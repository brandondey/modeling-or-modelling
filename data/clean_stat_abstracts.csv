"","id","title","categories","abstract","doi","created","updated","authors","one_l","astro.ph","cond.mat","gr.qc","hep.ex","hep.lat","hep.ph","hep.th","math.ph","nlin","nucl.ex","nucl.th","physics","quant.ph","math","CoRR","q.bio","q.fin","stat","stat.ap","stat.co","stat.ml","stat.me","stat.ot","stat.th"
"1",705.0418,"various approaches for predicting land cover in mountain areas","stat.ap stat.me","using former maps, geographers intend to study the evolution of the land cover in order to have a prospective approach on the future landscape; predictions of the future land cover, by the use of older maps and environmental variables, are usually done through the gis (geographic information system). we propose here to confront this classical geographical approach with statistical approaches: a linear parametric model (polychotomous regression modeling) and a nonparametric one (multilayer perceptron). these methodologies have been tested on two real areas on which the land cover is known at various dates; this allows us to emphasize the benefit of these two statistical approaches compared to gis and to discuss the way gis could be improved by the use of statistical models.","10.1080/03610910601096379","2007-05-03","","['nathalie villa', 'martin paegelow', 'maria t. camacho olmedo', 'laurence cornez', 'frédéric ferraty', 'louis ferré', 'pascal sarda']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"2",705.07,"inflated beta distributions","stat.me","this paper considers the issue of modeling fractional data observed in the interval [0,1), (0,1] or [0,1]. mixed continuous-discrete distributions are proposed. the beta distribution is used to describe the continuous component of the model since its density can have quite diferent shapes depending on the values of the two parameters that index the distribution. properties of the proposed distributions are examined. also, maximum likelihood and method of moments estimation is discussed. finally, practical applications that employ real data are presented.","10.1007/s00362-008-0125-4","2007-05-04","2007-11-11","['raydonal ospina', 'silvia l. p. ferrari']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"3",705.3693,"morphing ensemble kalman filters","math.ds cs.cv math.st physics.ao-ph stat.me stat.th","a new type of ensemble filter is proposed, which combines an ensemble kalman filter (enkf) with the ideas of morphing and registration from image processing. this results in filters suitable for nonlinear problems whose solutions exhibit moving coherent features, such as thin interfaces in wildfire modeling. the ensemble members are represented as the composition of one common state with a spatial transformation, called registration mapping, plus a residual. a fully automatic registration method is used that requires only gridded data, so the features in the model state do not need to be identified by the user. the morphing enkf operates on a transformed state consisting of the registration mapping and the residual. essentially, the morphing enkf uses intermediate states obtained by morphing instead of linear combinations of the states.","10.1111/j.1600-0870.2007.00275.x","2007-05-25","2007-08-23","['jonathan d. beezley', 'jan mandel']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"4",706.0073,"modeling hourly ozone concentration fields","stat.ap","this paper presents a dynamic linear model for modeling hourly ozone concentrations over the eastern united states. that model, which is developed within an bayesian hierarchical framework, inherits the important feature of such models that its coefficients, treated as states of the process, can change with time. thus the model includes a time--varying site invariant mean field as well as time varying coefficients for 24 and 12 diurnal cycle components. this cost of this model's great flexibility comes at the cost of computational complexity, forcing us to use an mcmc approach and to restrict application of our model domain to a small number of monitoring sites. we critically assess this model and discover some of its weaknesses in this type of application.","","2007-06-01","","['yiping dou', 'nhu d le', 'james v zidek']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"5",706.1401,"controlling for individual heterogeneity in longitudinal models, with   applications to student achievement","stat.ap","longitudinal data tracking repeated measurements on individuals are highly valued for research because they offer controls for unmeasured individual heterogeneity that might otherwise bias results. random effects or mixed models approaches, which treat individual heterogeneity as part of the model error term and use generalized least squares to estimate model parameters, are often criticized because correlation between unobserved individual effects and other model variables can lead to biased and inconsistent parameter estimates. starting with an examination of the relationship between random effects and fixed effects estimators in the standard unobserved effects model, this article demonstrates through analysis and simulation that the mixed model approach has a ``bias compression'' property under a general model for individual heterogeneity that can mitigate bias due to uncontrolled differences among individuals. the general model is motivated by the complexities of longitudinal student achievement measures, but the results have broad applicability to longitudinal modeling.","10.1214/07-ejs057","2007-06-11","","['j. r. lockwood', 'daniel f. mccaffrey']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"6",706.2281,"some questions of monte-carlo modeling on nontrivial bundles","math-ph math.mp stat.co","in this work are considered some questions of monte-carlo modeling on nontrivial bundles. as a basic example is used problem of generation of straight lines in 3d space, related with modeling of interaction of a solid body with a flux of particles and with some other tasks. space of lines used in given model is example of nontrivial fiber bundle, that is equivalent with tangent sheaf of a sphere.","","2007-06-15","2007-06-21","['alexander yu. vlasov']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"7",707.0878,"risk analysis in robust control -- making the case for probabilistic   robust control","math.oc cs.sy math.st stat.th","this paper offers a critical view of the ""worst-case"" approach that is the cornerstone of robust control design. it is our contention that a blind acceptance of worst-case scenarios may lead to designs that are actually more dangerous than designs based on probabilistic techniques with a built-in risk factor. the real issue is one of modeling. if one accepts that no mathematical model of uncertainties is perfect then a probabilistic approach can lead to more reliable control even if it cannot guarantee stability for all possible cases. our presentation is based on case analysis. we first establish that worst-case is not necessarily ""all-encompassing."" in fact, we show that for some uncertain control problems to have a conventional robust control solution it is necessary to make assumptions that leave out some feasible cases. once we establish that point, we argue that it is not uncommon for the risk of unaccounted cases in worst-case design to be greater than that of the accepted risk in a probabilistic approach. with an example, we quantify the risks and show that worst-case can be significantly more risky. finally, we join our analysis with existing results on computational complexity and probabilistic robustness to argue that the deterministic worst-case analysis is not necessarily the better tool.","","2007-07-05","","['xinjia chen', 'jorge aravena', 'kemin zhou']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"8",707.2257,"a bayes method for a bathtub failure rate via two $\mathbf{s}$-paths","stat.me","a class of semi-parametric hazard/failure rates with a bathtub shape is of interest. it does not only provide a great deal of flexibility over existing parametric methods in the modeling aspect but also results in a closed and tractable bayes estimator for the bathtub-shaped failure rate (bfr). such an estimator is derived to be a finite sum over two $\mathbf{s}$-paths due to an explicit posterior analysis in terms of two (conditionally independent) $\mathbf{s}$-paths. these, newly discovered, explicit results can be proved to be a rao-blackwellization of counterpart results in terms of partitions that are readily available by a specialization of james (2005)'s work. we develop both iterative and non-iterative computational procedures based on existing efficient monte carlo methods for sampling one single $\mathbf{s}$-path. nmerical simulations are given to demonstrate the practicality and the effectiveness of our methodology. last but not least, two applications of the proposed method are discussed, of which one is about a bayesian test for failure rates and the other is related to modeling with covariates.","","2007-07-15","","['man-wai ho']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"9",707.3013,"application of probabilistic pcr5 fusion rule for multisensor target   tracking","stat.ap","this paper defines and implements a non-bayesian fusion rule for combining densities of probabilities estimated by local (non-linear) filters for tracking a moving target by passive sensors. this rule is the restriction to a strict probabilistic paradigm of the recent and efficient proportional conflict redistribution rule no 5 (pcr5) developed in the dsmt framework for fusing basic belief assignments. a sampling method for probabilistic pcr5 (p-pcr5) is defined. it is shown that p-pcr5 is more robust to an erroneous modeling and allows to keep the modes of local densities and preserve as much as possible the whole information inherent to each densities to combine. in particular, p-pcr5 is able of maintaining multiple hypotheses/modes after fusion, when the hypotheses are too distant in regards to their deviations. this new p-pcr5 rule has been tested on a simple example of distributed non-linear filtering application to show the interest of such approach for future developments. the non-linear distributed filter is implemented through a basic particles filtering technique. the results obtained in our simulations show the ability of this p-pcr5-based filter to track the target even when the models are not well consistent in regards to the initialization and real cinematic.","","2007-07-20","","['alois kirchner', 'frederic dambreville', 'francis celeste', 'jean dezert', 'florentin smarandache']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"10",708.0343,"dynamic modeling and statistical analysis of event times","stat.me","this review article provides an overview of recent work in the modeling and analysis of recurrent events arising in engineering, reliability, public health, biomedicine and other areas. recurrent event modeling possesses unique facets making it different and more difficult to handle than single event settings. for instance, the impact of an increasing number of event occurrences needs to be taken into account, the effects of covariates should be considered, potential association among the interevent times within a unit cannot be ignored, and the effects of performed interventions after each event occurrence need to be factored in. a recent general class of models for recurrent events which simultaneously accommodates these aspects is described. statistical inference methods for this class of models are presented and illustrated through applications to real data sets. some existing open research problems are described.","10.1214/088342306000000349","2007-08-02","","['edsel a. peña']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"11",708.0362,"on the statistical modeling and analysis of repairable systems","stat.me","we review basic modeling approaches for failure and maintenance data from repairable systems. in particular we consider imperfect repair models, defined in terms of virtual age processes, and the trend-renewal process which extends the nonhomogeneous poisson process and the renewal process. in the case where several systems of the same kind are observed, we show how observed covariates and unobserved heterogeneity can be included in the models. we also consider various approaches to trend testing. modern reliability data bases usually contain information on the type of failure, the type of maintenance and so forth in addition to the failure times themselves. basing our work on recent literature we present a framework where the observed events are modeled as marked point processes, with marks labeling the types of events. throughout the paper the emphasis is more on modeling than on statistical inference.","10.1214/088342306000000448","2007-08-02","","['bo henry lindqvist']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"12",708.0471,"quantile regression with varying coefficients","math.st stat.th","quantile regression provides a framework for modeling statistical quantities of interest other than the conditional mean. the regression methodology is well developed for linear models, but less so for nonparametric models. we consider conditional quantiles with varying coefficients and propose a methodology for their estimation and assessment using polynomial splines. the proposed estimators are easy to compute via standard quantile regression algorithms and a stepwise knot selection algorithm. the proposed rao-score-type test that assesses the model against a linear model is also easy to implement. we provide asymptotic results on the convergence of the estimators and the null distribution of the test statistic. empirical results are also provided, including an application of the methodology to forced expiratory volume (fev) data.","10.1214/009053606000000966","2007-08-03","","['mi-ok kim']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"13",708.1929,"rank-based estimation for all-pass time series models","math.st stat.th","an autoregressive-moving average model in which all roots of the autoregressive polynomial are reciprocals of roots of the moving average polynomial and vice versa is called an all-pass time series model. all-pass models are useful for identifying and modeling noncausal and noninvertible autoregressive-moving average processes. we establish asymptotic normality and consistency for rank-based estimators of all-pass model parameters. the estimators are obtained by minimizing the rank-based residual dispersion function given by jaeckel [ann. math. statist. 43 (1972) 1449--1458]. these estimators can have the same asymptotic efficiency as maximum likelihood estimators and are robust. the behavior of the estimators for finite samples is studied via simulation and rank estimation is used in the deconvolution of a simulated water gun seismogram.","10.1214/009053606000001316","2007-08-14","","['beth andrews', 'richard a. davis', 'f. jay breidt']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"14",709.0394,"spatial variation of total column ozone on a global scale","stat.ap","the spatial dependence of total column ozone varies strongly with latitude, so that homogeneous models (invariant to all rotations) are clearly unsuitable. however, an assumption of axial symmetry, which means that the process model is invariant to rotations about the earth's axis, is much more plausible and considerably simplifies the modeling. using toms (total ozone mapping spectrometer) measurements of total column ozone over a six-day period, this work investigates the modeling of axially symmetric processes on the sphere using expansions in spherical harmonics. it turns out that one can capture many of the large scale features of the spatial covariance structure using a relatively small number of terms in such an expansion, but the resulting fitted model provides a horrible fit to the data when evaluated via its likelihood because of its inability to describe accurately the process's local behavior. thus, there remains the challenge of developing computationally tractable models that capture both the large and small scale structure of these data.","10.1214/07-aoas106","2007-09-04","","['michael l. stein']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"15",709.0427,"a multivariate semiparametric bayesian spatial modeling framework for   hurricane surface wind fields","stat.ap","storm surge, the onshore rush of sea water caused by the high winds and low pressure associated with a hurricane, can compound the effects of inland flooding caused by rainfall, leading to loss of property and loss of life for residents of coastal areas. numerical ocean models are essential for creating storm surge forecasts for coastal areas. these models are driven primarily by the surface wind forcings. currently, the gridded wind fields used by ocean models are specified by deterministic formulas that are based on the central pressure and location of the storm center. while these equations incorporate important physical knowledge about the structure of hurricane surface wind fields, they cannot always capture the asymmetric and dynamic nature of a hurricane. a new bayesian multivariate spatial statistical modeling framework is introduced combining data with physical knowledge about the wind fields to improve the estimation of the wind vectors. many spatial models assume the data follow a gaussian distribution. however, this may be overly-restrictive for wind fields data which often display erratic behavior, such as sudden changes in time or space. in this paper we develop a semiparametric multivariate spatial model for these data. our model builds on the stick-breaking prior, which is frequently used in bayesian modeling to capture uncertainty in the parametric form of an outcome. the stick-breaking prior is extended to the spatial setting by assigning each location a different, unknown distribution, and smoothing the distributions in space with a series of kernel functions. this semiparametric spatial model is shown to improve prediction compared to usual bayesian kriging methods for the wind field of hurricane ivan.","10.1214/07-aoas108","2007-09-04","","['brian j. reich', 'montserrat fuentes']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"16",709.2943,"on birnbaum-saunders inference","stat.me math.st stat.th","the birnbaum-saunders distribution, also known as the fatigue-life distribution, is frequently used in reliability studies. we obtain adjustments to the birnbaum--saunders profile likelihood function. the modified versions of the likelihood function were obtained for both the shape and scale parameters, i.e., we take the shape parameter to be of interest and the scale parameter to be of nuisance, and then consider the situation in which the interest lies in performing inference on the scale parameter with the shape parameter entering the modeling in nuisance fashion. modified profile maximum likelihood estimators are obtained by maximizing the corresponding adjusted likelihood functions. we present numerical evidence on the finite sample behavior of the different estimators and associated likelihood ratio tests. the results favor the adjusted estimators and tests we propose. a novel aspect of the profile likelihood adjustments obtained in this paper is that they yield improved point estimators and tests. the two profile likelihood adjustments work well when inference is made on the shape parameter, and one of them displays superior behavior when it comes to performing hypothesis testing inference on the scale parameter. two empirical applications are briefly presented.","","2007-09-18","2008-04-06","['audrey h. m. a. cysneiros', 'francisco cribari-neto', 'carlos a. g. araujo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"17",709.3545,"locally adaptive nonparametric binary regression","stat.me","a nonparametric and locally adaptive bayesian estimator is proposed for estimating a binary regression. flexibility is obtained by modeling the binary regression as a mixture of probit regressions with the argument of each probit regression having a thin plate spline prior with its own smoothing parameter and with the mixture weights depending on the covariates. the estimator is compared to a single spline estimator and to a recently proposed locally adaptive estimator. the methodology is illustrated by applying it to both simulated and real examples.","","2007-09-21","","['sally wood', 'robert kohn', 'remy cottet', 'wenxin jiang', 'martin tanner']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"18",710.2245,"size, power and false discovery rates","math.st stat.th","modern scientific technology has provided a new class of large-scale simultaneous inference problems, with thousands of hypothesis tests to consider at the same time. microarrays epitomize this type of technology, but similar situations arise in proteomics, spectroscopy, imaging, and social science surveys. this paper uses false discovery rate methods to carry out both size and power calculations on large-scale problems. a simple empirical bayes approach allows the false discovery rate (fdr) analysis to proceed with a minimum of frequentist or bayesian modeling assumptions. closed-form accuracy formulas are derived for estimated false discovery rates, and used to compare different methodologies: local or tail-area fdr's, theoretical, permutation, or empirical null hypothesis estimates. two microarray data sets as well as simulations are used to evaluate the methodology, the power diagnostics showing why nonnull cases might easily fail to appear on a list of ``significant'' discoveries.","10.1214/009053606000001460","2007-10-11","","['bradley efron']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"19",710.4536,"bayesian treed gaussian process models with an application to computer   modeling","stat.me stat.ap stat.co","motivated by a computer experiment for the design of a rocket booster, this paper explores nonstationary modeling methodologies that couple stationary gaussian processes with treed partitioning. partitioning is a simple but effective method for dealing with nonstationarity. the methodological developments and statistical computing details which make this approach efficient are described in detail. in addition to providing an analysis of the rocket booster simulator, our approach is demonstrated to be effective in other arenas.","","2007-10-24","2009-03-17","['robert b. gramacy', 'herbert k. h. lee']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"20",710.4614,"a family of generalized beta distributions for income","stat.me","the mathematical properties of a family of generalized beta distribution, including beta-normal, skewed-t, log-f, beta-exponential, beta-weibull distributions have recently been studied in several publications. this paper applies these distributions to the modeling of the size distribution of income and computes the maximum likelihood estimation estimates of parameters. their performances are compared to the widely used generalized beta distributions of the first and second types in terms of measures of goodness of fit.","","2007-10-25","","['j. h. sepanski', 'lingji kong']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"21",710.4618,"the use of unlabeled data in predictive modeling","stat.me","the incorporation of unlabeled data in regression and classification analysis is an increasing focus of the applied statistics and machine learning literatures, with a number of recent examples demonstrating the potential for unlabeled data to contribute to improved predictive accuracy. the statistical basis for this semisupervised analysis does not appear to have been well delineated; as a result, the underlying theory and rationale may be underappreciated, especially by nonstatisticians. there is also room for statisticians to become more fully engaged in the vigorous research in this important area of intersection of the statistical and computer sciences. much of the theoretical work in the literature has focused, for example, on geometric and structural properties of the unlabeled data in the context of particular algorithms, rather than probabilistic and statistical questions. this paper overviews the fundamental statistical foundations for predictive modeling and the general questions associated with unlabeled data, highlighting the relevance of venerable concepts of sampling design and prior specification. this theory, illustrated with a series of central illustrative examples and two substantial real data analyses, shows precisely when, why and how unlabeled data matter.","10.1214/088342307000000032","2007-10-25","","['feng liang', 'sayan mukherjee', 'mike west']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"22",710.4788,"a bayesian hierarchical model for the analysis of a longitudinal dynamic   contrast-enhanced mri cancer study","stat.ap physics.med-ph","imaging in clinical oncology trials provides a wealth of information that contributes to the drug development process, especially in early phase studies. this paper focuses on kinetic modeling in dce-mri, inspired by mixed-effects models that are frequently used in the analysis of clinical trials. instead of summarizing each scanning session as a single kinetic parameter -- such as median $\ktrans$ across all voxels in the tumor roi -- we propose to analyze all voxel time courses from all scans and across all subjects simultaneously in a single model. the kinetic parameters from the usual non-linear regression model are decomposed into unique components associated with factors from the longitudinal study; e.g., treatment, patient and voxel effects. a bayesian hierarchical model provides the framework in order to construct a data model, a parameter model, as well as prior distributions. the posterior distribution of the kinetic parameters is estimated using markov chain monte carlo (mcmc) methods. hypothesis testing at the study level for an overall treatment effect is straightforward and the patient- and voxel-level parameters capture random effects that provide additional information at various levels of resolution to allow a thorough evaluation of the clinical trial. the proposed method is validated with a breast cancer study, where the subjects were imaged before and after two cycles of chemotherapy, demonstrating the clinical potential of this method to longitudinal oncology studies.","","2007-10-25","","['volker j. schmid', 'brandon whitcher', 'anwar r. padhani', 'n. jane taylor', 'guang-zhong yang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"23",710.5009,"comment: struggles with survey weighting and regression modeling","stat.me","comment: struggles with survey weighting and regression modeling [arxiv:0710.5005]","10.1214/088342307000000177","2007-10-26","","['robert m. bell', 'michael l. cohen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"24",710.5012,"comment: struggles with survey weighting and regression modeling","stat.me","comment: struggles with survey weighting and regression modeling [arxiv:0710.5005]","10.1214/088342307000000195","2007-10-26","","['f. jay breidt', 'jean d. opsomer']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"25",710.5013,"comment: struggles with survey weighting and regression modeling","stat.me","comment: struggles with survey weighting and regression modeling [arxiv:0710.5005]","10.1214/088342307000000186","2007-10-26","","['roderick j. little']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"26",710.5015,"comment: struggles with survey weighting and regression modeling","stat.me","comment: struggles with survey weighting and regression modeling [arxiv:0710.5005]","10.1214/088342307000000159","2007-10-26","","['sharon l. lohr']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"27",710.5016,"comment: struggles with survey weighting and regression modeling","stat.me","comment: struggles with survey weighting and regression modeling [arxiv:0710.5005]","10.1214/088342307000000168","2007-10-26","","['danny pfeffermann']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"28",710.5019,"rejoinder: struggles with survey weighting and regression modeling","stat.me","rejoinder: struggles with survey weighting and regression modeling [arxiv:0710.5005]","10.1214/088342307000000203","2007-10-26","","['andrew gelman']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"29",711.2345,"models for dependent extremes using stable mixtures","stat.me math.st stat.th","this paper unifies and extends results on a class of multivariate extreme value (ev) models studied by hougaard, crowder, and tawn. in these models both unconditional and conditional distributions are ev, and all lower-dimensional marginals and maxima belong to the class. this leads to substantial economies of understanding, analysis and prediction. one interpretation of the models is as size mixtures of ev distributions, where the mixing is by positive stable distributions. a second interpretation is as exponential-stable location mixtures (for gumbel) or as power-stable scale mixtures (for non-gumbel ev distributions). a third interpretation is through a peaks over thresholds model with a positive stable intensity. the mixing variables are used as a modeling tool and for better understanding and model checking. we study extreme value analogues of components of variance models, and new time series, spatial, and continuous parameter models for extreme values. the results are applied to data from a pitting corrosion investigation.","10.1111/j.1467-9469.2008.00613.x","2007-11-15","","['anne-laure fougères', 'john p. nolan', 'holger rootzén']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"30",711.4555,"sparse additive models","math.st stat.th","we present a new class of methods for high-dimensional nonparametric regression and classification called sparse additive models (spam). our methods combine ideas from sparse linear modeling and additive nonparametric regression. we derive an algorithm for fitting the models that is practical and effective even when the number of covariates is larger than the sample size. spam is closely related to the cosso model of lin and zhang (2006), but decouples smoothing and sparsity, enabling the use of arbitrary nonparametric smoothers. an analysis of the theoretical properties of spam is given. we also study a greedy estimator that is a nonparametric version of forward stepwise regression. empirical results on synthetic and real data are presented, showing that spam can be effective in fitting sparse nonparametric models in high dimensional data.","","2007-11-28","2008-04-08","['pradeep ravikumar', 'john lafferty', 'han liu', 'larry wasserman']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"31",712.1962,"the barista: a model for bid arrivals in online auctions","stat.ap","the arrival process of bidders and bids in online auctions is important for studying and modeling supply and demand in the online marketplace. a popular assumption in the online auction literature is that a poisson bidder arrival process is a reasonable approximation. this approximation underlies theoretical derivations, statistical models and simulations used in field studies. however, when it comes to the bid arrivals, empirical research has shown that the process is far from poisson, with early bidding and last-moment bids taking place. an additional feature that has been reported by various authors is an apparent self-similarity in the bid arrival process. despite the wide evidence for the changing bidding intensities and the self-similarity, there has been no rigorous attempt at developing a model that adequately approximates bid arrivals and accounts for these features. the goal of this paper is to introduce a family of distributions that well-approximate the bid time distribution in hard-close auctions. we call this the barista process (bid arrivals in stages) because of its ability to generate different intensities at different stages. we describe the properties of this model, show how to simulate bid arrivals from it, and how to use it for estimation and inference. we illustrate its power and usefulness by fitting simulated and real data from ebay.com. finally, we show how a poisson bidder arrival process relates to a barista bid arrival process.","10.1214/07-aoas117","2007-12-12","","['galit shmueli', 'ralph p. russo', 'wolfgang jank']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"32",801.1599,"parametric and nonparametric models and methods in financial   econometrics","q-fin.st stat.me","financial econometrics has become an increasingly popular research field. in this paper we review a few parametric and nonparametric models and methods used in this area. after introducing several widely used continuous-time and discrete-time models, we study in detail dependence structures of discrete samples, including markovian property, hidden markovian structure, contaminated observations, and random samples. we then discuss several popular parametric and nonparametric estimation methods. to avoid model mis-specification, model validation plays a key role in financial modeling. we discuss several model validation techniques, including pseudo-likelihood ratio test, nonparametric curve regression based test, residuals based test, generalized likelihood ratio test, simultaneous confidence band construction, and density based test. finally, we briefly touch on tools for studying large sample properties.","10.1214/08-ss034","2008-01-10","2008-03-20","['zhibiao zhao']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"33",801.2555,"penalized clustering of large scale functional data with multiple   covariates","stat.me stat.co","in this article, we propose a penalized clustering method for large scale data with multiple covariates through a functional data approach. in the proposed method, responses and covariates are linked together through nonparametric multivariate functions (fixed effects), which have great flexibility in modeling a variety of function features, such as jump points, branching, and periodicity. functional anova is employed to further decompose multivariate functions in a reproducing kernel hilbert space and provide associated notions of main effect and interaction. parsimonious random effects are used to capture various correlation structures. the mixed-effect models are nested under a general mixture model, in which the heterogeneity of functional data is characterized. we propose a penalized henderson's likelihood approach for model-fitting and design a rejection-controlled em algorithm for the estimation. our method selects smoothing parameters through generalized cross-validation. furthermore, the bayesian confidence intervals are used to measure the clustering uncertainty. simulation studies and real-data examples are presented to investigate the empirical performance of the proposed method. open-source code is available in the r package mfda.","","2008-01-16","","['ping ma', 'wenxuan zhong']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"34",802.0436,"modeling all exceedances above a threshold using an extremal dependence   structure: inferences on several flood characteristics","stat.ap","flood quantile estimation is of great importance for many engineering studies and policy decisions. however, practitioners must often deal with small data available. thus, the information must be used optimally. in the last decades, to reduce the waste of data, inferential methodology has evolved from annual maxima modeling to peaks over a threshold one. to mitigate the lack of data, peaks over a threshold are sometimes combined with additional information - mostly regional and historical information. however, whatever the extra information is, the most precious information for the practitioner is found at the target site. in this study, a model that allows inferences on the whole time series is introduced. in particular, the proposed model takes into account the dependence between successive extreme observations using an appropriate extremal dependence structure. results show that this model leads to more accurate flood peak quantile estimates than conventional estimators. in addition, as the time dependence is taken into account, inferences on other flood characteristics can be performed. an illustration is given on flood duration. our analysis shows that the accuracy of the proposed models to estimate the flood duration is related to specific catchment characteristics. some suggestions to increase the flood duration predictions are introduced.","10.1029/2007wr006322","2008-02-04","","['mathieu ribatet', 'taha b. m. j. ouarda', 'eric sauquet', 'jean-michel grésillon']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"35",802.0443,"global sensitivity analysis of stochastic computer models with joint   metamodels","stat.me stat.ap","the global sensitivity analysis method, used to quantify the influence of uncertain input variables on the response variability of a numerical model, is applicable to deterministic computer code (for which the same set of input variables gives always the same output value). this paper proposes a global sensitivity analysis methodology for stochastic computer code (having a variability induced by some uncontrollable variables). the framework of the joint modeling of the mean and dispersion of heteroscedastic data is used. to deal with the complexity of computer experiment outputs, non parametric joint models (based on generalized additive models and gaussian processes) are discussed. the relevance of these new models is analyzed in terms of the obtained variance-based sensitivity indices with two case studies. results show that the joint modeling approach leads accurate sensitivity index estimations even when clear heteroscedasticity is present.","","2008-02-04","2009-06-08","['bertrand iooss', 'mathieu ribatet', 'amandine marrel']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"36",802.1008,"calculations of sobol indices for the gaussian process metamodel","stat.me math.st stat.th","global sensitivity analysis of complex numerical models can be performed by calculating variance-based importance measures of the input variables, such as the sobol indices. however, these techniques, requiring a large number of model evaluations, are often unacceptable for time expensive computer codes. a well known and widely used decision consists in replacing the computer code by a metamodel, predicting the model responses with a negligible computation time and rending straightforward the estimation of sobol indices. in this paper, we discuss about the gaussian process model which gives analytical expressions of sobol indices. two approaches are studied to compute the sobol indices: the first based on the predictor of the gaussian process model and the second based on the global stochastic process model. comparisons between the two estimates, made on analytical examples, show the superiority of the second approach in terms of convergence and robustness. moreover, the second approach allows to integrate the modeling error of the gaussian process model by directly giving some confidence intervals on the sobol indices. these techniques are finally applied to a real case of hydrogeological modeling.","","2008-02-07","","['amandine marrel', 'bertrand iooss', 'beatrice laurent', 'olivier roustant']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"37",802.1009,"global sensitivity analysis of computer models with functional inputs","stat.ap math.st stat.th","global sensitivity analysis is used to quantify the influence of uncertain input parameters on the response variability of a numerical model. the common quantitative methods are applicable to computer codes with scalar input variables. this paper aims to illustrate different variance-based sensitivity analysis techniques, based on the so-called sobol indices, when some input variables are functional, such as stochastic processes or random spatial fields. in this work, we focus on large cpu time computer codes which need a preliminary meta-modeling step before performing the sensitivity analysis. we propose the use of the joint modeling approach, i.e., modeling simultaneously the mean and the dispersion of the code outputs using two interlinked generalized linear models (glm) or generalized additive models (gam). the ``mean'' model allows to estimate the sensitivity indices of each scalar input variables, while the ``dispersion'' model allows to derive the total sensitivity index of the functional input variables. the proposed approach is compared to some classical sa methodologies on an analytical function. lastly, the proposed methodology is applied to a concrete industrial computer code that simulates the nuclear fuel irradiation.","","2008-02-07","2008-06-09","['bertrand iooss', 'mathieu ribatet']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE
"38",802.4317,"bayesball: a bayesian hierarchical model for evaluating fielding in   major league baseball","stat.ap","the use of statistical modeling in baseball has received substantial attention recently in both the media and academic community. we focus on a relatively under-explored topic: the use of statistical models for the analysis of fielding based on high-resolution data consisting of on-field location of batted balls. we combine spatial modeling with a hierarchical bayesian structure in order to evaluate the performance of individual fielders while sharing information between fielders at each position. we present results across four seasons of mlb data (2002--2005) and compare our approach to other fielding evaluation procedures.","10.1214/08-aoas228","2008-02-28","2009-08-14","['shane t. jensen', 'kenneth e. shirley', 'abraham j. wyner']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"39",803.0525,"an em algorithm for estimation in the mixture transition distribution   model","stat.co math.st stat.th","the mixture transition distribution (mtd) model was introduced by raftery to face the need for parsimony in the modeling of high-order markov chains in discrete time. the particularity of this model comes from the fact that the effect of each lag upon the present is considered separately and additively, so that the number of parameters required is drastically reduced. however, the efficiency for the mtd parameter estimations proposed up to date still remains problematic on account of the large number of constraints on the parameters. in this paper, an iterative procedure, commonly known as expectation-maximization (em) algorithm, is developed cooperating with the principle of maximum likelihood estimation (mle) to estimate the mtd parameters. some applications of modeling mtd show the proposed em algorithm is easier to be used than the algorithm developed by berchtold. moreover, the em estimations of parameters for high-order mtd models led on dna sequences outperform the corresponding fully parametrized markov chain in terms of bayesian information criterion. a software implementation of our algorithm is available in the library seq++ at http://stat.genopole.cnrs.fr/seqpp","","2008-03-04","","['sophie lèbre', 'pierre-yves bourguinon']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE
"40",803.0783,"the rank of the covariance matrix of an evanescent field","stat.me","evanescent random fields arise as a component of the 2-d wold decomposition of homogenous random fields. besides their theoretical importance, evanescent random fields have a number of practical applications, such as in modeling the observed signal in the space time adaptive processing (stap) of airborne radar data. in this paper we derive an expression for the rank of the low-rank covariance matrix of a finite dimension sample from an evanescent random field. it is shown that the rank of this covariance matrix is completely determined by the evanescent field spectral support parameters, alone. thus, the problem of estimating the rank lends itself to a solution that avoids the need to estimate the rank from the sample covariance matrix. we show that this result can be immediately applied to considerably simplify the estimation of the rank of the interference covariance matrix in the stap problem.","","2008-03-05","2009-10-05","['m. kliger', 'j. m. francos']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"41",803.1628,"component models for large networks","stat.ml","being among the easiest ways to find meaningful structure from discrete data, latent dirichlet allocation (lda) and related component models have been applied widely. they are simple, computationally fast and scalable, interpretable, and admit nonparametric priors. in the currently popular field of network modeling, relatively little work has taken uncertainty of data seriously in the bayesian sense, and component models have been introduced to the field only recently, by treating each node as a bag of out-going links. we introduce an alternative, interaction component model for communities (icmc), where the whole network is a bag of links, stemming from different components. the former finds both disassortative and assortative structure, while the alternative assumes assortativity and finds community-like structures like the earlier methods motivated by physics. with dirichlet process priors and an efficient implementation the models are highly scalable, as demonstrated with a social network from the last.fm web site, with 670,000 nodes and 1.89 million links.","","2008-03-11","","['janne sinkkonen', 'janne aukia', 'samuel kaski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"42",803.1931,"variable selection in semiparametric regression modeling","math.st stat.th","in this paper, we are concerned with how to select significant variables in semiparametric modeling. variable selection for semiparametric regression models consists of two components: model selection for nonparametric components and selection of significant variables for the parametric portion. thus, semiparametric variable selection is much more challenging than parametric variable selection (e.g., linear and generalized linear models) because traditional variable selection procedures including stepwise regression and the best subset selection now require separate model selection for the nonparametric components for each submodel. this leads to a very heavy computational burden. in this paper, we propose a class of variable selection procedures for semiparametric regression models using nonconcave penalized likelihood. we establish the rate of convergence of the resulting estimate. with proper choices of penalty functions and regularization parameters, we show the asymptotic normality of the resulting estimate and further demonstrate that the proposed procedures perform as well as an oracle procedure. a semiparametric generalized likelihood ratio test is proposed to select significant variables in the nonparametric component. we investigate the asymptotic behavior of the proposed test and demonstrate that its limiting null distribution follows a chi-square distribution which is independent of the nuisance parameters. extensive monte carlo simulation studies are conducted to examine the finite sample performance of the proposed variable selection procedures.","10.1214/009053607000000604","2008-03-13","","['runze li', 'hua liang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"43",803.2068,"optimal two-value zero-mean disintegration of zero-mean random variables","math.pr math.st stat.th","for any continuous zero-mean random variable (r.v.) x, a reciprocating function r is constructed, based only on the distribution of x, such that the conditional distribution of x given the (at-most-)two-point set {x,r(x)} is the zero-mean distribution on this set; in fact, a more general construction without the continuity assumption is given in this paper, as well as a large variety of other related results, including characterizations of the reciprocating function and modeling distribution asymmetry patterns. the mentioned disintegration of zero-mean r.v.'s implies, in particular, that an arbitrary zero-mean distribution is represented as the mixture of two-point zero-mean distributions; moreover, this mixture representation is most symmetric in a variety of senses. somewhat similar representations -- of any probability distribution as the mixture of two-point distributions with the same skewness coefficient (but possibly with different means) -- go back to kolmogorov; very recently, aizenman et al. further developed such representations and applied them to (anti-)concentration inequalities for functions of independent random variables and to spectral localization for random schroedinger operators. one kind of application given in the present paper is to construct certain statistical tests for asymmetry patterns and for location without symmetry conditions. exact inequalities implying conservative properties of such tests are presented. these developments extend results established earlier by efron, eaton, and pinelis under a symmetry condition.","","2008-03-13","","['iosif pinelis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"44",803.2679,"statistical aspects of birth--and--growth stochastic processes","stat.ap","the paper considers a particular family of set--valued stochastic processes modeling birth--and--growth processes. the proposed setting allows us to investigate the nucleation and the growth processes. a decomposition theorem is established to characterize the nucleation and the growth. as a consequence, different consistent set--valued estimators are studied for growth process. moreover, the nucleation process is studied via the hitting function, and a consistent estimator of the nucleation hitting function is derived.","10.1016/j.fss.2008.12.011","2008-03-18","2008-09-25","['giacomo aletti', 'enea g. bongiorno', 'vincenzo capasso']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"45",803.3436,"influence of speed limit on roadway safety in indiana","stat.ap","the influence of speed limits on roadway safety is an extremely important social issue and is subject to an extensive debate in the state of indiana and nationwide. with around 800-900 fatalities and thousands of injuries annually in indiana, traffic accidents place an incredible social and economic burden on the state. still, speed limits posted on highways and other roads are routinely exceeded as individual drivers try to balance safety and mobility (speed). this research explores the relationship between speed limits and roadway safety. namely, the research focuses on the influence of the posted speed limit on the causation and severity of accidents. data on individual accidents from the indiana electronic vehicle crash record system is used in the research, and appropriate statistical models are estimated for causation and severity of different types of accidents on all road classes. the results of the modeling show that speed limits do not have a statistically significant adverse effect on unsafe-speed-related causation of accidents on all roads, but generally increase the severity of accidents on the majority of roads other than highways (the accident severity on highways is unaffected by speed limits). our findings can perhaps save both lives and travel time by helping the indiana department of transportation determine optimal speed limit policies in the state.","","2008-03-24","","['nataliya v. malyshkina']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"46",803.374,"false discovery rate analysis of brain diffusion direction maps","stat.ap","diffusion tensor imaging (dti) is a novel modality of magnetic resonance imaging that allows noninvasive mapping of the brain's white matter. a particular map derived from dti measurements is a map of water principal diffusion directions, which are proxies for neural fiber directions. we consider a study in which diffusion direction maps were acquired for two groups of subjects. the objective of the analysis is to find regions of the brain in which the corresponding diffusion directions differ between the groups. this is attained by first computing a test statistic for the difference in direction at every brain location using a watson model for directional data. interesting locations are subsequently selected with control of the false discovery rate. more accurate modeling of the null distribution is obtained using an empirical null density based on the empirical distribution of the test statistics across the brain. further, substantial improvements in power are achieved by local spatial averaging of the test statistic map. although the focus is on one particular study and imaging technology, the proposed inference methods can be applied to other large scale simultaneous hypothesis testing problems with a continuous underlying spatial structure.","10.1214/07-aoas133","2008-03-26","","['armin schwartzman', 'robert f. dougherty', 'jonathan e. taylor']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"47",803.3904,"transcription factor binding site prediction with multivariate gene   expression data","stat.ap","multi-sample microarray experiments have become a standard experimental method for studying biological systems. a frequent goal in such studies is to unravel the regulatory relationships between genes. during the last few years, regression models have been proposed for the de novo discovery of cis-acting regulatory sequences using gene expression data. however, when applied to multi-sample experiments, existing regression based methods model each individual sample separately. to better capture the dynamic relationships in multi-sample microarray experiments, we propose a flexible method for the joint modeling of promoter sequence and multivariate expression data. in higher order eukaryotic genomes expression regulation usually involves combinatorial interaction between several transcription factors. experiments have shown that spacing between transcription factor binding sites can significantly affect their strength in activating gene expression. we propose an adaptive model building procedure to capture such spacing dependent cis-acting regulatory modules. we apply our methods to the analysis of microarray time-course experiments in yeast and in arabidopsis. these experiments exhibit very different dynamic temporal relationships. for both data sets, we have found all of the well-known cis-acting regulatory elements in the related context, as well as being able to predict novel elements.","10.1214/10.1214/07-aoas142","2008-03-27","","['nancy r. zhang', 'mary c. wildermuth', 'terence p. speed']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"48",803.4065,"statistical advances and challenges for analyzing correlated high   dimensional snp data in genomic study for complex diseases","stat.ap","recent advances of information technology in biomedical sciences and other applied areas have created numerous large diverse data sets with a high dimensional feature space, which provide us a tremendous amount of information and new opportunities for improving the quality of human life. meanwhile, great challenges are also created driven by the continuous arrival of new data that requires researchers to convert these raw data into scientific knowledge in order to benefit from it. association studies of complex diseases using snp data have become more and more popular in biomedical research in recent years. in this paper, we present a review of recent statistical advances and challenges for analyzing correlated high dimensional snp data in genomic association studies for complex diseases. the review includes both general feature reduction approaches for high dimensional correlated data and more specific approaches for snps data, which include unsupervised haplotype mapping, tag snp selection, and supervised snps selection using statistical testing/scoring, statistical modeling and machine learning methods with an emphasis on how to identify interacting loci.","10.1214/07-ss026","2008-03-28","","['yulan liang', 'arpad kelemen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"49",804.0741,"sequential change detection revisited","math.st stat.th","in sequential change detection, existing performance measures differ significantly in the way they treat the time of change. by modeling this quantity as a random time, we introduce a general framework capable of capturing and better understanding most well-known criteria and also propose new ones. for a specific new criterion that constitutes an extension to lorden's performance measure, we offer the optimum structure for detecting a change in the constant drift of a brownian motion and a formula for the corresponding optimum performance.","10.1214/009053607000000938","2008-04-04","","['george v. moustakides']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"50",804.3152,"bayesian computation for statistical models with intractable normalizing   constants","stat.co stat.me","this paper deals with some computational aspects in the bayesian analysis of statistical models with intractable normalizing constants. in the presence of intractable normalizing constants in the likelihood function, traditional mcmc methods cannot be applied. we propose an approach to sample from such posterior distributions. the method can be thought as a bayesian version of the mcmc-mle approach of geyer and thompson (1992). to the best of our knowledge, this is the first general and asymptotically consistent monte carlo method for such problems. we illustrate the method with examples from image segmentation and social network modeling. we study as well the asymptotic behavior of the algorithm and obtain a strong law of large numbers for empirical averages.","","2008-04-21","","['yves atchade', 'nicolas lartillot', 'christian p. robert']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"51",805.2258,"a bayesian test for excess zeros in a zero-inflated power series   distribution","math.st stat.th","power series distributions form a useful subclass of one-parameter discrete exponential families suitable for modeling count data. a zero-inflated power series distribution is a mixture of a power series distribution and a degenerate distribution at zero, with a mixing probability $p$ for the degenerate distribution. this distribution is useful for modeling count data that may have extra zeros. one question is whether the mixture model can be reduced to the power series portion, corresponding to $p=0$, or whether there are so many zeros in the data that zero inflation relative to the pure power series distribution must be included in the model i.e., $p\geq0$. the problem is difficult partially because $p=0$ is a boundary point. here, we present a bayesian test for this problem based on recognizing that the parameter space can be expanded to allow $p$ to be negative. negative values of $p$ are inconsistent with the interpretation of $p$ as a mixing probability, however, they index distributions that are physically and probabilistically meaningful. we compare our bayesian solution to two standard frequentist testing procedures and find that using a posterior probability as a test statistic has slightly higher power on the most important ranges of the sample size $n$ and parameter values than the score test and likelihood ratio test in simulations. our method also performs well on three real data sets.","10.1214/193940307000000068","2008-05-15","","['archan bhattacharya', 'bertrand s. clarke', 'gauri s. datta']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"52",805.2479,"a comparison of the benjamini-hochberg procedure with some bayesian   rules for multiple testing","math.st stat.th","in the spirit of modeling inference for microarrays as multiple testing for sparse mixtures, we present a similar approach to a simplified version of quantitative trait loci (qtl) mapping. unlike in case of microarrays, where the number of tests usually reaches tens of thousands, the number of tests performed in scans for qtl usually does not exceed several hundreds. however, in typical cases, the sparsity $p$ of significant alternatives for qtl mapping is in the same range as for microarrays. for methodological interest, as well as some related applications, we also consider non-sparse mixtures. using simulations as well as theoretical observations we study false discovery rate (fdr), power and misclassification probability for the benjamini-hochberg (bh) procedure and its modifications, as well as for various parametric and nonparametric bayes and parametric empirical bayes procedures. our results confirm the observation of genovese and wasserman (2002) that for small p the misclassification error of bh is close to optimal in the sense of attaining the bayes oracle. this property is shared by some of the considered bayes testing rules, which in general perform better than bh for large or moderate $p$'s.","10.1214/193940307000000158","2008-05-16","","['małgorzata bogdan', 'jayanta k. ghosh', 'surya t. tokdar']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"53",805.2756,"the remarkable simplicity of very high dimensional data: application of   model-based clustering","stat.me math.gm","an ultrametric topology formalizes the notion of hierarchical structure. an ultrametric embedding, referred to here as ultrametricity, is implied by a hierarchical embedding. such hierarchical structure can be global in the data set, or local. by quantifying extent or degree of ultrametricity in a data set, we show that ultrametricity becomes pervasive as dimensionality and/or spatial sparsity increases. this leads us to assert that very high dimensional data are of simple structure. we exemplify this finding through a range of simulated data cases. we discuss also application to very high frequency time series segmentation and modeling.","10.1007/s00357-009-9037-9","2008-05-18","2008-11-16","['fionn murtagh']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"54",805.3066,"j. k. ghosh's contribution to statistics: a brief outline","math.st stat.me stat.th","professor jayanta kumar ghosh has contributed massively to various areas of statistics over the last five decades. here, we survey some of his most important contributions. in roughly chronological order, we discuss his major results in the areas of sequential analysis, foundations, asymptotics, and bayesian inference. it is seen that he progressed from thinking about data points, to thinking about data summarization, to the limiting cases of data summarization in as they relate to parameter estimation, and then to more general aspects of modeling including prior and model selection.","10.1214/074921708000000011","2008-05-20","","['bertrand clarke', 'subhashis ghosal']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"55",805.3286,"an ensemble approach to improved prediction from multitype data","stat.ap stat.me stat.ml","we have developed a strategy for the analysis of newly available binary data to improve outcome predictions based on existing data (binary or non-binary). our strategy involves two modeling approaches for the newly available data, one combining binary covariate selection via lasso with logistic regression and one based on logic trees. the results of these models are then compared to the results of a model based on existing data with the objective of combining model results to achieve the most accurate predictions. the combination of model predictions is aided by the use of support vector machines to identify subspaces of the covariate space in which specific models lead to successful predictions. we demonstrate our approach in the analysis of single nucleotide polymorphism (snp) data and traditional clinical risk factors for the prediction of coronary heart disease.","10.1214/074921708000000219","2008-05-21","","['jennifer clarke', 'david seo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE
"56",805.4359,"adaptive design and analysis of supercomputer experiments","stat.ap stat.me","computer experiments are often performed to allow modeling of a response surface of a physical experiment that can be too costly or difficult to run except using a simulator. running the experiment over a dense grid can be prohibitively expensive, yet running over a sparse design chosen in advance can result in obtaining insufficient information in parts of the space, particularly when the surface calls for a nonstationary model. we propose an approach that automatically explores the space while simultaneously fitting the response surface, using predictive uncertainty to guide subsequent experimental runs. the newly developed bayesian treed gaussian process is used as the surrogate model, and a fully bayesian approach allows explicit measures of uncertainty. we develop an adaptive sequential design framework to cope with an asynchronous, random, agent--based supercomputing environment, by using a hybrid approach that melds optimal strategies from the statistics literature with flexible strategies from the active learning literature. the merits of this approach are borne out in several examples, including the motivating computational fluid dynamics simulation of a rocket booster.","","2008-05-28","2009-05-25","['robert b. gramacy', 'herbert k. h. lee']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"57",805.4598,"maximum likelihood estimation of cloud height from multi-angle satellite   imagery","stat.ap","we develop a new estimation technique for recovering depth-of-field from multiple stereo images. depth-of-field is estimated by determining the shift in image location resulting from different camera viewpoints. when this shift is not divisible by pixel width, the multiple stereo images can be combined to form a super-resolution image. by modeling this super-resolution image as a realization of a random field, one can view the recovery of depth as a likelihood estimation problem. we apply these modeling techniques to the recovery of cloud height from multiple viewing angles provided by the misr instrument on the terra satellite. our efforts are focused on a two layer cloud ensemble where both layers are relatively planar, the bottom layer is optically thick and textured, and the top layer is optically thin. our results demonstrate that with relative ease, we get comparable estimates to the m2 stereo matcher which is the same algorithm used in the current misr standard product (details can be found in [ieee transactions on geoscience and remote sensing 40 (2002) 1547--1559]). moreover, our techniques provide the possibility of modeling all of the misr data in a unified way for cloud height estimation. research is underway to extend this framework for fast, quality global estimates of cloud height.","10.1214/09-aoas243","2008-05-29","2009-10-07","['e. anderes', 'b. yu', 'v. jovanovic', 'c. moroney', 'm. garay', 'a. braverman', 'e. clothiaux']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"58",806.0582,"sampling spatially correlated clutter","stat.me","correlated ${\cal g}$ distributions can be used to describe the clutter seen in images obtained with coherent illumination, as is the case of b-scan ultrasound, laser, sonar and synthetic aperture radar (sar) imagery. these distributions are derived using the square root of the generalized inverse gaussian distribution for the amplitude backscatter within the multiplicative model. a two-parameters particular case of the amplitude ${\mathcal g}$ distribution, called ${\mathcal g}_{a}^{0}$, constitutes a modeling improvement with respect to the widespread ${\mathcal k}_{a}$ distribution when fitting urban, forested and deforested areas in remote sensing data. this article deals with the modeling and the simulation of correlated ${\mathcal g}_{a}^{0}$-distributed random fields. it is accomplished by means of the inverse transform method, applied to gaussian random fields with spatial correlation. the main feature of this approach is its generality, since it allows the introduction of negative correlation values in the resulting process, necessary for the proper explanation of the shadowing effect in many sar images.","10.1080/03610910903249536","2008-06-03","","['o. h. bustos', 'a. g. flesia', 'a. c. frery', 'm. m. lucini']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"59",806.2424,"developing bayesian information entropy-based techniques for spatially   explicit model assessment","stat.me stat.co","the aim of this paper is to explore and develop advanced spatial bayesian assessment methods and techniques for land use modeling. the paper provides a comprehensive guide for assessing additional informational entropy value of model predictions at the spatially explicit domain of knowledge, and proposes a few alternative metrics and indicators for extracting higher-order information dynamics from simulation tournaments. a seven-county study area in south-eastern wisconsin (sewi) has been used to simulate and assess the accuracy of historical land use changes (1963-1990) using artificial neural network simulations of the land transformation model (ltm). the use of the analysis and the performance of the metrics helps: (a) understand and learn how well the model runs fits to different combinations of presence and absence of transitions in a landscape, not simply how well the model fits our given data; (b) derive (estimate) a theoretical accuracy that we would expect a model to assess under the presence of incomplete information and measurement; (c) understand the spatially explicit role and patterns of uncertainty in simulations and model estimations, by comparing results across simulation runs; (d) compare the significance or estimation contribution of transitional presence and absence (change versus no change) to model performance, and the contribution of the spatial drivers and variables to the explanatory value of our model; and (e) compare measurements of informational uncertainty at different scales of spatial resolution.","","2008-06-15","","['kostas alexandridis', 'bryan c. pijanowski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"60",806.285,"decoding beta-decay systematics: a global statistical model for beta^-   halflives","nucl-th astro-ph cond-mat.dis-nn cs.lg stat.ml","statistical modeling of nuclear data provides a novel approach to nuclear systematics complementary to established theoretical and phenomenological approaches based on quantum theory. continuing previous studies in which global statistical modeling is pursued within the general framework of machine learning theory, we implement advances in training algorithms designed to improved generalization, in application to the problem of reproducing and predicting the halflives of nuclear ground states that decay 100% by the beta^- mode. more specifically, fully-connected, multilayer feedforward artificial neural network models are developed using the levenberg-marquardt optimization algorithm together with bayesian regularization and cross-validation. the predictive performance of models emerging from extensive computer experiments is compared with that of traditional microscopic and phenomenological models as well as with the performance of other learning systems, including earlier neural network models as well as the support vector machines recently applied to the same problem. in discussing the results, emphasis is placed on predictions for nuclei that are far from the stability line, and especially those involved in the r-process nucleosynthesis. it is found that the new statistical models can match or even surpass the predictive performance of conventional models for beta-decay systematics and accordingly should provide a valuable additional tool for exploring the expanding nuclear landscape.","10.1103/physrevc.80.044332","2008-06-17","","['n. j. costiris', 'e. mavrommatis', 'k. a. gernoth', 'j. w. clark']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"61",807.347,"inference with discriminative posterior","stat.ml math.st stat.th","we study bayesian discriminative inference given a model family $p(c,\x, \theta)$ that is assumed to contain all our prior information but still known to be incorrect. this falls in between ""standard"" bayesian generative modeling and bayesian regression, where the margin $p(\x,\theta)$ is known to be uninformative about $p(c|\x,\theta)$. we give an axiomatic proof that discriminative posterior is consistent for conditional inference; using the discriminative posterior is standard practice in classical bayesian regression, but we show that it is theoretically justified for model families of joint densities as well. a practical benefit compared to bayesian regression is that the standard methods of handling missing values in generative modeling can be extended into discriminative inference, which is useful if the amount of data is small. compared to standard generative modeling, discriminative posterior results in better conditional inference if the model family is incorrect. if the model family contains also the true model, the discriminative posterior gives the same result as standard bayesian generative modeling. practical computation is done with markov chain monte carlo.","","2008-07-22","2008-11-18","['jarkko salojärvi', 'kai puolamäki', 'eerika savia', 'samuel kaski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"62",807.4071,"forecasting time series of inhomogeneous poisson processes with   application to call center workforce management","stat.ap","we consider forecasting the latent rate profiles of a time series of inhomogeneous poisson processes. the work is motivated by operations management of queueing systems, in particular, telephone call centers, where accurate forecasting of call arrival rates is a crucial primitive for efficient staffing of such centers. our forecasting approach utilizes dimension reduction through a factor analysis of poisson variables, followed by time series modeling of factor score series. time series forecasts of factor scores are combined with factor loadings to yield forecasts of future poisson rate profiles. penalized poisson regressions on factor loadings guided by time series forecasts of factor scores are used to generate dynamic within-process rate updating. methods are also developed to obtain distributional forecasts. our methods are illustrated using simulation and real data. the empirical results demonstrate how forecasting and dynamic updating of call arrival rates can affect the accuracy of call center staffing.","10.1214/08-aoas164","2008-07-25","","['haipeng shen', 'jianhua z. huang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"63",807.4649,"hidden markov models for the assessment of chromosomal alterations using   high-throughput snp arrays","stat.ap","chromosomal dna is characterized by variation between individuals at the level of entire chromosomes (e.g., aneuploidy in which the chromosome copy number is altered), segmental changes (including insertions, deletions, inversions, and translocations), and changes to small genomic regions (including single nucleotide polymorphisms). a variety of alterations that occur in chromosomal dna, many of which can be detected using high density single nucleotide polymorphism (snp) microarrays, are linked to normal variation as well as disease and are therefore of particular interest. these include changes in copy number (deletions and duplications) and genotype (e.g., the occurrence of regions of homozygosity). hidden markov models (hmm) are particularly useful for detecting such alterations, modeling the spatial dependence between neighboring snps. here, we improve previous approaches that utilize hmm frameworks for inference in high throughput snp arrays by integrating copy number, genotype calls, and the corresponding measures of uncertainty when available. using simulated and experimental data, we, in particular, demonstrate how confidence scores control smoothing in a probabilistic framework. software for fitting hmms to snp array data is available in the r package vanillaice.","10.1214/07-aoas155","2008-07-29","","['robert b. scharpf', 'giovanni parmigiani', 'jonathan pevsner', 'ingo ruczinski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"64",807.4663,"gamma shape mixtures for heavy-tailed distributions","stat.ap","an important question in health services research is the estimation of the proportion of medical expenditures that exceed a given threshold. typically, medical expenditures present highly skewed, heavy tailed distributions, for which (a) simple variable transformations are insufficient to achieve a tractable low-dimensional parametric form and (b) nonparametric methods are not efficient in estimating exceedance probabilities for large thresholds. motivated by this context, in this paper we propose a general bayesian approach for the estimation of tail probabilities of heavy-tailed distributions, based on a mixture of gamma distributions in which the mixing occurs over the shape parameter. this family provides a flexible and novel approach for modeling heavy-tailed distributions, it is computationally efficient, and it only requires to specify a prior distribution for a single parameter. by carrying out simulation studies, we compare our approach with commonly used methods, such as the log-normal model and nonparametric alternatives. we found that the mixture-gamma model significantly improves predictive performance in estimating tail probabilities, compared to these alternatives. we also applied our method to the medical current beneficiary survey (mcbs), for which we estimate the probability of exceeding a given hospitalization cost for smoking attributable diseases. we have implemented the method in the open source gsm package, available from the comprehensive r archive network.","10.1214/07-aoas156","2008-07-29","","['sergio venturini', 'francesca dominici', 'giovanni parmigiani']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"65",808.0383,"an overview of mixture models","math.st stat.th","this paper has been withdrawn. with the advancement of statistical theory and computing power, data sets are providing a greater amount of insight into the problems of today. statisticians have an ever increasing number of tools to attack these problems, some of which can be implemented in the area of mixture modeling. there is a great deal of literature on mixture models and this work attempts to provide a general overview of the subject, including the discussion of relevant issues and algorithms. the reader can hope to gain a broad understanding of concepts in mixture modeling and find the references cited within as a valuable resource for the next stage of their research.","","2008-08-04","2012-12-18","['derek s. young']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"66",808.0597,"comment: microarrays, empirical bayes and the two-groups model","stat.me","brad efron's paper [arxiv:0808.0572] has inspired a return to the ideas behind bayes, frequency and empirical bayes. the latter preferably would not be limited to exchangeable models for the data and hyperparameters. parallels are revealed between microarray analyses and profiling of hospitals, with advances suggesting more decision modeling for gene identification also. then good multilevel and empirical bayes models for random effects should be sought when regression toward the mean is anticipated.","10.1214/08-sts236d","2008-08-05","","['carl n. morris']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"67",808.0989,"semiparametric detection of significant activation for brain fmri","math.st stat.th","functional magnetic resonance imaging (fmri) aims to locate activated regions in human brains when specific tasks are performed. the conventional tool for analyzing fmri data applies some variant of the linear model, which is restrictive in modeling assumptions. to yield more accurate prediction of the time-course behavior of neuronal responses, the semiparametric inference for the underlying hemodynamic response function is developed to identify significantly activated voxels. under mild regularity conditions, we demonstrate that a class of the proposed semiparametric test statistics, based on the local linear estimation technique, follow $\chi^2$ distributions under null hypotheses for a number of useful hypotheses. furthermore, the asymptotic power functions of the constructed tests are derived under the fixed and contiguous alternatives. simulation evaluations and real fmri data application suggest that the semiparametric inference procedure provides more efficient detection of activated brain areas than the popular imaging analysis tools afni and fsl.","10.1214/07-aos519","2008-08-07","","['chunming zhang', 'tao yu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"68",808.171,"dynamic modeling of mean-reverting spreads for statistical arbitrage","q-fin.st q-fin.pm stat.ap stat.me","statistical arbitrage strategies, such as pairs trading and its generalizations, rely on the construction of mean-reverting spreads enjoying a certain degree of predictability. gaussian linear state-space processes have recently been proposed as a model for such spreads under the assumption that the observed process is a noisy realization of some hidden states. real-time estimation of the unobserved spread process can reveal temporary market inefficiencies which can then be exploited to generate excess returns. building on previous work, we embrace the state-space framework for modeling spread processes and extend this methodology along three different directions. first, we introduce time-dependency in the model parameters, which allows for quick adaptation to changes in the data generating process. second, we provide an on-line estimation algorithm that can be constantly run in real-time. being computationally fast, the algorithm is particularly suitable for building aggressive trading strategies based on high-frequency data and may be used as a monitoring device for mean-reversion. finally, our framework naturally provides informative uncertainty measures of all the estimated parameters. experimental results based on monte carlo simulations and historical equity data are discussed, including a co-integration relationship involving two exchange-traded funds.","","2008-08-12","2009-05-19","['kostas triantafyllopoulos', 'giovanni montana']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"69",808.3864,"rejoinder: gibbs sampling, exponential families and orthogonal   polynomials","stat.me","we are thankful to the discussants for their hard, interesting work. the main purpose of our paper was to give reasonably sharp rates of convergence for some simple examples of the gibbs sampler. we chose examples from expository accounts where direct use of available techniques gave practically useless answers. careful treatment of these simple examples grew into bivariate modeling and lancaster families. since bounding rates of convergence is our primary focus, let us begin there. [arxiv:0808.3852]","10.1214/08-sts252rej","2008-08-28","","['persi diaconis', 'kshitij khare', 'laurent saloff-coste']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"70",809.2703,"the weibull-geometric distribution","stat.me","in this paper we introduce, for the first time, the weibull-geometric distribution which generalizes the exponential-geometric distribution proposed by adamidis and loukas (1998). the hazard function of the last distribution is monotone decreasing but the hazard function of the new distribution can take more general forms. unlike the weibull distribution, the proposed distribution is useful for modeling unimodal failure rates. we derive the cumulative distribution and hazard functions, the density of the order statistics and calculate expressions for its moments and for the moments of the order statistics. we give expressions for the r\'enyi and shannon entropies. the maximum likelihood estimation procedure is discussed and an algorithm em (dempster et al., 1977; mclachlan and krishnan, 1997) is provided for estimating the parameters. we obtain the information matrix and discuss inference. applications to real data sets are given to show the flexibility and potentiality of the proposed distribution.","10.1080/00949650903436554","2008-09-16","","['wagner barreto-souza', 'alice lemos de morais', 'gauss m. cordeiro']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"71",810.3724,"foundations of a multi-way spectral clustering framework for hybrid   linear modeling","stat.ml math.mg","the problem of hybrid linear modeling (hlm) is to model and segment data using a mixture of affine subspaces. different strategies have been proposed to solve this problem, however, rigorous analysis justifying their performance is missing. this paper suggests the theoretical spectral curvature clustering (tscc) algorithm for solving the hlm problem, and provides careful analysis to justify it. the tscc algorithm is practically a combination of govindu's multi-way spectral clustering framework (cvpr 2005) and ng et al.'s spectral clustering algorithm (nips 2001). the main result of this paper states that if the given data is sampled from a mixture of distributions concentrated around affine subspaces, then with high sampling probability the tscc algorithm segments well the different underlying clusters. the goodness of clustering depends on the within-cluster errors, the between-clusters interaction, and a tuning parameter applied by tscc. the proof also provides new insights for the analysis of ng et al. (nips 2001).","10.1007/s10208-009-9043-7","2008-10-20","2009-01-14","['guangliang chen', 'gilad lerman']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"72",810.5655,"gibbs posterior for variable selection in high-dimensional   classification and data mining","stat.me stat.ml","in the popular approach of ""bayesian variable selection"" (bvs), one uses prior and posterior distributions to select a subset of candidate variables to enter the model. a completely new direction will be considered here to study bvs with a gibbs posterior originating in statistical mechanics. the gibbs posterior is constructed from a risk function of practical interest (such as the classification error) and aims at minimizing a risk function without modeling the data probabilistically. this can improve the performance over the usual bayesian approach, which depends on a probability model which may be misspecified. conditions will be provided to achieve good risk performance, even in the presence of high dimensionality, when the number of candidate variables ""$k$"" can be much larger than the sample size ""$n$."" in addition, we develop a convenient markov chain monte carlo algorithm to implement bvs with the gibbs posterior.","10.1214/07-aos547","2008-10-31","","['wenxin jiang', 'martin a. tanner']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"73",811.0659,"estimation of missing data by using the filtering process in a time   series modeling","stat.me","this paper proposed a new method to estimate the missing data by using the filtering process. we used datasets without missing data and randomly missing data to evaluate the new method of estimation by using the box - jenkins modeling technique to predict monthly average rainfall for site 5504035 lahar ikan mati at kepala batas, p. pinang station in malaysia. the rainfall data was collected from the $1^{st}$ january 1969 to $31^{st}$ december 1997 in the station. the data used in the development of the model to predict rainfall were represented by an autoregressive integrated moving - average (arima) model. the model for both datasets was arima$(1,0,0)(0,1,1)_s$. the result checked with the naive test, which is the thiel's statistic and was found to be equal to $u=0.72086$ for the complete data and $u=0.726352$ for the missing data, which mean they were good models.","","2008-11-05","","['r. ahmad mahir', 'a. m. h. al-khazaleh']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"74",811.0683,"an algorithmic and a geometric characterization of coarsening at random","math.st stat.th","we show that the class of conditional distributions satisfying the coarsening at random (car) property for discrete data has a simple and robust algorithmic description based on randomized uniform multicovers: combinatorial objects generalizing the notion of partition of a set. however, the complexity of a given car mechanism can be large: the maximal ""height"" of the needed multicovers can be exponential in the number of points in the sample space. the results stem from a geometric interpretation of the set of car distributions as a convex polytope and a characterization of its extreme points. the hierarchy of car models defined in this way could be useful in parsimonious statistical modeling of car mechanisms, though the results also raise doubts in applied work as to the meaningfulness of the car assumption in its full generality.","10.1214/07-aos532","2008-11-05","","['richard d. gill', 'peter d. grünwald']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"75",811.1686,"sequential category aggregation and partitioning approaches for   multi-way contingency tables based on survey and census data","stat.ap","large contingency tables arise in many contexts but especially in the collection of survey and census data by government statistical agencies. because the vast majority of the variables in this context have a large number of categories, agencies and users need a systematic way of constructing tables which are summaries of such contingency tables. we propose such an approach in this paper by finding members of a class of restricted log-linear models which maximize the likelihood of the data and use this to find a parsimonious means of representing the table. in contrast with more standard approaches for model search in hierarchical log-linear models (hllm), our procedure systematically reduces the number of categories of the variables. through a series of examples, we illustrate the extent to which it can preserve the interaction structure found with hllms and be used as a data simplification procedure prior to hll modeling. a feature of the procedure is that it can easily be applied to many tables with millions of cells, providing a new way of summarizing large data sets in many disciplines. the focus is on information and description rather than statistical testing. the procedure may treat each variable in the table in different ways, preserving full detail, treating it as fully nominal, or preserving ordinality.","10.1214/08-aoas175","2008-11-11","","['l. fraser jackson', 'alistair g. gray', 'stephen e. fienberg']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"76",811.3644,"markov switching multinomial logit model: an application to accident   injury severities","stat.ap stat.me","in this study, two-state markov switching multinomial logit models are proposed for statistical modeling of accident injury severities. these models assume markov switching in time between two unobserved states of roadway safety. the states are distinct, in the sense that in different states accident severity outcomes are generated by separate multinomial logit processes. to demonstrate the applicability of the approach presented herein, two-state markov switching multinomial logit models are estimated for severity outcomes of accidents occurring on indiana roads over a four-year time interval. bayesian inference methods and markov chain monte carlo (mcmc) simulations are used for model estimation. the estimated markov switching models result in a superior statistical fit relative to the standard (single-state) multinomial logit models. it is found that the more frequent state of roadway safety is correlated with better weather conditions. the less frequent state is found to be correlated with adverse weather conditions.","","2008-11-21","","['nataliya v. malyshkina', 'fred l. mannering']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"77",812.3253,"estimation of the distribution of random shifts deformation","math.st stat.th","consider discrete values of functions shifted by unobserved translation effects, which are independent realizations of a random variable with unknown distribution $\mu$, modeling the variability in the response of each individual. our aim is to construct a nonparametric estimator of the density of these random translation deformations using semiparametric preliminary estimates of the shifts. building on results of dalalyan et al. (2006), semiparametric estimators are obtained in our discrete framework and their performance studied. from these estimates we construct a nonparametric estimator of the target density. both rates of convergence and an algorithm to construct the estimator are provided.","","2008-12-17","","['ismael castillo', 'jean-michel loubes']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"78",812.5061,"thresholding-based iterative selection procedures for model selection   and shrinkage","math.st stat.th","this paper discusses a class of thresholding-based iterative selection procedures (tisp) for model selection and shrinkage. people have long before noticed the weakness of the convex $l_1$-constraint (or the soft-thresholding) in wavelets and have designed many different forms of nonconvex penalties to increase model sparsity and accuracy. but for a nonorthogonal regression matrix, there is great difficulty in both investigating the performance in theory and solving the problem in computation. tisp provides a simple and efficient way to tackle this so that we successfully borrow the rich results in the orthogonal design to solve the nonconvex penalized regression for a general design matrix. our starting point is, however, thresholding rules rather than penalty functions. indeed, there is a universal connection between them. but a drawback of the latter is its non-unique form, and our approach greatly facilitates the computation and the analysis. in fact, we are able to build the convergence theorem and explore theoretical properties of the selection and estimation via tisp nonasymptotically. more importantly, a novel hybrid-tisp is proposed based on hard-thresholding and ridge-thresholding. it provides a fusion between the $l_0$-penalty and the $l_2$-penalty, and adaptively achieves the right balance between shrinkage and selection in statistical modeling. in practice, hybrid-tisp shows superior performance in test-error and is parsimonious.","","2008-12-30","2009-11-29","['yiyuan she']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"79",901.2503,"linear processes for functional data","math.st stat.th","linear processes on functional spaces were born about fifteen years ago. and this original topic went through the same fast development as the other areas of functional data modeling such as pca or regression. they aim at generalizing to random curves the classical arma models widely known in time series analysis. they offer a wide spectrum of models suited to the statistical inference on continuous time stochastic processes within the paradigm of functional data. essentially designed to improve the quality and the range of prediction, they give birth to challenging theoretical and applied problems. we propose here a state of the art which emphasizes recent advances and we present some promising perspectives based on our experience in this area.","","2009-01-16","","['andré mas', 'besnik pumo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"80",901.3484,"probabilistic quantitative precipitation field forecasting using a   two-stage spatial model","stat.ap","short-range forecasts of precipitation fields are needed in a wealth of agricultural, hydrological, ecological and other applications. forecasts from numerical weather prediction models are often biased and do not provide uncertainty information. here we present a postprocessing technique for such numerical forecasts that produces correlated probabilistic forecasts of precipitation accumulation at multiple sites simultaneously. the statistical model is a spatial version of a two-stage model that represents the distribution of precipitation by a mixture of a point mass at zero and a gamma density for the continuous distribution of precipitation accumulation. spatial correlation is captured by assuming that two gaussian processes drive precipitation occurrence and precipitation amount, respectively. the first process is latent and drives precipitation occurrence via a threshold. the second process explains the spatial correlation in precipitation accumulation. it is related to precipitation via a site-specific transformation function, so as to retain the marginal right-skewed distribution of precipitation while modeling spatial dependence. both processes take into account the information contained in the numerical weather forecast and are modeled as stationary isotropic spatial processes with an exponential correlation function. the two-stage spatial model was applied to 48-hour-ahead forecasts of daily precipitation accumulation over the pacific northwest in 2004. the predictive distributions from the two-stage spatial model were calibrated and sharp, and outperformed reference forecasts for spatially composite and areally averaged quantities.","10.1214/08-aoas203","2009-01-22","","['veronica j. berrocal', 'adrian e. raftery', 'tilmann gneiting']",1,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"81",901.3806,"modeling long-term longitudinal hiv dynamics with application to an aids   clinical study","stat.ap","a virologic marker, the number of hiv rna copies or viral load, is currently used to evaluate antiretroviral (arv) therapies in aids clinical trials. this marker can be used to assess the arv potency of therapies, but is easily affected by drug exposures, drug resistance and other factors during the long-term treatment evaluation process. hiv dynamic studies have significantly contributed to the understanding of hiv pathogenesis and arv treatment strategies. however, the models of these studies are used to quantify short-term hiv dynamics ($<$ 1 month), and are not applicable to describe long-term virological response to arv treatment due to the difficulty of establishing a relationship of antiviral response with multiple treatment factors such as drug exposure and drug susceptibility during long-term treatment. long-term therapy with arv agents in hiv-infected patients often results in failure to suppress the viral load. pharmacokinetics (pk), drug resistance and imperfect adherence to prescribed antiviral drugs are important factors explaining the resurgence of virus. to better understand the factors responsible for the virological failure, this paper develops the mechanism-based nonlinear differential equation models for characterizing long-term viral dynamics with arv therapy. the models directly incorporate drug concentration, adherence and drug susceptibility into a function of treatment efficacy and, hence, fully integrate virologic, pk, drug adherence and resistance from an aids clinical trial into the analysis. a bayesian nonlinear mixed-effects modeling approach in conjunction with the rescaled version of dynamic differential equations is investigated to estimate dynamic parameters and make inference. in addition, the correlations of baseline factors with estimated dynamic parameters are explored and some biologically meaningful correlation results are presented. further, the estimated dynamic parameters in patients with virologic success were compared to those in patients with virologic failure and significantly important findings were summarized. these results suggest that viral dynamic parameters may play an important role in understanding hiv pathogenesis, designing new treatment strategies for long-term care of aids patients.","10.1214/08-aoas192","2009-01-26","","['yangxin huang', 'tao lu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"82",901.4025,"a bayesian framework for estimating vaccine efficacy per infectious   contact","stat.ap","in vaccine studies for infectious diseases such as human immunodeficiency virus (hiv), the frequency and type of contacts between study participants and infectious sources are among the most informative risk factors, but are often not adequately adjusted for in standard analyses. such adjustment can improve the assessment of vaccine efficacy as well as the assessment of risk factors. it can be attained by modeling transmission per contact with infectious sources. however, information about contacts that rely on self-reporting by study participants are subject to nontrivial measurement error in many studies. we develop a bayesian hierarchical model fitted using markov chain monte carlo (mcmc) sampling to estimate the vaccine efficacy controlled for exposure to infection, while adjusting for measurement error in contact-related factors. our method is used to re-analyze two recent hiv vaccine studies, and the results are compared with the published primary analyses that used standard methods. the proposed method could also be used for other vaccines where contact information is collected, such as human papilloma virus vaccines.","10.1214/08-aoas193","2009-01-26","","['yang yang', 'peter gilbert', 'ira m. longini,', 'm. elizabeth halloran']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"83",901.4213,"state-space based mass event-history model i: many decision-making   agents with one target","stat.ap","a dynamic decision-making system that includes a mass of indistinguishable agents could manifest impressive heterogeneity. this kind of nonhomogeneity is postulated to result from macroscopic behavioral tactics employed by almost all involved agents. a state-space based (ssb) mass event-history model is developed here to explore the potential existence of such macroscopic behaviors. by imposing an unobserved internal state-space variable into the system, each individual's event-history is made into a composition of a common state duration and an individual specific time to action. with the common state modeling of the macroscopic behavior, parametric statistical inferences are derived under the current-status data structure and conditional independence assumptions. identifiability and computation related problems are also addressed. from the dynamic perspectives of system-wise heterogeneity, this ssb mass event-history model is shown to be very distinct from a random effect model via the principle component analysis (pca) in a numerical experiment. real data showing the mass invasion by two species of parasitic nematode into two species of host larvae are also analyzed. the analysis results not only are found coherent in the context of the biology of the nematode as a parasite, but also include new quantitative interpretations.","10.1214/08-aoas189","2009-01-27","","['hsieh fushing', 'li zhu', 'david i. shapiro-ilan', 'james f. campbell', 'edwin e. lewis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"84",902.306,"likelihood-based inference for max-stable processes","stat.me","the last decade has seen max-stable processes emerge as a common tool for the statistical modeling of spatial extremes. however, their application is complicated due to the unavailability of the multivariate density function, and so likelihood-based methods remain far from providing a complete and flexible framework for inference. in this article we develop inferentially practical, likelihood-based methods for fitting max-stable processes derived from a composite-likelihood approach. the procedure is sufficiently reliable and versatile to permit the simultaneous modeling of marginal and dependence parameters in the spatial context at a moderate computational cost. the utility of this methodology is examined via simulation, and illustrated by the analysis of u.s. precipitation extremes.","","2009-02-18","2009-02-23","['simone a. padoan', 'mathieu ribatet', 'scott a. sisson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"85",902.3725,"statistical inference of functional connectivity in neuronal networks   using frequent episodes","q-bio.nc cond-mat.dis-nn cs.db q-bio.qm stat.me","identifying the spatio-temporal network structure of brain activity from multi-neuronal data streams is one of the biggest challenges in neuroscience. repeating patterns of precisely timed activity across a group of neurons is potentially indicative of a microcircuit in the underlying neural tissue. frequent episode discovery, a temporal data mining framework, has recently been shown to be a computationally efficient method of counting the occurrences of such patterns. in this paper, we propose a framework to determine when the counts are statistically significant by modeling the counting process. our model allows direct estimation of the strengths of functional connections between neurons with improved resolution over previously published methods. it can also be used to rank the patterns discovered in a network of neurons according to their strengths and begin to reconstruct the graph structure of the network that produced the spike data. we validate our methods on simulated data and present analysis of patterns discovered in data from cultures of cortical neurons.","","2009-02-21","","['casey diekman', 'kohinoor dasgupta', 'vijay nair', 'p. s. sastry', 'k. p. unnikrishnan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"86",904.0776,"induction of high-level behaviors from problem-solving traces using   machine learning tools","stat.ml cs.lg","this paper applies machine learning techniques to student modeling. it presents a method for discovering high-level student behaviors from a very large set of low-level traces corresponding to problem-solving actions in a learning environment. basic actions are encoded into sets of domain-dependent attribute-value patterns called cases. then a domain-independent hierarchical clustering identifies what we call general attitudes, yielding automatic diagnosis expressed in natural language, addressed in principle to teachers. the method can be applied to individual students or to entire groups, like a class. we exhibit examples of this system applied to thousands of students' actions in the domain of algebraic transformations.","","2009-04-05","","['vivien robinet', 'gilles bisson', 'mirta b. gordon', 'benoît lemaire']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"87",904.0838,"finding exogenous variables in data with many more variables than   observations","stat.ml","many statistical methods have been proposed to estimate causal models in classical situations with fewer variables than observations (p<n, p: the number of variables and n: the number of observations). however, modern datasets including gene expression data need high-dimensional causal modeling in challenging situations with orders of magnitude more variables than observations (p>>n). in this paper, we propose a method to find exogenous variables in a linear non-gaussian causal model, which requires much smaller sample sizes than conventional methods and works even when p>>n. the key idea is to identify which variables are exogenous based on non-gaussianity instead of estimating the entire structure of the model. exogenous variables work as triggers that activate a causal chain in the model, and their identification leads to more efficient experimental designs and better understanding of the causal mechanism. we present experiments with artificial data and real-world gene expression data to evaluate the method.","10.1007/978-3-642-15819-3_10","2009-04-05","2011-04-07","['shohei shimizu', 'takashi washio', 'aapo hyvarinen', 'seiya imoto']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"88",904.0951,"inference on counterfactual distributions","stat.me econ.em stat.ap","counterfactual distributions are important ingredients for policy analysis and decomposition analysis in empirical economics. in this article we develop modeling and inference tools for counterfactual distributions based on regression methods. the counterfactual scenarios that we consider consist of ceteris paribus changes in either the distribution of covariates related to the outcome of interest or the conditional distribution of the outcome given covariates. for either of these scenarios we derive joint functional central limit theorems and bootstrap validity results for regression-based estimators of the status quo and counterfactual outcome distributions. these results allow us to construct simultaneous confidence sets for function-valued effects of the counterfactual changes, including the effects on the entire distribution and quantile functions of the outcome as well as on related functionals. these confidence sets can be used to test functional hypotheses such as no-effect, positive effect, or stochastic dominance. our theory applies to general counterfactual changes and covers the main regression methods including classical, quantile, duration, and distribution regressions. we illustrate the results with an empirical application to wage decompositions using data for the united states.   as a part of developing the main results, we introduce distribution regression as a comprehensive and flexible tool for modeling and estimating the \textit{entire} conditional distribution. we show that distribution regression encompasses the cox duration regression and represents a useful alternative to quantile regression. we establish functional central limit theorems and bootstrap validity results for the empirical distribution regression process and various related functionals.","","2009-04-06","2013-09-18","['victor chernozhukov', 'ivan fernandez-val', 'blaise melly']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"89",904.151,"decomposition and model selection for large contingency tables","stat.me","large contingency tables summarizing categorical variables arise in many areas. for example in biology when a large number of biomarkers are cross-tabulated according to their discrete expression level. interactions of the variables are generally studied with log-linear models and the structure of a log-linear model can be visually represented by a graph from which the conditional independence structure can then be read off. however, since the number of parameters in a saturated model grows exponentially in the number of variables, this generally comes with a heavy burden as far as computational power is concerned. if we restrict ourselves to models of lower order interactions or other sparse structures we face similar problems as the number of cells remains unchanged. we therefore present a divide-and-conquer approach, where we first divide the problem into several lower-dimensional problems and then combine these to form a global solution. our methodology is computationally feasible for log-linear interaction modeling with many categorical variables each or some of them having many categories. we demonstrate the proposed method on simulated data and apply it to a bio-medical problem in cancer research.","","2009-04-09","2009-11-17","['corinne dahinden', 'markus kalisch', 'peter bühlmann']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"90",905.0668,"small-sample corrections for score tests in birnbaum-saunders   regressions","stat.me","in this paper we deal with the issue of performing accurate small-sample inference in the birnbaum-saunders regression model, which can be useful for modeling lifetime or reliability data. we derive a bartlett-type correction for the score test and numerically compare the corrected test with the usual score test, the likelihood ratio test and its bartlett-corrected version. our simulation results suggest that the corrected test we propose is more reliable than the other tests.","10.1080/03610920903402613","2009-05-05","2010-07-15","['artur j. lemonte', 'silvia l. p. ferrari']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"91",905.2236,"percolation thresholds of updated posteriors for tracking causal markov   processes in complex networks","stat.ml stat.ap","percolation on complex networks has been used to study computer viruses, epidemics, and other casual processes. here, we present conditions for the existence of a network specific, observation dependent, phase transition in the updated posterior of node states resulting from actively monitoring the network. since traditional percolation thresholds are derived using observation independent markov chains, the threshold of the posterior should more accurately model the true phase transition of a network, as the updated posterior more accurately tracks the process. these conditions should provide insight into modeling the dynamic response of the updated posterior to active intervention and control policies while monitoring large complex networks.","","2009-05-13","","['patrick l. harrington', 'alfred o. hero']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"92",906.0865,"nonparametric estimation of composite functions","math.st stat.th","we study the problem of nonparametric estimation of a multivariate function $g:\mathbb {r}^d\to\mathbb{r}$ that can be represented as a composition of two unknown smooth functions $f:\mathbb{r}\to\mathbb{r}$ and $g:\mathbb{r}^d\to \mathbb{r}$. we suppose that $f$ and $g$ belong to known smoothness classes of functions, with smoothness $\gamma$ and $\beta$, respectively. we obtain the full description of minimax rates of estimation of $g$ in terms of $\gamma$ and $\beta$, and propose rate-optimal estimators for the sup-norm loss. for the construction of such estimators, we first prove an approximation result for composite functions that may have an independent interest, and then a result on adaptation to the local structure. interestingly, the construction of rate-optimal estimators for composite functions (with given, fixed smoothness) needs adaptation, but not in the traditional sense: it is now adaptation to the local structure. we prove that composition models generate only two types of local structures: the local single-index model and the local model with roughness isolated to a single dimension (i.e., a model containing elements of both additive and single-index structure). we also find the zones of ($\gamma$, $\beta$) where no local structure is generated, as well as the zones where the composition modeling leads to faster rates, as compared to the classical nonparametric rates that depend only to the overall smoothness of $g$.","10.1214/08-aos611","2009-06-04","","['anatoli b. juditsky', 'oleg v. lepski', 'alexandre b. tsybakov']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"93",906.1117,"on multi-view learning with additive models","stat.ap","in many scientific settings data can be naturally partitioned into variable groupings called views. common examples include environmental (1st view) and genetic information (2nd view) in ecological applications, chemical (1st view) and biological (2nd view) data in drug discovery. multi-view data also occur in text analysis and proteomics applications where one view consists of a graph with observations as the vertices and a weighted measure of pairwise similarity between observations as the edges. further, in several of these applications the observations can be partitioned into two sets, one where the response is observed (labeled) and the other where the response is not (unlabeled). the problem for simultaneously addressing viewed data and incorporating unlabeled observations in training is referred to as multi-view transductive learning. in this work we introduce and study a comprehensive generalized fixed point additive modeling framework for multi-view transductive learning, where any view is represented by a linear smoother. the problem of view selection is discussed using a generalized akaike information criterion, which provides an approach for testing the contribution of each view. an efficient implementation is provided for fitting these models with both backfitting and local-scoring type algorithms adjusted to semi-supervised graph-based learning. the proposed technique is assessed on both synthetic and real data sets and is shown to be competitive to state-of-the-art co-training and graph-based techniques.","10.1214/08-aoas202","2009-06-05","","['mark culp', 'george michailidis', 'kjell johnson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"94",906.1428,"practical large-scale spatio-temporal modeling of particulate matter   concentrations","stat.ap","the last two decades have seen intense scientific and regulatory interest in the health effects of particulate matter (pm). influential epidemiological studies that characterize chronic exposure of individuals rely on monitoring data that are sparse in space and time, so they often assign the same exposure to participants in large geographic areas and across time. we estimate monthly pm during 1988--2002 in a large spatial domain for use in studying health effects in the nurses' health study. we develop a conceptually simple spatio-temporal model that uses a rich set of covariates. the model is used to estimate concentrations of $pm_{10}$ for the full time period and $pm_{2.5}$ for a subset of the period. for the earlier part of the period, 1988--1998, few $pm_{2.5}$ monitors were operating, so we develop a simple extension to the model that represents $pm_{2.5}$ conditionally on $pm_{10}$ model predictions. in the epidemiological analysis, model predictions of $pm_{10}$ are more strongly associated with health effects than when using simpler approaches to estimate exposure. our modeling approach supports the application in estimating both fine-scale and large-scale spatial heterogeneity and capturing space--time interaction through the use of monthly-varying spatial surfaces. at the same time, the model is computationally feasible, implementable with standard software, and readily understandable to the scientific audience. despite simplifying assumptions, the model has good predictive performance and uncertainty characterization.","10.1214/08-aoas204","2009-06-08","","['christopher j. paciorek', 'jeff d. yanosky', 'robin c. puett', 'francine laden', 'helen h. suh']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"95",906.1433,"$\mathcal{g}$-selc: optimization by sequential elimination of level   combinations using genetic algorithms and gaussian processes","stat.ap","identifying promising compounds from a vast collection of feasible compounds is an important and yet challenging problem in the pharmaceutical industry. an efficient solution to this problem will help reduce the expenditure at the early stages of drug discovery. in an attempt to solve this problem, mandal, wu and johnson [technometrics 48 (2006) 273--283] proposed the selc algorithm. although powerful, it fails to extract substantial information from the data to guide the search efficiently, as this methodology is not based on any statistical modeling. the proposed approach uses gaussian process (gp) modeling to improve upon selc, and hence named $\mathcal{g}$-selc. the performance of the proposed methodology is illustrated using four and five dimensional test functions. finally, we implement the new algorithm on a real pharmaceutical data set for finding a group of chemical compounds with optimal properties.","10.1214/08-aoas199","2009-06-08","","['abhyuday mandal', 'pritam ranjan', 'c. f. jeff wu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"96",906.1698,"multiscale local change point detection with applications to   value-at-risk","math.st stat.th","this paper offers a new approach to modeling and forecasting of nonstationary time series with applications to volatility modeling for financial data. the approach is based on the assumption of local homogeneity: for every time point, there exists a historical interval of homogeneity, in which the volatility parameter can be well approximated by a constant. the proposed procedure recovers this interval from the data using the local change point (lcp) analysis. afterward, the estimate of the volatility can be simply obtained by local averaging. the approach carefully addresses the question of choosing the tuning parameters of the procedure using the so-called ``propagation'' condition. the main result claims a new ``oracle'' inequality in terms of the modeling bias which measures the quality of the local constant approximation. this result yields the optimal rate of estimation for smooth and piecewise constant volatility functions. then, the new procedure is applied to some data sets and a comparison with a standard garch model is also provided. finally, we discuss applications of the new method to the value at risk problem. the numerical results demonstrate a very reasonable performance of the new method.","10.1214/08-aos612","2009-06-09","","['vladimir spokoiny']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"97",906.2099,"a cluster identification framework illustrated by a filtering model for   earthquake occurrences","math.st stat.th","a general dynamical cluster identification framework including both modeling and computation is developed. the earthquake declustering problem is studied to demonstrate how this framework applies. a stochastic model is proposed for earthquake occurrences that considers the sequence of occurrences as composed of two parts: earthquake clusters and single earthquakes. we suggest that earthquake clusters contain a ``mother quake'' and her ``offspring.'' applying the filtering techniques, we use the solution of filtering equations as criteria for declustering. a procedure for calculating maximum likelihood estimations (mle's) and the most likely cluster sequence is also presented.","10.3150/08-bej159","2009-06-11","","['zhengxiao wu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"98",906.2855,"a three-parameter binomial approximation","math.pr math.st stat.th","we approximate the distribution of the sum of independent but not necessarily identically distributed bernoulli random variables using a shifted binomial distribution where the three parameters (the number of trials, the probability of success, and the shift amount) are chosen to match up the first three moments of the two distributions. we give a bound on the approximation error in terms of the total variation metric using stein's method. a numerical study is discussed that shows shifted binomial approximations typically are more accurate than poisson or standard binomial approximations. the application of the approximation to solving a problem arising in bayesian hierarchical modeling is also discussed.","10.1239/jap/1261670689","2009-06-16","","['vydas čekanavičius', 'erol a. peköz', 'adrian röllin', 'michael shwartz']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"99",907.2337,"sparsistent estimation of time-varying discrete markov random fields","stat.ml","network models have been popular for modeling and representing complex relationships and dependencies between observed variables. when data comes from a dynamic stochastic process, a single static network model cannot adequately capture transient dependencies, such as, gene regulatory dependencies throughout a developmental cycle of an organism. kolar et al (2010b) proposed a method based on kernel-smoothing l1-penalized logistic regression for estimating time-varying networks from nodal observations collected from a time-series of observational data. in this paper, we establish conditions under which the proposed method consistently recovers the structure of a time-varying network. this work complements previous empirical findings by providing sound theoretical guarantees for the proposed estimation procedure. for completeness, we include numerical simulations in the paper.","","2009-07-14","2013-04-02","['mladen kolar', 'eric p. xing']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"100",907.3355,"a network-based approach for surveillance of occupational health   exposures","stat.me","in the context of surveillance of health problems, the research carried out by the french national occupational disease surveillance and prevention network (r\'eseau national de vigilance et de pr\'evention des pathologies professionnelles, rnv3p) aims to develop, among other approaches, methods of surveillance, statistical analysis and modeling in order to study the structure and change over time of relationships between disease and exposure, and to detect emerging disease-exposure associations. in this perspective, this paper aims to present the concept of the ""exposome"" and to explain on what bases it is constructed. the exposome is defined as a network of relationships between occupational health problems that have in common one or several elements of occupational exposure (exposures, occupation and/or activity sector). the paper also aims to outline its potential for the study and programmed surveillance of composite disease-occupational exposure associations. we illustrate this approach by applying it to a sample from the rnv3p data, taking malignant tumours and focusing on the subgroup of non-hodgkin lymphomas.","","2009-07-20","","['laurie faisandier', 'vincent bonneterre', 'régis de gaudemaris', 'dominique j bicout']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"101",907.4903,"random effects compound poisson model to represent data with extra zeros","stat.ap","this paper describes a compound poisson-based random effects structure for modeling zero-inflated data. data with large proportion of zeros are found in many fields of applied statistics, for example in ecology when trying to model and predict species counts (discrete data) or abundance distributions (continuous data). standard methods for modeling such data include mixture and two-part conditional models. conversely to these methods, the stochastic models proposed here behave coherently with regards to a change of scale, since they mimic the harvesting of a marked poisson process in the modeling steps. random effects are used to account for inhomogeneity. in this paper, model design and inference both rely on conditional thinking to understand the links between various layers of quantities : parameters, latent variables including random effects and zero-inflated observations. the potential of these parsimonious hierarchical models for zero-inflated data is exemplified using two marine macroinvertebrate abundance datasets from a large scale scientific bottom-trawl survey. the em algorithm with a monte carlo step based on importance sampling is checked for this model structure on a simulated dataset : it proves to work well for parameter estimation but parameter values matter when re-assessing the actual coverage level of the confidence regions far from the asymptotic conditions.","","2009-07-28","","['marie-pierre etienne', 'eric parent', 'benoit hugues', 'bernier jacques']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"102",908.0145,"empirical assessment of the impact of highway design exceptions on the   frequency and severity of vehicle accidents","stat.ap stat.me","compliance to standardized highway design criteria is considered essential to ensure the roadway safety. however, for a variety of reasons, situations arise where exceptions to standard-design criteria are requested and accepted after review. this research explores the impact that design exceptions have on the accident severity and accident frequency in indiana. data on accidents at roadway sites with and without design exceptions are used to estimate appropriate statistical models for the frequency and severity accidents at these sites using some of the most recent statistical advances with mixing distributions. the results of the modeling process show that presence of approved design exceptions has not had a statistically significant effect on the average frequency or severity of accidents -- suggesting that current procedures for granting design exceptions have been sufficiently rigorous to avoid adverse safety impacts.","","2009-08-02","","['nataliya v. malyshkina', 'fred l. mannering']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"103",908.0618,"functional partial linear model","stat.me","when predicting scalar responses in the situation where the explanatory variables are functions, it is sometimes the case that some functional variables are related to responses linearly while other variables have more complicated relationships with the responses. in this paper, we propose a new semi-parametric model to take advantage of both parametric and nonparametric functional modeling. asymptotic properties of the proposed estimators are established and finite sample behavior is investigated through a small simulation experiment.","","2009-08-05","2012-11-27","['heng lian']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"104",908.1258,"discrete temporal models of social networks","stat.ml stat.me","we propose a family of statistical models for social network evolution over time, which represents an extension of exponential random graph models (ergms). many of the methods for ergms are readily adapted for these models, including maximum likelihood estimation algorithms. we discuss models of this type and their properties, and give examples, as well as a demonstration of their use for hypothesis testing and classification. we believe our temporal erg models represent a useful new framework for modeling time-evolving social networks, and rewiring networks from other domains such as gene regulation circuitry, and communication networks.","","2009-08-09","","['steve hanneke', 'wenjie fu', 'eric xing']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"105",908.1882,"asymptotics for posterior hazards","math.st stat.th","an important issue in survival analysis is the investigation and the modeling of hazard rates. within a bayesian nonparametric framework, a natural and popular approach is to model hazard rates as kernel mixtures with respect to a completely random measure. in this paper we provide a comprehensive analysis of the asymptotic behavior of such models. we investigate consistency of the posterior distribution and derive fixed sample size central limit theorems for both linear and quadratic functionals of the posterior hazard rate. the general results are then specialized to various specific kernels and mixing measures yielding consistency under minimal conditions and neat central limit theorems for the distribution of functionals.","10.1214/08-aos631","2009-08-13","","['pierpaolo de blasi', 'giovanni peccati', 'igor prünster']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"106",908.2862,"sensitivity of inferences in forensic genetics to assumptions about   founding genes","stat.ap","many forensic genetics problems can be handled using structured systems of discrete variables, for which bayesian networks offer an appealing practical modeling framework, and allow inferences to be computed by probability propagation methods. however, when standard assumptions are violated--for example, when allele frequencies are unknown, there is identity by descent or the population is heterogeneous--dependence is generated among founding genes, that makes exact calculation of conditional probabilities by propagation methods less straightforward. here we illustrate different methodologies for assessing sensitivity to assumptions about founders in forensic genetics problems. these include constrained steepest descent, linear fractional programming and representing dependence by structure. we illustrate these methods on several forensic genetics examples involving criminal identification, simple and complex disputed paternity and dna mixtures.","10.1214/09-aoas235","2009-08-20","","['peter j. green', 'julia mortera']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"107",909.1123,"shrinkage tuning parameter selection in precision matrices estimation","stat.me","recent literature provides many computational and modeling approaches for covariance matrices estimation in a penalized gaussian graphical models but relatively little study has been carried out on the choice of the tuning parameter. this paper tries to fill this gap by focusing on the problem of shrinkage parameter selection when estimating sparse precision matrices using the penalized likelihood approach. previous approaches typically used k-fold cross-validation in this regard. in this paper, we first derived the generalized approximate cross-validation for tuning parameter selection which is not only a more computationally efficient alternative, but also achieves smaller error rate for model fitting compared to leave-one-out cross-validation. for consistency in the selection of nonzero entries in the precision matrix, we employ a bayesian information criterion which provably can identify the nonzero conditional correlations in the gaussian model. our simulations demonstrate the general superiority of the two proposed selectors in comparison with leave-one-out cross-validation, ten-fold cross-validation and akaike information criterion.","","2009-09-06","","['heng lian']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"108",909.4821,"hierarchical subspace models for contingency tables","math.st stat.th","for statistical analysis of multiway contingency tables we propose modeling interaction terms in each maximal compact component of a hierarchical model. by this approach we can search for parsimonious models with smaller degrees of freedom than the usual hierarchical model, while preserving conditional independence structures in the hierarchical model. we discuss estimation and exacts tests of the proposed model and illustrate the advantage of the proposed modeling with some data sets.","10.1016/j.jmva.2011.06.003","2009-09-25","2011-06-09","['hisayuki hara', 'tomonari sei', 'akimichi takemura']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"109",910.0745,"estimating the null distribution for conditional inference and   genome-scale screening","stat.me math.st stat.th","in a novel approach to the multiple testing problem, efron (2004; 2007) formulated estimators of the distribution of test statistics or nominal p-values under a null distribution suitable for modeling the data of thousands of unaffected genes, non-associated single-nucleotide polymorphisms, or other biological features. estimators of the null distribution can improve not only the empirical bayes procedure for which it was originally intended, but also many other multiple comparison procedures. such estimators serve as the groundwork for the proposed multiple comparison procedure based on a recent frequentist method of minimizing posterior expected loss, exemplified with a non-additive loss function designed for genomic screening rather than for validation.   the merit of estimating the null distribution is examined from the vantage point of conditional inference in the remainder of the paper. in a simulation study of genome-scale multiple testing, conditioning the observed confidence level on the estimated null distribution as an approximate ancillary statistic markedly improved conditional inference. to enable researchers to determine whether to rely on a particular estimated null distribution for inference or decision making, an information-theoretic score is provided that quantifies the benefit of conditioning. as the sum of the degree of ancillarity and the degree of inferential relevance, the score reflects the balance conditioning would strike between the two conflicting terms.   applications to gene expression microarray data illustrate the methods introduced.","10.1111/j.1541-0420.2010.01491.x","2009-10-05","","['david r. bickel']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"110",910.1122,"a selective overview of variable selection in high dimensional feature   space (invited review article)","math.st stat.th","high dimensional statistical problems arise from diverse fields of scientific research and technological development. variable selection plays a pivotal role in contemporary statistical learning and scientific discoveries. the traditional idea of best subset selection methods, which can be regarded as a specific form of penalized likelihood, is computationally too expensive for many modern statistical applications. other forms of penalized likelihood methods have been successfully developed over the last decade to cope with high dimensionality. they have been widely applied for simultaneously selecting important variables and estimating their effects in high dimensional statistical inference. in this article, we present a brief account of the recent developments of theory, methods, and implementations for high dimensional variable selection. what limits of the dimensionality such methods can handle, what the role of penalty functions is, and what the statistical properties are rapidly drive the advances of the field. the properties of non-concave penalized likelihood and its roles in high dimensional statistical modeling are emphasized. we also review some recent advances in ultra-high dimensional variable selection, with emphasis on independence screening and two-scale methods.","","2009-10-06","","['jianqing fan', 'jinchi lv']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"111",910.1455,"statistical modeling of the time course of tantrum anger","stat.ap","although anger is an important emotion that underlies much overt aggression at great social cost, little is known about how to quantify anger or to specify the relationship between anger and the overt behaviors that express it. this paper proposes a novel statistical model which provides both a metric for the intensity of anger and an approach to determining the quantitative relationship between anger intensity and the specific behaviors that it controls. from observed angry behaviors, we reconstruct the time course of the latent anger intensity and the linkage between anger intensity and the probability of each angry behavior. the data on which this analysis is based consist of observed tantrums had by 296 children in the madison wi area during the period 1994--1996. for each tantrum, eight angry behaviors were recorded as occurring or not within each consecutive 30-second unit. so, the data can be characterized as a multivariate, binary, longitudinal (mbl) dataset with a latent variable (anger intensity) involved. data such as these are common in biomedical, psychological and other areas of the medical and social sciences. thus, the proposed modeling approach has broad applications.","10.1214/09-aoas242","2009-10-08","","['peihua qiu', 'rong yang', 'michael potegal']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"112",910.149,"hierarchical spatial models for predicting tree species assemblages   across large domains","stat.ap","spatially explicit data layers of tree species assemblages, referred to as forest types or forest type groups, are a key component in large-scale assessments of forest sustainability, biodiversity, timber biomass, carbon sinks and forest health monitoring. this paper explores the utility of coupling georeferenced national forest inventory (nfi) data with readily available and spatially complete environmental predictor variables through spatially-varying multinomial logistic regression models to predict forest type groups across large forested landscapes. these models exploit underlying spatial associations within the nfi plot array and the spatially-varying impact of predictor variables to improve the accuracy of forest type group predictions. the richness of these models incurs onerous computational burdens and we discuss dimension reducing spatial processes that retain the richness in modeling. we illustrate using nfi data from michigan, usa, where we provide a comprehensive analysis of this large study area and demonstrate improved prediction with associated measures of uncertainty.","10.1214/09-aoas250","2009-10-08","","['andrew o. finley', 'sudipto banerjee', 'ronald e. mcroberts']",1,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"113",910.3752,"the essential role of pair matching in cluster-randomized experiments,   with application to the mexican universal health insurance evaluation","stat.me","a basic feature of many field experiments is that investigators are only able to randomize clusters of individuals--such as households, communities, firms, medical practices, schools or classrooms--even when the individual is the unit of interest. to recoup the resulting efficiency loss, some studies pair similar clusters and randomize treatment within pairs. however, many other studies avoid pairing, in part because of claims in the literature, echoed by clinical trials standards organizations, that this matched-pair, cluster-randomization design has serious problems. we argue that all such claims are unfounded. we also prove that the estimator recommended for this design in the literature is unbiased only in situations when matching is unnecessary; its standard error is also invalid. to overcome this problem without modeling assumptions, we develop a simple design-based estimator with much improved statistical properties. we also propose a model-based approach that includes some of the benefits of our design-based estimator as well as the estimator in the literature. our methods also address individual-level noncompliance, which is common in applications but not allowed for in most existing methods. we show that from the perspective of bias, efficiency, power, robustness or research costs, and in large or small samples, pairing should be used in cluster-randomized experiments whenever feasible; failing to do so is equivalent to discarding a considerable fraction of one's data. we develop these techniques in the context of a randomized evaluation we are conducting of the mexican universal health insurance program.","10.1214/08-sts274","2009-10-20","","['kosuke imai', 'gary king', 'clayton nall']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"114",911.1015,"extreme-value copulas","math.st stat.th","being the limits of copulas of componentwise maxima in independent random samples, extreme-value copulas can be considered to provide appropriate models for the dependence structure between rare events. extreme-value copulas not only arise naturally in the domain of extreme-value theory, they can also be a convenient choice to model general positive dependence structures. the aim of this survey is to present the reader with the state-of-the-art in dependence modeling via extreme-value copulas. both probabilistic and statistical issues are reviewed, in a nonparametric as well as a parametric context.","","2009-11-05","2009-12-07","['gordon gudendorf', 'johan segers']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"115",911.1189,"global sensitivity analysis for models with spatially dependent outputs","stat.co math.st stat.ap stat.th","the global sensitivity analysis of a complex numerical model often calls for the estimation of variance-based importance measures, named sobol' indices. metamodel-based techniques have been developed in order to replace the cpu time-expensive computer code with an inexpensive mathematical function, which predicts the computer code output. the common metamodel-based sensitivity analysis methods are well-suited for computer codes with scalar outputs. however, in the environmental domain, as in many areas of application, the numerical model outputs are often spatial maps, which may also vary with time. in this paper, we introduce an innovative method to obtain a spatial map of sobol' indices with a minimal number of numerical model computations. it is based upon the functional decomposition of the spatial output onto a wavelet basis and the metamodeling of the wavelet coefficients by the gaussian process. an analytical example is presented to clarify the various steps of our methodology. this technique is then applied to a real hydrogeological case: for each model input variable, a spatial map of sobol' indices is thus obtained.","","2009-11-06","2010-09-23","['amandine marrel', 'bertrand iooss', 'michel jullien', 'beatrice laurent', 'elena volkova']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE
"116",911.2634,"local statistical modeling by cluster-weighted","stat.me stat.co","we investigate statistical properties of cluster-weighted modeling, which is a framework for supervised learning originally developed in order to recreate a digital violin with traditional inputs and realistic sound. the analysis is carried out in comparison with finite mixtures of regression models. based on some geometrical arguments, we highlight that cluster-weightedmodeling provides a quite general framework for local statistical modeling. theoretical results are illustrated on the ground of some numerical simulations.","","2009-11-13","2011-06-15","['salvatore ingrassia', 'simona c. minotti', 'giorgio vittadini']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"117",911.3501,"quantile regression in partially linear varying coefficient models","math.st stat.th","semiparametric models are often considered for analyzing longitudinal data for a good balance between flexibility and parsimony. in this paper, we study a class of marginal partially linear quantile models with possibly varying coefficients. the functional coefficients are estimated by basis function approximations. the estimation procedure is easy to implement, and it requires no specification of the error distributions. the asymptotic properties of the proposed estimators are established for the varying coefficients as well as for the constant coefficients. we develop rank score tests for hypotheses on the coefficients, including the hypotheses on the constancy of a subset of the varying coefficients. hypothesis testing of this type is theoretically challenging, as the dimensions of the parameter spaces under both the null and the alternative hypotheses are growing with the sample size. we assess the finite sample performance of the proposed method by monte carlo simulation studies, and demonstrate its value by the analysis of an aids data set, where the modeling of quantiles provides more comprehensive information than the usual least squares approach.","10.1214/09-aos695","2009-11-18","","['huixia judy wang', 'zhongyi zhu', 'jianhui zhou']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"118",911.546,"an iterative algorithm for fitting nonconvex penalized generalized   linear models with grouped predictors","stat.ml stat.me","high-dimensional data pose challenges in statistical learning and modeling. sometimes the predictors can be naturally grouped where pursuing the between-group sparsity is desired. collinearity may occur in real-world high-dimensional applications where the popular $l_1$ technique suffers from both selection inconsistency and prediction inaccuracy. moreover, the problems of interest often go beyond gaussian models. to meet these challenges, nonconvex penalized generalized linear models with grouped predictors are investigated and a simple-to-implement algorithm is proposed for computation. a rigorous theoretical result guarantees its convergence and provides tight preliminary scaling. this framework allows for grouped predictors and nonconvex penalties, including the discrete $l_0$ and the `$l_0+l_2$' type penalties. penalty design and parameter tuning for nonconvex penalties are examined. applications of super-resolution spectrum estimation in signal processing and cancer classification with joint gene selection in bioinformatics show the performance improvement by nonconvex penalized estimation.","","2009-11-29","2011-11-10","['yiyuan she']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"119",912.0171,"under-determined reverberant audio source separation using a full-rank   spatial covariance model","stat.ml","this article addresses the modeling of reverberant recording environments in the context of under-determined convolutive blind source separation. we model the contribution of each source to all mixture channels in the time-frequency domain as a zero-mean gaussian random variable whose covariance encodes the spatial characteristics of the source. we then consider four specific covariance models, including a full-rank unconstrained model. we derive a family of iterative expectationmaximization (em) algorithms to estimate the parameters of each model and propose suitable procedures to initialize the parameters and to align the order of the estimated sources across all frequency bins based on their estimated directions of arrival (doa). experimental results over reverberant synthetic mixtures and live recordings of speech data show the effectiveness of the proposed approach.","","2009-12-01","2009-12-14","['ngoc duong', 'emmanuel vincent', 'remi gribonval']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"120",912.2412,"modeling sparse connectivity between underlying brain sources for   eeg/meg","stat.me stat.ap stat.ml","we propose a novel technique to assess functional brain connectivity in eeg/meg signals. our method, called sparsely-connected sources analysis (scsa), can overcome the problem of volume conduction by modeling neural data innovatively with the following ingredients: (a) the eeg is assumed to be a linear mixture of correlated sources following a multivariate autoregressive (mvar) model, (b) the demixing is estimated jointly with the source mvar parameters, (c) overfitting is avoided by using the group lasso penalty. this approach allows to extract the appropriate level cross-talk between the extracted sources and in this manner we obtain a sparse data-driven model of functional connectivity. we demonstrate the usefulness of scsa with simulated data, and compare to a number of existing algorithms with excellent results.","10.1109/tbme.2010.2046325","2009-12-12","","['stefan haufe', 'ryota tomioka', 'guido nolte', 'klaus-robert mueller', 'motoaki kawanabe']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE
"121",912.3516,"tails of correlation mixtures of elliptical copulas","math.st q-fin.rm q-fin.st stat.th","correlation mixtures of elliptical copulas arise when the correlation parameter is driven itself by a latent random process. for such copulas, both penultimate and asymptotic tail dependence are much larger than for ordinary elliptical copulas with the same unconditional correlation. furthermore, for gaussian and student t-copulas, tail dependence at sub-asymptotic levels is generally larger than in the limit, which can have serious consequences for estimation and evaluation of extreme risk. finally, although correlation mixtures of gaussian copulas inherit the property of asymptotic independence, at the same time they fall in the newly defined category of near asymptotic dependence. the consequences of these findings for modeling are assessed by means of a simulation study and a case study involving financial time series.","","2009-12-17","","['hans manner', 'johan segers']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"122",912.4686,"robust estimation of the scale and of the autocovariance function of   gaussian short and long-range dependent processes","math.st stat.th","a desirable property of an autocovariance estimator is to be robust to the presence of additive outliers. it is well-known that the sample autocovariance, being based on moments, does not have this property. hence, the use of an autocovariance estimator which is robust to additive outliers can be very useful for time-series modeling. in this paper, the asymptotic properties of the robust scale and autocovariance estimators proposed by rousseeuw and croux (1993) and genton and ma (2000) are established for gaussian processes, with either short-range or long-range dependence. it is shown in the short-range dependence setting that this robust estimator is asymptotically normal at the rate $\sqrt{n}$, where $n$ is the number of observations. an explicit expression of the asymptotic variance is also given and compared to the asymptotic variance of the classical autocovariance estimator. in the long-range dependence setting, the limiting distribution displays the same behavior than that of the classical autocovariance estimator, with a gaussian limit and rate $\sqrt{n}$ when the hurst parameter $h$ is less 3/4 and with a non-gaussian limit (belonging to the second wiener chaos) with rate depending on the hurst parameter when $h \in (3/4,1)$. some monte-carlo experiments are presented to illustrate our claims and the nile river data is analyzed as an application. the theoretical results and the empirical evidence strongly suggest using the robust estimators as an alternative to estimate the dependence structure of gaussian processes.","","2009-12-23","","['céline lévy-leduc', 'hélène boistard', 'eric moulines', 'murad s. taqqu', 'valderio a. reisen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"123",912.541,"a survey of statistical network models","stat.me cs.lg physics.soc-ph q-bio.mn stat.ml","networks are ubiquitous in science and have become a focal point for discussion in everyday life. formal statistical models for the analysis of network data have emerged as a major topic of interest in diverse areas of study, and most of these involve a form of graphical representation. probability models on graphs date back to 1959. along with empirical studies in social psychology and sociology from the 1960s, these early works generated an active network community and a substantial literature in the 1970s. this effort moved into the statistical literature in the late 1970s and 1980s, and the past decade has seen a burgeoning network literature in statistical physics and computer science. the growth of the world wide web and the emergence of online networking communities such as facebook, myspace, and linkedin, and a host of more specialized professional network communities has intensified interest in the study of networks and network data. our goal in this review is to provide the reader with an entry point to this burgeoning literature. we begin with an overview of the historical development of statistical network modeling and then we introduce a number of examples that have been studied in the network literature. our subsequent discussion focuses on a number of prominent static and dynamic network models and their interconnections. we emphasize formal model descriptions, and pay special attention to the interpretation of parameters and their estimation. we end with a description of some open problems and challenges for machine learning and statistics.","","2009-12-29","","['anna goldenberg', 'alice x zheng', 'stephen e fienberg', 'edoardo m airoldi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"124",1001.0597,"inference of global clusters from locally distributed data","stat.me cs.lg stat.ml","we consider the problem of analyzing the heterogeneity of clustering distributions for multiple groups of observed data, each of which is indexed by a covariate value, and inferring global clusters arising from observations aggregated over the covariate domain. we propose a novel bayesian nonparametric method reposing on the formalism of spatial modeling and a nested hierarchy of dirichlet processes. we provide an analysis of the model properties, relating and contrasting the notions of local and global clusters. we also provide an efficient inference algorithm, and demonstrate the utility of our method in several data examples, including the problem of object tracking and a global clustering analysis of functional data where the functional identity information is not available.","","2010-01-04","2011-01-21","['xuanlong nguyen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"125",1001.2685,"relaxation penalties and priors for plausible modeling of nonidentified   bias sources","stat.me","in designed experiments and surveys, known laws or design feat ures provide checks on the most relevant aspects of a model and identify the target parameters. in contrast, in most observational studies in the health and social sciences, the primary study data do not identify and may not even bound target parameters. discrepancies between target and analogous identified parameters (biases) are then of paramount concern, which forces a major shift in modeling strategies. conventional approaches are based on conditional testing of equality constraints, which correspond to implausible point-mass priors. when these constraints are not identified by available data, however, no such testing is possible. in response, implausible constraints can be relaxed into penalty functions derived from plausible prior distributions. the resulting models can be fit within familiar full or partial likelihood frameworks. the absence of identification renders all analyses part of a sensitivity analysis. in this view, results from single models are merely examples of what might be plausibly inferred. nonetheless, just one plausible inference may suffice to demonstrate inherent limitations of the data. points are illustrated with misclassified data from a study of sudden infant death syndrome. extensions to confounding, selection bias and more complex data structures are outlined.","10.1214/09-sts291","2010-01-15","","['sander greenland']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"126",1001.3209,"detection of an anomalous cluster in a network","math.st stat.th","we consider the problem of detecting whether or not, in a given sensor network, there is a cluster of sensors which exhibit an ""unusual behavior."" formally, suppose we are given a set of nodes and attach a random variable to each node. we observe a realization of this process and want to decide between the following two hypotheses: under the null, the variables are i.i.d. standard normal; under the alternative, there is a cluster of variables that are i.i.d. normal with positive mean and unit variance, while the rest are i.i.d. standard normal. we also address surveillance settings where each sensor in the network collects information over time. the resulting model is similar, now with a time series attached to each node. we again observe the process over time and want to decide between the null, where all the variables are i.i.d. standard normal, and the alternative, where there is an emerging cluster of i.i.d. normal variables with positive mean and unit variance. the growth models used to represent the emerging cluster are quite general and, in particular, include cellular automata used in modeling epidemics. in both settings, we consider classes of clusters that are quite general, for which we obtain a lower bound on their respective minimax detection rate and show that some form of scan statistic, by far the most popular method in practice, achieves that same rate to within a logarithmic factor. our results are not limited to the normal location model, but generalize to any one-parameter exponential family when the anomalous clusters are large enough.","10.1214/10-aos839","2010-01-19","2011-03-09","['ery arias-castro', 'emmanuel j. candès', 'arnaud durand']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"127",1001.4425,"robust quantile estimation and prediction for spatial processes","math.st stat.th","in this paper, we present a statistical framework for modeling conditional quantiles of spatial processes assumed to be strongly mixing in space. we establish the $l_1$ consistency and the asymptotic normality of the kernel conditional quantile estimator in the case of random fields. we also define a nonparametric spatial predictor and illustrate the methodology used with some simulations.","","2010-01-25","","['sophie dabo niang', 'baba thiam']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"128",1002.1142,"clustering and variable selection for categorical multivariate data","math.st stat.th","this article investigates unsupervised classification techniques for categorical multivariate data. the study employs multivariate multinomial mixture modeling, which is a type of model particularly applicable to multilocus genotypic data. a model selection procedure is used to simultaneously select the number of components and the relevant variables. a non-asymptotic oracle inequality is obtained, leading to the proposal of a new penalized maximum likelihood criterion. the selected model proves to be asymptotically consistent under weak assumptions on the true probability underlying the observations. the main theoretical result obtained in this study suggests a penalty function defined to within a multiplicative parameter. in practice, the data-driven calibration of the penalty function is made possible by slope heuristics. based on simulated data, this procedure is found to improve the performance of the selection procedure with respect to classical criteria such as bic and aic. the new criterion provides an answer to the question ""which criterion for which sample size?"" examples of real dataset applications are also provided.","10.1214/13-ejs844","2010-02-05","2014-03-08","['dominique bontemps', 'wilson toussile']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"129",1002.1247,"manifold-based signal recovery and parameter estimation from compressive   measurements","stat.ml math.st stat.th","a field known as compressive sensing (cs) has recently emerged to help address the growing challenges of capturing and processing high-dimensional signals and data sets. cs exploits the surprising fact that the information contained in a sparse signal can be preserved in a small number of compressive (or random) linear measurements of that signal. strong theoretical guarantees have been established on the accuracy to which sparse or near-sparse signals can be recovered from noisy compressive measurements. in this paper, we address similar questions in the context of a different modeling framework. instead of sparse models, we focus on the broad class of manifold models, which can arise in both parametric and non-parametric signal families. building upon recent results concerning the stable embeddings of manifolds within the measurement space, we establish both deterministic and probabilistic instance-optimal bounds in $\ell_2$ for manifold-based signal recovery and parameter estimation from noisy compressive measurements. in line with analogous results for sparsity-based cs, we conclude that much stronger bounds are possible in the probabilistic setting. our work supports the growing empirical evidence that manifold-based models can be used with high accuracy in compressive signal processing.","","2010-02-05","","['michael b. wakin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"130",1002.1994,"probabilistic recovery of multiple subspaces in point clouds by   geometric lp minimization","stat.ml","we assume data independently sampled from a mixture distribution on the unit ball of the d-dimensional euclidean space with k+1 components: the first component is a uniform distribution on that ball representing outliers and the other k components are uniform distributions along k d-dimensional linear subspaces restricted to that ball. we study both the simultaneous recovery of all k underlying subspaces and the recovery of the best l0 subspace (i.e., with largest number of points) by minimizing the lp-averaged distances of data points from d-dimensional subspaces of the d-dimensional space. unlike other lp minimization problems, this minimization is non-convex for all p>0 and thus requires different methods for its analysis. we show that if 0<p <= 1, then both all underlying subspaces and the best l0 subspace can be precisely recovered by lp minimization with overwhelming probability. this result extends to additive homoscedastic uniform noise around the subspaces (i.e., uniform distribution in a strip around them) and near recovery with an error proportional to the noise level. on the other hand, if k>1 and p>1, then we show that both all underlying subspaces and the best l0 subspace cannot be recovered and even nearly recovered. further relaxations are also discussed. we use the results of this paper for partially justifying recent effective algorithms for modeling data by mixtures of multiple subspaces as well as for discussing the effect of using variants of lp minimizations in ransac-type strategies for single subspace recovery.","","2010-02-09","2012-04-19","['gilad lerman', 'teng zhang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"131",1003.3984,"on mmse and map denoising under sparse representation modeling over a   unitary dictionary","cs.cv stat.ap","among the many ways to model signals, a recent approach that draws considerable attention is sparse representation modeling. in this model, the signal is assumed to be generated as a random linear combination of a few atoms from a pre-specified dictionary. in this work we analyze two bayesian denoising algorithms -- the maximum-aposteriori probability (map) and the minimum-mean-squared-error (mmse) estimators, under the assumption that the dictionary is unitary. it is well known that both these estimators lead to a scalar shrinkage on the transformed coefficients, albeit with a different response curve. in this work we start by deriving closed-form expressions for these shrinkage curves and then analyze their performance. upper bounds on the map and the mmse estimation errors are derived. we tie these to the error obtained by a so-called oracle estimator, where the support is given, establishing a worst-case gain-factor between the map/mmse estimation errors and the oracle's performance. these denoising algorithms are demonstrated on synthetic signals and on true data (images).","10.1109/tsp.2011.2151190","2010-03-21","","['javier turek', 'irad yavneh', 'matan protter', 'michael elad']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"132",1003.4944,"incorporating side information in probabilistic matrix factorization   with gaussian processes","stat.ml cs.lg","probabilistic matrix factorization (pmf) is a powerful method for modeling data associated with pairwise relationships, finding use in collaborative filtering, computational biology, and document analysis, among other areas. in many domains, there is additional information that can assist in prediction. for example, when modeling movie ratings, we might know when the rating occurred, where the user lives, or what actors appear in the movie. it is difficult, however, to incorporate this side information into the pmf model. we propose a framework for incorporating side information by coupling together multiple pmf problems via gaussian process priors. we replace scalar latent features with functions that vary over the space of side information. the gp priors on these functions require them to vary smoothly and share information. we successfully use this new method to predict the scores of professional basketball games, where side information about the venue and date of the game are relevant for the outcome.","","2010-03-25","","['ryan prescott adams', 'george e. dahl', 'iain murray']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"133",1003.5165,"new consistent and asymptotically normal estimators for random graph   mixture models","math.st stat.me stat.th","random graph mixture models are now very popular for modeling real data networks. in these setups, parameter estimation procedures usually rely on variational approximations, either combined with the expectation-maximisation (\textsc{em}) algorithm or with bayesian approaches. despite good results on synthetic data, the validity of the variational approximation is however not established. moreover, the behavior of the maximum likelihood or of the maximum a posteriori estimators approximated by these procedures is not known in these models, due to the dependency structure on the variables. in this work, we show that in many different affiliation contexts (for binary or weighted graphs), estimators based either on moment equations or on the maximization of some composite likelihood are strongly consistent and $\sqrt{n}$-convergent, where $n$ is the number of nodes. as a consequence, our result establishes that the overall structure of an affiliation model can be caught by the description of the network in terms of its number of triads (order 3 structures) and edges (order 2 structures). we illustrate the efficiency of our method on simulated data and compare its performances with other existing procedures. a data set of cross-citations among economics journals is also analyzed.","","2010-03-26","2010-12-08","['christophe ambroise', 'catherine matias']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"134",1003.5956,"unbiased offline evaluation of contextual-bandit-based news article   recommendation algorithms","cs.lg cs.ai cs.ro stat.ml","contextual bandit algorithms have become popular for online recommendation systems such as digg, yahoo! buzz, and news recommendation in general. \emph{offline} evaluation of the effectiveness of new algorithms in these applications is critical for protecting online user experiences but very challenging due to their ""partial-label"" nature. common practice is to create a simulator which simulates the online environment for the problem at hand and then run an algorithm against this simulator. however, creating simulator itself is often difficult and modeling bias is usually unavoidably introduced. in this paper, we introduce a \emph{replay} methodology for contextual bandit algorithm evaluation. different from simulator-based approaches, our method is completely data-driven and very easy to adapt to different applications. more importantly, our method can provide provably unbiased evaluations. our empirical results on a large-scale news article recommendation dataset collected from yahoo! front page conform well with our theoretical results. furthermore, comparisons between our offline replay and online bucket evaluation of several contextual bandit algorithms show accuracy and effectiveness of our offline evaluation method.","10.1145/1935826.1935878","2010-03-30","2012-03-01","['lihong li', 'wei chu', 'john langford', 'xuanhui wang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"135",1004.0209,"inference with transposable data: modeling the effects of row and column   correlations","stat.me stat.ap stat.ml","we consider the problem of large-scale inference on the row or column variables of data in the form of a matrix. often this data is transposable, meaning that both the row variables and column variables are of potential interest. an example of this scenario is detecting significant genes in microarrays when the samples or arrays may be dependent due to underlying relationships. we study the effect of both row and column correlations on commonly used test-statistics, null distributions, and multiple testing procedures, by explicitly modeling the covariances with the matrix-variate normal distribution. using this model, we give both theoretical and simulation results revealing the problems associated with using standard statistical methodology on transposable data. we solve these problems by estimating the row and column covariances simultaneously, with transposable regularized covariance models, and de-correlating or sphering the data as a pre-processing step. under reasonable assumptions, our method gives test statistics that follow the scaled theoretical null distribution and are approximately independent. simulations based on various models with structured and observed covariances from real microarray data reveal that our method offers substantial improvements in two areas: 1) increased statistical power and 2) correct estimation of false discovery rates.","","2010-04-01","","['genevera i. allen', 'robert tibshirani']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE
"136",1004.4856,"on the dynamic interplay between positive and negative affects","stat.ap","emotional disorders and psychological flourishing are the result of complex interactions between positive and negative affects that depend on external events and the subject's internal representations. based on psychological data, we mathematically model the dynamical balance between positive and negative affects as a function of the response to external positive and negative events. this modeling allows the investigation of the relative impact of two leading forms of therapy on affect balance. the model uses a delay differential equation to analytically study the complete bifurcation diagram of the system. we compare the results of the model to psychological data on a single, recurrently depressed patient that was administered the two types of therapies considered (viz., coping-focused vs. affect-focused). the model leads to the prediction that stabilization at a normal state may rely on evaluating one's emotional state through an historical ongoing emotional state rather than in a narrow present window. the simple mathematical model proposed here offers a theoretically grounded quantitative framework for investigating the temporal process of change and parameters of resilience to relapse.","","2010-04-27","2015-05-18","['jonathan touboul', 'alberto romagnoni', 'robert schwartz']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"137",1004.5178,"variance estimation using refitted cross-validation in ultrahigh   dimensional regression","stat.me math.st stat.th","variance estimation is a fundamental problem in statistical modeling. in ultrahigh dimensional linear regressions where the dimensionality is much larger than sample size, traditional variance estimation techniques are not applicable. recent advances on variable selection in ultrahigh dimensional linear regressions make this problem accessible. one of the major problems in ultrahigh dimensional regression is the high spurious correlation between the unobserved realized noise and some of the predictors. as a result, the realized noises are actually predicted when extra irrelevant variables are selected, leading to serious underestimate of the noise level. in this paper, we propose a two-stage refitted procedure via a data splitting technique, called refitted cross-validation (rcv), to attenuate the influence of irrelevant variables with high spurious correlations. our asymptotic results show that the resulting procedure performs as well as the oracle estimator, which knows in advance the mean regression function. the simulation studies lend further support to our theoretical claims. the naive two-stage estimator which fits the selected variables in the first stage and the plug-in one stage estimators using lasso and scad are also studied and compared. their performances can be improved by the proposed rcv method.","","2010-04-28","2010-12-24","['jianqing fan', 'shaojun guo', 'ning hao']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"138",1004.5265,"sparse linear identifiable multivariate modeling","stat.ml","in this paper we consider sparse and identifiable linear latent variable (factor) and linear bayesian network models for parsimonious analysis of multivariate data. we propose a computationally efficient method for joint parameter and model inference, and model comparison. it consists of a fully bayesian hierarchy for sparse models using slab and spike priors (two-component delta-function and continuous mixtures), non-gaussian latent factors and a stochastic search over the ordering of the variables. the framework, which we call slim (sparse linear identifiable multivariate modeling), is validated and bench-marked on artificial and real biological data sets. slim is closest in spirit to lingam (shimizu et al., 2006), but differs substantially in inference, bayesian network structure learning and model comparison. experimentally, slim performs equally well or better than lingam with comparable computational complexity. we attribute this mainly to the stochastic search strategy used, and to parsimony (sparsity and identifiability), which is an explicit part of the model. we propose two extensions to the basic i.i.d. linear framework: non-linear dependence on observed variables, called snim (sparse non-linear identifiable multivariate modeling) and allowing for correlations between latent variables, called cslim (correlated slim), for the temporal and/or spatial data. the source code and scripts are available from http://cogsys.imm.dtu.dk/slim/.","","2010-04-29","2011-06-23","['ricardo henao', 'ole winther']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"139",1005.0312,"conditional sampling for spectrally discrete max-stable random fields","stat.co math.pr","max-stable random fields play a central role in modeling extreme value phenomena. we obtain an explicit formula for the conditional probability in general max-linear models, which include a large class of max-stable random fields. as a consequence, we develop an algorithm for efficient and exact sampling from the conditional distributions. our method provides a computational solution to the prediction problem for spectrally discrete max-stable random fields. this work offers new tools and a new perspective to many statistical inference problems for spatial extremes, arising, for example, in meteorology, geology, and environmental applications.","","2010-05-03","2010-11-25","['yizao wang', 'stilian a. stoev']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"140",1005.3014,"on the computability of conditional probability","math.lo cs.lo math.pr math.st stat.ml stat.th","as inductive inference and machine learning methods in computer science see continued success, researchers are aiming to describe ever more complex probabilistic models and inference algorithms. it is natural to ask whether there is a universal computational procedure for probabilistic inference. we investigate the computability of conditional probability, a fundamental notion in probability theory and a cornerstone of bayesian statistics. we show that there are computable joint distributions with noncomputable conditional distributions, ruling out the prospect of general inference algorithms, even inefficient ones. specifically, we construct a pair of computable random variables in the unit interval such that the conditional distribution of the first variable given the second encodes the halting problem. nevertheless, probabilistic inference is possible in many common modeling settings, and we prove several results giving broadly applicable conditions under which conditional distributions are computable. in particular, conditional distributions become computable when measurements are corrupted by independent computable noise with a sufficiently smooth bounded density.","10.1145/3321699","2010-05-17","2019-11-16","['nathanael l. ackerman', 'cameron e. freer', 'daniel m. roy']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"141",1005.3325,"size and power properties of some tests in the birnbaum-saunders   regression model","stat.me","the birnbaum-saunders distribution has been used quite effectively to model times to failure for materials subject to fatigue and for modeling lifetime data. in this paper we obtain asymptotic expansions, up to order $n^{-1/2}$ and under a sequence of pitman alternatives, for the nonnull distribution functions of the likelihood ratio, wald, score and gradient test statistics in the birnbaum-saunders regression model. the asymptotic distributions of all four statistics are obtained for testing a subset of regression parameters and for testing the shape parameter. monte carlo simulation is presented in order to compare the finite-sample performance of these tests. we also present an empirical application.","10.1016/j.csda.2010.09.008","2010-05-18","","['artur lemonte', 'silvia ferrari']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"142",1005.4337,"global modeling and prediction of computer network traffic","cs.ni math.st stat.th","we develop a probabilistic framework for global modeling of the traffic over a computer network. this model integrates existing single-link (-flow) traffic models with the routing over the network to capture the global traffic behavior. it arises from a limit approximation of the traffic fluctuations as the time--scale and the number of users sharing the network grow. the resulting probability model is comprised of a gaussian and/or a stable, infinite variance components. they can be succinctly described and handled by certain 'space-time' random fields. the model is validated against simulated and real data. it is then applied to predict traffic fluctuations over unobserved links from a limited set of observed links. further, applications to anomaly detection and network management are briefly discussed.","","2010-05-24","","['stilian a. stoev', 'george michailidis', 'joel vaughan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"143",1005.5081,"bayesian clustering in decomposable graphs","stat.me stat.ap stat.ml","in this paper we propose a class of prior distributions on decomposable graphs, allowing for improved modeling flexibility. while existing methods solely penalize the number of edges, the proposed work empowers practitioners to control clustering, level of separation, and other features of the graph. emphasis is placed on a particular prior distribution which derives its motivation from the class of product partition models; the properties of this prior relative to existing priors is examined through theory and simulation. we then demonstrate the use of graphical models in the field of agriculture, showing how the proposed prior distribution alleviates the inflexibility of previous approaches in properly modeling the interactions between the yield of different crop varieties.","","2010-05-27","2012-05-03","['luke bornn', 'françois caron']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE
"144",1005.5483,"model selection principles in misspecified models","math.st stat.me stat.th","model selection is of fundamental importance to high dimensional modeling featured in many contemporary applications. classical principles of model selection include the kullback-leibler divergence principle and the bayesian principle, which lead to the akaike information criterion and bayesian information criterion when models are correctly specified. yet model misspecification is unavoidable when we have no knowledge of the true model or when we have the correct family of distributions but miss some true predictor. in this paper, we propose a family of semi-bayesian principles for model selection in misspecified models, which combine the strengths of the two well-known principles. we derive asymptotic expansions of the semi-bayesian principles in misspecified generalized linear models, which give the new semi-bayesian information criteria (sic). a specific form of sic admits a natural decomposition into the negative maximum quasi-log-likelihood, a penalty on model dimensionality, and a penalty on model misspecification directly. numerical studies demonstrate the advantage of the newly proposed sic methodology for model selection in both correctly specified and misspecified models.","10.1111/rssb.12023","2010-05-29","2016-05-11","['jinchi lv', 'jun s. liu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"145",1006.103,"rasch-based high-dimensionality data reduction and class prediction with   applications to microarray gene expression data","cs.ai stat.ap stat.me stat.ml","class prediction is an important application of microarray gene expression data analysis. the high-dimensionality of microarray data, where number of genes (variables) is very large compared to the number of samples (obser- vations), makes the application of many prediction techniques (e.g., logistic regression, discriminant analysis) difficult. an efficient way to solve this prob- lem is by using dimension reduction statistical techniques. increasingly used in psychology-related applications, rasch model (rm) provides an appealing framework for handling high-dimensional microarray data. in this paper, we study the potential of rm-based modeling in dimensionality reduction with binarized microarray gene expression data and investigate its prediction ac- curacy in the context of class prediction using linear discriminant analysis. two different publicly available microarray data sets are used to illustrate a general framework of the approach. performance of the proposed method is assessed by re-randomization scheme using principal component analysis (pca) as a benchmark method. our results show that rm-based dimension reduction is as effective as pca-based dimension reduction. the method is general and can be applied to the other high-dimensional data problems.","10.1016/j.eswa.2009.12.074","2010-06-05","","['andrej kastrin', 'borut peterlin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE
"146",1006.1062,"tree-structured stick breaking processes for hierarchical data","stat.me stat.ml","many data are naturally modeled by an unobserved hierarchical structure. in this paper we propose a flexible nonparametric prior over unknown data hierarchies. the approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are infinitely exchangeable. one can view our model as providing infinite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. by using a stick-breaking approach, we can apply markov chain monte carlo methods based on slice sampling to perform bayesian inference and simulate from the posterior distribution on trees. we apply our method to hierarchical clustering of images and topic modeling of text data.","","2010-06-05","","['ryan prescott adams', 'zoubin ghahramani', 'michael i. jordan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"147",1006.1328,"uncovering the riffled independence structure of rankings","cs.lg cs.ai stat.ap stat.ml","representing distributions over permutations can be a daunting task due to the fact that the number of permutations of $n$ objects scales factorially in $n$. one recent way that has been used to reduce storage complexity has been to exploit probabilistic independence, but as we argue, full independence assumptions impose strong sparsity constraints on distributions and are unsuitable for modeling rankings. we identify a novel class of independence structures, called \emph{riffled independence}, encompassing a more expressive family of distributions while retaining many of the properties necessary for performing efficient inference and reducing sample complexity. in riffled independence, one draws two permutations independently, then performs the \emph{riffle shuffle}, common in card games, to combine the two permutations to form a single permutation. within the context of ranking, riffled independence corresponds to ranking disjoint sets of objects independently, then interleaving those rankings. in this paper, we provide a formal introduction to riffled independence and present algorithms for using riffled independence within fourier-theoretic frameworks which have been explored by a number of recent papers. additionally, we propose an automated method for discovering sets of items which are riffle independent from a training set of rankings. we show that our clustering-like algorithms can be used to discover meaningful latent coalitions from real preference ranking datasets and to learn the structure of hierarchically decomposable models based on riffled independence.","","2010-06-07","","['jonathan huang', 'carlos guestrin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"148",1006.1346,"c-hilasso: a collaborative hierarchical sparse modeling framework","stat.ml cs.cv","sparse modeling is a powerful framework for data analysis and processing. traditionally, encoding in this framework is performed by solving an l1-regularized linear regression problem, commonly referred to as lasso or basis pursuit. in this work we combine the sparsity-inducing property of the lasso model at the individual feature level, with the block-sparsity property of the group lasso model, where sparse groups of features are jointly encoded, obtaining a sparsity pattern hierarchically structured. this results in the hierarchical lasso (hilasso), which shows important practical modeling advantages. we then extend this approach to the collaborative case, where a set of simultaneously coded signals share the same sparsity pattern at the higher (group) level, but not necessarily at the lower (inside the group) level, obtaining the collaborative hilasso model (c-hilasso). such signals then share the same active groups, or classes, but not necessarily the same active set. this model is very well suited for applications such as source identification and separation. an efficient optimization procedure, which guarantees convergence to the global optimum, is developed for these new models. the underlying presentation of the new framework and optimization approach is complemented with experimental examples and theoretical results regarding recovery guarantees for the proposed models.","10.1109/tsp.2011.2157912","2010-06-07","2011-03-04","['pablo sprechmann', 'ignacio ramírez', 'guillermo sapiro', 'yonina eldar']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"149",1006.23,"a group model for stable multi-subject ica on fmri datasets","stat.ap stat.me","spatial independent component analysis (ica) is an increasingly used data-driven method to analyze functional magnetic resonance imaging (fmri) data. to date, it has been used to extract sets of mutually correlated brain regions without prior information on the time course of these regions. some of these sets of regions, interpreted as functional networks, have recently been used to provide markers of brain diseases and open the road to paradigm-free population comparisons. such group studies raise the question of modeling subject variability within ica: how can the patterns representative of a group be modeled and estimated via ica for reliable inter-group comparisons? in this paper, we propose a hierarchical model for patterns in multi-subject fmri datasets, akin to mixed-effect group models used in linear-model-based analysis. we introduce an estimation procedure, canica (canonical ica), based on i) probabilistic dimension reduction of the individual data, ii) canonical correlation analysis to identify a data subspace common to the group iii) ica-based pattern extraction. in addition, we introduce a procedure based on cross-validation to quantify the stability of ica patterns at the level of the group. we compare our method with state-of-the-art multi-subject fmri ica methods and show that the features extracted using our procedure are more reproducible at the group level on two datasets of 12 healthy controls: a resting-state and a functional localizer study.","10.1016/j.neuroimage.2010.02.010","2010-06-11","","['g. varoquaux', 's. sadaghiani', 'p. pinel', 'a. kleinschmidt', 'j. b. poline', 'b. thirion']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"150",1006.2592,"outlier detection using nonconvex penalized regression","stat.me cs.lg stat.co","this paper studies the outlier detection problem from the point of view of penalized regressions. our regression model adds one mean shift parameter for each of the $n$ data points. we then apply a regularization favoring a sparse vector of mean shift parameters. the usual $l_1$ penalty yields a convex criterion, but we find that it fails to deliver a robust estimator. the $l_1$ penalty corresponds to soft thresholding. we introduce a thresholding (denoted by $\theta$) based iterative procedure for outlier detection ($\theta$-ipod). a version based on hard thresholding correctly identifies outliers on some hard test problems. we find that $\theta$-ipod is much faster than iteratively reweighted least squares for large data because each iteration costs at most $o(np)$ (and sometimes much less) avoiding an $o(np^2)$ least squares estimate. we describe the connection between $\theta$-ipod and $m$-estimators. our proposed method has one tuning parameter with which to both identify outliers and estimate regression coefficients. a data-dependent choice can be made based on bic. the tuned $\theta$-ipod shows outstanding performance in identifying outliers in various situations in comparison to other existing approaches. this methodology extends to high-dimensional modeling with $p\gg n$, if both the coefficient vector and the outlier pattern are sparse.","","2010-06-13","2011-10-16","['yiyuan she', 'art b. owen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"151",1006.2871,"group variable selection via a hierarchical lasso and its oracle   property","stat.me","in many engineering and scientific applications, prediction variables are grouped, for example, in biological applications where assayed genes or proteins can be grouped by biological roles or biological pathways. common statistical analysis methods such as anova, factor analysis, and functional modeling with basis sets also exhibit natural variable groupings. existing successful group variable selection methods such as antoniadis and fan (2001), yuan and lin (2006) and zhao, rocha and yu (2009) have the limitation of selecting variables in an ""all-in-all-out"" fashion, i.e., when one variable in a group is selected, all other variables in the same group are also selected. in many real problems, however, we may want to keep the flexibility of selecting variables within a group, such as in gene-set selection. in this paper, we develop a new group variable selection method that not only removes unimportant groups effectively, but also keeps the flexibility of selecting variables within a group. we also show that the new method offers the potential for achieving the theoretical ""oracle"" property as in fan and li (2001) and fan and peng (2004).","","2010-06-14","2010-08-09","['nengfeng zhou', 'ji zhu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"152",1006.364,"gaussian mixture modeling with gaussian process latent variable models","stat.ml","density modeling is notoriously difficult for high dimensional data. one approach to the problem is to search for a lower dimensional manifold which captures the main characteristics of the data. recently, the gaussian process latent variable model (gplvm) has successfully been used to find low dimensional manifolds in a variety of complex data. the gplvm consists of a set of points in a low dimensional latent space, and a stochastic map to the observed space. we show how it can be interpreted as a density model in the observed space. however, the gplvm is not trained as a density model and therefore yields bad density estimates. we propose a new training strategy and obtain improved generalisation performance and better density estimates in comparative evaluations on several benchmark data sets.","","2010-06-18","2010-07-13","['hannes nickisch', 'carl edward rasmussen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"153",1006.3718,"g1-renewal process as repairable system model","stat.me","this paper considers a point process model with a monotonically decreasing or increasing rocof and the underlying distributions from the location-scale family, known as the geometric process (lam, 1988). in terms of repairable system reliability analysis, the process is capable of modeling various restoration types including ""better-than-new"", i.e., the one not covered by the popular g-renewal model (kijima & sumita, 1986). the distinctive property of the process is that the times between successive events are obtained from the underlying distributions as the scale parameter of each is monotonically decreasing or increasing. the paper discusses properties and maximum likelihood estimation of the model for the case of the exponential and weibull underlying distributions.","","2010-06-18","2010-10-26","['mark kaminskiy', 'vasiliy krivtsov']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"154",1006.3764,"using integrated nested laplace approximation for modeling spatial   healthcare utilization","stat.ap stat.me","in recent years, spatial and spatio-temporal modeling have become an important area of research in many fields (epidemiology, environmental studies, disease mapping). in this work we propose different spatial models to study hospital recruitment, including some potentially explicative variables. interest is on the distribution per geographical unit of the ratio between the number of patients living in this geographical unit and the population in the same unit. models considered are within the framework of bayesian latent gaussian models. our response variable is assumed to follow a binomial distribution, with logit link, whose parameters are the population in the geographical unit and the corresponding relative risk. the structured additive predictor accounts for effects of various covariates in an additive way, including smoothing functions of the covariates (for example spatial effect), linear effect of covariates. to approximate posterior marginals, which not available in closed form, we use integrated nested laplace approximations (inla), recently proposed for approximate bayesian inference in latent gaussian models. inla has the advantage of giving very accurate approximations and being faster than mcmc methods when the number of parameters does not exceed 6 (as it is in our case). model comparisons are assessed using dic criterion.","","2010-06-18","","['erik a. sauleau', 'valentina mameli', 'monica musio']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"155",1006.4432,"a statistical social network model for consumption data in food webs","stat.me q-bio.pe stat.ap","we adapt existing statistical modeling techniques for social networks to study consumption data observed in trophic food webs. these data describe the feeding volume (non-negative) among organisms grouped into nodes, called trophic species, that form the food web. model complexity arises due to the extensive amount of zeros in the data, as each node in the web is predator/prey to only a small number of other trophic species. many of the zeros are regarded as structural (non-random) in the context of feeding behavior. the presence of basal prey and top predator nodes (those who never consume and those who are never consumed, with probability 1) creates additional complexity to the statistical modeling. we develop a special statistical social network model to account for such network features. the model is applied to two empirical food webs; focus is on the web for which the population size of seals is of concern to various commercial fisheries.","10.1016/j.stamet.2013.09.001","2010-06-23","2013-09-06","['grace s. chiu', 'anton h. westveld']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"156",1006.4645,"spot: an r package for automatic and interactive tuning of optimization   algorithms by sequential parameter optimization","cs.ne cs.ai math.oc stat.ap","the sequential parameter optimization (spot) package for r is a toolbox for tuning and understanding simulation and optimization algorithms. model-based investigations are common approaches in simulation and optimization. sequential parameter optimization has been developed, because there is a strong need for sound statistical analysis of simulation and optimization algorithms. spot includes methods for tuning based on classical regression and analysis of variance techniques; tree-based models such as cart and random forest; gaussian process models (kriging), and combinations of different meta-modeling approaches. this article exemplifies how spot can be used for automatic and interactive tuning.","","2010-06-23","","['thomas bartz-beielstein']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"157",1007.323,"exponential random graph modeling for complex brain networks","stat.ap q-bio.nc q-bio.qm stat.me","exponential random graph models (ergms), also known as p* models, have been utilized extensively in the social science literature to study complex networks and how their global structure depends on underlying structural components. however, the literature on their use in biological networks (especially brain networks) has remained sparse. descriptive models based on a specific feature of the graph (clustering coefficient, degree distribution, etc.) have dominated connectivity research in neuroscience. corresponding generative models have been developed to reproduce one of these features. however, the complexity inherent in whole-brain network data necessitates the development and use of tools that allow the systematic exploration of several features simultaneously and how they interact to form the global network architecture. ergms provide a statistically principled approach to the assessment of how a set of interacting local brain network features gives rise to the global structure. we illustrate the utility of ergms for modeling, analyzing, and simulating complex whole-brain networks with network data from normal subjects. we also provide a foundation for the selection of important local features through the implementation and assessment of three selection approaches: a traditional p-value based backward selection approach, an information criterion approach (aic), and a graphical goodness of fit (gof) approach. the graphical gof approach serves as the best method given the scientific interest in being able to capture and reproduce the structure of fitted brain networks.","10.1371/journal.pone.0020039","2010-07-19","2011-06-01","['sean l. simpson', 'satoru hayasaka', 'paul j. laurienti']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"158",1008.129,"latent variable graphical model selection via convex optimization","math.st math.oc stat.th","suppose we observe samples of a subset of a collection of random variables. no additional information is provided about the number of latent variables, nor of the relationship between the latent and observed variables. is it possible to discover the number of latent components, and to learn a statistical model over the entire collection of variables? we address this question in the setting in which the latent and observed variables are jointly gaussian, with the conditional statistics of the observed variables conditioned on the latent variables being specified by a graphical model. as a first step we give natural conditions under which such latent-variable gaussian graphical models are identifiable given marginal statistics of only the observed variables. essentially these conditions require that the conditional graphical model among the observed variables is sparse, while the effect of the latent variables is ""spread out"" over most of the observed variables. next we propose a tractable convex program based on regularized maximum-likelihood for model selection in this latent-variable setting; the regularizer uses both the $\ell_1$ norm and the nuclear norm. our modeling framework can be viewed as a combination of dimensionality reduction (to identify latent variables) and graphical modeling (to capture remaining statistical structure not attributable to the latent variables), and it consistently estimates both the number of latent components and the conditional graphical model structure among the observed variables. these results are applicable in the high-dimensional setting in which the number of latent/observed variables grows with the number of samples of the observed variables. the geometric properties of the algebraic varieties of sparse matrices and of low-rank matrices play an important role in our analysis.","10.1214/11-aos949","2010-08-06","2012-11-02","['venkat chandrasekaran', 'pablo a. parrilo', 'alan s. willsky']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"159",1008.2218,"combining spatial information sources while accounting for systematic   errors in proxies","stat.me stat.ap stat.co","environmental research increasingly uses high-dimensional remote sensing and numerical model output to help fill space-time gaps between traditional observations. such output is often a noisy proxy for the process of interest. thus one needs to separate and assess the signal and noise (often called discrepancy) in the proxy given complicated spatio-temporal dependencies. here i extend a popular two-likelihood hierarchical model using a more flexible representation for the discrepancy. i employ the little-used markov random field approximation to a thin plate spline, which can capture small-scale discrepancy in a computationally efficient manner while better modeling smooth processes than standard conditional auto-regressive models. the increased flexibility reduces identifiability, but the lack of identifiability is inherent in the scientific context. i model particulate matter air pollution using satellite aerosol and atmospheric model output proxies. the estimated discrepancies occur at a variety of spatial scales, with small-scale discrepancy particularly important. the examples indicate little predictive improvement over modeling the observations alone. similarly, in simulations with an informative proxy, the presence of discrepancy and resulting identifiability issues prevent improvement in prediction. the results highlight but do not resolve the critical question of how best to use proxy information while minimizing the potential for proxy-induced error.","10.1111/j.1467-9876.2011.01035.x","2010-08-12","2011-09-13","['christopher j. paciorek']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"160",1008.4988,"sparse group restricted boltzmann machines","stat.ml","since learning is typically very slow in boltzmann machines, there is a need to restrict connections within hidden layers. however, the resulting states of hidden units exhibit statistical dependencies. based on this observation, we propose using $l_1/l_2$ regularization upon the activation possibilities of hidden units in restricted boltzmann machines to capture the loacal dependencies among hidden units. this regularization not only encourages hidden units of many groups to be inactive given observed data but also makes hidden units within a group compete with each other for modeling observed data. thus, the $l_1/l_2$ regularization on rbms yields sparsity at both the group and the hidden unit levels. we call rbms trained with the regularizer \emph{sparse group} rbms. the proposed sparse group rbms are applied to three tasks: modeling patches of natural images, modeling handwritten digits and pretaining a deep networks for a classification task. furthermore, we illustrate the regularizer can also be applied to deep boltzmann machines, which lead to sparse group deep boltzmann machines. when adapted to the mnist data set, a two-layer sparse group boltzmann machine achieves an error rate of $0.84\%$, which is, to our knowledge, the best published result on the permutation-invariant version of the mnist task.","","2010-08-29","","['heng luo', 'ruimin shen', 'cahngyong niu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"161",1008.5071,"brain covariance selection: better individual functional connectivity   models using population prior","stat.ml q-bio.nc","spontaneous brain activity, as observed in functional neuroimaging, has been shown to display reproducible structure that expresses brain architecture and carries markers of brain pathologies. an important view of modern neuroscience is that such large-scale structure of coherent activity reflects modularity properties of brain connectivity graphs. however, to date, there has been no demonstration that the limited and noisy data available in spontaneous activity observations could be used to learn full-brain probabilistic models that generalize to new data. learning such models entails two main challenges: i) modeling full brain connectivity is a difficult estimation problem that faces the curse of dimensionality and ii) variability between subjects, coupled with the variability of functional signals between experimental runs, makes the use of multiple datasets challenging. we describe subject-level brain functional connectivity structure as a multivariate gaussian process and introduce a new strategy to estimate it from group data, by imposing a common structure on the graphical model in the population. we show that individual models learned from functional magnetic resonance imaging (fmri) data using this population prior generalize better to unseen data than models based on alternative regularization schemes. to our knowledge, this is the first report of a cross-validated model of spontaneous brain activity. finally, we use the estimated graphical model to explore the large-scale characteristics of functional architecture and show for the first time that known cognitive networks appear as the integrated communities of functional connectivity graph.","","2010-08-30","2010-11-12","['gaël varoquaux', 'alexandre gramfort', 'jean baptiste poline', 'bertrand thirion']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"162",1008.5386,"mixed cumulative distribution networks","stat.ml cs.lg","directed acyclic graphs (dags) are a popular framework to express multivariate probability distributions. acyclic directed mixed graphs (admgs) are generalizations of dags that can succinctly capture much richer sets of conditional independencies, and are especially useful in modeling the effects of latent variables implicitly. unfortunately there are currently no good parameterizations of general admgs. in this paper, we apply recent work on cumulative distribution networks and copulas to propose one one general construction for admg models. we consider a simple parameter estimation approach, and report some encouraging experimental results.","","2010-08-31","","['ricardo silva', 'charles blundell', 'yee whye teh']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"163",1009.0891,"predicting sequences of progressive events times with time-dependent   covariates","stat.me math.st stat.th","this paper presents an approach to modeling progressive event-history data when the overall objective is prediction based on time-dependent covariates. this approach does not model the hazard function directly. instead, it models the process of the state indicators of the event history so that the time-dependent covariates can be incorporated and predictors of the future events easily formulated. our model can be applied to a range of real-world problems in medical and agricultural science.","","2010-09-05","","['song cai', 'james v. zidek', 'nathaniel newlands']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"164",1009.1436,"a mixed effects model for longitudinal relational and network data, with   applications to international trade and conflict","stat.me stat.ap","the focus of this paper is an approach to the modeling of longitudinal social network or relational data. such data arise from measurements on pairs of objects or actors made at regular temporal intervals, resulting in a social network for each point in time. in this article we represent the network and temporal dependencies with a random effects model, resulting in a stochastic process defined by a set of stationary covariance matrices. our approach builds upon the social relations models of warner, kenny and stoto [journal of personality and social psychology 37 (1979) 1742--1757] and gill and swartz [canad. j. statist. 29 (2001) 321--331] and allows for an intra- and inter-temporal representation of network structures. we apply the methodology to two longitudinal data sets: international trade (continuous response) and militarized interstate disputes (binary response).","10.1214/10-aoas403","2010-09-07","2011-08-17","['anton h. westveld', 'peter d. hoff']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"165",1009.2722,"learning latent tree graphical models","stat.ml cs.it math.it","we study the problem of learning a latent tree graphical model where samples are available only from a subset of variables. we propose two consistent and computationally efficient algorithms for learning minimal latent trees, that is, trees without any redundant hidden nodes. unlike many existing methods, the observed nodes (or variables) are not constrained to be leaf nodes. our first algorithm, recursive grouping, builds the latent tree recursively by identifying sibling groups using so-called information distances. one of the main contributions of this work is our second algorithm, which we refer to as clgrouping. clgrouping starts with a pre-processing procedure in which a tree over the observed variables is constructed. this global step groups the observed nodes that are likely to be close to each other in the true latent tree, thereby guiding subsequent recursive grouping (or equivalent procedures) on much smaller subsets of variables. this results in more accurate and efficient learning of latent trees. we also present regularized versions of our algorithms that learn latent tree approximations of arbitrary distributions. we compare the proposed algorithms to other methods by performing extensive numerical experiments on various latent tree graphical models such as hidden markov models and star graphs. in addition, we demonstrate the applicability of our methods on real-world datasets by modeling the dependency structure of monthly stock returns in the s&p index and of the words in the 20 newsgroups dataset.","","2010-09-14","","['myung jin choi', 'vincent y. f. tan', 'animashree anandkumar', 'alan s. willsky']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"166",1009.2898,"multivariate copula expressed by lower dimensional copulas","math.st stat.th","modeling of high order multivariate probability distribution is a difficult problem which occurs in many fields. copula approach is a good choice for this purpose, but the curse of dimensionality still remains a problem. in this paper we give a theorem which expresses a multivariate copula by using only some lower dimensional ones based on the conditional independences between the variables. in general the construction of a multivariate copula using this theorem is quite difficult, due the consistency properties which have to be fulfilled. for this purpose we introduce the sample derivated copula, and prove that the dependence between the random variables involved depends just on this copula and on the partition. by using the sample derivated copula the theorem can be successfully applied, in order to to construct a multivariate discrete copula by using some of its marginals.","","2010-09-15","","['edith kovacs', 'tamas szantai']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"167",1009.3507,"hierarchical modeling of abundance in closed population   capture-recapture models under heterogeneity","stat.ap stat.co","hierarchical modeling of abundance in space or time using closed-population mark-recapture under heterogeneity (model m$_{h}$) presents two challenges: (i) finding a flexible likelihood in which abundance appears as an explicit parameter and (ii) fitting the hierarchical model for abundance. the first challenge arises because abundance not only indexes the population size, it also determines the dimension of the capture probabilities in heterogeneity models. a common approach is to use data augmentation to include these capture probabilities directly into the likelihood and fit the model using bayesian inference via markov chain monte carlo (mcmc). two such examples of this approach are (i) explicit trans-dimensional mcmc, and (ii) superpopulation data augmentation. the superpopulation approach has the advantage of simple specification that is easily implemented in bugs and related software. however, it reparameterizes the model so that abundance is no longer included, except as a derived quantity. this is a drawback when hierarchical models for abundance, or related parameters, are desired. here, we analytically compare the two approaches and show that they are more closely related than might appear superficially. we exploit this relationship to specify the model in a way that allows us to include abundance as a parameter and that facilitates hierarchical modeling using readily available software such as bugs. we use this approach to model trends in grizzly bear abundance in yellowstone national park from 1986-1998.","","2010-09-17","2015-04-08","['matthew r. schofield', 'richard j. barker']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE
"168",1009.3516,"full open population capture-recapture models with individual covariates","stat.ap","traditional analyses of capture-recapture data are based on likelihood functions that explicitly integrate out all missing data. we use a complete data likelihood (cdl) to show how a wide range of capture-recapture models can be easily fitted using readily available software jags/bugs even when there are individual-specific time-varying covariates. the models we describe extend those that condition on first capture to include abundance parameters, or parameters related to abundance, such as population size, birth rates or lifetime. the use of a cdl means that any missing data, including uncertain individual covariates, can be included in models without the need for customized likelihood functions. this approach also facilitates modeling processes of demographic interest rather than the complexities caused by non-ignorable missing data. we illustrate using two examples, (i) open population modeling in the presence of a censored time-varying individual covariate in a full robust-design, and (ii) full open population multi-state modeling in the presence of a partially observed categorical variable.","","2010-09-17","","['matthew r. schofield', 'richard j. barker']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"169",1009.5358,"task-driven dictionary learning","stat.ml","modeling data with linear combinations of a few elements from a learned dictionary has been the focus of much recent research in machine learning, neuroscience and signal processing. for signals such as natural images that admit such sparse representations, it is now well established that these models are well suited to restoration tasks. in this context, learning the dictionary amounts to solving a large-scale matrix factorization problem, which can be done efficiently with classical optimization tools. the same approach has also been used for learning features from data for other purposes, e.g., image classification, but tuning the dictionary in a supervised way for these tasks has proven to be more difficult. in this paper, we present a general formulation for supervised dictionary learning adapted to a wide variety of tasks, and present an efficient algorithm for solving the corresponding optimization problem. experiments on handwritten digit classification, digital art identification, nonlinear inverse image problems, and compressed sensing demonstrate that our approach is effective in large-scale settings, and is well suited to supervised and semi-supervised classification, as well as regression tasks for data that admit sparse representations.","10.1109/tpami.2011.156","2010-09-27","2013-09-09","['julien mairal', 'francis bach', 'jean ponce']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"170",1009.5869,"nonparametric bayesian multiple testing for longitudinal performance   stratification","stat.ap","this paper describes a framework for flexible multiple hypothesis testing of autoregressive time series. the modeling approach is bayesian, though a blend of frequentist and bayesian reasoning is used to evaluate procedures. nonparametric characterizations of both the null and alternative hypotheses will be shown to be the key robustification step necessary to ensure reasonable type-i error performance. the methodology is applied to part of a large database containing up to 50 years of corporate performance statistics on 24,157 publicly traded american companies, where the primary goal of the analysis is to flag companies whose historical performance is significantly different from that expected due to chance.","10.1214/09-aoas252","2010-09-29","","['james g. scott']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"171",1010.0303,"likelihood inference for models with unobservables: another view","stat.me","there have been controversies among statisticians on (i) what to model and (ii) how to make inferences from models with unobservables. one such controversy concerns the difference between estimation methods for the marginal means not necessarily having a probabilistic basis and statistical models having unobservables with a probabilistic basis. another concerns likelihood-based inference for statistical models with unobservables. this needs an extended-likelihood framework, and we show how one such extension, hierarchical likelihood, allows this to be done. modeling of unobservables leads to rich classes of new probabilistic models from which likelihood-type inferences can be made naturally with hierarchical likelihood.","10.1214/09-sts277","2010-10-02","2010-10-06","['youngjo lee', 'john a. nelder']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"172",1010.0305,"inference and modeling with log-concave distributions","stat.me","log-concave distributions are an attractive choice for modeling and inference, for several reasons: the class of log-concave distributions contains most of the commonly used parametric distributions and thus is a rich and flexible nonparametric class of distributions. further, the mle exists and can be computed with readily available algorithms. thus, no tuning parameter, such as a bandwidth, is necessary for estimation. due to these attractive properties, there has been considerable recent research activity concerning the theory and applications of log-concave distributions. this article gives a review of these results.","10.1214/09-sts303","2010-10-02","","['guenther walther']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"173",1010.0306,"interval estimation for messy observational data","stat.me","we review some aspects of bayesian and frequentist interval estimation, focusing first on their relative strengths and weaknesses when used in ""clean"" or ""textbook"" contexts. we then turn attention to observational-data situations which are ""messy,"" where modeling that acknowledges the limitations of study design and data collection leads to nonidentifiability. we argue, via a series of examples, that bayesian interval estimation is an attractive way to proceed in this context even for frequentists, because it can be supplied with a diagnostic in the form of a calibration-sensitivity simulation analysis. we illustrate the basis for this approach in a series of theoretical considerations, simulations and an application to a study of silica exposure and lung cancer.","10.1214/09-sts305","2010-10-02","","['paul gustafson', 'sander greenland']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"174",1010.0427,"on the consistency of fr\'echet means in deformable models for curve and   image analysis","math.st stat.th","a new class of statistical deformable models is introduced to study high-dimensional curves or images. in addition to the standard measurement error term, these deformable models include an extra error term modeling the individual variations in intensity around a mean pattern. it is shown that an appropriate tool for statistical inference in such models is the notion of sample fr\'echet means, which leads to estimators of the deformation parameters and the mean pattern. the main contribution of this paper is to study how the behavior of these estimators depends on the number n of design points and the number j of observed curves (or images). numerical experiments are given to illustrate the finite sample performances of the procedure.","","2010-10-03","2011-08-22","['jérémie bigot', 'benjamin charlier']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"175",1010.0581,"approximation of conditional densities by smooth mixtures of regressions","math.st stat.th","this paper shows that large nonparametric classes of conditional multivariate densities can be approximated in the kullback--leibler distance by different specifications of finite mixtures of normal regressions in which normal means and variances and mixing probabilities can depend on variables in the conditioning set (covariates). these models are a special case of models known as ""mixtures of experts"" in statistics and computer science literature. flexible specifications include models in which only mixing probabilities, modeled by multinomial logit, depend on the covariates and, in the univariate case, models in which only means of the mixed normals depend flexibly on the covariates. modeling the variance of the mixed normals by flexible functions of the covariates can weaken restrictions on the class of the approximable densities. obtained results can be generalized to mixtures of general location scale densities. rates of convergence and easy to interpret bounds are also obtained for different model specifications. these approximation results can be useful for proving consistency of bayesian and maximum likelihood density estimators based on these models. the results also have interesting implications for applied researchers.","10.1214/09-aos765","2010-10-04","","['andriy norets']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"176",1010.0621,"local optimality of user choices and collaborative competitive filtering","stat.ml cs.ir cs.si stat.ap","while a user's preference is directly reflected in the interactive choice process between her and the recommender, this wealth of information was not fully exploited for learning recommender models. in particular, existing collaborative filtering (cf) approaches take into account only the binary events of user actions but totally disregard the contexts in which users' decisions are made. in this paper, we propose collaborative competitive filtering (ccf), a framework for learning user preferences by modeling the choice process in recommender systems. ccf employs a multiplicative latent factor model to characterize the dyadic utility function. but unlike cf, ccf models the user behavior of choices by encoding a local competition effect. in this way, ccf allows us to leverage dyadic data that was previously lumped together with missing data in existing cf models. we present two formulations and an efficient large scale optimization algorithm. experiments on three real-world recommendation data sets demonstrate that ccf significantly outperforms standard cf approaches in both offline and online evaluations.","","2010-10-04","2011-02-25","['shuang hong yang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"177",1010.1357,"model misspecification in peaks over threshold analysis","stat.ap","classical peaks over threshold analysis is widely used for statistical modeling of sample extremes, and can be supplemented by a model for the sizes of clusters of exceedances. under mild conditions a compound poisson process model allows the estimation of the marginal distribution of threshold exceedances and of the mean cluster size, but requires the choice of a threshold and of a run parameter, $k$, that determines how exceedances are declustered. we extend a class of estimators of the reciprocal mean cluster size, known as the extremal index, establish consistency and asymptotic normality, and use the compound poisson process to derive misspecification tests of model validity and of the choice of run parameter and threshold. simulated examples and real data on temperatures and rainfall illustrate the ideas, both for estimating the extremal index in nonstandard situations and for assessing the validity of extremal models.","10.1214/09-aoas292","2010-10-07","","['mária süveges', 'anthony c. davison']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"178",1010.143,"a latent factor model for spatial data with informative missingness","stat.ap","a large amount of data is typically collected during a periodontal exam. analyzing these data poses several challenges. several types of measurements are taken at many locations throughout the mouth. these spatially-referenced data are a mix of binary and continuous responses, making joint modeling difficult. also, most patients have missing teeth. periodontal disease is a leading cause of tooth loss, so it is likely that the number and location of missing teeth informs about the patient's periodontal health. in this paper we develop a multivariate spatial framework for these data which jointly models the binary and continuous responses as a function of a single latent spatial process representing general periodontal health. we also use the latent spatial process to model the location of missing teeth. we show using simulated and real data that exploiting spatial associations and jointly modeling the responses and locations of missing teeth mitigates the problems presented by these data.","10.1214/09-aoas278","2010-10-07","","['brian j. reich', 'dipankar bandyopadhyay']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"179",1010.1435,"estimation of constant and time-varying dynamic parameters of hiv   infection in a nonlinear differential equation model","stat.ap","modeling viral dynamics in hiv/aids studies has resulted in a deep understanding of pathogenesis of hiv infection from which novel antiviral treatment guidance and strategies have been derived. viral dynamics models based on nonlinear differential equations have been proposed and well developed over the past few decades. however, it is quite challenging to use experimental or clinical data to estimate the unknown parameters (both constant and time-varying parameters) in complex nonlinear differential equation models. therefore, investigators usually fix some parameter values, from the literature or by experience, to obtain only parameter estimates of interest from clinical or experimental data. however, when such prior information is not available, it is desirable to determine all the parameter estimates from data. in this paper we intend to combine the newly developed approaches, a multi-stage smoothing-based (mssb) method and the spline-enhanced nonlinear least squares (snls) approach, to estimate all hiv viral dynamic parameters in a nonlinear differential equation model. in particular, to the best of our knowledge, this is the first attempt to propose a comparatively thorough procedure, accounting for both efficiency and accuracy, to rigorously estimate all key kinetic parameters in a nonlinear differential equation model of hiv dynamics from clinical data. these parameters include the proliferation rate and death rate of uninfected hiv-targeted cells, the average number of virions produced by an infected cell, and the infection rate which is related to the antiviral treatment effect and is time-varying. to validate the estimation methods, we verified the identifiability of the hiv viral dynamic model and performed simulation studies.","10.1214/09-aoas290","2010-10-07","","['hua liang', 'hongyu miao', 'hulin wu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"180",1010.1437,"mixed-membership stochastic block-models for transactional networks","stat.ml cs.ai cs.si stat.ap stat.me","transactional network data can be thought of as a list of one-to-many communications(e.g., email) between nodes in a social network. most social network models convert this type of data into binary relations between pairs of nodes. we develop a latent mixed membership model capable of modeling richer forms of transactional network data, including relations between more than two nodes. the model can cluster nodes and predict transactions. the block-model nature of the model implies that groups can be characterized in very general ways. this flexible notion of group structure enables discovery of rich structure in transactional networks. estimation and inference are accomplished via a variational em algorithm. simulations indicate that the learning algorithm can recover the correct generative model. interesting structure is discovered in the enron email dataset and another dataset extracted from the reddit website. analysis of the reddit data is facilitated by a novel performance measure for comparing two soft clusterings. the new model is superior at discovering mixed membership in groups and in predicting transactions.","","2010-10-07","","['mahdi shafiei', 'hugh chipman']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE
"181",1010.1604,"downscaling extremes: a comparison of extreme value distributions in   point-source and gridded precipitation data","stat.ap","there is substantial empirical and climatological evidence that precipitation extremes have become more extreme during the twentieth century, and that this trend is likely to continue as global warming becomes more intense. however, understanding these issues is limited by a fundamental issue of spatial scaling: most evidence of past trends comes from rain gauge data, whereas trends into the future are produced by climate models, which rely on gridded aggregates. to study this further, we fit the generalized extreme value (gev) distribution to the right tail of the distribution of both rain gauge and gridded events. the results of this modeling exercise confirm that return values computed from rain gauge data are typically higher than those computed from gridded data; however, the size of the difference is somewhat surprising, with the rain gauge data exhibiting return values sometimes two or three times that of the gridded data. the main contribution of this paper is the development of a family of regression relationships between the two sets of return values that also take spatial variations into account. based on these results, we now believe it is possible to project future changes in precipitation extremes at the point-location level based on results from climate models.","10.1214/09-aoas287","2010-10-08","","['elizabeth c. mannshardt-shamseldin', 'richard l. smith', 'stephan r. sain', 'linda o. mearns', 'daniel cooley']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"182",1010.1613,"nonparametric inference procedure for percentiles of the random effects   distribution in meta-analysis","stat.ap","to investigate whether treating cancer patients with erythropoiesis-stimulating agents (esas) would increase the mortality risk, bennett et al. [journal of the american medical association 299 (2008) 914--924] conducted a meta-analysis with the data from 52 phase iii trials comparing esas with placebo or standard of care. with a standard parametric random effects modeling approach, the study concluded that esa administration was significantly associated with increased average mortality risk. in this article we present a simple nonparametric inference procedure for the distribution of the random effects. we re-analyzed the esa mortality data with the new method. our results about the center of the random effects distribution were markedly different from those reported by bennett et al. moreover, our procedure, which estimates the distribution of the random effects, as opposed to just a simple population average, suggests that the esa may be beneficial to mortality for approximately a quarter of the study populations. this new meta-analysis technique can be implemented with study-level summary statistics. in contrast to existing methods for parametric random effects models, the validity of our proposal does not require the number of studies involved to be large. from the results of an extensive numerical study, we find that the new procedure performs well even with moderate individual study sample sizes.","10.1214/09-aoas280","2010-10-08","","['rui wang', 'lu tian', 'tianxi cai', 'l. j. wei']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"183",1010.2334,"screening and metamodeling of computer experiments with functional   outputs. application to thermal-hydraulic computations","math.st stat.th","to perform uncertainty, sensitivity or optimization analysis on scalar variables calculated by a cpu time expensive computer code, a widely accepted methodology consists in first identifying the most influential uncertain inputs (by screening techniques), and then in replacing the cpu time expensive model by a cpu inexpensive mathematical function, called a metamodel. this paper extends this methodology to the functional output case, for instance when the model output variables are curves. the screening approach is based on the analysis of variance and principal component analysis of output curves. the functional metamodeling consists in a curve classification step, a dimension reduction step, then a classical metamodeling step. an industrial nuclear reactor application (dealing with uncertainties in the pressurized thermal shock analysis) illustrates all these steps.","","2010-10-12","2011-11-03","['benjamin auder', 'agnes de crecy', 'bertrand iooss', 'michel marques']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"184",1010.346,"hybrid linear modeling via local best-fit flats","cs.cv stat.ml","we present a simple and fast geometric method for modeling data by a union of affine subspaces. the method begins by forming a collection of local best-fit affine subspaces, i.e., subspaces approximating the data in local neighborhoods. the correct sizes of the local neighborhoods are determined automatically by the jones' $\beta_2$ numbers (we prove under certain geometric conditions that our method finds the optimal local neighborhoods). the collection of subspaces is further processed by a greedy selection procedure or a spectral method to generate the final model. we discuss applications to tracking-based motion segmentation and clustering of faces under different illuminating conditions. we give extensive experimental evidence demonstrating the state of the art accuracy and speed of the suggested algorithms on these problems and also on synthetic hybrid linear data as well as the mnist handwritten digits data; and we demonstrate how to use our algorithms for fast determination of the number of affine subspaces.","10.1007/s11263-012-0535-6","2010-10-17","2012-05-01","['teng zhang', 'arthur szlam', 'yi wang', 'gilad lerman']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"185",1010.3586,"a nonparametric urn-based approach to interacting failing systems with   an application to credit risk modeling","stat.ap math.st stat.th","in this paper we propose a new nonparametric approach to interacting failing systems (fs), that is systems whose probability of failure is not negligible in a fixed time horizon, a typical example being firms and financial bonds. the main purpose when studying a fs is to calculate the probability of default and the distribution of the number of failures that may occur during the observation period. a model used to study a failing system is defined default model. in particular, we present a general recursive model constructed by the means of inter- acting urns. after introducing the theoretical model and its properties we show a first application to credit risk modeling, showing how to assess the idiosyncratic probability of default of an obligor and the joint probability of failure of a set of obligors in a portfolio of risks, that are divided into reliability classes.","","2010-10-18","","['pasquale cirillo', 'jürg hüsler', 'pietro muliere']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE
"186",1010.3882,"introduction to papers on the modeling and analysis of network data","stat.ap","introduction to papers on the modeling and analysis of network data","10.1214/10-aoas346","2010-10-19","","['stephen e. fienberg']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"187",1010.467,"a bayesian method for detecting and characterizing allelic heterogeneity   and boosting signals in genome-wide association studies","stat.me q-bio.gn","the standard paradigm for the analysis of genome-wide association studies involves carrying out association tests at both typed and imputed snps. these methods will not be optimal for detecting the signal of association at snps that are not currently known or in regions where allelic heterogeneity occurs. we propose a novel association test, complementary to the snp-based approaches, that attempts to extract further signals of association by explicitly modeling and estimating both unknown snps and allelic heterogeneity at a locus. at each site we estimate the genealogy of the case-control sample by taking advantage of the hapmap haplotypes across the genome. allelic heterogeneity is modeled by allowing more than one mutation on the branches of the genealogy. our use of bayesian methods allows us to assess directly the evidence for a causative snp not well correlated with known snps and for allelic heterogeneity at each locus. using simulated data and real data from the wtccc project, we show that our method (i) produces a significant boost in signal and accurately identifies the form of the allelic heterogeneity in regions where it is known to exist, (ii) can suggest new signals that are not found by testing typed or imputed snps and (iii) can provide more accurate estimates of effect sizes in regions of association.","10.1214/09-sts311","2010-10-22","","['zhan su', 'niall cardin', 'the wellcome trust case control consortium', 'peter donnelly', 'jonathan marchini']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"188",1010.4686,"structures and assumptions: strategies to harness gene $\times$ gene and   gene $\times$ environment interactions in gwas","stat.me q-bio.gn","genome-wide association studies, in which as many as a million single nucleotide polymorphisms (snp) are measured on several thousand samples, are quickly becoming a common type of study for identifying genetic factors associated with many phenotypes. there is a strong assumption that interactions between snps or genes and interactions between genes and environmental factors substantially contribute to the genetic risk of a disease. identification of such interactions could potentially lead to increased understanding about disease mechanisms; drug $\times$ gene interactions could have profound applications for personalized medicine; strong interaction effects could be beneficial for risk prediction models. in this paper we provide an overview of different approaches to model interactions, emphasizing approaches that make specific use of the structure of genetic data, and those that make specific modeling assumptions that may (or may not) be reasonable to make. we conclude that to identify interactions it is often necessary to do some selection of snps, for example, based on prior hypothesis or marginal significance, but that to identify snps that are marginally associated with a disease it may also be useful to consider larger numbers of interactions.","10.1214/09-sts287","2010-10-22","","['charles kooperberg', 'michael leblanc', 'james y. dai', 'indika rajapakse']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"189",1010.4751,"sparse coding and dictionary learning based on the mdl principle","cs.it math.it math.st stat.th","the power of sparse signal coding with learned dictionaries has been demonstrated in a variety of applications and fields, from signal processing to statistical inference and machine learning. however, the statistical properties of these models, such as underfitting or overfitting given sets of data, are still not well characterized in the literature. this work aims at filling this gap by means of the minimum description length (mdl) principle -- a well established information-theoretic approach to statistical inference. the resulting framework derives a family of efficient sparse coding and modeling (dictionary learning) algorithms, which by virtue of the mdl principle, are completely parameter free. furthermore, such framework allows to incorporate additional prior information in the model, such as markovian dependencies, in a natural way. we demonstrate the performance of the proposed framework with results for image denoising and classification tasks.","","2010-10-22","","['ignacio ramírez', 'guillermo sapiro']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"190",1011.0626,"semi-parametric dynamic time series modelling with applications to   detecting neural dynamics","stat.ap","this paper illustrates novel methods for nonstationary time series modeling along with their applications to selected problems in neuroscience. these methods are semi-parametric in that inferences are derived by combining sequential bayesian updating with a non-parametric change-point test. as a test statistic, we propose a kullback--leibler (kl) divergence between posterior distributions arising from different sets of data. a closed form expression of this statistic is derived for exponential family models, whereas standard markov chain monte carlo output is used to approximate its value and its critical region for more general models. the behavior of one-step ahead predictive distributions under our semi-parametric framework is described analytically for a dynamic linear time series model. conditions under which our approach reduces to fully parametric state-space modeling are also illustrated. we apply our methods to estimating the functional dynamics of a wide range of neural data, including multi-channel electroencephalogram recordings, longitudinal behavioral experiments and in-vivo multiple spike trains recordings. the estimated dynamics are related to the presentation of visual stimuli, to the evaluation of a learning performance and to changes in the functional connections between neurons over a sequence of experiments.","10.1214/09-aoas275","2010-11-02","","['fabio rigat', 'jim q. smith']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"191",1011.0748,"response of double-auction markets to instantaneous selling-buying   signals with stochastic bid-ask spread","q-fin.tr stat.ap","statistical properties of order-driven double-auction markets with bid-ask spread are investigated through the dynamical quantities such as response function. we first attempt to utilize the so-called {\it madhavan-richardson-roomans model} (mrr for short) to simulate the stochastic process of the price-change in empirical data sets (say, eur/jpy or usd/jpy exchange rates) in which the bid-ask spread fluctuates in time. we find that the mrr theory apparently fails to simulate so much as the qualitative behaviour ('non-monotonic' behaviour) of the response function $r(l)$ ($l$ denotes the difference of times at which the response function is evaluated) calculated from the data. especially, we confirm that the stochastic nature of the bid-ask spread causes apparent deviations from a linear relationship between the $r(l)$ and the auto-correlation function $c(l)$, namely, $r(l) \propto -c(l)$. to make the microscopic model of double-auction markets having stochastic bid-ask spread, we use the minority game with a finite market history length and find numerically that appropriate extension of the game shows quite similar behaviour of the response function to the empirical evidence. we also reveal that the minority game modeling with the adaptive ('annealed') look-up table reproduces the non-linear relationship $r(l) \propto -f(c(l))$ ($f(x)$ stands for a non-linear function leading to '$\lambda$-shapes') more effectively than the fixed (`quenched') look-up table does.","","2010-11-02","2011-03-31","['takero ibuki', 'jun-ichi inoue']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"192",1011.081,"make research data public? -- not always so simple: a dialogue for   statisticians and science editors","stat.me cs.dl physics.data-an","putting data into the public domain is not the same thing as making those data accessible for intelligent analysis. a distinguished group of editors and experts who were already engaged in one way or another with the issues inherent in making research data public came together with statisticians to initiate a dialogue about policies and practicalities of requiring published research to be accompanied by publication of the research data. this dialogue carried beyond the broad issues of the advisability, the intellectual integrity, the scientific exigencies to the relevance of these issues to statistics as a discipline and the relevance of statistics, from inference to modeling to data exploration, to science and social science policies on these issues.","10.1214/10-sts320","2010-11-03","","['nell sedransk', 'lawrence h. cox', 'deborah nolan', 'keith soper', 'cliff spiegelman', 'linda j. young', 'katrina l. kelner', 'robert a. moffitt', 'ani thakar', 'jordan raddick', 'edward j. ungvarsky', 'richard w. carlson', 'rolf apweiler']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"193",1011.1518,"robust matrix decomposition with outliers","stat.ml cs.lg math.na","suppose a given observation matrix can be decomposed as the sum of a low-rank matrix and a sparse matrix (outliers), and the goal is to recover these individual components from the observed sum. such additive decompositions have applications in a variety of numerical problems including system identification, latent variable graphical modeling, and principal components analysis. we study conditions under which recovering such a decomposition is possible via a combination of $\ell_1$ norm and trace norm minimization. we are specifically interested in the question of how many outliers are allowed so that convex programming can still achieve accurate recovery, and we obtain stronger recovery guarantees than previous studies. moreover, we do not assume that the spatial pattern of outliers is random, which stands in contrast to related analyses under such assumptions via matrix completion.","","2010-11-05","2010-12-03","['daniel hsu', 'sham m. kakade', 'tong zhang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"194",1011.1717,"introduction to papers on the modeling and analysis of network data---ii","stat.ap","introduction to papers on the modeling and analysis of network data---ii","10.1214/10-aoas365","2010-11-08","","['stephen e. fienberg']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"195",1011.1937,"a separable model for dynamic networks","stat.me","models of dynamic networks --- networks that evolve over time --- have manifold applications. we develop a discrete-time generative model for social network evolution that inherits the richness and flexibility of the class of exponential-family random graph models. the model --- a separable temporal ergm (stergm) --- facilitates separable modeling of the tie duration distributions and the structural dynamics of tie formation. we develop likelihood-based inference for the model, and provide computational algorithms for maximum likelihood estimation. we illustrate the interpretability of the model in analyzing a longitudinal network of friendship ties within a school.","10.1111/rssb.12014","2010-11-08","2012-08-19","['pavel n. krivitsky', 'mark s. handcock']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"196",1011.2065,"a dirichlet process mixture of hidden markov models for protein   structure prediction","stat.ap","by providing new insights into the distribution of a protein's torsion angles, recent statistical models for this data have pointed the way to more efficient methods for protein structure prediction. most current approaches have concentrated on bivariate models at a single sequence position. there is, however, considerable value in simultaneously modeling angle pairs at multiple sequence positions in a protein. one area of application for such models is in structure prediction for the highly variable loop and turn regions. such modeling is difficult due to the fact that the number of known protein structures available to estimate these torsion angle distributions is typically small. furthermore, the data is ""sparse"" in that not all proteins have angle pairs at each sequence position. we propose a new semiparametric model for the joint distributions of angle pairs at multiple sequence positions. our model accommodates sparse data by leveraging known information about the behavior of protein secondary structure. we demonstrate our technique by predicting the torsion angles in a loop from the globin fold family. our results show that a template-based approach can now be successfully extended to modeling the notoriously difficult loop and turn regions.","10.1214/09-aoas296","2010-11-09","","['kristin p. lennox', 'david b. dahl', 'marina vannucci', 'ryan day', 'jerry w. tsai']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"197",1011.2077,"a flexible regression model for count data","stat.ap","poisson regression is a popular tool for modeling count data and is applied in a vast array of applications from the social to the physical sciences and beyond. real data, however, are often over- or under-dispersed and, thus, not conducive to poisson regression. we propose a regression model based on the conway--maxwell-poisson (com-poisson) distribution to address this problem. the com-poisson regression generalizes the well-known poisson and logistic regression models, and is suitable for fitting count data with a wide range of dispersion levels. with a glm approach that takes advantage of exponential family properties, we discuss model estimation, inference, diagnostics, and interpretation, and present a test for determining the need for a com-poisson regression over a standard poisson regression. we compare the com-poisson to several alternatives and illustrate its advantages and usefulness using three data sets with varying dispersion.","10.1214/09-aoas306","2010-11-09","","['kimberly f. sellers', 'galit shmueli']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"198",1011.2104,"bayesian meta-analysis for identifying periodically expressed genes in   fission yeast cell cycle","stat.ap","the effort to identify genes with periodic expression during the cell cycle from genome-wide microarray time series data has been ongoing for a decade. however, the lack of rigorous modeling of periodic expression as well as the lack of a comprehensive model for integrating information across genes and experiments has impaired the effort for the accurate identification of periodically expressed genes. to address the problem, we introduce a bayesian model to integrate multiple independent microarray data sets from three recent genome-wide cell cycle studies on fission yeast. a hierarchical model was used for data integration. in order to facilitate an efficient monte carlo sampling from the joint posterior distribution, we develop a novel metropolis--hastings group move. a surprising finding from our integrated analysis is that more than 40% of the genes in fission yeast are significantly periodically expressed, greatly enhancing the reported 10--15% of the genes in the current literature. it calls for a reconsideration of the periodically expressed gene detection problem.","10.1214/09-aoas300","2010-11-09","","['xiaodan fan', 'saumyadipta pyne', 'jun s. liu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"199",1011.2553,"modeling non-stationary processes through dimension expansion","stat.me stat.ap","in this paper, we propose a novel approach to modeling nonstationary spatial fields. the proposed method works by expanding the geographic plane over which these processes evolve into higher dimensional spaces, transforming and clarifying complex patterns in the physical plane. by combining aspects of multi-dimensional scaling, group lasso, and latent variables models, a dimensionally sparse projection is found in which the originally nonstationary field exhibits stationarity. following a comparison with existing methods in a simulated environment, dimension expansion is studied on a classic test-bed data set historically used to study nonstationary models. following this, we explore the use of dimension expansion in modeling air pollution in the united kingdom, a process known to be strongly influenced by rural/urban effects, amongst others, which gives rise to a nonstationary field.","","2010-11-10","2011-06-02","['luke bornn', 'gavin shaddick', 'james v zidek']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"200",1011.2851,"age- and time-varying proportional hazards models for employment   discrimination","stat.ap","we use a discrete-time proportional hazards model of time to involuntary employment termination. this model enables us to examine both the continuous effect of the age of an employee and whether that effect has varied over time, generalizing earlier work [kadane and woodworth j. bus. econom. statist. 22 (2004) 182--193]. we model the log hazard surface (over age and time) as a thin-plate spline, a bayesian smoothness-prior implementation of penalized likelihood methods of surface-fitting [wahba (1990) spline models for observational data. siam]. the nonlinear component of the surface has only two parameters, smoothness and anisotropy. the first, a scale parameter, governs the overall smoothness of the surface, and the second, anisotropy, controls the relative smoothness over time and over age. for any fixed value of the anisotropy parameter, the prior is equivalent to a gaussian process with linear drift over the time--age plane with easily computed eigenvectors and eigenvalues that depend only on the configuration of data in the time--age plane and the anisotropy parameter. this model has application to legal cases in which a company is charged with disproportionately disadvantaging older workers when deciding whom to terminate. we illustrate the application of the modeling approach using data from an actual discrimination case.","10.1214/10-aoas330","2010-11-12","","['george woodworth', 'joseph kadane']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"201",1011.3319,"poisson point process models solve the ""pseudo-absence problem"" for   presence-only data in ecology","stat.ap","presence-only data, point locations where a species has been recorded as being present, are often used in modeling the distribution of a species as a function of a set of explanatory variables---whether to map species occurrence, to understand its association with the environment, or to predict its response to environmental change. currently, ecologists most commonly analyze presence-only data by adding randomly chosen ""pseudo-absences"" to the data such that it can be analyzed using logistic regression, an approach which has weaknesses in model specification, in interpretation, and in implementation. to address these issues, we propose poisson point process modeling of the intensity of presences. we also derive a link between the proposed approach and logistic regression---specifically, we show that as the number of pseudo-absences increases (in a regular or uniform random arrangement), logistic regression slope parameters and their standard errors converge to those of the corresponding poisson point process model. we discuss the practical implications of these results. in particular, point process modeling offers a framework for choice of the number and location of pseudo-absences, both of which are currently chosen by ad hoc and sometimes ineffective methods in ecology, a point which we illustrate by example.","10.1214/10-aoas331","2010-11-15","","['david i. warton', 'leah c. shepherd']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"202",1011.3327,"modeling large scale species abundance with latent spatial processes","stat.ap","modeling species abundance patterns using local environmental features is an important, current problem in ecology. the cape floristic region (cfr) in south africa is a global hot spot of diversity and endemism, and provides a rich class of species abundance data for such modeling. here, we propose a multi-stage bayesian hierarchical model for explaining species abundance over this region. our model is specified at areal level, where the cfr is divided into roughly $37{,}000$ one minute grid cells; species abundance is observed at some locations within some cells. the abundance values are ordinally categorized. environmental and soil-type factors, likely to influence the abundance pattern, are included in the model. we formulate the empirical abundance pattern as a degraded version of the potential pattern, with the degradation effect accomplished in two stages. first, we adjust for land use transformation and then we adjust for measurement error, hence misclassification error, to yield the observed abundance classifications. an important point in this analysis is that only $28%$ of the grid cells have been sampled and that, for sampled grid cells, the number of sampled locations ranges from one to more than one hundred. still, we are able to develop potential and transformed abundance surfaces over the entire region. in the hierarchical framework, categorical abundance classifications are induced by continuous latent surfaces. the degradation model above is built on the latent scale. on this scale, an areal level spatial regression model was used for modeling the dependence of species abundance on the environmental factors.","10.1214/10-aoas335","2010-11-15","2010-11-23","['avishek chakraborty', 'alan e. gelfand', 'adam m. wilson', 'andrew m. latimer', 'john a. silander']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"203",1011.3351,"prediction-based classification for longitudinal biomarkers","stat.ap","assessment of circulating cd4 count change over time in hiv-infected subjects on antiretroviral therapy (art) is a central component of disease monitoring. the increasing number of hiv-infected subjects starting therapy and the limited capacity to support cd4 count testing within resource-limited settings have fueled interest in identifying correlates of cd4 count change such as total lymphocyte count, among others. the application of modeling techniques will be essential to this endeavor due to the typically nonlinear cd4 trajectory over time and the multiple input variables necessary for capturing cd4 variability. we propose a prediction-based classification approach that involves first stage modeling and subsequent classification based on clinically meaningful thresholds. this approach draws on existing analytical methods described in the receiver operating characteristic curve literature while presenting an extension for handling a continuous outcome. application of this method to an independent test sample results in greater than 98% positive predictive value for cd4 count change. the prediction algorithm is derived based on a cohort of $n=270$ hiv-1 infected individuals from the royal free hospital, london who were followed for up to three years from initiation of art. a test sample comprised of $n=72$ individuals from philadelphia and followed for a similar length of time is used for validation. results suggest that this approach may be a useful tool for prioritizing limited laboratory resources for cd4 testing after subjects start antiretroviral therapy.","10.1214/10-aoas326","2010-11-15","","['andrea s. foulkes', 'livio azzoni', 'xiaohong li', 'margaret a. johnson', 'colette smith', 'karam mounzer', 'luis j. montaner']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"204",1011.3371,"an approach for jointly modeling multivariate longitudinal measurements   and discrete time-to-event data","stat.ap","in many medical studies, patients are followed longitudinally and interest is on assessing the relationship between longitudinal measurements and time to an event. recently, various authors have proposed joint modeling approaches for longitudinal and time-to-event data for a single longitudinal variable. these joint modeling approaches become intractable with even a few longitudinal variables. in this paper we propose a regression calibration approach for jointly modeling multiple longitudinal measurements and discrete time-to-event data. ideally, a two-stage modeling approach could be applied in which the multiple longitudinal measurements are modeled in the first stage and the longitudinal model is related to the time-to-event data in the second stage. biased parameter estimation due to informative dropout makes this direct two-stage modeling approach problematic. we propose a regression calibration approach which appropriately accounts for informative dropout. we approximate the conditional distribution of the multiple longitudinal measurements given the event time by modeling all pairwise combinations of the longitudinal measurements using a bivariate linear mixed model which conditions on the event time. complete data are then simulated based on estimates from these pairwise conditional models, and regression calibration is used to estimate the relationship between longitudinal data and time-to-event data using the complete data. we show that this approach performs well in estimating the relationship between multivariate longitudinal measurements and the time-to-event data and in estimating the parameters of the multiple longitudinal process subject to informative dropout. we illustrate this methodology with simulations and with an analysis of primary biliary cirrhosis (pbc) data.","10.1214/10-aoas339","2010-11-15","","['paul s. albert', 'joanna h. shih']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"205",1011.3494,"learning planar ising models","stat.ml cs.ai","inference and learning of graphical models are both well-studied problems in statistics and machine learning that have found many applications in science and engineering. however, exact inference is intractable in general graphical models, which suggests the problem of seeking the best approximation to a collection of random variables within some tractable family of graphical models. in this paper, we focus our attention on the class of planar ising models, for which inference is tractable using techniques of statistical physics [kac and ward; kasteleyn]. based on these techniques and recent methods for planarity testing and planar embedding [chrobak and payne], we propose a simple greedy algorithm for learning the best planar ising model to approximate an arbitrary collection of binary random variables (possibly from sample data). given the set of all pairwise correlations among variables, we select a planar graph and optimal planar ising model defined on this graph to best approximate that set of correlations. we demonstrate our method in some simulations and for the application of modeling senate voting records.","","2010-11-15","","['jason k. johnson', 'praneeth netrapalli', 'michael chertkov']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"206",1011.4058,"modeling image structure with factorized phase-coupled boltzmann   machines","cs.cv cond-mat.dis-nn q-bio.nc stat.ml","we describe a model for capturing the statistical structure of local amplitude and local spatial phase in natural images. the model is based on a recently developed, factorized third-order boltzmann machine that was shown to be effective at capturing higher-order structure in images by modeling dependencies among squared filter outputs (ranzato and hinton, 2010). here, we extend this model to $l_p$-spherically symmetric subspaces. in order to model local amplitude and phase structure in images, we focus on the case of two dimensional subspaces, and the $l_2$-norm. when trained on natural images the model learns subspaces resembling quadrature-pair gabor filters. we then introduce an additional set of hidden units that model the dependencies among subspace phases. these hidden units form a combinatorial mixture of phase coupling distributions, concentrated in the sum and difference of phase pairs. when adapted to natural images, these distributions capture local spatial phase structure in natural images.","","2010-11-17","","['charles f. cadieu', 'kilian koepsell']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"207",1011.4088,"an introduction to conditional random fields","stat.ml","often we wish to predict a large number of variables that depend on each other as well as on other observed variables. structured prediction methods are essentially a combination of classification and graphical modeling, combining the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. this tutorial describes conditional random fields, a popular probabilistic method for structured prediction. crfs have seen wide application in natural language processing, computer vision, and bioinformatics. we describe methods for inference and parameter estimation for crfs, including practical issues for implementing large scale crfs. we do not assume previous knowledge of graphical modeling, so this tutorial is intended to be useful to practitioners in a wide variety of fields.","","2010-11-17","","['charles sutton', 'andrew mccallum']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"208",1011.6086,"in all likelihood, deep belief is not enough","stat.ml cs.lg","statistical models of natural stimuli provide an important tool for researchers in the fields of machine learning and computational neuroscience. a canonical way to quantitatively assess and compare the performance of statistical models is given by the likelihood. one class of statistical models which has recently gained increasing popularity and has been applied to a variety of complex data are deep belief networks. analyses of these models, however, have been typically limited to qualitative analyses based on samples due to the computationally intractable nature of the model likelihood. motivated by these circumstances, the present article provides a consistent estimator for the likelihood that is both computationally tractable and simple to apply in practice. using this estimator, a deep belief network which has been suggested for the modeling of natural image patches is quantitatively investigated and compared to other models of natural image patches. contrary to earlier claims based on qualitative results, the results presented in this article provide evidence that the model under investigation is not a particularly good model for natural images","","2010-11-28","","['lucas theis', 'sebastian gerwinn', 'fabian sinz', 'matthias bethge']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"209",1011.6293,"nonparametric bayesian sparse factor models with application to gene   expression modeling","stat.ap cs.ai stat.ml","a nonparametric bayesian extension of factor analysis (fa) is proposed where observed data $\mathbf{y}$ is modeled as a linear superposition, $\mathbf{g}$, of a potentially infinite number of hidden factors, $\mathbf{x}$. the indian buffet process (ibp) is used as a prior on $\mathbf{g}$ to incorporate sparsity and to allow the number of latent features to be inferred. the model's utility for modeling gene expression data is investigated using randomly generated data sets based on a known sparse connectivity matrix for e. coli, and on three biological data sets of increasing complexity.","10.1214/10-aoas435","2010-11-29","2011-07-28","['david knowles', 'zoubin ghahramani']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"210",1011.6649,"dimension reduction and alleviation of confounding for spatial   generalized linear mixed models","stat.me math.st stat.th","non-gaussian spatial data are very common in many disciplines. for instance, count data are common in disease mapping, and binary data are common in ecology. when fitting spatial regressions for such data, one needs to account for dependence to ensure reliable inference for the regression coefficients. the spatial generalized linear mixed model (sglmm) offers a very popular and flexible approach to modeling such data, but the sglmm suffers from three major shortcomings: (1) uninterpretability of parameters due to spatial confounding, (2) variance inflation due to spatial confounding, and (3) high-dimensional spatial random effects that make fully bayesian inference for such models computationally challenging. we propose a new parameterization of the sglmm that alleviates spatial confounding and speeds computation by greatly reducing the dimension of the spatial random effects. we illustrate the application of our approach to simulated binary, count, and gaussian spatial datasets, and to a large infant mortality dataset.","","2010-11-30","","['john hughes', 'murali haran']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"211",1012.0866,"generalized species sampling priors with latent beta reinforcements","math.st cs.lg stat.me stat.th","many popular bayesian nonparametric priors can be characterized in terms of exchangeable species sampling sequences. however, in some applications, exchangeability may not be appropriate. we introduce a {novel and probabilistically coherent family of non-exchangeable species sampling sequences characterized by a tractable predictive probability function with weights driven by a sequence of independent beta random variables. we compare their theoretical clustering properties with those of the dirichlet process and the two parameters poisson-dirichlet process. the proposed construction provides a complete characterization of the joint process, differently from existing work. we then propose the use of such process as prior distribution in a hierarchical bayes modeling framework, and we describe a markov chain monte carlo sampler for posterior inference. we evaluate the performance of the prior and the robustness of the resulting inference in a simulation study, providing a comparison with popular dirichlet processes mixtures and hidden markov models. finally, we develop an application to the detection of chromosomal aberrations in breast cancer by leveraging array cgh data.","","2010-12-03","2014-08-01","['edoardo m. airoldi', 'thiago costa', 'federico bassetti', 'fabrizio leisen', 'michele guindani']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"212",1012.1258,"simultaneous sequential detection of multiple interacting faults","cs.it cs.sy math.it math.st stat.th","single fault sequential change point problems have become important in modeling for various phenomena in large distributed systems, such as sensor networks. but such systems in many situations present multiple interacting faults. for example, individual sensors in a network may fail and detection is performed by comparing measurements between sensors, resulting in statistical dependency among faults. we present a new formulation for multiple interacting faults in a distributed system. the formulation includes specifications of how individual subsystems composing the large system may fail, the information that can be shared among these subsystems and the interaction pattern between faults. we then specify a new sequential algorithm for detecting these faults. the main feature of the algorithm is that it uses composite stopping rules for a subsystem that depend on the decision of other subsystems. we provide asymptotic false alarm and detection delay analysis for this algorithm in the bayesian setting and show that under certain conditions the algorithm is optimal. the analysis methodology relies on novel detailed comparison techniques between stopping times. we validate the approach with some simulations.","","2010-12-06","","['ram rajagopal', 'xuanlong nguyen', 'sinem coleri ergen', 'pravin varaiya']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"213",1012.2098,"multinomial inverse regression for text analysis","stat.me","text data, including speeches, stories, and other document forms, are often connected to sentiment variables that are of interest for research in marketing, economics, and elsewhere. it is also very high dimensional and difficult to incorporate into statistical analyses. this article introduces a straightforward framework of sentiment-preserving dimension reduction for text data. multinomial inverse regression is introduced as a general tool for simplifying predictor sets that can be represented as draws from a multinomial distribution, and we show that logistic regression of phrase counts onto document annotations can be used to obtain low dimension document representations that are rich in sentiment information. to facilitate this modeling, a novel estimation technique is developed for multinomial logistic regression with very high-dimension response. in particular, independent laplace priors with unknown variance are assigned to each regression coefficient, and we detail an efficient routine for maximization of the joint posterior over coefficients and their prior scale. this ""gamma-lasso"" scheme yields stable and effective estimation for general high-dimension logistic regression, and we argue that it will be superior to current methods in many settings. guidelines for prior specification are provided, algorithm convergence is detailed, and estimator properties are outlined from the perspective of the literature on non-concave likelihood penalization. related work on sentiment analysis from statistics, econometrics, and machine learning is surveyed and connected. finally, the methods are applied in two detailed examples and we provide out-of-sample prediction studies to illustrate their effectiveness.","","2010-12-09","2013-08-08","['matt taddy']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"214",1012.2105,"mixture modeling for marked poisson processes","stat.me","we propose a general modeling framework for marked poisson processes observed over time or space. the modeling approach exploits the connection of the nonhomogeneous poisson process intensity with a density function. nonparametric dirichlet process mixtures for this density, combined with nonparametric or semiparametric modeling for the mark distribution, yield flexible prior models for the marked poisson process. in particular, we focus on fully nonparametric model formulations that build the mark density and intensity function from a joint nonparametric mixture, and provide guidelines for straightforward application of these techniques. a key feature of such models is that they can yield flexible inference about the conditional distribution for multivariate marks without requiring specification of a complicated dependence scheme. we address issues relating to choice of the dirichlet process mixture kernels, and develop methods for prior specification and posterior simulation for full inference about functionals of the marked poisson process. moreover, we discuss a method for model checking that can be used to assess and compare goodness of fit of different model specifications under the proposed framework. the methodology is illustrated with simulated and real data sets.","","2010-12-09","2011-11-01","['matthew a. taddy', 'athanasios kottas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"215",1012.2902,"on the stationary distribution of iterative imputations","math.st stat.th","iterative imputation, in which variables are imputed one at a time each given a model predicting from all the others, is a popular technique that can be convenient and flexible, as it replaces a potentially difficult multivariate modeling problem with relatively simple univariate regressions.   in this paper, we begin to characterize the stationary distributions of iterative imputations and their statistical properties. more precisely, when the conditional models are compatible (defined in the text), we give a set of sufficient conditions under which the imputation distribution converges in total variation to the posterior distribution of a bayesian model. when the conditional models are incompatible but are valid, we show that the combined imputation estimator is consistent.","","2010-12-13","2012-04-02","['jingchen liu', 'andrew gelman', 'jennifer hill', 'yu-sung su']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"216",1012.4769,"scalable inference of customer similarities from interactions data using   dirichlet processes","stat.ap stat.co stat.me","under the sociological theory of homophily, people who are similar to one another are more likely to interact with one another. marketers often have access to data on interactions among customers from which, with homophily as a guiding principle, inferences could be made about the underlying similarities. however, larger networks face a quadratic explosion in the number of potential interactions that need to be modeled. this scalability problem renders probability models of social interactions computationally infeasible for all but the smallest networks. in this paper we develop a probabilistic framework for modeling customer interactions that is both grounded in the theory of homophily, and is flexible enough to account for random variation in who interacts with whom. in particular, we present a novel bayesian nonparametric approach, using dirichlet processes, to moderate the scalability problems that marketing researchers encounter when working with networked data. we find that this framework is a powerful way to draw insights into latent similarities of customers, and we discuss how marketers can apply these insights to segmentation and targeting activities.","10.1287/mksc.1110.0640","2010-12-21","","['michael braun', 'andré bonfrer']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"217",1101.0891,"to explain or to predict?","stat.me","statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description. in many disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are inherently of high predictive power. conflation between explanation and prediction is common, yet the distinction must be understood for progressing scientific knowledge. while this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the many differences that arise in the process of modeling for an explanatory versus a predictive goal. the purpose of this article is to clarify the distinction between explanatory and predictive modeling, to discuss its sources, and to reveal the practical implications of the distinction to each step in the modeling process.","10.1214/10-sts330","2011-01-05","","['galit shmueli']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"218",1101.1359,"exponential-family random graph models for valued networks","stat.me math.st stat.th","exponential-family random graph models (ergms) provide a principled and flexible way to model and simulate features common in social networks, such as propensities for homophily, mutuality, and friend-of-a-friend triad closure, through choice of model terms (sufficient statistics). however, those ergms modeling the more complex features have, to date, been limited to binary data: presence or absence of ties. thus, analysis of valued networks, such as those where counts, measurements, or ranks are observed, has necessitated dichotomizing them, losing information and introducing biases.   in this work, we generalize ergms to valued networks. focusing on modeling counts, we formulate an ergm for networks whose ties are counts and discuss issues that arise when moving beyond the binary case. we introduce model terms that generalize and model common social network features for such data and apply these methods to a network dataset whose values are counts of interactions.","10.1214/12-ejs696","2011-01-07","2012-01-19","['pavel n. krivitsky']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"219",1101.1373,"generalized extreme value regression for binary response data: an   application to b2b electronic payments system adoption","stat.ap","in the information system research, a question of particular interest is to interpret and to predict the probability of a firm to adopt a new technology such that market promotions are targeted to only those firms that were more likely to adopt the technology. typically, there exists significant difference between the observed number of ``adopters'' and ``nonadopters,'' which is usually coded as binary response. a critical issue involved in modeling such binary response data is the appropriate choice of link functions in a regression model. in this paper we introduce a new flexible skewed link function for modeling binary response data based on the generalized extreme value (gev) distribution. we show how the proposed gev links provide more flexible and improved skewed link regression models than the existing skewed links, especially when dealing with imbalance between the observed number of 0's and 1's in a data. the flexibility of the proposed model is illustrated through simulated data sets and a billing data set of the electronic payments system adoption from a fortune 100 company in 2005.","10.1214/10-aoas354","2011-01-07","","['xia wang', 'dipak k. dey']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"220",1101.1377,"a bayesian graphical modeling approach to microrna regulatory network   inference","stat.ap","it has been estimated that about 30% of the genes in the human genome are regulated by micrornas (mirnas). these are short rna sequences that can down-regulate the levels of mrnas or proteins in animals and plants. genes regulated by mirnas are called targets. typically, methods for target prediction are based solely on sequence data and on the structure information. in this paper we propose a bayesian graphical modeling approach that infers the mirna regulatory network by integrating expression levels of mirnas with their potential mrna targets and, via the prior probability model, with their sequence/structure information. we use a directed graphical model with a particular structure adapted to our data based on biological considerations. we then achieve network inference using stochastic search methods for variable selection that allow us to explore the huge model space via mcmc. a time-dependent coefficients model is also implemented. we consider experimental data from a study on a very well-known developmental toxicant causing neural tube defects, hyperthermia. some of the pairs of target gene and mirna we identify seem very plausible and warrant future investigation. our proposed method is general and can be easily applied to other types of network inference by integrating multiple data sources.","10.1214/10-aoas360","2011-01-07","","['francesco c. stingo', 'yian a. chen', 'marina vannucci', 'marianne barrier', 'philip e. mirkes']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"221",1101.1402,"model-robust regression and a bayesian ``sandwich'' estimator","stat.ap","we present a new bayesian approach to model-robust linear regression that leads to uncertainty estimates with the same robustness properties as the huber--white sandwich estimator. the sandwich estimator is known to provide asymptotically correct frequentist inference, even when standard modeling assumptions such as linearity and homoscedasticity in the data-generating mechanism are violated. our derivation provides a compelling bayesian justification for using this simple and popular tool, and it also clarifies what is being estimated when the data-generating mechanism is not linear. we demonstrate the applicability of our approach using a simulation study and health care cost data from an evaluation of the washington state basic health plan.","10.1214/10-aoas362","2011-01-07","","['adam a. szpiro', 'kenneth m. rice', 'thomas lumley']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"222",1101.2017,"bayesian nonparametric covariance regression","stat.me stat.ml","although there is a rich literature on methods for allowing the variance in a univariate regression model to vary with predictors, time and other factors, relatively little has been done in the multivariate case. our focus is on developing a class of nonparametric covariance regression models, which allow an unknown p x p covariance matrix to change flexibly with predictors. the proposed modeling framework induces a prior on a collection of covariance matrices indexed by predictors through priors for predictor-dependent loadings matrices in a factor model. in particular, the predictor-dependent loadings are characterized as a sparse combination of a collection of unknown dictionary functions (e.g, gaussian process random functions). the induced covariance is then a regularized quadratic function of these dictionary elements. our proposed framework leads to a highly-flexible, but computationally tractable formulation with simple conjugate posterior updates that can readily handle missing data. theoretical properties are discussed and the methods are illustrated through simulations studies and an application to the google flu trends data.","","2011-01-10","2011-02-08","['emily fox', 'david dunson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"223",1101.2157,"modeling and control of thermostatically controlled loads","cond-mat.stat-mech nlin.ao stat.ap","as the penetration of intermittent energy sources grows substantially, loads will be required to play an increasingly important role in compensating the fast time-scale fluctuations in generated power. recent numerical modeling of thermostatically controlled loads (tcls) has demonstrated that such load following is feasible, but analytical models that satisfactorily quantify the aggregate power consumption of a group of tcls are desired to enable controller design. we develop such a model for the aggregate power response of a homogeneous population of tcls to uniform variation of all tcl setpoints. a linearized model of the response is derived, and a linear quadratic regulator (lqr) has been designed. using the tcl setpoint as the control input, the lqr enables aggregate power to track reference signals that exhibit step, ramp and sinusoidal variations. although much of the work assumes a homogeneous population of tcls with deterministic dynamics, we also propose a method for probing the dynamics of systems where load characteristics are not well known.","","2011-01-11","","['soumya kundu', 'nikolai sinitsyn', 'scott backhaus', 'ian hiskens']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"224",1101.2592,"an exponential random graph modeling approach to creating group-based   representative whole-brain connectivity networks","stat.ap q-bio.nc q-bio.qm","group-based brain connectivity networks have great appeal for researchers interested in gaining further insight into complex brain function and how it changes across different mental states and disease conditions. accurately constructing these networks presents a daunting challenge given the difficulties associated with accounting for inter-subject topological variability. viable approaches to this task must engender networks that capture the constitutive topological properties of the group of subjects' networks that it is aiming to represent. the conventional approach has been to use a mean or median correlation network (achard et al., 2006; song et al., 2009) to embody a group of networks. however, the degree to which their topological properties conform with those of the groups that they are purported to represent has yet to be explored. here we investigate the performance of these mean and median correlation networks. we also propose an alternative approach based on an exponential random graph modeling framework and compare its performance to that of the aforementioned conventional approach. simpson et al. (2010) illustrated the utility of exponential random graph models (ergms) for creating brain networks that capture the topological characteristics of a single subject's brain network. however, their advantageousness in the context of producing a brain network that ""represents"" a group of brain networks has yet to be examined. here we show that our proposed ergm approach outperforms the conventional mean and median correlation based approaches and provides an accurate and flexible method for constructing group-based representative brain networks.","","2011-01-13","2011-11-16","['sean l. simpson', 'malaak n. moussa', 'paul j. laurienti']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"225",1102.1492,"on nonparametric guidance for learning autoencoder representations","stat.ml","unsupervised discovery of latent representations, in addition to being useful for density modeling, visualisation and exploratory data analysis, is also increasingly important for learning features relevant to discriminative tasks. autoencoders, in particular, have proven to be an effective way to learn latent codes that reflect meaningful variations in data. a continuing challenge, however, is guiding an autoencoder toward representations that are useful for particular tasks. a complementary challenge is to find codes that are invariant to irrelevant transformations of the data. the most common way of introducing such problem-specific guidance in autoencoders has been through the incorporation of a parametric component that ties the latent representation to the label information. in this work, we argue that a preferable approach relies instead on a nonparametric guidance mechanism. conceptually, it ensures that there exists a function that can predict the label information, without explicitly instantiating that function. the superiority of this guidance mechanism is confirmed on two datasets. in particular, this approach is able to incorporate invariance information (lighting, elevation, etc.) from the small norb object recognition dataset and yields state-of-the-art performance for a single layer, non-convolutional network.","","2011-02-07","2011-10-25","['jasper snoek', 'ryan prescott adams', 'hugo larochelle']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"226",1102.4399,"semi-supervised logistic discrimination for functional data","stat.me stat.ml","multi-class classification methods based on both labeled and unlabeled functional data sets are discussed. we present a semi-supervised logistic model for classification in the context of functional data analysis. unknown parameters in our proposed model are estimated by regularization with the help of em algorithm. a crucial point in the modeling procedure is the choice of a regularization parameter involved in the semi-supervised functional logistic model. in order to select the adjusted parameter, we introduce model selection criteria from information-theoretic and bayesian viewpoints. monte carlo simulations and a real data analysis are given to examine the effectiveness of our proposed modeling strategy.","","2011-02-21","2012-05-28","['shuichi kawano', 'sadanori konishi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"227",1102.5509,"probabilistic analysis of the human transcriptome with side information","stat.ml cs.ce q-bio.gn q-bio.mn q-bio.qm stat.ap stat.me","understanding functional organization of genetic information is a major challenge in modern biology. following the initial publication of the human genome sequence in 2001, advances in high-throughput measurement technologies and efficient sharing of research material through community databases have opened up new views to the study of living organisms and the structure of life. in this thesis, novel computational strategies have been developed to investigate a key functional layer of genetic information, the human transcriptome, which regulates the function of living cells through protein synthesis. the key contributions of the thesis are general exploratory tools for high-throughput data analysis that have provided new insights to cell-biological networks, cancer mechanisms and other aspects of genome function.   a central challenge in functional genomics is that high-dimensional genomic observations are associated with high levels of complex and largely unknown sources of variation. by combining statistical evidence across multiple measurement sources and the wealth of background information in genomic data repositories it has been possible to solve some the uncertainties associated with individual observations and to identify functional mechanisms that could not be detected based on individual measurement sources. statistical learning and probabilistic models provide a natural framework for such modeling tasks. open source implementations of the key methodological contributions have been released to facilitate further adoption of the developed methods by the research community.","","2011-02-27","","['leo lahti']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE
"228",1103.0769,"sparse volterra and polynomial regression models: recoverability and   estimation","cs.lg cs.it math.it stat.ml","volterra and polynomial regression models play a major role in nonlinear system identification and inference tasks. exciting applications ranging from neuroscience to genome-wide association analysis build on these models with the additional requirement of parsimony. this requirement has high interpretative value, but unfortunately cannot be met by least-squares based or kernel regression methods. to this end, compressed sampling (cs) approaches, already successful in linear regression settings, can offer a viable alternative. the viability of cs for sparse volterra and polynomial models is the core theme of this work. a common sparse regression task is initially posed for the two models. building on (weighted) lasso-based schemes, an adaptive rls-type algorithm is developed for sparse polynomial regressions. the identifiability of polynomial models is critically challenged by dimensionality. however, following the cs principle, when these models are sparse, they could be recovered by far fewer measurements. to quantify the sufficient number of measurements for a given level of sparsity, restricted isometry properties (rip) are investigated in commonly met polynomial regression settings, generalizing known results for their linear counterparts. the merits of the novel (weighted) adaptive cs algorithms to sparse polynomial modeling are verified through synthetic as well as real data tests for genotype-phenotype analysis.","10.1109/tsp.2011.2165952","2011-03-03","2011-09-06","['vassilis kekatos', 'georgios b. giannakis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"229",1103.1046,"bayesian design of synthetic biological systems","q-bio.mn stat.ap","here we introduce a new design framework for synthetic biology that exploits the advantages of bayesian model selection. we will argue that the difference between inference and design is that in the former we try to reconstruct the system that has given rise to the data that we observe, while in the latter, we seek to construct the system that produces the data that we would like to observe, i.e. the desired behavior. our approach allows us to exploit methods from bayesian statistics, including efficient exploration of models spaces and high-dimensional parameter spaces, and the ability to rank models with respect to their ability to generate certain types of data. bayesian model selection furthermore automatically strikes a balance between complexity and (predictive or explanatory) performance of mathematical models. in order to deal with the complexities of molecular systems we employ an approximate bayesian computation scheme which only requires us to simulate from different competing models in order to arrive at rational criteria for choosing between them. we illustrate the advantages resulting from combining the design and modeling (or in-silico prototyping) stages currently seen as separate in synthetic biology by reference to deterministic and stochastic model systems exhibiting adaptive and switch-like behavior, as well as bacterial two-component signaling systems.","10.1073/pnas.1017972108","2011-03-05","","['chris barnes', 'daniel silk', 'xia sheng', 'michael p. h. stumpf']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"230",1103.1805,"probability boxes on totally preordered spaces for multivariate   modelling","math.pr math.st stat.co stat.th","a pair of lower and upper cumulative distribution functions, also called probability box or p-box, is among the most popular models used in imprecise probability theory. they arise naturally in expert elicitation, for instance in cases where bounds are specified on the quantiles of a random variable, or when quantiles are specified only at a finite number of points. many practical and formal results concerning p-boxes already exist in the literature. in this paper, we provide new efficient tools to construct multivariate p-boxes and develop algorithms to draw inferences from them. for this purpose, we formalise and extend the theory of p-boxes using walley's behavioural theory of imprecise probabilities, and heavily rely on its notion of natural extension and existing results about independence modeling. in particular, we allow p-boxes to be defined on arbitrary totally preordered spaces, hence thereby also admitting multivariate p-boxes via probability bounds over any collection of nested sets. we focus on the cases of independence (using the factorization property), and of unknown dependence (using the fr\'echet bounds), and we show that our approach extends the probabilistic arithmetic of williamson and downs. two design problems---a damped oscillator, and a river dike---demonstrate the practical feasibility of our results.","10.1016/j.ijar.2011.02.001","2011-03-09","2011-03-29","['matthias c. m. troffaes', 'sebastien destercke']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE
"231",1103.3967,"a new selection method for high-dimensionial instrumental setting:   application to the growth rate convergence hypothesis","math.st stat.th","this paper investigates the problem of selecting variables in regression-type models for an ""instrumental"" setting. our study is motivated by empirically verifying the conditional convergence hypothesis used in the economical literature concerning the growth rate. to avoid unnecessary discussion about the choice and the pertinence of instrumental variables, we embed the model in a very high dimensional setting. we propose a selection procedure with no optimization step called lola, for learning out of leaders with adaptation. lola is an auto-driven algorithm with two thresholding steps. the consistency of the procedure is proved under sparsity conditions and simulations are conducted to illustrate the practical good performances of lola. the behavior of the algorithm is studied when instrumental variables are artificially added without a priori significant connection to the model. using our algorithm, we provide a solution for modeling the link between the growth rate and the initial level of the gross domestic product and empirically prove the convergence hypothesis.","","2011-03-21","","['mathilde mougeot', 'dominique picard', 'karine tribouley']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"232",1103.4615,"statistical inference for valued-edge networks: generalized exponential   random graph models","physics.data-an stat.me","across the sciences, the statistical analysis of networks is central to the production of knowledge on relational phenomena. because of their ability to model the structural generation of networks, exponential random graph models are a ubiquitous means of analysis. however, they are limited by an inability to model networks with valued edges. we solve this problem by introducing a class of generalized exponential random graph models capable of modeling networks whose edges are valued, thus greatly expanding the scope of networks applied researchers can subject to statistical analysis.","10.1371/journal.pone.0030136","2011-03-23","","['bruce a. desmarais', 'skyler j. cranmer']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"233",1103.4789,"the discrete infinite logistic normal distribution","stat.ml","we present the discrete infinite logistic normal distribution (diln), a bayesian nonparametric prior for mixed membership models. diln is a generalization of the hierarchical dirichlet process (hdp) that models correlation structure between the weights of the atoms at the group level. we derive a representation of diln as a normalized collection of gamma-distributed random variables, and study its statistical properties. we consider applications to topic modeling and derive a variational inference algorithm for approximate posterior inference. we study the empirical performance of the diln topic model on four corpora, comparing performance with the hdp and the correlated topic model (ctm). to deal with large-scale data sets, we also develop an online inference algorithm for diln and compare with online hdp and online lda on the nature magazine, which contains approximately 350,000 articles.","","2011-03-24","2012-04-19","['john paisley', 'chong wang', 'david blei']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"234",1103.5178,"logistic network regression for scalable analysis of networks with joint   edge/vertex dynamics","stat.me stat.ap","network dynamics may be viewed as a process of change in the edge structure of a network, in the vertex set on which edges are defined, or in both simultaneously. though early studies of such processes were primarily descriptive, recent work on this topic has increasingly turned to formal statistical models. while showing great promise, many of these modern dynamic models are computationally intensive and scale very poorly in the size of the network under study and/or the number of time points considered. likewise, currently employed models focus on edge dynamics, with little support for endogenously changing vertex sets. here, we show how an existing approach based on logistic network regression can be extended to serve as highly scalable framework for modeling large networks with dynamic vertex sets. we place this approach within a general dynamic exponential family (ergm) context, clarifying the assumptions underlying the framework (and providing a clear path for extensions), and show how model assessment methods for cross-sectional networks can be extended to the dynamic case. finally, we illustrate this approach on a classic data set involving interactions among windsurfers on a california beach.","","2011-03-26","","['zack w. almquist', 'carter t. butts']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"235",1103.5921,"a new bivariate extension of fgm copulas","math.st stat.th","we propose a new family of copulas generalizing the   farlie-gumbel-morgenstern family and generated by two univariate functions. the main feature of this family is to permit the modeling of high positive dependence. in particular, it is established that the range of the spearman's rho is [-3/4,1] and that the upper tail dependence coefficient can reach any value in [0,1]. necessary and sufficient conditions are given on the generating functions in order to obtain various dependence properties. some examples of parametric subfamilies are provided.","","2011-03-30","","['cécile amblard', 'stéphane girard']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"236",1104.0455,"robust nonparametric regression via sparsity control with application to   load curve data cleansing","stat.ml","nonparametric methods are widely applicable to statistical inference problems, since they rely on a few modeling assumptions. in this context, the fresh look advocated here permeates benefits from variable selection and compressive sampling, to robustify nonparametric regression against outliers - that is, data markedly deviating from the postulated models. a variational counterpart to least-trimmed squares regression is shown closely related to an l0-(pseudo)norm-regularized estimator, that encourages sparsity in a vector explicitly modeling the outliers. this connection suggests efficient solvers based on convex relaxation, which lead naturally to a variational m-type estimator equivalent to the least-absolute shrinkage and selection operator (lasso). outliers are identified by judiciously tuning regularization parameters, which amounts to controlling the sparsity of the outlier vector along the whole robustification path of lasso solutions. reduced bias and enhanced generalization capability are attractive features of an improved estimator obtained after replacing the l0-(pseudo)norm with a nonconvex surrogate. the novel robust spline-based smoother is adopted to cleanse load curve data, a key task aiding operational decisions in the envisioned smart grid system. computer simulations and tests on real load curve data corroborate the effectiveness of the novel sparsity-controlling robust estimators.","10.1109/tsp.2011.2181837","2011-04-03","","['gonzalo mateos', 'georgios b. giannakis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"237",1104.0554,"high frequency sampling of a continuous-time arma process","math.st math.pr math.sp stat.th","continuous-time autoregressive moving average (carma) processes have recently been used widely in the modeling of non-uniformly spaced data and as a tool for dealing with high-frequency data of the form $y_{n\delta}, n=0,1,2,...$, where $\delta$ is small and positive. such data occur in many fields of application, particularly in finance and the study of turbulence. this paper is concerned with the characteristics of the process $(y_{n\delta})_{n\in\bbz}$, when $\delta$ is small and the underlying continuous-time process $(y_t)_{t\in\bbr}$ is a specified carma process.","10.1111/j.1467-9892.2011.00748.x","2011-04-04","","['peter j. brockwell', 'vincenzo ferrazzano', 'claudia klüppelberg']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"238",1104.1162,"gemtools: a fast and efficient approach to estimating genetic ancestry","stat.ap","to uncover the genetic basis of complex disease, individuals are often measured at a large number of genetic variants (usually snps) across the genome. gemtools provides computationally efficient tools for modeling genetic ancestry based on snp genotypes. the main algorithm creates an eigenmap based on genetic similarities, and then clusters subjects based on their map position. this process is continued iteratively until each cluster is relatively homogeneous. for genetic association studies, gemtools matches cases and controls based on genetic similarity.","","2011-04-06","","['lambertus klei', 'brian p. kent', 'nadine melhem', 'bernie devlin', 'kathryn roeder']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"239",1104.1992,"unified treatment of hidden markov switching models","stat.ml","many real-world problems encountered in several disciplines deal with the modeling of time-series containing different underlying dynamical regimes, for which probabilistic approaches are very often employed. in this paper we describe several such approaches in the common framework of graphical models. we give a unified overview of models previously introduced in the literature, which is simpler and more comprehensive than previous descriptions and enables us to highlight commonalities and differences among models that were not observed in the past. in addition, we present several new models and inference routines, which are naturally derived within this unified viewpoint.","","2011-04-11","","['silvia chiappa']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"240",1104.2703,"a spatial analysis of multivariate output from regional climate models","stat.ap","climate models have become an important tool in the study of climate and climate change, and ensemble experiments consisting of multiple climate-model runs are used in studying and quantifying the uncertainty in climate-model output. however, there are often only a limited number of model runs available for a particular experiment, and one of the statistical challenges is to characterize the distribution of the model output. to that end, we have developed a multivariate hierarchical approach, at the heart of which is a new representation of a multivariate markov random field. this approach allows for flexible modeling of the multivariate spatial dependencies, including the cross-dependencies between variables. we demonstrate this statistical model on an ensemble arising from a regional-climate-model experiment over the western united states, and we focus on the projected change in seasonal temperature and precipitation over the next 50 years.","10.1214/10-aoas369","2011-04-14","","['stephan r. sain', 'reinhard furrer', 'noel cressie']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"241",1104.2805,"encoding and decoding v1 fmri responses to natural images with sparse   nonparametric models","stat.ap","functional mri (fmri) has become the most common method for investigating the human brain. however, fmri data present some complications for statistical analysis and modeling. one recently developed approach to these data focuses on estimation of computational encoding models that describe how stimuli are transformed into brain activity measured in individual voxels. here we aim at building encoding models for fmri signals recorded in the primary visual cortex of the human brain. we use residual analyses to reveal systematic nonlinearity across voxels not taken into account by previous models. we then show how a sparse nonparametric method [j. roy. statist. soc. ser. b 71 (2009b) 1009-1030] can be used together with correlation screening to estimate nonlinear encoding models effectively. our approach produces encoding models that predict about 25% more accurately than models estimated using other methods [nature 452 (2008a) 352--355]. the estimated nonlinearity impacts the inferred properties of individual voxels, and it has a plausible biological interpretation. one benefit of quantitative encoding models is that estimated models can be used to decode brain activity, in order to identify which specific image was seen by an observer. encoding models estimated by our approach also improve such image identification by about 12% when the correct image is one of 11,500 possible images.","10.1214/11-aoas476","2011-04-14","2011-07-25","['vincent q. vu', 'pradeep ravikumar', 'thomas naselaris', 'kendrick n. kay', 'jack l. gallant', 'bin yu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"242",1104.3464,"a dynamic bayesian nonlinear mixed-effects model of hiv response   incorporating medication adherence, drug resistance and covariates","stat.ap","hiv dynamic studies have contributed significantly to the understanding of hiv pathogenesis and antiviral treatment strategies for aids patients. establishing the relationship of virologic responses with clinical factors and covariates during long-term antiretroviral (arv) therapy is important to the development of effective treatments. medication adherence is an important predictor of the effectiveness of arv treatment, but an appropriate determinant of adherence rate based on medication event monitoring system (mems) data is critical to predict virologic outcomes. the primary objective of this paper is to investigate the effects of a number of summary determinants of mems adherence rates on virologic response measured repeatedly over time in hiv-infected patients. we developed a mechanism-based differential equation model with consideration of drug adherence, interacted by virus susceptibility to drug and baseline characteristics, to characterize the long-term virologic responses after initiation of therapy. this model fully integrates viral load, mems adherence, drug resistance and baseline covariates into the data analysis. in this study we employed the proposed model and associated bayesian nonlinear mixed-effects modeling approach to assess how to efficiently use the mems adherence data for prediction of virologic response, and to evaluate the predicting power of each summary metric of the mems adherence rates.","10.1214/10-aoas376","2011-04-18","","['yangxin huang', 'hulin wu', 'jeanne holden-wiltse', 'edward p. acosta']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"243",1104.3476,"metamodel-based importance sampling for the simulation of rare events","stat.me stat.ml","in the field of structural reliability, the monte-carlo estimator is considered as the reference probability estimator. however, it is still untractable for real engineering cases since it requires a high number of runs of the model. in order to reduce the number of computer experiments, many other approaches known as reliability methods have been proposed. a certain approach consists in replacing the original experiment by a surrogate which is much faster to evaluate. nevertheless, it is often difficult (or even impossible) to quantify the error made by this substitution. in this paper an alternative approach is developed. it takes advantage of the kriging meta-modeling and importance sampling techniques. the proposed alternative estimator is finally applied to a finite element based structural reliability analysis.","","2011-04-18","","['v. dubourg', 'f. deheeger', 'b. sudret']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"244",1104.3479,"reliability-based design optimization of shells with uncertain geometry   using adaptive kriging metamodels","stat.me stat.ap","optimal design under uncertainty has gained much attention in the past ten years due to the ever increasing need for manufacturers to build robust systems at the lowest cost. reliability-based design optimization (rbdo) allows the analyst to minimize some cost function while ensuring some minimal performances cast as admissible failure probabilities for a set of performance functions. in order to address real-world engineering problems in which the performance is assessed through computational models (e.g., finite element models in structural mechanics) metamodeling techniques have been developed in the past decade. this paper introduces adaptive kriging surrogate models to solve the rbdo problem. the latter is cast in an augmented space that ""sums up"" the range of the design space and the aleatory uncertainty in the design parameters and the environmental conditions. the surrogate model is used (i) for evaluating robust estimates of the failure probabilities (and for enhancing the computational experimental design by adaptive sampling) in order to achieve the requested accuracy and (ii) for applying a gradient-based optimization algorithm to get optimal values of the design parameters. the approach is applied to the optimal design of ring-stiffened cylindrical shells used in submarine engineering under uncertain geometric imperfections. for this application the performance of the structure is related to buckling which is addressed here by means of a finite element solution based on the asymptotic numerical method.","","2011-04-18","2017-04-12","['v. dubourg', 'j. -m. bourinet', 'b. sudret']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"245",1104.3667,"reliability-based design optimization using kriging surrogates and   subset simulation","stat.me stat.ml","the aim of the present paper is to develop a strategy for solving reliability-based design optimization (rbdo) problems that remains applicable when the performance models are expensive to evaluate. starting with the premise that simulation-based approaches are not affordable for such problems, and that the most-probable-failure-point-based approaches do not permit to quantify the error on the estimation of the failure probability, an approach based on both metamodels and advanced simulation techniques is explored. the kriging metamodeling technique is chosen in order to surrogate the performance functions because it allows one to genuinely quantify the surrogate error. the surrogate error onto the limit-state surfaces is propagated to the failure probabilities estimates in order to provide an empirical error measure. this error is then sequentially reduced by means of a population-based adaptive refinement technique until the kriging surrogates are accurate enough for reliability analysis. this original refinement strategy makes it possible to add several observations in the design of experiments at the same time. reliability and reliability sensitivity analyses are performed by means of the subset simulation technique for the sake of numerical efficiency. the adaptive surrogate-based strategy for reliability estimation is finally involved into a classical gradient-based optimization algorithm in order to solve the rbdo problem. the kriging surrogates are built in a so-called augmented reliability space thus making them reusable from one nested rbdo iteration to the other. the strategy is compared to other approaches available in the literature on three academic examples in the field of structural mechanics.","10.1007/s00158-011-0653-8","2011-04-19","","['v. dubourg', 'b. sudret', 'j. -m. bourinet']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"246",1104.377,"robust recovery of multiple subspaces by geometric l_p minimization","stat.ml math.st stat.th","we assume i.i.d. data sampled from a mixture distribution with k components along fixed d-dimensional linear subspaces and an additional outlier component. for p>0, we study the simultaneous recovery of the k fixed subspaces by minimizing the l_p-averaged distances of the sampled data points from any k subspaces. under some conditions, we show that if $0<p\leq1$, then all underlying subspaces can be precisely recovered by l_p minimization with overwhelming probability. on the other hand, if k>1 and p>1, then the underlying subspaces cannot be recovered or even nearly recovered by l_p minimization. the results of this paper partially explain the successes and failures of the basic approach of l_p energy minimization for modeling data by multiple subspaces.","10.1214/11-aos914","2011-04-19","2012-02-01","['gilad lerman', 'teng zhang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"247",1104.421,"curve registration by nonparametric goodness-of-fit testing","math.st stat.th","the problem of curve registration appears in many different areas of applications ranging from neuroscience to road traffic modeling. in the present work, we propose a nonparametric testing framework in which we develop a generalized likelihood ratio test to perform curve registration. we first prove that, under the null hypothesis, the resulting test statistic is asymptotically distributed as a chi-squared random variable. this result, often referred to as wilks' phenomenon, provides a natural threshold for the test of a prescribed asymptotic significance level and a natural measure of lack-of-fit in terms of the $p$-value of the $\chi^2$-test. we also prove that the proposed test is consistent, \textit{i.e.}, its power is asymptotically equal to $1$. finite sample properties of the proposed methodology are demonstrated by numerical simulations. as an application, a new local descriptor for digital images is introduced and an experimental evaluation of its discriminative power is conducted.","","2011-04-21","2015-02-19","['olivier collier', 'arnak s. dalalyan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"248",1104.4385,"convex approaches to model wavelet sparsity patterns","cs.cv stat.ml","statistical dependencies among wavelet coefficients are commonly represented by graphical models such as hidden markov trees(hmts). however, in linear inverse problems such as deconvolution, tomography, and compressed sensing, the presence of a sensing or observation matrix produces a linear mixing of the simple markovian dependency structure. this leads to reconstruction problems that are non-convex optimizations. past work has dealt with this issue by resorting to greedy or suboptimal iterative reconstruction methods. in this paper, we propose new modeling approaches based on group-sparsity penalties that leads to convex optimizations that can be solved exactly and efficiently. we show that the methods we develop perform significantly better in deconvolution and compressed sensing applications, while being as computationally efficient as standard coefficient-wise approaches such as lasso.","","2011-04-22","","['nikhil s rao', 'robert d. nowak', 'stephen j. wright', 'nick g. kingsbury']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"249",1104.441,"semi-parametric regression: efficiency gains from modeling the   nonparametric part","math.st stat.th","it is widely admitted that structured nonparametric modeling that circumvents the curse of dimensionality is important in nonparametric estimation. in this paper we show that the same holds for semi-parametric estimation. we argue that estimation of the parametric component of a semi-parametric model can be improved essentially when more structure is put into the nonparametric part of the model. we illustrate this for the partially linear model, and investigate efficiency gains when the nonparametric part of the model has an additive structure. we present the semi-parametric fisher information bound for estimating the parametric part of the partially linear additive model and provide semi-parametric efficient estimators for which we use a smooth backfitting technique to deal with the additive nonparametric part. we also present the finite sample performances of the proposed estimators and analyze boston housing data as an illustration.","10.3150/10-bej296","2011-04-22","","['kyusang yu', 'enno mammen', 'byeong u. park']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"250",1104.4605,"compressive network analysis","stat.ml cs.dm cs.lg cs.si physics.soc-ph","modern data acquisition routinely produces massive amounts of network data. though many methods and models have been proposed to analyze such data, the research of network data is largely disconnected with the classical theory of statistical learning and signal processing. in this paper, we present a new framework for modeling network data, which connects two seemingly different areas: network data analysis and compressed sensing. from a nonparametric perspective, we model an observed network using a large dictionary. in particular, we consider the network clique detection problem and show connections between our formulation with a new algebraic tool, namely randon basis pursuit in homogeneous spaces. such a connection allows us to identify rigorous recovery conditions for clique detection problems. though this paper is mainly conceptual, we also develop practical approximation algorithms for solving empirical problems and demonstrate their usefulness on real-world datasets.","","2011-04-24","","['xiaoye jiang', 'yuan yao', 'han liu', 'leonidas guibas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"251",1104.5429,"unsupervised classification for tiling arrays: chip-chip and   transcriptome","stat.me q-bio.gn q-bio.qm stat.ap","tiling arrays make possible a large scale exploration of the genome thanks to probes which cover the whole genome with very high density until 2 000 000 probes. biological questions usually addressed are either the expression difference between two conditions or the detection of transcribed regions. in this work we propose to consider simultaneously both questions as an unsupervised classification problem by modeling the joint distribution of the two conditions. in contrast to previous methods, we account for all available information on the probes as well as biological knowledge like annotation and spatial dependence between probes. since probes are not biologically relevant units we propose a classification rule for non-connected regions covered by several probes. applications to transcriptomic and chip-chip data of arabidopsis thaliana obtained with a nimblegen tiling array highlight the importance of a precise modeling and the region classification.","","2011-04-28","","['caroline bérard', 'marie-laure martin-magniette', 'véronique brunaud', 'sébastien aubourg', 'stéphane robin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"252",1105.0543,"hiv dynamics and natural history studies: joint modeling with doubly   interval-censored event time and infrequent longitudinal data","stat.ap","hepatitis c virus (hcv) coinfection has become one of the most challenging clinical situations to manage in hiv-infected patients. recently the effect of hcv coinfection on hiv dynamics following initiation of highly active antiretroviral therapy (haart) has drawn considerable attention. post-haart hiv dynamics are commonly studied in short-term clinical trials with frequent data collection design. for example, the elimination process of plasma virus during treatment is closely monitored with daily assessments in viral dynamics studies of aids clinical trials. in this article instead we use infrequent cohort data from long-term natural history studies and develop a model for characterizing post-haart hiv dynamics and their associations with hcv coinfection. specifically, we propose a joint model for doubly interval-censored data for the time between haart initiation and viral suppression, and the longitudinal cd4 count measurements relative to the viral suppression. inference is accomplished using a fully bayesian approach. doubly interval-censored data are modeled semiparametrically by dirichlet process priors and bayesian penalized splines are used for modeling population-level and individual-level mean cd4 count profiles. we use the proposed methods and data from the hiv epidemiology research study (hers) to investigate the effect of hcv coinfection on the response to haart.","10.1214/10-aoas391","2011-05-03","","['li su', 'joseph w. hogan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"253",1105.078,"homogeneity tests for michaelis-menten curves with application to   fluorescence resonance energy transfer data","q-bio.bm q-bio.qm stat.ap","resonance energy transfer methods are in wide use for evaluating protein-protein interactions and protein conformational changes in living cells. fluorescence resonance energy transfer (fret) measures energy transfer as a function of the acceptor:donor ratio, generating fret saturation curves. modeling these curves by michaelis-menten kinetics allows characterization by two parameters, which serve to evaluate apparent affinity between two proteins and to compare this affinity in different experimental conditions. to reduce the effect of sampling variability, several statistical samples of the saturation curve are generated in the same biological conditions. here we study three procedures to determine whether statistical samples in a collection are homogeneous, in the sense that they are extracted from the same regression model. from the hypothesis testing viewpoint, we considered an f test and a procedure based on bootstrap resampling. the third method analyzed the problem from the model selection viewpoint, and used the akaike information criterion (aic). although we only considered the michaelis-menten model, all statistical procedures would be applicable to any other nonlinear regression model. we compared the performance of the homogeneity testing methods in a monte carlo study and through analysis in living cells of fret saturation curves for dimeric complexes of cxcr4, a seven-transmembrane receptor of the g protein-coupled receptor family. we show that the f test, the bootstrap procedure and the model selection method lead in general to similar conclusions, although aic gave the best results when sample sizes were small, whereas the f test and the bootstrap method were more appropriate for large samples. in practice, all three methods are easy to use simultaneously and show consistency, facilitating conclusions on sample homogeneity.","","2011-05-04","","['amparo baíllo', 'laura martínez-muñoz', 'mario mellado']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"254",1105.0871,"bounding rare event probabilities in computer experiments","stat.co","we are interested in bounding probabilities of rare events in the context of computer experiments. these rare events depend on the output of a physical model with random input variables. since the model is only known through an expensive black box function, standard efficient monte carlo methods designed for rare events cannot be used. we then propose a strategy to deal with this difficulty based on importance sampling methods. this proposal relies on kriging metamodeling and is able to achieve sharp upper confidence bounds on the rare event probabilities. the variability due to the kriging metamodeling step is properly taken into account. the proposed methodology is applied to a toy example and compared to more standard bayesian bounds. finally, a challenging real case study is analyzed. it consists of finding an upper bound of the probability that the trajectory of an airborne load will collide with the aircraft that has released it.","","2011-05-04","2012-03-20","['yves auffray', 'pierre barbillon', 'jean-michel marin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"255",1105.0902,"modeling network evolution using graph motifs","stat.me cs.si physics.soc-ph stat.co","network structures are extremely important to the study of political science. much of the data in its subfields are naturally represented as networks. this includes trade, diplomatic and conflict relationships. the social structure of several organization is also of interest to many researchers, such as the affiliations of legislators or the relationships among terrorist. a key aspect of studying social networks is understanding the evolutionary dynamics and the mechanism by which these structures grow and change over time. while current methods are well suited to describe static features of networks, they are less capable of specifying models of change and simulating network evolution. in the following paper i present a new method for modeling network growth and evolution. this method relies on graph motifs to generate simulated network data with particular structural characteristic. this technique departs notably from current methods both in form and function. rather than a closed-form model, or stochastic implementation from a single class of graphs, the proposed ""graph motif model"" provides a framework for building flexible and complex models of network evolution. the paper proceeds as follows: first a brief review of the current literature on network modeling is provided to place the graph motif model in context. next, the graph motif model is introduced, and a simple example is provided. as a proof of concept, three classic random graph models are recovered using the graph motif modeling method: the erdos-renyi binomial random graph, the watts-strogatz ""small world"" model, and the barabasi-albert preferential attachment model. in the final section i discuss the results of these simulations and subsequent advantage and disadvantages presented by using this technique to model social networks.","","2011-05-04","","['drew conway']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"256",1105.3169,"semiparametric bivariate zero-inflated poisson models with application   to studies of abundance for multiple species","stat.me stat.ap","ecological studies involving counts of abundance, presence-absence or occupancy rates often produce data having a substantial proportion of zeros. furthermore, these types of processes are typically multivariate and only adequately described by complex nonlinear relationships involving externally measured covariates. ignoring these aspects of the data and implementing standard approaches can lead to models that fail to provide adequate scientific understanding of the underlying ecological processes, possibly resulting in a loss of inferential power. one method of dealing with data having excess zeros is to consider the class of univariate zero-inflated generalized linear models. however, this class of models fails to address the multivariate and nonlinear aspects associated with the data usually encountered in practice. therefore, we propose a semiparametric bivariate zero-inflated poisson model that takes into account both of these data attributes. the general modeling framework is hierarchical bayes and is suitable for a broad range of applications. we demonstrate the effectiveness of our model through a motivating example on modeling catch per unit area for multiple species using data from the missouri river benthic fish study, implemented by the united states geological survey.","","2011-05-16","","['ali arab', 'scott h. holan', 'christopher k. wikle', 'mark l. wildhaber']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"257",1105.4151,"towards realistic vehicular network modeling using planet-scale public   webcams","cs.ni stat.ap","realistic modeling of vehicular mobility has been particularly challenging due to a lack of large libraries of measurements in the research community. in this paper we introduce a novel method for large-scale monitoring, analysis, and identification of spatio-temporal models for vehicular mobility using the freely available online webcams in cities across the globe. we collect vehicular mobility traces from 2,700 traffic webcams in 10 different cities for several months and generate a mobility dataset of 7.5 terabytes consisting of 125 million of images. to the best of our knowl- edge, this is the largest data set ever used in such study. to process and analyze this data, we propose an efficient and scalable algorithm to estimate traffic density based on background image subtraction. initial results show that at least 82% of individual cameras with less than 5% deviation from four cities follow loglogistic distribution and also 94% cameras from toronto follow gamma distribution. the aggregate results from each city also demonstrate that log- logistic and gamma distribution pass the ks-test with 95% confidence. furthermore, many of the camera traces exhibit long range dependence, with self-similarity evident in the aggregates of traffic (per city). we believe our novel data collection method and dataset provide a much needed contribution to the research community for realistic modeling of vehicular networks and mobility.","","2011-05-19","","['gautam s. thakur', 'pan hui', 'hamed ketabdar', 'ahmed helmy']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"258",1105.6344,"prototype selection for parameter estimation in complex models","stat.ap astro-ph.im","parameter estimation in astrophysics often requires the use of complex physical models. in this paper we study the problem of estimating the parameters that describe star formation history (sfh) in galaxies. here, high-dimensional spectral data from galaxies are appropriately modeled as linear combinations of physical components, called simple stellar populations (ssps), plus some nonlinear distortions. theoretical data for each ssp is produced for a fixed parameter vector via computer modeling. though the parameters that define each ssp are continuous, optimizing the signal model over a large set of ssps on a fine parameter grid is computationally infeasible and inefficient. the goal of this study is to estimate the set of parameters that describes the sfh of each galaxy. these target parameters, such as the average ages and chemical compositions of the galaxy's stellar populations, are derived from the ssp parameters and the component weights in the signal model. here, we introduce a principled approach of choosing a small basis of ssp prototypes for sfh parameter estimation. the basic idea is to quantize the vector space and effective support of the model components. in addition to greater computational efficiency, we achieve better estimates of the sfh target parameters. in simulations, our proposed quantization method obtains a substantial improvement in estimating the target parameters over the common method of employing a parameter grid. sparse coding techniques are not appropriate for this problem without proper constraints, while constrained sparse coding methods perform poorly for parameter estimation because their objective is signal reconstruction, not estimation of the target parameters.","10.1214/11-aoas500","2011-05-31","2012-03-20","['joseph w. richards', 'ann b. lee', 'chad m. schafer', 'peter e. freeman']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"259",1106.198,"how do markov approximations compare with other methods for large   spatial data sets?","stat.co","the mat\'ern covariance function is a popular choice for modeling dependence in spatial environmental data. standard mat\'ern covariance models are, however, often computationally infeasible for large data sets. in this work, recent results for markov approximations of gaussian mat\'{e}rn fields based on hilbert space approximations are extended using wavelet basis functions. these markov approximations are compared with two of the most popular methods for efficient covariance approximations; covariance tapering and the process convolution method. the results show that, for a given computational cost, the markov methods have a substantial gain in accuracy compared with the other methods.","","2011-06-10","2011-11-04","['david bolin', 'finn lindgren']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"260",1106.2363,"random design analysis of ridge regression","math.st cs.ai cs.lg stat.ml stat.th","this work gives a simultaneous analysis of both the ordinary least squares estimator and the ridge regression estimator in the random design setting under mild assumptions on the covariate/response distributions. in particular, the analysis provides sharp results on the ``out-of-sample'' prediction error, as opposed to the ``in-sample'' (fixed design) error. the analysis also reveals the effect of errors in the estimated covariance structure, as well as the effect of modeling errors, neither of which effects are present in the fixed design setting. the proofs of the main results are based on a simple decomposition lemma combined with concentration inequalities for random vectors and matrices.","","2011-06-12","2014-03-24","['daniel hsu', 'sham m. kakade', 'tong zhang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"261",1106.2697,"a tutorial on bayesian nonparametric models","stat.ml stat.me","a key problem in statistical modeling is model selection, how to choose a model at an appropriate level of complexity. this problem appears in many settings, most prominently in choosing the number ofclusters in mixture models or the number of factors in factor analysis. in this tutorial we describe bayesian nonparametric methods, a class of methods that side-steps this issue by allowing the data to determine the complexity of the model. this tutorial is a high-level introduction to bayesian nonparametric methods and contains several examples of their application.","","2011-06-14","2011-08-04","['samuel j. gershman', 'david m. blei']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"262",1106.3181,"variable selection for nonparametric gaussian process priors: models and   computational strategies","stat.me","this paper presents a unified treatment of gaussian process models that extends to data from the exponential dispersion family and to survival data. our specific interest is in the analysis of data sets with predictors that have an a priori unknown form of possibly nonlinear associations to the response. the modeling approach we describe incorporates gaussian processes in a generalized linear model framework to obtain a class of nonparametric regression models where the covariance matrix depends on the predictors. we consider, in particular, continuous, categorical and count responses. we also look into models that account for survival outcomes. we explore alternative covariance formulations for the gaussian process prior and demonstrate the flexibility of the construction. next, we focus on the important problem of selecting variables from the set of possible predictors and describe a general framework that employs mixture priors. we compare alternative mcmc strategies for posterior inference and achieve a computationally efficient and practical approach. we demonstrate performances on simulated and benchmark data sets.","10.1214/11-sts354","2011-06-16","","['terrance savitsky', 'marina vannucci', 'naijun sha']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"263",1106.3703,"prediction and modularity in dynamical systems","nlin.ao cs.ai cs.it cs.lg cs.sy math.it q-bio.qm stat.me","identifying and understanding modular organizations is centrally important in the study of complex systems. several approaches to this problem have been advanced, many framed in information-theoretic terms. our treatment starts from the complementary point of view of statistical modeling and prediction of dynamical systems. it is known that for finite amounts of training data, simpler models can have greater predictive power than more complex ones. we use the trade-off between model simplicity and predictive accuracy to generate optimal multiscale decompositions of dynamical networks into weakly-coupled, simple modules. state-dependent and causal versions of our method are also proposed.","","2011-06-19","2015-01-16","['artemy kolchinsky', 'luis m. rocha']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"264",1106.3921,"dynamic large spatial covariance matrix estimation in application to   semiparametric model construction via variable clustering: the sce approach","stat.ml q-fin.rm q-fin.st stat.me","to better understand the spatial structure of large panels of economic and financial time series and provide a guideline for constructing semiparametric models, this paper first considers estimating a large spatial covariance matrix of the generalized $m$-dependent and $\beta$-mixing time series (with $j$ variables and $t$ observations) by hard thresholding regularization as long as ${{\log j \, \cx^*(\ct)}}/{t} = \co(1)$ (the former scheme with some time dependence measure $\cx^*(\ct)$) or $\log j /{t} = \co(1)$ (the latter scheme with some upper bounded mixing coefficient). we quantify the interplay between the estimators' consistency rate and the time dependence level, discuss an intuitive resampling scheme for threshold selection, and also prove a general cross-validation result justifying this. given a consistently estimated covariance (correlation) matrix, by utilizing its natural links with graphical models and semiparametrics, after ""screening"" the (explanatory) variables, we implement a novel forward (and backward) label permutation procedure to cluster the ""relevant"" variables and construct the corresponding semiparametric model, which is further estimated by the groupwise dimension reduction method with sign constraints. we call this the sce (screen - cluster - estimate) approach for modeling high dimensional data with complex spatial structure. finally we apply this method to study the spatial structure of large panels of economic and financial time series and find the proper semiparametric structure for estimating the consumer price index (cpi) to illustrate its superiority over the linear models.","","2011-06-20","2011-06-23","['song song']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"265",1106.5779,"efficient gaussian process regression for large data sets","stat.me","gaussian processes (gps) are widely used in nonparametric regression, classification and spatio-temporal modeling, motivated in part by a rich literature on theoretical properties. however, a well known drawback of gps that limits their use is the expensive computation, typically o($n^3$) in performing the necessary matrix inversions with $n$ denoting the number of data points. in large data sets, data storage and processing also lead to computational bottlenecks and numerical stability of the estimates and predicted values degrades with $n$. to address these problems, a rich variety of methods have been proposed, with recent options including predictive processes in spatial data analysis and subset of regressors in machine learning. the underlying idea in these approaches is to use a subset of the data, leading to questions of sensitivity to the subset and limitations in estimating fine scale structure in regions that are not well covered by the subset. motivated by the literature on compressive sensing, we propose an alternative random projection of all the data points onto a lower-dimensional subspace. we demonstrate the superiority of this approach from a theoretical perspective and through the use of simulated and real data examples. some keywords: bayesian; compressive sensing; dimension reduction; gaussian processes; random projections; subset selection","","2011-06-28","","['anjishnu banerjee', 'david dunson', 'surya tokdar']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"266",1107.0789,"distributed matrix completion and robust factorization","cs.lg cs.ds cs.na math.na stat.ml","if learning methods are to scale to the massive sizes of modern datasets, it is essential for the field of machine learning to embrace parallel and distributed computing. inspired by the recent development of matrix factorization methods with rich theory but poor computational complexity and by the relative ease of mapping matrices onto distributed architectures, we introduce a scalable divide-and-conquer framework for noisy matrix factorization. we present a thorough theoretical analysis of this framework in which we characterize the statistical errors introduced by the ""divide"" step and control their magnitude in the ""conquer"" step, so that the overall algorithm enjoys high-probability estimation guarantees comparable to those of its base algorithm. we also present experiments in collaborative filtering and video background modeling that demonstrate the near-linear to superlinear speed-ups attainable with this approach.","","2011-07-05","2013-10-28","['lester mackey', 'ameet talwalkar', 'michael i. jordan']",1,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"267",1107.2462,"statistical topic models for multi-label document classification","stat.ml cs.lg","machine learning approaches to multi-label document classification have to date largely relied on discriminative modeling techniques such as support vector machines. a drawback of these approaches is that performance rapidly drops off as the total number of labels and the number of labels per document increase. this problem is amplified when the label frequencies exhibit the type of highly skewed distributions that are often observed in real-world datasets. in this paper we investigate a class of generative statistical topic models for multi-label documents that associate individual word tokens with different labels. we investigate the advantages of this approach relative to discriminative models, particularly with respect to classification problems involving large numbers of relatively rare labels. we compare the performance of generative and discriminative approaches on document labeling tasks ranging from datasets with several thousand labels to datasets with tens of labels. the experimental results indicate that probabilistic generative models can achieve competitive multi-label classification performance compared to discriminative methods, and have advantages for datasets with many labels and skewed label frequencies.","","2011-07-13","2011-11-09","['timothy n. rubin', 'america chambers', 'padhraic smyth', 'mark steyvers']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"268",1107.3382,"special section on statistics in neuroscience","stat.ap","this article provides a brief introduction to seven papers that are included in this special section on statistics in neuroscience: (1) xiaoyan shi, joseph g. ibrahim, jeffrey lieberman, martin styner, yimei li and hongtu zhu: two-state empirical likelihood for longitudinal neuroimaging data (2) vincent q. vu, pradeep ravikumar, thomas naselaris, kendrick n. kay, jack l. gallant and bin yu: encoding and decoding v1 fmri responses to natural images with sparse nonparametric models (3) sourabh bhattacharya and ranjan maitra: a nonstationary nonparametric bayesian approach to dynamically modeling effective connectivity in functional magnetic resonance imaging experiments (4) christopher j. long, patrick l. purdon, simona temereanca, neil u. desai, matti s. h\""{a}m\""{a}l\""{a}inen and emery neal brown: state-space solutions to the dynamic magnetoencephalography inverse problem using high performance computing (5) yuriy mishchencko, joshua t. vogelstein and liam paninski: a bayesian approach for inferring neuronal connectivity from calcium fluorescent imaging data (6) robert e. kass, ryan c. kelly and wei-liem loh: assessment of synchrony in multiple neural spike trains using loglinear point process models (7) sofia olhede and brandon whitcher: nonparametric tests of structure for high angular resolution diffusion imaging in q-space","10.1214/11-aoas485","2011-07-18","","['karen kafadar']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"269",1107.3618,"varying-coefficient modeling via regularized basis functions","stat.me","we address the problem of constructing varying-coefficient models based on basis expansions along with the technique of regularization. a crucial point in our modeling procedure is the selection of smoothing parameters in the regularization method. in order to choose the parameters objectively, we derive model selection criteria from the viewpoints of information-theoretic and bayesian approach. we demonstrate the effectiveness of proposed modeling strategy through monte carlo simulations and analyzing a real data set.","10.1080/00949655.2013.785548","2011-07-18","","['hidetoshi matsui', 'toshihiro misumi', 'shuichi kawano']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"270",1107.4852,"network routing in a dynamic environment","stat.ap","recently, there has been an explosion of work on network routing in hostile environments. hostile environments tend to be dynamic, and the motivation for this work stems from the scenario of ied placements by insurgents in a logistical network. for discussion, we consider here a sub-network abstracted from a real network, and propose a framework for route selection. what distinguishes our work from related work is its decision theoretic foundation, and statistical considerations pertaining to probability assessments. the latter entails the fusion of data from diverse sources, modeling the socio-psychological behavior of adversaries, and likelihood functions that are induced by simulation. this paper demonstrates the role of statistical inference and data analysis on problems that have traditionally belonged in the domain of computer science, communications, transportation science, and operations research.","10.1214/10-aoas453","2011-07-25","","['nozer d. singpurwalla']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"271",1107.4855,"causal inference in transportation safety studies: comparison of   potential outcomes and causal diagrams","stat.ap","the research questions that motivate transportation safety studies are causal in nature. safety researchers typically use observational data to answer such questions, but often without appropriate causal inference methodology. the field of causal inference presents several modeling frameworks for probing empirical data to assess causal relations. this paper focuses on exploring the applicability of two such modeling frameworks---causal diagrams and potential outcomes---for a specific transportation safety problem. the causal effects of pavement marking retroreflectivity on safety of a road segment were estimated. more specifically, the results based on three different implementations of these frameworks on a real data set were compared: inverse propensity score weighting with regression adjustment and propensity score matching with regression adjustment versus causal bayesian network. the effect of increased pavement marking retroreflectivity was generally found to reduce the probability of target nighttime crashes. however, we found that the magnitude of the causal effects estimated are sensitive to the method used and to the assumptions being violated.","10.1214/10-aoas440","2011-07-25","","['vishesh karwa', 'aleksandra b. slavković', 'eric t. donnell']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"272",1107.4905,"bayesian hierarchical modeling for temperature reconstruction from   geothermal data","stat.ap","we present a bayesian hierarchical modeling approach to paleoclimate reconstruction using borehole temperature profiles. the approach relies on modeling heat conduction in solids via the heat equation with step function, surface boundary conditions. our analysis includes model error and assumes that the boundary conditions are random processes. the formulation also enables separation of measurement error and model error. we apply the analysis to data from nine borehole temperature records from the san rafael region in utah. we produce ground surface temperature histories with uncertainty estimates for the past 400 years. we pay special attention to use of prior parameter models that illustrate borrowing strength in a combined analysis for all nine boreholes. in addition, we review selected sensitivity analyses.","10.1214/10-aoas452","2011-07-25","","['jenný brynjarsdóttir', 'l. mark berliner']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"273",1107.5712,"degradation modeling applied to residual lifetime prediction using   functional data analysis","stat.ap","sensor-based degradation signals measure the accumulation of damage of an engineering system using sensor technology. degradation signals can be used to estimate, for example, the distribution of the remaining life of partially degraded systems and/or their components. in this paper we present a nonparametric degradation modeling framework for making inference on the evolution of degradation signals that are observed sparsely or over short intervals of times. furthermore, an empirical bayes approach is used to update the stochastic parameters of the degradation model in real-time using training degradation signals for online monitoring of components operating in the field. the primary application of this bayesian framework is updating the residual lifetime up to a degradation threshold of partially degraded components. we validate our degradation modeling approach using a real-world crack growth data set as well as a case study of simulated degradation signals.","10.1214/10-aoas448","2011-07-28","","['rensheng r. zhou', 'nicoleta serban', 'nagi gebraeel']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"274",1107.5872,"assessment of synchrony in multiple neural spike trains using loglinear   point process models","stat.ap q-bio.nc","neural spike trains, which are sequences of very brief jumps in voltage across the cell membrane, were one of the motivating applications for the development of point process methodology. early work required the assumption of stationarity, but contemporary experiments often use time-varying stimuli and produce time-varying neural responses. more recently, many statistical methods have been developed for nonstationary neural point process data. there has also been much interest in identifying synchrony, meaning events across two or more neurons that are nearly simultaneous at the time scale of the recordings. a natural statistical approach is to discretize time, using short time bins, and to introduce loglinear models for dependency among neurons, but previous use of loglinear modeling technology has assumed stationarity. we introduce a succinct yet powerful class of time-varying loglinear models by (a) allowing individual-neuron effects (main effects) to involve time-varying intensities; (b) also allowing the individual-neuron effects to involve autocovariation effects (history effects) due to past spiking, (c) assuming excess synchrony effects (interaction effects) do not depend on history, and (d) assuming all effects vary smoothly across time.","10.1214/10-aoas429","2011-07-29","","['robert e. kass', 'ryan c. kelly', 'wei-liem loh']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"275",1107.5935,"bayesian synthesis: combining subjective analyses, with an application   to ozone data","stat.ap","bayesian model averaging enables one to combine the disparate predictions of a number of models in a coherent fashion, leading to superior predictive performance. the improvement in performance arises from averaging models that make different predictions. in this work, we tap into perhaps the biggest driver of different predictions---different analysts---in order to gain the full benefits of model averaging. in a standard implementation of our method, several data analysts work independently on portions of a data set, eliciting separate models which are eventually updated and combined through a specific weighting method. we call this modeling procedure bayesian synthesis. the methodology helps to alleviate concerns about the sizable gap between the foundational underpinnings of the bayesian paradigm and the practice of bayesian statistics. in experimental work we show that human modeling has predictive performance superior to that of many automatic modeling techniques, including aic, bic, smoothing splines, cart, bagged cart, bayes cart, bma and lars, and only slightly inferior to that of bart. we also show that bayesian synthesis further improves predictive performance. additionally, we examine the predictive performance of a simple average across analysts, which we dub convex synthesis, and find that it also produces an improvement.","10.1214/10-aoas444","2011-07-29","","['qingzhao yu', 'steven n. maceachern', 'mario peruggia']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"276",1108.024,"analysis of rolling group therapy data using conditionally   autoregressive priors","stat.ap","group therapy is a central treatment modality for behavioral health disorders such as alcohol and other drug use (aod) and depression. group therapy is often delivered under a rolling (or open) admissions policy, where new clients are continuously enrolled into a group as space permits. rolling admissions policies result in a complex correlation structure among client outcomes. despite the ubiquity of rolling admissions in practice, little guidance on the analysis of such data is available. we discuss the limitations of previously proposed approaches in the context of a study that delivered group cognitive behavioral therapy for depression to clients in residential substance abuse treatment. we improve upon previous rolling group analytic approaches by fully modeling the interrelatedness of client depressive symptom scores using a hierarchical bayesian model that assumes a conditionally autoregressive prior for session-level random effects. we demonstrate improved performance using our method for estimating the variance of model parameters and the enhanced ability to learn about the complex correlation structure among participants in rolling therapy groups. our approach broadly applies to any group therapy setting where groups have changing client composition. it will lead to more efficient analyses of client-level data and improve the group therapy research community's ability to understand how the dynamics of rolling groups lead to client outcomes.","10.1214/10-aoas434","2011-08-01","","['susan m. paddock', 'sarah b. hunter', 'katherine e. watkins', 'daniel f. mccaffrey']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"277",1108.0298,"network model-assisted inference from respondent-driven sampling data","stat.me","respondent-driven sampling is a method to sample hard-to-reach human populations by link-tracing over their social networks. beginning with a convenience sample, each person sampled is given a small number of uniquely identified coupons to distribute to other members of the target population, making them eligible for enrollment in the study. this can be an effective means to collect large diverse samples from many populations.   inference from such data requires specialized techniques for two reasons. unlike in standard sampling designs, the sampling process is both partially beyond the control of the researcher, and partially implicitly defined. therefore, it is not generally possible to directly compute the sampling weights necessary for traditional design-based inference. any likelihood-based inference requires the modeling of the complex sampling process often beginning with a convenience sample. we introduce a model-assisted approach, resulting in a design-based estimator leveraging a working model for the structure of the population over which sampling is conducted.   we demonstrate that the new estimator has improved performance compared to existing estimators and is able to adjust for the bias induced by the selection of the initial sample. we present sensitivity analyses for unknown population sizes and the misspecification of the working network model. we develop a bootstrap procedure to compute measures of uncertainty. we apply the method to the estimation of hiv prevalence in a population of injecting drug users (idu) in the ukraine, and show how it can be extended to include application-specific information.","","2011-08-01","","['krista j. gile', 'mark s. handcock']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"278",1108.0793,"bayesian hierarchical modeling for signaling pathway inference from   single cell interventional data","stat.ap","recent technological advances have made it possible to simultaneously measure multiple protein activities at the single cell level. with such data collected under different stimulatory or inhibitory conditions, it is possible to infer the causal relationships among proteins from single cell interventional data. in this article we propose a bayesian hierarchical modeling framework to infer the signaling pathway based on the posterior distributions of parameters in the model. under this framework, we consider network sparsity and model the existence of an association between two proteins both at the overall level across all experiments and at each individual experimental level. this allows us to infer the pairs of proteins that are associated with each other and their causal relationships. we also explicitly consider both intrinsic noise and measurement error. markov chain monte carlo is implemented for statistical inference. we demonstrate that this hierarchical modeling can effectively pool information from different interventional experiments through simulation studies and real data analysis.","10.1214/10-aoas425","2011-08-03","","['ruiyan luo', 'hongyu zhao']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"279",1108.0996,"mean--variance portfolio optimization when means and covariances are   unknown","stat.ap q-fin.pm","markowitz's celebrated mean--variance portfolio optimization theory assumes that the means and covariances of the underlying asset returns are known. in practice, they are unknown and have to be estimated from historical data. plugging the estimates into the efficient frontier that assumes known parameters has led to portfolios that may perform poorly and have counter-intuitive asset allocation weights; this has been referred to as the ""markowitz optimization enigma."" after reviewing different approaches in the literature to address these difficulties, we explain the root cause of the enigma and propose a new approach to resolve it. not only is the new approach shown to provide substantial improvements over previous methods, but it also allows flexible modeling to incorporate dynamic features and fundamental analysis of the training sample of historical data, as illustrated in simulation and empirical studies.","10.1214/10-aoas422","2011-08-04","","['tze leung lai', 'haipeng xing', 'zehao chen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"280",1108.2167,"missing data in value-added modeling of teacher effects","stat.ap","the increasing availability of longitudinal student achievement data has heightened interest among researchers, educators and policy makers in using these data to evaluate educational inputs, as well as for school and possibly teacher accountability. researchers have developed elaborate ""value-added models"" of these longitudinal data to estimate the effects of educational inputs (e.g., teachers or schools) on student achievement while using prior achievement to adjust for nonrandom assignment of students to schools and classes. a challenge to such modeling efforts is the extensive numbers of students with incomplete records and the tendency for those students to be lower achieving. these conditions create the potential for results to be sensitive to violations of the assumption that data are missing at random, which is commonly used when estimating model parameters. the current study extends recent value-added modeling approaches for longitudinal student achievement data lockwood et al. [j. educ. behav. statist. 32 (2007) 125--150] to allow data to be missing not at random via random effects selection and pattern mixture models, and applies those methods to data from a large urban school district to estimate effects of elementary school mathematics teachers. we find that allowing the data to be missing not at random has little impact on estimated teacher effects. the robustness of estimated teacher effects to the missing data assumptions appears to result from both the relatively small impact of model specification on estimated student effects compared with the large variability in teacher effects and the downweighting of scores from students with incomplete data.","10.1214/10-aoas405","2011-08-10","","['daniel f. mccaffrey', 'j. r. lockwood']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"281",1108.2334,"two-stage empirical likelihood for longitudinal neuroimaging data","stat.ap","longitudinal imaging studies are essential to understanding the neural development of neuropsychiatric disorders, substance use disorders, and the normal brain. the main objective of this paper is to develop a two-stage adjusted exponentially tilted empirical likelihood (tetel) for the spatial analysis of neuroimaging data from longitudinal studies. the tetel method as a frequentist approach allows us to efficiently analyze longitudinal data without modeling temporal correlation and to classify different time-dependent covariate types. to account for spatial dependence, the tetel method developed here specifically combines all the data in the closest neighborhood of each voxel (or pixel) on a 3-dimensional (3d) volume (or 2d surface) with appropriate weights to calculate adaptive parameter estimates and adaptive test statistics. simulation studies are used to examine the finite sample performance of the adjusted exponential tilted likelihood ratio statistic and tetel. we demonstrate the application of our statistical methods to the detection of the difference in the morphological changes of the hippocampus across time between schizophrenia patients and healthy subjects in a longitudinal schizophrenia study.","10.1214/11-aoas480","2011-08-11","","['xiaoyan shi', 'joseph g. ibrahim', 'jeffrey lieberman', 'martin styner', 'yimei li', 'hongtu zhu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"282",1108.272,"latent factor models for density estimation","math.st stat.th","although discrete mixture modeling has formed the backbone of the literature on bayesian density estimation, there are some well known disadvantages. we propose an alternative class of priors based on random nonlinear functions of a uniform latent variable with an additive residual. the induced prior for the density is shown to have desirable properties including ease of centering on an initial guess for the density, large support, posterior consistency and straightforward computation via gibbs sampling. some advantages over discrete mixtures, such as dirichlet process mixtures of gaussian kernels, are discussed and illustrated via simulations and an epidemiology application.","","2011-08-12","2011-09-19","['suprateek kundu', 'david b. dunson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"283",1108.282,"ensemble risk modeling method for robust learning on scarce data","cs.lg stat.ml","in medical risk modeling, typical data are ""scarce"": they have relatively small number of training instances (n), censoring, and high dimensionality (m). we show that the problem may be effectively simplified by reducing it to bipartite ranking, and introduce new bipartite ranking algorithm, smooth rank, for robust learning on scarce data. the algorithm is based on ensemble learning with unsupervised aggregation of predictors. the advantage of our approach is confirmed in comparison with two ""gold standard"" risk modeling methods on 10 real life survival analysis datasets, where the new approach has the best results on all but two datasets with the largest ratio n/m. for systematic study of the effects of data scarcity on modeling by all three methods, we conducted two types of computational experiments: on real life data with randomly drawn training sets of different sizes, and on artificial data with increasing number of features. both experiments demonstrated that smooth rank has critical advantage over the popular methods on the scarce data; it does not suffer from overfitting where other methods do.","","2011-08-13","2012-01-28","['marina sapir']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"284",1108.3298,"a machine learning perspective on predictive coding with paq","cs.lg cs.ai cs.cv cs.ir stat.ml","paq8 is an open source lossless data compression algorithm that currently achieves the best compression rates on many benchmarks. this report presents a detailed description of paq8 from a statistical machine learning perspective. it shows that it is possible to understand some of the modules of paq8 and use this understanding to improve the method. however, intuitive statistical explanations of the behavior of other modules remain elusive. we hope the description in this report will be a starting point for discussions that will increase our understanding, lead to improvements to paq8, and facilitate a transfer of knowledge from paq8 to other machine learning methods, such a recurrent neural networks and stochastic memoizers. finally, the report presents a broad range of new applications of paq to machine learning tasks including language modeling and adaptive text prediction, adaptive game playing, classification, and compression using features from the field of deep learning.","","2011-08-16","","['byron knoll', 'nando de freitas']",1,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"285",1108.391,"automated analysis of quantitative image data using isomorphic   functional mixed models, with application to proteomics data","stat.ap","image data are increasingly encountered and are of growing importance in many areas of science. much of these data are quantitative image data, which are characterized by intensities that represent some measurement of interest in the scanned images. the data typically consist of multiple images on the same domain and the goal of the research is to combine the quantitative information across images to make inference about populations or interventions. in this paper we present a unified analysis framework for the analysis of quantitative image data using a bayesian functional mixed model approach. this framework is flexible enough to handle complex, irregular images with many local features, and can model the simultaneous effects of multiple factors on the image intensities and account for the correlation between images induced by the design. we introduce a general isomorphic modeling approach to fitting the functional mixed model, of which the wavelet-based functional mixed model is one special case. with suitable modeling choices, this approach leads to efficient calculations and can result in flexible modeling and adaptive smoothing of the salient features in the data. the proposed method has the following advantages: it can be run automatically, it produces inferential plots indicating which regions of the image are associated with each factor, it simultaneously considers the practical and statistical significance of findings, and it controls the false discovery rate.","10.1214/10-aoas407","2011-08-19","","['jeffrey s. morris', 'veerabhadran baladandayuthapani', 'richard c. herrick', 'pietro sanna', 'howard gutstein']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"286",1108.4079,"toward parts-based scene understanding with pixel-support parts-sparse   pictorial structures","cs.cv stat.ml","scene understanding remains a significant challenge in the computer vision community. the visual psychophysics literature has demonstrated the importance of interdependence among parts of the scene. yet, the majority of methods in computer vision remain local. pictorial structures have arisen as a fundamental parts-based model for some vision problems, such as articulated object detection. however, the form of classical pictorial structures limits their applicability for global problems, such as semantic pixel labeling. in this paper, we propose an extension of the pictorial structures approach, called pixel-support parts-sparse pictorial structures, or ps3, to overcome this limitation. our model extends the classical form in two ways: first, it defines parts directly based on pixel-support rather than in a parametric form, and second, it specifies a space of plausible parts-based scene models and permits one to be used for inference on any given image. ps3 makes strides toward unifying object-level and pixel-level modeling of scene elements. in this report, we implement the first half of our model and rely upon external knowledge to provide an initial graph structure for a given image. our experimental results on benchmark datasets demonstrate the capability of this new parts-based view of scene modeling.","","2011-08-19","","['jason j. corso']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"287",1108.5244,"semi-supervised logistic discrimination via labeled data and unlabeled   data from different sampling distributions","stat.ml stat.me","this article addresses the problem of classification method based on both labeled and unlabeled data, where we assume that a density function for labeled data is different from that for unlabeled data. we propose a semi-supervised logistic regression model for classification problem along with the technique of covariate shift adaptation. unknown parameters involved in proposed models are estimated by regularization with em algorithm. a crucial issue in the modeling process is the choices of tuning parameters in our semi-supervised logistic models. in order to select the parameters, a model selection criterion is derived from an information-theoretic approach. some numerical studies show that our modeling procedure performs well in various cases.","10.1002/sam.11204","2011-08-26","2012-10-13","['shuichi kawano']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"288",1108.5838,"off-grid direction of arrival estimation using sparse bayesian inference","stat.ap cs.it math.it stat.ml","direction of arrival (doa) estimation is a classical problem in signal processing with many practical applications. its research has recently been advanced owing to the development of methods based on sparse signal reconstruction. while these methods have shown advantages over conventional ones, there are still difficulties in practical situations where true doas are not on the discretized sampling grid. to deal with such an off-grid doa estimation problem, this paper studies an off-grid model that takes into account effects of the off-grid doas and has a smaller modeling error. an iterative algorithm is developed based on the off-grid model from a bayesian perspective while joint sparsity among different snapshots is exploited by assuming a laplace prior for signals at all snapshots. the new approach applies to both single snapshot and multi-snapshot cases. numerical simulations show that the proposed algorithm has improved accuracy in terms of mean squared estimation error. the algorithm can maintain high estimation accuracy even under a very coarse sampling grid.","10.1109/tsp.2012.2222378","2011-08-30","2012-09-17","['zai yang', 'lihua xie', 'cishen zhang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"289",1108.6271,"optimizing the quantity/quality trade-off in connectome inference","q-bio.nc stat.ap","we demonstrate a meaningful prospective power analysis for an (admittedly idealized) illustrative connectome inference task. modeling neurons as vertices and synapses as edges in a simple random graph model, we optimize the trade-off between the number of (putative) edges identified and the accuracy of the edge identification procedure. we conclude that explicit analysis of the quantity/quality trade-off is imperative for optimal neuroscientific experimental design. in particular, more though more errorful edge identification can yield superior inferential performance.","","2011-08-31","2011-10-11","['carey e. priebe', 'joshua t. vogelstein', 'davi bock']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"290",1109.2411,"efficient algorithm to select tuning parameters in sparse regression   modeling with regularization","stat.me stat.co stat.ml","in sparse regression modeling via regularization such as the lasso, it is important to select appropriate values of tuning parameters including regularization parameters. the choice of tuning parameters can be viewed as a model selection and evaluation problem. mallows' $c_p$ type criteria may be used as a tuning parameter selection tool in lasso-type regularization methods, for which the concept of degrees of freedom plays a key role. in the present paper, we propose an efficient algorithm that computes the degrees of freedom by extending the generalized path seeking algorithm. our procedure allows us to construct model selection criteria for evaluating models estimated by regularization with a wide variety of convex and non-convex penalties. monte carlo simulations demonstrate that our methodology performs well in various situations. a real data example is also given to illustrate our procedure.","","2011-09-12","2012-01-03","['kei hirose', 'shohei tateishi', 'sadanori konishi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE
"291",1109.299,"extrapolation of urn models via poissonization: accurate measurements of   the microbial unknown","stat.me math.pr q-bio.gn q-bio.pe","the availability of high-throughput parallel methods for sequencing microbial communities is increasing our knowledge of the microbial world at an unprecedented rate. though most attention has focused on determining lower-bounds on the alpha-diversity i.e. the total number of different species present in the environment, tight bounds on this quantity may be highly uncertain because a small fraction of the environment could be composed of a vast number of different species. to better assess what remains unknown, we propose instead to predict the fraction of the environment that belongs to unsampled classes. modeling samples as draws with replacement of colored balls from an urn with an unknown composition, and under the sole assumption that there are still undiscovered species, we show that conditionally unbiased predictors and exact prediction intervals (of constant length in logarithmic scale) are possible for the fraction of the environment that belongs to unsampled classes. our predictions are based on a poissonization argument, which we have implemented in what we call the embedding algorithm. in fixed i.e. non-randomized sample sizes, the algorithm leads to very accurate predictions on a sub-sample of the original sample. we quantify the effect of fixed sample sizes on our prediction intervals and test our methods and others found in the literature against simulated environments, which we devise taking into account datasets from a human-gut and -hand microbiota. our methodology applies to any dataset that can be conceptualized as a sample with replacement from an urn. in particular, it could be applied, for example, to quantify the proportion of all the unseen solutions to a binding site problem in a random rna pool, or to reassess the surveillance of a certain terrorist group, predicting the conditional probability that it deploys a new tactic in a next attack.","10.1371/journal.pone.0021105","2011-09-14","","['manuel lladser', 'raúl gouet', 'jens reeder']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"292",1109.3829,"an adaptive interacting wang-landau algorithm for automatic density   exploration","stat.co stat.ap stat.me","while statisticians are well-accustomed to performing exploratory analysis in the modeling stage of an analysis, the notion of conducting preliminary general-purpose exploratory analysis in the monte carlo stage (or more generally, the model-fitting stage) of an analysis is an area which we feel deserves much further attention. towards this aim, this paper proposes a general-purpose algorithm for automatic density exploration. the proposed exploration algorithm combines and expands upon components from various adaptive markov chain monte carlo methods, with the wang-landau algorithm at its heart. additionally, the algorithm is run on interacting parallel chains -- a feature which both decreases computational cost as well as stabilizes the algorithm, improving its ability to explore the density. performance is studied in several applications. through a bayesian variable selection example, the authors demonstrate the convergence gains obtained with interacting chains. the ability of the algorithm's adaptive proposal to induce mode-jumping is illustrated through a trimodal density and a bayesian mixture modeling application. lastly, through a 2d ising model, the authors demonstrate the ability of the algorithm to overcome the high correlations encountered in spatial models.","","2011-09-17","2012-06-14","['luke bornn', 'pierre jacob', 'pierre del moral', 'arnaud doucet']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"293",1109.4389,"mixtures of conditional gaussian scale mixtures applied to multiscale   image representations","stat.ml","we present a probabilistic model for natural images which is based on gaussian scale mixtures and a simple multiscale representation. in contrast to the dominant approach to modeling whole images focusing on markov random fields, we formulate our model in terms of a directed graphical model. we show that it is able to generate images with interesting higher-order correlations when trained on natural images or samples from an occlusion based model. more importantly, the directed model enables us to perform a principled evaluation. while it is easy to generate visually appealing images, we demonstrate that our model also yields the best performance reported to date when evaluated with respect to the cross-entropy rate, a measure tightly linked to the average log-likelihood.","10.1371/journal.pone.0039857","2011-09-20","","['lucas theis', 'reshad hosseini', 'matthias bethge']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"294",1109.4777,"beta-product poisson-dirichlet processes","math.st math.pr stat.co stat.th","time series data may exhibit clustering over time and, in a multiple time series context, the clustering behavior may differ across the series. this paper is motivated by the bayesian non--parametric modeling of the dependence between the clustering structures and the distributions of different time series. we follow a dirichlet process mixture approach and introduce a new class of multivariate dependent dirichlet processes (ddp). the proposed ddp are represented in terms of vector of stick-breaking processes with dependent weights. the weights are beta random vectors that determine different and dependent clustering effects along the dimension of the ddp vector. we discuss some theoretical properties and provide an efficient monte carlo markov chain algorithm for posterior computation. the effectiveness of the method is illustrated with a simulation study and an application to the united states and the european union industrial production indexes.","","2011-09-22","","['federico bassetti', 'roberto casarin', 'fabrizio leisen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE
"295",1109.6804,"comparing probabilistic models for melodic sequences","stat.ml","modelling the real world complexity of music is a challenge for machine learning. we address the task of modeling melodic sequences from the same music genre. we perform a comparative analysis of two probabilistic models; a dirichlet variable length markov model (dirichlet-vmm) and a time convolutional restricted boltzmann machine (tc-rbm). we show that the tc-rbm learns descriptive music features, such as underlying chords and typical melody transitions and dynamics. we assess the models for future prediction and compare their performance to a vmm, which is the current state of the art in melody generation. we show that both models perform significantly better than the vmm, with the dirichlet-vmm marginally outperforming the tc-rbm. finally, we evaluate the short order statistics of the models, using the kullback-leibler divergence between test sequences and model samples, and show that our proposed methods match the statistics of the music genre significantly better than the vmm.","","2011-09-30","","['athina spiliopoulou', 'amos storkey']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"296",1110.0721,"properties and applications of fisher distribution on the rotation group","stat.me stat.co","we study properties of fisher distribution (von mises-fisher distribution, matrix langevin distribution) on the rotation group so(3). in particular we apply the holonomic gradient descent, introduced by nakayama et al. (2011), and a method of series expansion for evaluating the normalizing constant of the distribution and for computing the maximum likelihood estimate. the rotation group can be identified with the stiefel manifold of two orthonormal vectors. therefore from the viewpoint of statistical modeling, it is of interest to compare fisher distributions on these manifolds. we illustrate the difference with an example of near-earth objects data.","","2011-10-04","2013-02-03","['tomonari sei', 'hiroki shibata', 'akimichi takemura', 'katsuyoshi ohara', 'nobuki takayama']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"297",1110.2388,"selectivity in probabilistic causality: where psychology runs into   quantum physics","physics.data-an math.st q-bio.qm quant-ph stat.th","given a set of several inputs into a system (e.g., independent variables characterizing stimuli) and a set of several stochastically non-independent outputs (e.g., random variables describing different aspects of responses), how can one determine, for each of the outputs, which of the inputs it is influenced by? the problem has applications ranging from modeling pairwise comparisons to reconstructing mental processing architectures to conjoint testing. a necessary and sufficient condition for a given pattern of selective influences is provided by the joint distribution criterion, according to which the problem of ""what influences what"" is equivalent to that of the existence of a joint distribution for a certain set of random variables. for inputs and outputs with finite sets of values this criterion translates into a test of consistency of a certain system of linear equations and inequalities (linear feasibility test) which can be performed by means of linear programming. while new in the behavioral context, both this test and the joint distribution criterion on which it is based have been previously proposed in quantum physics, in dealing with generalizations of bell inequalities for the quantum entanglement problem. the parallels between this problem and that of selective influences in behavioral sciences is established by observing that noncommuting measurements in quantum physics are mutually exclusive and can therefore be treated as different levels of one and the same factor.","","2011-10-10","2012-01-13","['ehtibar n. dzhafarov', 'janne v. kujala']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"298",1110.2436,"an mdl framework for sparse coding and dictionary learning","cs.it math.it stat.ml","the power of sparse signal modeling with learned over-complete dictionaries has been demonstrated in a variety of applications and fields, from signal processing to statistical inference and machine learning. however, the statistical properties of these models, such as under-fitting or over-fitting given sets of data, are still not well characterized in the literature. as a result, the success of sparse modeling depends on hand-tuning critical parameters for each data and application. this work aims at addressing this by providing a practical and objective characterization of sparse models by means of the minimum description length (mdl) principle -- a well established information-theoretic approach to model selection in statistical inference. the resulting framework derives a family of efficient sparse coding and dictionary learning algorithms which, by virtue of the mdl principle, are completely parameter free. furthermore, such framework allows to incorporate additional prior information to existing models, such as markovian dependencies, or to define completely new problem formulations, including in the matrix analysis area, in a natural way. these virtues will be demonstrated with parameter-free algorithms for the classic image denoising and classification problems, and for low-rank matrix recovery in video applications.","10.1109/tsp.2012.2187203","2011-10-11","","['ignacio ramírez', 'guillermo sapiro']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"299",1110.2868,"anomalous diffusion models: different types of subordinator distribution","math-ph math.mp stat.me","subordinated processes play an important role in modeling anomalous diffusion-type behavior. in such models the observed constant time periods are described by the subordinator distribution. therefore, on the basis of the observed time series, it is possible to conclude on the main properties of the subordinator. in this paper we analyze the anomalous diffusion models with three types of subordinator distribution: \alpha-stable, tempered stable and gamma. we present similarities and differences between the analyzed processes and point at their main properties (like the behavior of moments or the mean squared displacement).","","2011-10-13","","['joanna janczura', 'agnieszka wyłomańska']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"300",1110.3151,"minimum penalized hellinger distance for model selection in small   samples","stat.me","in statistical modeling area, the akaike information criterion aic, is a widely known and extensively used tool for model choice. the {\phi}-divergence test statistic is a recently developed tool for statistical model selection. the popularity of the divergence criterion is however tempered by their known lack of robustness in small sample. in this paper the penalized minimum hellinger distance type statistics are considered and some properties are established. the limit laws of the estimates and test statistics are given under both the null and the alternative hypotheses, and approximations of the power functions are deduced. a model selection criterion relative to these divergence measures are developed for parametric inference. our interest is in the problem to testing for choosing between two models using some informational type statistics, when independent sample are drawn from a discrete population. here, we discuss the asymptotic properties and the performance of new procedure tests and investigate their small sample behavior.","","2011-10-14","2011-10-27","['papa ngom', 'bertrand ntep']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"301",1110.5454,"distance dependent infinite latent feature models","stat.ml math.st stat.th","latent feature models are widely used to decompose data into a small number of components. bayesian nonparametric variants of these models, which use the indian buffet process (ibp) as a prior over latent features, allow the number of features to be determined from the data. we present a generalization of the ibp, the distance dependent indian buffet process (dd-ibp), for modeling non-exchangeable data. it relies on distances defined between data points, biasing nearby data to share more features. the choice of distance measure allows for many kinds of dependencies, including temporal and spatial. further, the original ibp is a special case of the dd-ibp. in this paper, we develop the dd-ibp and theoretically characterize its feature-sharing properties. we derive a markov chain monte carlo sampler for a linear gaussian model with a dd-ibp prior and study its performance on several non-exchangeable data sets.","","2011-10-25","2012-09-10","['samuel j. gershman', 'peter i. frazier', 'david m. blei']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"302",1110.5789,"an empirical test for eurozone contagion using an asset-pricing model   with heavy-tailed stochastic volatility","q-fin.st stat.ap","this paper proposes an empirical test of financial contagion in european equity markets during the tumultuous period of 2008-2011. our analysis shows that traditional garch and gaussian stochastic-volatility models are unable to explain two key stylized features of global markets during presumptive contagion periods: shocks to aggregate market volatility can be sudden and explosive, and they are associated with specific directional biases in the cross-section of country-level returns. our model repairs this deficit by assuming that the random shocks to volatility are heavy-tailed and correlated cross-sectionally, both with each other and with returns. the fundamental conclusion of our analysis is that great care is needed in modeling volatility if one wishes to characterize the relationship between volatility and contagion that is predicted by economic theory.   in analyzing daily data, we find evidence for significant contagion effects during the major eu crisis periods of may 2010 and august 2011, where contagion is defined as excess correlation in the residuals from a factor model incorporating global and regional market risk factors. some of this excess correlation can be explained by quantifying the impact of shocks to aggregate volatility in the cross-section of expected returns - but only, it turns out, if one is extremely careful in accounting for the explosive nature of these shocks. we show that global markets have time-varying cross-sectional sensitivities to these shocks, and that high sensitivities strongly predict periods of financial crisis. moreover, the pattern of temporal changes in correlation structure between volatility and returns is readily interpretable in terms of the major events of the periods in question.","","2011-10-26","2012-03-26","['nicholas g. polson', 'james g. scott']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"303",1110.6631,"new first trimester crown-rump length's equations optimized by   structured data collection from a french general population","stat.ap","--- objectives --- prior to foetal karyotyping, the likelihood of down's syndrome is often determined combining maternal age, serum free beta-hcg, papp-a levels and embryonic measurements of crown-rump length and nuchal translucency for gestational ages between 11 and 13 weeks. it appeared important to get a precise knowledge of these scan parameters' normal values during the first trimester. this paper focused on crown-rump length. --- methods --- 402 pregnancies from in-vitro fertilization allowing a precise estimation of foetal ages (fa) were used to determine the best model that describes crown-rump length (crl) as a function of fa. scan measures by a single operator from 3846 spontaneous pregnancies representative of the general population from northern france were used to build a mathematical model linking fa and crl in a context as close as possible to normal scan screening used in down's syndrome likelihood determination. we modeled both crl as a function of fa and fa as a function of crl. for this, we used a clear methodology and performed regressions with heteroskedastic corrections and robust regressions. the results were compared by cross-validation to retain the equations with the best predictive power. we also studied the errors between observed and predicted values. --- results --- data from 513 spontaneous pregnancies allowed to model crl as a function of age of foetal age. the best model was a polynomial of degree 2. datation with our equation that models spontaneous pregnancies from a general population was in quite agreement with objective datations obtained from 402 ivf pregnancies and thus support the validity of our model. the most precise measure of crl was when the sd was minimal (1.83mm), for a crl of 23.6 mm where our model predicted a 49.4 days of foetal age. our study allowed to model the sd from 30 to 90 days of foetal age and offers the opportunity of using zscores in the future to detect growth abnormalities. --- conclusion --- with powerful statistical tools we report a good modeling of the first trimester embryonic growth in the general population allowing a better knowledge of the date of fertilization useful in the ultrasound screening of down's syndrome. the optimal period to measure crl and predict foetal age was 49.4 days (9 weeks of gestational age). our results open the way to the detection of foetal growth abnormalities using crl zscores throughout the first trimester.","10.1159/000339272","2011-10-30","","['marc constant', 'viet chi tran', 'bernard benoît', 'francis vasseur']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"304",1111.0352,"revisiting k-means: new algorithms via bayesian nonparametrics","cs.lg stat.ml","bayesian models offer great flexibility for clustering applications---bayesian nonparametrics can be used for modeling infinite mixtures, and hierarchical bayesian models can be utilized for sharing clusters across multiple data sets. for the most part, such flexibility is lacking in classical clustering methods such as k-means. in this paper, we revisit the k-means clustering algorithm from a bayesian nonparametric viewpoint. inspired by the asymptotic connection between k-means and mixtures of gaussians, we show that a gibbs sampling algorithm for the dirichlet process mixture approaches a hard clustering algorithm in the limit, and further that the resulting algorithm monotonically minimizes an elegant underlying k-means-like clustering objective that includes a penalty for the number of clusters. we generalize this analysis to the case of clustering multiple data sets through a similar asymptotic argument with the hierarchical dirichlet process. we also discuss further extensions that highlight the benefits of our analysis: i) a spectral relaxation involving thresholded eigenvectors, and ii) a normalized cut graph clustering algorithm that does not fix the number of clusters in the graph.","","2011-11-01","2012-06-14","['brian kulis', 'michael i. jordan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"305",1111.1788,"robust pca as bilinear decomposition with outlier-sparsity   regularization","stat.ml cs.it math.it","principal component analysis (pca) is widely used for dimensionality reduction, with well-documented merits in various applications involving high-dimensional data, including computer vision, preference measurement, and bioinformatics. in this context, the fresh look advocated here permeates benefits from variable selection and compressive sampling, to robustify pca against outliers. a least-trimmed squares estimator of a low-rank bilinear factor analysis model is shown closely related to that obtained from an $\ell_0$-(pseudo)norm-regularized criterion encouraging sparsity in a matrix explicitly modeling the outliers. this connection suggests robust pca schemes based on convex relaxation, which lead naturally to a family of robust estimators encompassing huber's optimal m-class as a special case. outliers are identified by tuning a regularization parameter, which amounts to controlling sparsity of the outlier matrix along the whole robustification path of (group) least-absolute shrinkage and selection operator (lasso) solutions. beyond its neat ties to robust statistics, the developed outlier-aware pca framework is versatile to accommodate novel and scalable algorithms to: i) track the low-rank signal subspace robustly, as new data are acquired in real time; and ii) determine principal components robustly in (possibly) infinite-dimensional feature spaces. synthetic and real data tests corroborate the effectiveness of the proposed robust pca schemes, when used to identify aberrant responses in personality assessment surveys, as well as unveil communities in social networks, and intruders from video surveillance data.","10.1109/tsp.2012.2204986","2011-11-07","","['gonzalo mateos', 'georgios b. giannakis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"306",1111.273,"a statistical and computational theory for robust and sparse kalman   smoothing","math.oc math.st stat.ap stat.co stat.th","kalman smoothers reconstruct the state of a dynamical system starting from noisy output samples. while the classical estimator relies on quadratic penalization of process deviations and measurement errors, extensions that exploit piecewise linear quadratic (plq) penalties have been recently proposed in the literature. these new formulations include smoothers robust with respect to outliers in the data, and smoothers that keep better track of fast system dynamics, e.g. jumps in the state values. in addition to l2, well known examples of plq penalties include the l1, huber and vapnik losses. in this paper, we use a dual representation for plq penalties to build a statistical modeling framework and a computational theory for kalman smoothing.   we develop a statistical framework by establishing conditions required to interpret plq penalties as negative logs of true probability densities. then, we present a computational framework, based on interior-point methods, that solves the kalman smoothing problem with plq penalties and maintains the linear complexity in the size of the time series, just as in the l2 case. the framework presented extends the computational efficiency of the mayne-fraser and rauch-tung-striebel algorithms to a much broader non-smooth setting, and includes many known robust and sparse smoothers as special cases.","","2011-11-11","","['aleksandr y. aravkin', 'james v. burke', 'gianluigi pillonetto']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE
"307",1111.3149,"cmb map restoration","astro-ph.co stat.ap","estimating the cosmological microwave background is of utmost importance for cosmology. however, its estimation from full-sky surveys such as wmap or more recently planck is challenging: cmb maps are generally estimated via the application of some source separation techniques which never prevent the final map from being contaminated with noise and foreground residuals. these spurious contaminations whether noise or foreground residuals are well-known to be a plague for most cosmologically relevant tests or evaluations; this includes cmb lensing reconstruction or non-gaussian signatures search. noise reduction is generally performed by applying a simple wiener filter in spherical harmonics; however this does not account for the non-stationarity of the noise. foreground contamination is usually tackled by masking the most intense residuals detected in the map, which makes cmb evaluation harder to perform. in this paper, we introduce a novel noise reduction framework coined liw-filtering for linear iterative wavelet filtering which is able to account for the noise spatial variability thanks to a wavelet-based modeling while keeping the highly desired linearity of the wiener filter. we further show that the same filtering technique can effectively perform foreground contamination reduction thus providing a globally cleaner cmb map. numerical results on simulated but realistic planck data are provided.","10.1155/2012/703217","2011-11-14","","['j. bobin', 'j. -l. starck', 'f. sureau', 'j. fadili']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"308",1111.4226,"joint modeling of multiple related time series via the beta process","stat.me stat.ml","we propose a bayesian nonparametric approach to the problem of jointly modeling multiple related time series. our approach is based on the discovery of a set of latent, shared dynamical behaviors. using a beta process prior, the size of the set and the sharing pattern are both inferred from data. we develop efficient markov chain monte carlo methods based on the indian buffet process representation of the predictive distribution of the beta process, without relying on a truncated model. in particular, our approach uses the sum-product algorithm to efficiently compute metropolis-hastings acceptance probabilities, and explores new dynamical behaviors via birth and death proposals. we examine the benefits of our proposed feature-based model on several synthetic datasets, and also demonstrate promising results on unsupervised segmentation of visual motion capture data.","","2011-11-17","","['emily b. fox', 'erik b. sudderth', 'michael i. jordan', 'alan s. willsky']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"309",1111.4296,"conditional modeling and the jitter method of spike re-sampling:   supplement","stat.me","this technical report accompanies the manuscript ""conditional modeling and the jitter method of spike re-sampling."" it contains further details, comments, references, and equations concerning various simulations and data analyses presented in that manuscript, as well as a self-contained mathematical appendix that provides a formal treatment of jitter-based spike re-sampling methods.","","2011-11-18","","['asohan amarasingham', 'matthew t. harrison', 'nicholas g. hatsopoulos', 'stuart geman']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"310",1111.4639,"cancer gene prioritization by integrative analysis of mrna expression   and dna copy number data: a comparative review","cs.ce q-bio.gn stat.ap stat.me","a variety of genome-wide profiling techniques are available to probe complementary aspects of genome structure and function. integrative analysis of heterogeneous data sources can reveal higher-level interactions that cannot be detected based on individual observations. a standard integration task in cancer studies is to identify altered genomic regions that induce changes in the expression of the associated genes based on joint analysis of genome-wide gene expression and copy number profiling measurements. in this review, we provide a comparison among various modeling procedures for integrating genome-wide profiling data of gene copy number and transcriptional alterations and highlight common approaches to genomic data integration. a transparent benchmarking procedure is introduced to quantitatively compare the cancer gene prioritization performance of the alternative methods. the benchmarking algorithms and data sets are available at http://intcomp.r-forge.r-project.org","10.1093/bib/bbs005","2011-11-20","","['leo lahti', 'martin schäfer', 'hans-ulrich klein', 'silvio bicciato', 'martin dugas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"311",1111.6822,"optimal phase transitions in compressed sensing","cs.it math.it math.st stat.th","compressed sensing deals with efficient recovery of analog signals from linear encodings. this paper presents a statistical study of compressed sensing by modeling the input signal as an i.i.d. process with known distribution. three classes of encoders are considered, namely optimal nonlinear, optimal linear and random linear encoders. focusing on optimal decoders, we investigate the fundamental tradeoff between measurement rate and reconstruction fidelity gauged by error probability and noise sensitivity in the absence and presence of measurement noise, respectively. the optimal phase transition threshold is determined as a functional of the input distribution and compared to suboptimal thresholds achieved by popular reconstruction algorithms. in particular, we show that gaussian sensing matrices incur no penalty on the phase transition threshold with respect to optimal nonlinear encoding. our results also provide a rigorous justification of previous results based on replica heuristics in the weak-noise regime.","","2011-11-29","2012-07-10","['yihong wu', 'sergio verdú']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"312",1111.6925,"structure learning of probabilistic graphical models: a comprehensive   survey","stat.ml cs.lg","probabilistic graphical models combine the graph theory and probability theory to give a multivariate statistical modeling. they provide a unified description of uncertainty using probability and complexity using the graphical model. especially, graphical models provide the following several useful properties:   - graphical models provide a simple and intuitive interpretation of the structures of probabilistic models. on the other hand, they can be used to design and motivate new models.   - graphical models provide additional insights into the properties of the model, including the conditional independence properties.   - complex computations which are required to perform inference and learning in sophisticated models can be expressed in terms of graphical manipulations, in which the underlying mathematical expressions are carried along implicitly.   the graphical models have been applied to a large number of fields, including bioinformatics, social science, control theory, image processing, marketing analysis, among others. however, structure learning for graphical models remains an open challenge, since one must cope with a combinatorial search over the space of all possible structures.   in this paper, we present a comprehensive survey of the existing structure learning algorithms.","","2011-11-29","","['yang zhou']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"313",1111.7091,"spatial modeling of extreme snow depth","stat.ap","the spatial modeling of extreme snow is important for adequate risk management in alpine and high altitude countries. a natural approach to such modeling is through the theory of max-stable processes, an infinite-dimensional extension of multivariate extreme value theory. in this paper we describe the application of such processes in modeling the spatial dependence of extreme snow depth in switzerland, based on data for the winters 1966--2008 at 101 stations. the models we propose rely on a climate transformation that allows us to account for the presence of climate regions and for directional effects, resulting from synoptic weather patterns. estimation is performed through pairwise likelihood inference and the models are compared using penalized likelihood criteria. the max-stable models provide a much better fit to the joint behavior of the extremes than do independence or full dependence models.","10.1214/11-aoas464","2011-11-30","","['juliette blanchet', 'anthony c. davison']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"314",1111.712,"a space--time varying coefficient model: the equity of service   accessibility","stat.ap","research in examining the equity of service accessibility has emerged as economic and social equity advocates recognized that where people live influences their opportunities for economic development, access to quality health care and political participation. in this research paper service accessibility equity is concerned with where and when services have been and are accessed by different groups of people, identified by location or underlying socioeconomic variables. using new statistical methods for modeling spatial-temporal data, this paper estimates demographic association patterns to financial service accessibility varying over a large geographic area (georgia) and over a period of 13 years. the underlying model is a space--time varying coefficient model including both separable space and time varying coefficients and space--time interaction terms. the model is extended to a multilevel response where the varying coefficients account for both the within- and between-variability. we introduce an inference procedure for assessing the shape of the varying regression coefficients using confidence bands.","10.1214/11-aoas473","2011-11-30","","['nicoleta serban']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"315",1112.0698,"machine learning with operational costs","stat.ml cs.ai math.oc","this work proposes a way to align statistical modeling with decision making. we provide a method that propagates the uncertainty in predictive modeling to the uncertainty in operational cost, where operational cost is the amount spent by the practitioner in solving the problem. the method allows us to explore the range of operational costs associated with the set of reasonable statistical models, so as to provide a useful way for practitioners to understand uncertainty. to do this, the operational cost is cast as a regularization term in a learning algorithm's objective function, allowing either an optimistic or pessimistic view of possible costs, depending on the regularization parameter. from another perspective, if we have prior knowledge about the operational cost, for instance that it should be low, this knowledge can help to restrict the hypothesis space, and can help with generalization. we provide a theoretical generalization bound for this scenario. we also show that learning with operational costs is related to robust optimization.","","2011-12-03","2013-06-18","['theja tulabandhula', 'cynthia rudin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"316",1112.0712,"estimation and inference for high-dimensional non-sparse models","stat.me math.st stat.th","to successfully work on variable selection, sparse model structure has become a basic assumption for all existing methods. however, this assumption is questionable as it is hard to hold in most of cases and none of existing methods may provide consistent estimation and accurate model prediction in nons-parse scenarios. in this paper, we propose semiparametric re-modeling and inference when the linear regression model under study is possibly non-sparse. after an initial working model is selected by a method such as the dantzig selector adopted in this paper, we re-construct a globally unbiased semiparametric model by use of suitable instrumental variables and nonparametric adjustment. the newly defined model is identifiable, and the estimator of parameter vector is asymptotically normal. the consistency, together with the re-built model, promotes model prediction. this method naturally works when the model is indeed sparse and thus is of robustness against non-sparseness in certain sense. simulation studies show that the new approach has, particularly when $p$ is much larger than $n$, significant improvement of estimation and prediction accuracies over the gaussian dantzig selector and other classical methods. even when the model under study is sparse, our method is also comparable to the existing methods designed for sparse models.","","2011-12-03","","['lu lin', 'lixing zhu', 'yujie gai']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"317",1112.084,"on the question of effective sample size in network modeling: an   asymptotic inquiry","math.st stat.me stat.th","the modeling and analysis of networks and network data has seen an explosion of interest in recent years and represents an exciting direction for potential growth in statistics. despite the already substantial amount of work done in this area to date by researchers from various disciplines, however, there remain many questions of a decidedly foundational nature - natural analogues of standard questions already posed and addressed in more classical areas of statistics - that have yet to even be posed, much less addressed. here we raise and consider one such question in connection with network modeling. specifically, we ask, ""given an observed network, what is the sample size?"" using simple, illustrative examples from the class of exponential random graph models, we show that the answer to this question can very much depend on basic properties of the networks expected under the model, as the number of vertices $n_v$ in the network grows. in particular, adopting the (asymptotic) scaling of the variance of the maximum likelihood parameter estimates as a notion of effective sample size ($n_{\mathrm{eff}}$), we show that when modeling the overall propensity to have ties and the propensity to reciprocate ties, whether the networks are sparse or not under the model (i.e., having a constant or an increasing number of ties per vertex, respectively) is sufficient to yield an order of magnitude difference in $n_{\mathrm{eff}}$, from $o(n_v)$ to $o(n^2_v)$. in addition, we report simulation study results that suggest similar properties for models for triadic (friend-of-a-friend) effects. we then explore some practical implications of this result, using both simulation and data on food-sharing from lamalera, indonesia.","10.1214/14-sts502","2011-12-05","2015-08-05","['pavel n. krivitsky', 'eric d. kolaczyk']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"318",1112.149,"fragility index of block tailed vectors","math.st stat.th","financial crises are a recurrent phenomenon with important effects on the real economy. the financial system is inherently fragile and it is therefore of great importance to be able to measure and characterize its systemic stability. multivariate extreme value theory provide us such a framework through the \emph{fragility index} (geluk \cite{gel+}, \emph{et al.}, 2007; falk and tichy, \cite{falk+tichy1,falk+tichy2} 2010, 2011). here we generalize this concept and contribute to the modeling of the stability of a stochastic system divided into blocks. we will find several relations with well-known tail dependence measures in literature, which will provide us immediate estimators. we end with an application to financial data.","","2011-12-07","","['helena ferreira', 'marta ferreira']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"319",1112.2093,"green's function based unparameterised multi-dimensional kernel density   and likelihood ratio estimator","stat.ml math.st stat.th","this paper introduces a probability density estimator based on green's function identities. a density model is constructed under the sole assumption that the probability density is differentiable. the method is implemented as a binary likelihood estimator for classification purposes, so issues such as mis-modeling and overtraining are also discussed. the identity behind the density estimator can be interpreted as a real-valued, non-scalar kernel method which is able to reconstruct differentiable density functions.","10.1088/1742-6596/368/1/012041","2011-12-09","2012-04-18","['peter kovesarki', 'ian c. brock', 'a. elizabeth nuncio quiroz']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"320",1112.3652,"understanding better (some) astronomical data using bayesian methods","astro-ph.im astro-ph.co astro-ph.sr physics.data-an stat.ap","current analysis of astronomical data are confronted with the daunting task of modeling the awkward features of astronomical data, among which heteroscedastic (point-dependent) errors, intrinsic scatter, non-ignorable data collection (selection effects), data structure, non-uniform populations (often called malmquist bias), non-gaussian data, and upper/lower limits. this chapter shows, by examples, how modeling all these features using bayesian methods. in short, one just need to formalize, using maths, the logical link between the involved quantities, how the data arise and what we already known on the quantities we want to study. the posterior probability distribution summarizes what we known on the studied quantities after the data, and we should not be afraid about their actual numerical computation, because it is left to (special) monte carlo programs such as jags. as examples, we show how to predict the mass of a new object disposing of a calibrating sample, how to constraint cosmological parameters from supernovae data and how to check if the fitted data are in tension with the adopted fitting model. examples are given with their coding. these examples can be easily used as template for completely different analysis, on totally unrelated astronomical objects, requiring to model the same awkward data features.","","2011-12-15","","['s. andreon']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"321",1112.3891,"conditional simulations of brown-resnick processes","stat.me","since many environmental processes such as heat waves or precipitation are spatial in extent, it is likely that a single extreme event affects several locations and the areal modeling of extremes is therefore essential if the spatial dependence of extremes has to be appropriately taken into account. although some progress has been made to develop a geostatistic of extremes, conditional simulation of max-stable processes is still in its early stage. this paper proposes a framework to get conditional simulations of brown-resnick processes. although closed forms for the regular conditional distribution of brown-resnick processes were recently found, sampling from this conditional distribution is a considerable challenge as it leads quickly to a combinatorial explosion. to bypass this computational burden, a markov chain monte-carlo algorithm is presented. we test the method on simulated data and give an application to extreme rainfall around zurich. results show that the proposed framework provides accurate conditional simulations of brown-resnick processes and can handle real-sized problems.","","2011-12-16","2012-08-27","['clément dombry', 'frédéric éyi-minko', 'mathieu ribatet']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"322",1112.418,"generalization of the normal-exponential model: exploration of a more   accurate parametrisation for the signal distribution on illumina beadarrays","stat.ap","motivation: illumina beadarray technology includes negative control features that allow a precise estimation of the background noise. as an alternative to the background subtraction proposed in beadstudio which leads to an important loss of information by generating negative values, a background correction method modeling the observed intensities as the sum of the exponentially distributed signal and normally distributed noise has been developed. nevertheless, wang and ye (2011) display a kernel-based estimator of the signal distribution on illumina beadarrays and suggest that a gamma distribution would represent a better modeling of the signal density. hence, the normal-exponential modeling may not be appropriate for illumina data and background corrections derived from this model may lead to wrong estimation. results: we propose a more flexible modeling based on a gamma distributed signal and a normal distributed background noise and develop the associated background correction. our model proves to be markedly more accurate to model illumina beadarrays: on the one hand, this model offers a more correct fit of the observed intensities. on the other hand, the comparison of the operating characteristics of several background correction procedures on spike-in and on normal-gamma simulated data shows high similarities, reinforcing the validation of the normal-gamma modeling. the performance of the background corrections based on the normal-gamma and normal-exponential models are compared on two dilution data sets. surprisingly, we observe that the implementation of a more accurate parametrisation in the model-based background correction does not increase the sensitivity. these results may be explained by the operating characteristics of the estimators: the normal-gamma background correction offers an improvement in terms of bias, but at the cost of a loss in precision.","","2011-12-18","2012-09-17","['sandra plancade', 'yves rozenholc', 'eiliv lund']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"323",1112.5635,"bayesian model choice and information criteria in sparse generalized   linear models","math.st stat.th","we consider bayesian model selection in generalized linear models that are high-dimensional, with the number of covariates p being large relative to the sample size n, but sparse in that the number of active covariates is small compared to p. treating the covariates as random and adopting an asymptotic scenario in which p increases with n, we show that bayesian model selection using certain priors on the set of models is asymptotically equivalent to selecting a model using an extended bayesian information criterion. moreover, we prove that the smallest true model is selected by either of these methods with probability tending to one. having addressed random covariates, we are also able to give a consistency result for pseudo-likelihood approaches to high-dimensional sparse graphical modeling. experiments on real data demonstrate good performance of the extended bayesian information criterion for regression and for graphical models.","","2011-12-23","","['rina foygel', 'mathias drton']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"324",1201.0794,"sparse nonparametric graphical models","stat.ml cs.lg stat.me","we present some nonparametric methods for graphical modeling. in the discrete case, where the data are binary or drawn from a finite alphabet, markov random fields are already essentially nonparametric, since the cliques can take only a finite number of values. continuous data are different. the gaussian graphical model is the standard parametric model for continuous data, but it makes distributional assumptions that are often unrealistic. we discuss two approaches to building more flexible graphical models. one allows arbitrary graphs and a nonparametric extension of the gaussian; the other uses kernel density estimation and restricts the graphs to trees and forests. examples of both methods are presented. we also discuss possible future research directions for nonparametric graphical modeling.","10.1214/12-sts391","2012-01-03","2013-01-07","['john lafferty', 'han liu', 'larry wasserman']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"325",1201.1356,"discussion of ""feature matching in time series modeling"" by y. xia and   h. tong","stat.me","discussion of ""feature matching in time series modeling"" by y. xia and h. tong [arxiv:1104.3073]","10.1214/11-sts345a","2012-01-06","","['bruce e. hansen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"326",1201.1367,"discussion of ""feature matching in time series modeling"" by y. xia and   h. tong","stat.me","discussion of ""feature matching in time series modeling"" by y. xia and h. tong [arxiv:1104.3073]","10.1214/11-sts345b","2012-01-06","","['kung-sik chan', 'ruey s. tsay']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"327",1201.1373,"discussion of ""feature matching in time series modeling"" by y. xia and   h. tong","stat.me","discussion of ""feature matching in time series modeling"" by y. xia and h. tong [arxiv:1104.3073]","10.1214/11-sts345c","2012-01-06","","['edward l. ionides']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"328",1201.1376,"discussion of ""feature matching in time series modeling"" by y. xia and   h. tong","stat.me","discussion of ""feature matching in time series modeling"" by y. xia and h. tong [arxiv:1104.3073]","10.1214/11-sts345d","2012-01-06","","['qiwei yao']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"329",1201.1379,"rejoinder to ""feature matching in time series modeling""","stat.me","rejoinder to ""feature matching in time series modeling"" by y. xia and h. tong [arxiv:1104.3073]","10.1214/11-sts345rej","2012-01-06","","['yingcun xia', 'howell tong']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"330",1201.145,"the interaction of entropy-based discretization and sample size: an   empirical study","stat.ml cs.lg","an empirical investigation of the interaction of sample size and discretization - in this case the entropy-based method caim (class-attribute interdependence maximization) - was undertaken to evaluate the impact and potential bias introduced into data mining performance metrics due to variation in sample size as it impacts the discretization process. of particular interest was the effect of discretizing within cross-validation folds averse to outside discretization folds. previous publications have suggested that discretizing externally can bias performance results; however, a thorough review of the literature found no empirical evidence to support such an assertion. this investigation involved construction of over 117,000 models on seven distinct datasets from the uci (university of california-irvine) machine learning library and multiple modeling methods across a variety of configurations of sample size and discretization, with each unique ""setup"" being independently replicated ten times. the analysis revealed a significant optimistic bias as sample sizes decreased and discretization was employed. the study also revealed that there may be a relationship between the interaction that produces such bias and the numbers and types of predictor attributes, extending the ""curse of dimensionality"" concept from feature selection into the discretization realm. directions for further exploration are laid out, as well some general guidelines about the proper application of discretization in light of these results.","","2012-01-06","","['casey bennett']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"331",1201.1658,"bayesian hierarchical modeling of simply connected 2d shapes","stat.me","models for distributions of shapes contained within images can be widely used in biomedical applications ranging from tumor tracking for targeted radiation therapy to classifying cells in a blood sample. our focus is on hierarchical probability models for the shape and size of simply connected 2d closed curves, avoiding the need to specify landmarks through modeling the entire curve while borrowing information across curves for related objects. prevalent approaches follow a fundamentally different strategy in providing an initial point estimate of the curve and/or locations of landmarks, which are then fed into subsequent statistical analyses. such two-stage methods ignore uncertainty in the first stage, and do not allow borrowing of information across objects in estimating object shapes and sizes. our fully bayesian hierarchical model is based on multiscale deformations within a linear combination of cyclic basis characterization, which facilitates automatic alignment of the different curves accounting for uncertainty. the characterization is shown to be highly flexible in representing 2d closed curves, leading to a nonparametric bayesian prior with large support. efficient markov chain monte carlo methods are developed for simultaneous analysis of many objects. the methods are evaluated through simulation examples and applied to yeast cell imaging data.","","2012-01-08","","['kelvin gu', 'debdeep pati', 'david b. dunson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"332",1201.2375,"mixed beta regression: a bayesian perspective","stat.co","this paper builds on recent research that focuses on regression modeling of continuous bounded data, such as proportions measured on a continuous scale. specifically, it deals with beta regression models with mixed effects from a bayesian approach. we use a suitable parameterization of the beta law in terms of its mean and a precision parameter, and allow both parameters to be modeled through regression structures that may involve fixed and random effects. specification of prior distributions is discussed, computational implementation via gibbs sampling is provided, and illustrative examples are presented.","","2012-01-11","2012-11-14","['jorge i. figueroa-zuñiga', 'reinaldo b. arellano-valle', 'silvia l. p. ferrari']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"333",1201.4114,"sines, steps and droplets: semiparametric bayesian modeling of arrival   time series","astro-ph.im stat.ap","i describe ongoing work developing bayesian methods for flexible modeling of arrival time series data without binning, aiming to improve detection and measurement of x-ray and gamma-ray pulsars, and of pulses in gamma-ray bursts. the methods use parametric and semiparametric poisson point process models for the event rate, and by design have close connections to conventional frequentist methods currently used in time-domain astronomy.","10.1017/s1743921312000300","2012-01-19","","['thomas j. loredo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"334",1202.0501,"global modeling of transcriptional responses in interaction networks","q-bio.mn cs.ce q-bio.qm stat.ap stat.ml","motivation: cell-biological processes are regulated through a complex network of interactions between genes and their products. the processes, their activating conditions, and the associated transcriptional responses are often unknown. organism-wide modeling of network activation can reveal unique and shared mechanisms between physiological conditions, and potentially as yet unknown processes. we introduce a novel approach for organism-wide discovery and analysis of transcriptional responses in interaction networks. the method searches for local, connected regions in a network that exhibit coordinated transcriptional response in a subset of conditions. known interactions between genes are used to limit the search space and to guide the analysis. validation on a human pathway network reveals physiologically coherent responses, functional relatedness between physiological conditions, and coordinated, context-specific regulation of the genes. availability: implementation is freely available in r and matlab at http://netpro.r-forge.r-project.org","10.1093/bioinformatics/btq500","2012-02-02","","['leo lahti', 'juha e. a. knuuttila', 'samuel kaski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"335",1202.133,"a dual modelling of evolving political opinion networks","physics.soc-ph cs.si stat.co","we present the result of a dual modeling of opinion network. the model complements the agent-based opinion models by attaching to the social agent (voters) network a political opinion (party) network having its own intrinsic mechanisms of evolution. these two sub-networks form a global network which can be either isolated from or dependent on the external influence. basically, the evolution of the agent network includes link adding and deleting, the opinion changes influenced by social validation, the political climate, the attractivity of the parties and the interaction between them. the opinion network is initially composed of numerous nodes representing opinions or parties which are located on a one dimensional axis according to their political positions. the mechanism of evolution includes union, splitting, change of position and of attractivity, taken into account the pairwise node interaction decaying with node distance in power law. the global evolution ends in a stable distribution of the social agents over a quasi-stable and fluctuating stationary number of remaining parties. empirical study on the lifetime distribution of numerous parties and vote results is carried out to verify numerical results.","10.1103/physreve.84.036108","2012-02-06","","['ru wang', 'qiuping alexandre wang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"336",1202.1425,"fast joint detection-estimation of evoked brain activity in   event-related fmri using a variational approach","stat.ap","in standard clinical within-subject analyses of event-related fmri data, two steps are usually performed separately: detection of brain activity and estimation of the hemodynamic response. because these two steps are inherently linked, we adopt the so-called region-based joint detection-estimation (jde) framework that addresses this joint issue using a multivariate inference for detection and estimation. jde is built by making use of a regional bilinear generative model of the bold response and constraining the parameter estimation by physiological priors using temporal and spatial information in a markovian modeling. in contrast to previous works that use markov chain monte carlo (mcmc) techniques to approximate the resulting intractable posterior distribution, we recast the jde into a missing data framework and derive a variational expectation-maximization (vem) algorithm for its inference. a variational approximation is used to approximate the markovian model in the unsupervised spatially adaptive jde inference, which allows fine automatic tuning of spatial regularisation parameters. it follows a new algorithm that exhibits interesting properties compared to the previously used mcmc-based approach. experiments on artificial and real data show that vem-jde is robust to model mis-specification and provides computational gain while maintaining good performance in terms of activation detection and hemodynamic shape recovery.","","2012-02-07","","['lotfi chaari', 'thomas vincent', 'florence forbes', 'michel dojat', 'philippe ciuciu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"337",1202.1661,"covariance estimation: the glm and regularization perspectives","stat.me","finding an unconstrained and statistically interpretable reparameterization of a covariance matrix is still an open problem in statistics. its solution is of central importance in covariance estimation, particularly in the recent high-dimensional data environment where enforcing the positive-definiteness constraint could be computationally expensive. we provide a survey of the progress made in modeling covariance matrices from two relatively complementary perspectives: (1) generalized linear models (glm) or parsimony and use of covariates in low dimensions, and (2) regularization or sparsity for high-dimensional data. an emerging, unifying and powerful trend in both perspectives is that of reducing a covariance estimation problem to that of estimating a sequence of regression problems. we point out several instances of the regression-based formulation. a notable case is in sparse estimation of a precision matrix or a gaussian graphical model leading to the fast graphical lasso algorithm. some advantages and limitations of the regression-based cholesky decomposition relative to the classical spectral (eigenvalue) and variance-correlation decompositions are highlighted. the former provides an unconstrained and statistically interpretable reparameterization, and guarantees the positive-definiteness of the estimated covariance matrix. it reduces the unintuitive task of covariance estimation to that of modeling a sequence of regressions at the cost of imposing an a priori order among the variables. elementwise regularization of the sample covariance matrix such as banding, tapering and thresholding has desirable asymptotic properties and the sparse estimated covariance matrix is positive definite with probability tending to one for large samples and dimensions.","10.1214/11-sts358","2012-02-08","","['mohsen pourahmadi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"338",1202.2002,"selecting and estimating regular vine copulae and application to   financial returns","stat.me","regular vine distributions which constitute a flexible class of multivariate dependence models are discussed. since multivariate copulae constructed through pair-copula decompositions were introduced to the statistical community, interest in these models has been growing steadily and they are finding successful applications in various fields. research so far has however been concentrating on so-called canonical and d-vine copulae, which are more restrictive cases of regular vine copulae. it is shown how to evaluate the density of arbitrary regular vine specifications. this opens the vine copula methodology to the flexible modeling of complex dependencies even in larger dimensions. in this regard, a new automated model selection and estimation technique based on graph theoretical considerations is presented. this comprehensive search strategy is evaluated in a large simulation study and applied to a 16-dimensional financial data set of international equity, fixed income and commodity indices which were observed over the last decade, in particular during the recent financial crisis. the analysis provides economically well interpretable results and interesting insights into the dependence structure among these indices.","","2012-02-09","2012-04-19","['jeffrey dissmann', 'eike christian brechmann', 'claudia czado', 'dorota kurowicka']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"339",1202.2008,"modeling high dimensional time-varying dependence using d-vine scar   models","stat.me stat.co","we consider the problem of modeling the dependence among many time series. we build high dimensional time-varying copula models by combining pair-copula constructions (pcc) with stochastic autoregressive copula (scar) models to capture dependence that changes over time. we show how the estimation of this highly complex model can be broken down into the estimation of a sequence of bivariate scar models, which can be achieved by using the method of simulated maximum likelihood. further, by restricting the conditional dependence parameter on higher cascades of the pcc to be constant, we can greatly reduce the number of parameters to be estimated without losing much flexibility. we study the performance of our estimation method by a large scale monte carlo simulation. an application to a large dataset of stock returns of all constituents of the dax 30 illustrates the usefulness of the proposed model class.","","2012-02-09","","['carlos almeida', 'claudia czado', 'hans manner']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"340",1202.2169,"high dimensional semiparametric gaussian copula graphical models","stat.ml","in this paper, we propose a semiparametric approach, named nonparanormal skeptic, for efficiently and robustly estimating high dimensional undirected graphical models. to achieve modeling flexibility, we consider gaussian copula graphical models (or the nonparanormal) as proposed by liu et al. (2009). to achieve estimation robustness, we exploit nonparametric rank-based correlation coefficient estimators, including spearman's rho and kendall's tau. in high dimensional settings, we prove that the nonparanormal skeptic achieves the optimal parametric rate of convergence in both graph and parameter estimation. this celebrating result suggests that the gaussian copula graphical models can be used as a safe replacement of the popular gaussian graphical models, even when the data are truly gaussian. besides theoretical analysis, we also conduct thorough numerical simulations to compare different estimators for their graph recovery performance under both ideal and noisy settings. the proposed methods are then applied on a large-scale genomic dataset to illustrate their empirical usefulness. the r language software package huge implementing the proposed methods is available on the comprehensive r archive network: http://cran. r-project.org/.","","2012-02-09","2012-07-27","['han liu', 'fang han', 'ming yuan', 'john lafferty', 'larry wasserman']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"341",1202.3734,"efficient probabilistic inference with partial ranking queries","cs.lg cs.ai stat.ml","distributions over rankings are used to model data in various settings such as preference analysis and political elections. the factorial size of the space of rankings, however, typically forces one to make structural assumptions, such as smoothness, sparsity, or probabilistic independence about these underlying distributions. we approach the modeling problem from the computational principle that one should make structural assumptions which allow for efficient calculation of typical probabilistic queries. for ranking models, ""typical"" queries predominantly take the form of partial ranking queries (e.g., given a user's top-k favorite movies, what are his preferences over remaining movies?). in this paper, we argue that riffled independence factorizations proposed in recent literature [7, 8] are a natural structural assumption for ranking distributions, allowing for particularly efficient processing of partial ranking queries.","","2012-02-14","","['jonathan huang', 'ashish kapoor', 'carlos e. guestrin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"342",1202.3748,"conditional restricted boltzmann machines for structured output   prediction","cs.lg stat.ml","conditional restricted boltzmann machines (crbms) are rich probabilistic models that have recently been applied to a wide range of problems, including collaborative filtering, classification, and modeling motion capture data. while much progress has been made in training non-conditional rbms, these algorithms are not applicable to conditional models and there has been almost no work on training and generating predictions from conditional rbms for structured output problems. we first argue that standard contrastive divergence-based learning may not be suitable for training crbms. we then identify two distinct types of structured output prediction problems and propose an improved learning algorithm for each. the first problem type is one where the output space has arbitrary structure but the set of likely output configurations is relatively small, such as in multi-label classification. the second problem is one where the output space is arbitrarily structured but where the output space variability is much greater, such as in image denoising or pixel labeling. we show that the new learning algorithms can work much better than contrastive divergence on both types of problems.","","2012-02-14","","['volodymyr mnih', 'hugo larochelle', 'geoffrey e. hinton']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"343",1202.3752,"multidimensional counting grids: inferring word order from disordered   bags of words","cs.ir cs.cl cs.lg stat.ml","models of bags of words typically assume topic mixing so that the words in a single bag come from a limited number of topics. we show here that many sets of bag of words exhibit a very different pattern of variation than the patterns that are efficiently captured by topic mixing. in many cases, from one bag of words to the next, the words disappear and new ones appear as if the theme slowly and smoothly shifted across documents (providing that the documents are somehow ordered). examples of latent structure that describe such ordering are easily imagined. for example, the advancement of the date of the news stories is reflected in a smooth change over the theme of the day as certain evolving news stories fall out of favor and new events create new stories. overlaps among the stories of consecutive days can be modeled by using windows over linearly arranged tight distributions over words. we show here that such strategy can be extended to multiple dimensions and cases where the ordering of data is not readily obvious. we demonstrate that this way of modeling covariation in word occurrences outperforms standard topic models in classification and prediction tasks in applications in biology, text modeling and computer vision.","","2012-02-14","","['nebojsa jojic', 'alessandro perina']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"344",1202.4437,"on the wavelet-based simulation of anomalous diffusion","stat.me math.st stat.th","the characterization of particle diffusion is a classical problem in physics and probability theory. the field of microrheology is based on experiments in which microscopic tracer beads are placed into a non-newtonian fluid and tracked using high speed video capture. the modeling of the behavior of these beads is now an active scientific area which demands multiple stochastic and statistical methods.   we propose an approximate wavelet-based simulation technique for two classes of continuous time anomalous diffusion models, the fractional ornstein-uhlenbeck process and the fractional generalized langevin equation. the proposed algorithm is an iterative method that provides approximate discretizations that converge quickly and in an appropriate sense to the continuous time target process. as compared to previous works, it covers cases where the natural discretization of the target process does not have closed form in the time domain. moreover, we propose smoothing procedures as to speed the time domain decay of the filters.","","2012-02-20","2012-07-02","['gustavo didier', 'john fricks']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"345",1202.5695,"training restricted boltzmann machines on word observations","cs.lg stat.ml","the restricted boltzmann machine (rbm) is a flexible tool for modeling complex data, however there have been significant computational difficulties in using rbms to model high-dimensional multinomial observations. in natural language processing applications, words are naturally modeled by k-ary discrete distributions, where k is determined by the vocabulary size and can easily be in the hundreds of thousands. the conventional approach to training rbms on word observations is limited because it requires sampling the states of k-way softmax visible units during block gibbs updates, an operation that takes time linear in k. in this work, we address this issue by employing a more general class of markov chain monte carlo operators on the visible units, yielding updates with computational complexity independent of k. we demonstrate the success of our approach by training rbms on hundreds of millions of word n-grams using larger vocabularies than previously feasible and using the learned features to improve performance on chunking and sentiment classification tasks, achieving state-of-the-art results on the latter.","","2012-02-25","2012-07-05","['george e. dahl', 'ryan p. adams', 'hugo larochelle']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"346",1202.5846,"instrumental variable bayesian model averaging via conditional bayes   factors","stat.me stat.ap","we develop a method to perform model averaging in two-stage linear regression systems subject to endogeneity. our method extends an existing gibbs sampler for instrumental variables to incorporate a component of model uncertainty. direct evaluation of model probabilities is intractable in this setting. we show that by nesting model moves inside the gibbs sampler, model comparison can be performed via conditional bayes factors, leading to straightforward calculations. this new gibbs sampler is only slightly more involved than the original algorithm and exhibits no evidence of mixing difficulties. we conclude with a study of two different modeling challenges: incorporating uncertainty into the determinants of macroeconomic growth, and estimating a demand function by instrumenting wholesale on retail prices.","","2012-02-27","2012-03-19","['anna karl', 'alex lenkoski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"347",1202.5858,"two-stage bayesian model averaging in endogenous variable models","stat.me stat.ap","economic modeling in the presence of endogeneity is subject to model uncertainty at both the instrument and covariate level. we propose a two-stage bayesian model averaging (2sbma) methodology that extends the two-stage least squares (2sls) estimator. by constructing a two-stage unit information prior in the endogenous variable model, we are able to efficiently combine established methods for addressing model uncertainty in regression models with the classic technique of 2sls. to assess the validity of instruments in the 2sbma context, we develop bayesian tests of the identification restriction that are based on model averaged posterior predictive p-values. a simulation study showed that 2sbma has the ability to recover structure in both the instrument and covariate set, and substantially improves the sharpness of resulting coefficient estimates in comparison to 2sls using the full specification in an automatic fashion. due to the increased parsimony of the 2sbma estimate, the bayesian sargan test had a power of 50 percent in detecting a violation of the exogeneity assumption, while the method based on 2sls using the full specification had negligible power. we apply our approach to the problem of development accounting, and find support not only for institutions, but also for geography and integration as development determinants, once both model uncertainty and endogeneity have been jointly addressed.","","2012-02-27","","['a. lenkoski', 't. s. eicher', 'a. e. raftery']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"348",1202.5999,"survival-supervised latent dirichlet allocation models for genomic   analysis of time-to-event outcomes","stat.me stat.ap","two challenging problems in the clinical study of cancer are the characterization of cancer subtypes and the classification of individual patients according to those subtypes. statistical approaches addressing these problems are hampered by population heterogeneity and challenges inherent in data integration across high-dimensional, diverse covariates. we have developed a survival-supervised latent dirichlet allocation (survlda) modeling framework to address these concerns. lda models have proven extremely effective at identifying themes common across large collections of text, but applications to genomics have been limited. our framework extends lda to the genome by considering each patient as a `document' with `text' constructed from clinical and high-dimensional genomic measurements. we then further extend the framework to allow for supervision by a time-to-event response. the model enables the efficient identification of collections of clinical and genomic features that co-occur within patient subgroups, and then characterizes each patient by those features. an application of survlda to the cancer genome atlas (tcga) ovarian project identifies informative patient subgroups that are characterized by different propensities for exhibiting abnormal mrna expression and methylations, corresponding to differential rates of survival from primary therapy.","","2012-02-27","","['john a. dawson', 'christina kendziorski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"349",1203.0098,"bayesian matching of unlabeled marked point sets using random fields,   with an application to molecular alignment","stat.ap","statistical methodology is proposed for comparing unlabeled marked point sets, with an application to aligning steroid molecules in chemoinformatics. methods from statistical shape analysis are combined with techniques for predicting random fields in spatial statistics in order to define a suitable measure of similarity between two marked point sets. bayesian modeling of the predicted field overlap between pairs of point sets is proposed, and posterior inference of the alignment is carried out using markov chain monte carlo simulation. by representing the fields in reproducing kernel hilbert spaces, the degree of overlap can be computed without expensive numerical integration. superimposing entire fields rather than the configuration matrices of point coordinates thereby avoids the problem that there is usually no clear one-to-one correspondence between the points. in addition, mask parameters are introduced in the model, so that partial matching of the marked point sets can be carried out. we also propose an adaptation of the generalized procrustes analysis algorithm for the simultaneous alignment of multiple point sets. the methodology is illustrated with a simulation study and then applied to a data set of 31 steroid molecules, where the relationship between shape and binding activity to the corticosteroid binding globulin receptor is explored.","10.1214/11-aoas486","2012-03-01","","['irina czogiel', 'ian l. dryden', 'christopher j. brignell']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"350",1203.0133,"covariance approximation for large multivariate spatial data sets with   an application to multiple climate model errors","stat.ap","this paper investigates the cross-correlations across multiple climate model errors. we build a bayesian hierarchical model that accounts for the spatial dependence of individual models as well as cross-covariances across different climate models. our method allows for a nonseparable and nonstationary cross-covariance structure. we also present a covariance approximation approach to facilitate the computation in the modeling and analysis of very large multivariate spatial data sets. the covariance approximation consists of two parts: a reduced-rank part to capture the large-scale spatial dependence, and a sparse covariance matrix to correct the small-scale dependence error induced by the reduced rank approximation. we pay special attention to the case that the second part of the approximation has a block-diagonal structure. simulation results of model fitting and prediction show substantial improvement of the proposed approximation over the predictive process approximation and the independent blocks analysis. we then apply our computational approach to the joint statistical modeling of multiple climate model errors.","10.1214/11-aoas478","2012-03-01","","['huiyan sang', 'mikyoung jun', 'jianhua z. huang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"351",1203.1365,"bayesian nonparametric hidden semi-markov models","stat.me stat.ap stat.ml","there is much interest in the hierarchical dirichlet process hidden markov model (hdp-hmm) as a natural bayesian nonparametric extension of the ubiquitous hidden markov model for learning from sequential and time-series data. however, in many settings the hdp-hmm's strict markovian constraints are undesirable, particularly if we wish to learn or encode non-geometric state durations. we can extend the hdp-hmm to capture such structure by drawing upon explicit-duration semi-markovianity, which has been developed mainly in the parametric frequentist setting, to allow construction of highly interpretable models that admit natural prior information on state durations.   in this paper we introduce the explicit-duration hierarchical dirichlet process hidden semi-markov model (hdp-hsmm) and develop sampling algorithms for efficient posterior inference. the methods we introduce also provide new methods for sampling inference in the finite bayesian hsmm. our modular gibbs sampling methods can be embedded in samplers for larger hierarchical bayesian models, adding semi-markov chain modeling as another tool in the bayesian inference toolbox. we demonstrate the utility of the hdp-hsmm and our inference methods on both synthetic and real experiments.","","2012-03-06","2012-09-07","['matthew j. johnson', 'alan s. willsky']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE
"352",1203.1547,"roostats for searches","physics.data-an hep-ex stat.co","the roostats toolkit, which is distributed with the root software package, provides a large collection of software tools that implement statistical methods commonly used by the high energy physics community. the toolkit is based on roofit, a high-level data analysis modeling package that implements various methods of statistical data analysis. roostats enforces a clear mapping of statistical concepts to c++ classes and methods and emphasizes the ability to easily combine analyses within and across experiments. we present an overview of the roostats toolkit, describe some of the methods used for hypothesis testing and estimation of confidence intervals and finally discuss some of the latest developments.","","2012-03-07","","['grégory schott']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"353",1203.2313,"automatic parametrization of age/ sex leslie matrices for human   populations","q-bio.pe stat.ap","in this paper, we present a technique for parameterizing leslie transition matrices from simple age and sex population counts, using an implementation of ""wood's method"" [wood]; these matrices can forecast population by age and sex (the ""cohort component"" method) using simple matrix multiplication and a starting population. our approach improves on previous methods for creating leslie matrices in two respects: it eliminates the need to calculate input demographic rates from ""raw"" data, and our new format for the leslie matrix more elegantly reveals the population's demographic components of change (fertility, mortality, and migration). the paper is organized around three main themes. first, we describe the underlying algorithm, ""wood's method,"" which uses quadratic optimization to fit a transition matrix to age and sex population counts. second, we use demographic theory to create constraint sets that make the algorithm useable for human populations. finally, we use the method to forecast 3,120 us counties and show that it holds promise for automating cohort-component forecasts. this paper describes the first published successful application of wood's method to human populations; it also points to more general promise of constrained optimization techniques in demographic modeling.","","2012-03-10","2012-04-22","['w. webb sprague']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"354",1203.2433,"accurate emulators for large-scale computer experiments","math.st stat.th","large-scale computer experiments are becoming increasingly important in science. a multi-step procedure is introduced to statisticians for modeling such experiments, which builds an accurate interpolator in multiple steps. in practice, the procedure shows substantial improvements in overall accuracy, but its theoretical properties are not well established. we introduce the terms nominal and numeric error and decompose the overall error of an interpolator into nominal and numeric portions. bounds on the numeric and nominal error are developed to show theoretically that substantial gains in overall accuracy can be attained with the multi-step approach.","10.1214/11-aos929","2012-03-12","","['ben haaland', 'peter z. g. qian']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"355",1203.2879,"an imputation method for estimating the learning curve in classification   problems","stat.ap","the learning curve expresses the error rate of a predictive modeling procedure as a function of the sample size of the training dataset. it typically is a decreasing, convex function with a positive limiting value. an estimate of the learning curve can be used to assess whether a modeling procedure should be expected to become substantially more accurate if additional training data become available. this article proposes a new procedure for estimating learning curves using imputation. we focus on classification, although the idea is applicable to other predictive modeling settings. simulation studies indicate that the learning curve can be estimated with useful accuracy for a roughly four-fold increase in the size of the training set relative to the available data, and that the proposed imputation approach outperforms an alternative estimation approach based on parameterizing the learning curve. we illustrate the method with an application that predicts the risk of disease progression for people with chronic lymphocytic leukemia.","","2012-03-13","","['eric b. laber', 'kerby shedden', 'yang yang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"356",1203.3328,"copar - multivariate time series modeling using the copula   autoregressive model","stat.me","analysis of multivariate time series is a common problem in areas like finance and economics. the classical tool for this purpose are vector autoregressive models. these however are limited to the modeling of linear and symmetric dependence. we propose a novel copula-based model which allows for non-linear and asymmetric modeling of serial as well as between-series dependencies. the model exploits the flexibility of vine copulas which are built up by bivariate copulas only. we describe statistical inference techniques for the new model and demonstrate its usefulness in three relevant applications: we analyze time series of macroeconomic indicators, of electricity load demands and of bond portfolio returns.","","2012-03-15","2012-04-04","['eike christian brechmann', 'claudia czado']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"357",1203.3462,"gaussian process topic models","cs.lg stat.ml","we introduce gaussian process topic models (gptms), a new family of topic models which can leverage a kernel among documents while extracting correlated topics. gptms can be considered a systematic generalization of the correlated topic models (ctms) using ideas from gaussian process (gp) based embedding. since gptms work with both a topic covariance matrix and a document kernel matrix, learning gptms involves a novel component-solving a suitable sylvester equation capturing both topic and document dependencies. the efficacy of gptms is demonstrated with experiments evaluating the quality of both topic modeling and embedding.","","2012-03-15","","['amrudin agovic', 'arindam banerjee']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"358",1203.3489,"bayesian exponential family projections for coupled data sources","cs.lg stat.ml","exponential family extensions of principal component analysis (epca) have received a considerable amount of attention in recent years, demonstrating the growing need for basic modeling tools that do not assume the squared loss or gaussian distribution. we extend the epca model toolbox by presenting the first exponential family multi-view learning methods of the partial least squares and canonical correlation analysis, based on a unified representation of epca as matrix factorization of the natural parameters of exponential family. the models are based on a new family of priors that are generally usable for all such factorizations. we also introduce new inference strategies, and demonstrate how the methods outperform earlier ones when the gaussianity assumption does not hold.","","2012-03-15","","['arto klami', 'seppo virtanen', 'samuel kaski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"359",1203.351,"irregular-time bayesian networks","cs.ai cs.lg stat.ml","in many fields observations are performed irregularly along time, due to either measurement limitations or lack of a constant immanent rate. while discrete-time markov models (as dynamic bayesian networks) introduce either inefficient computation or an information loss to reasoning about such processes, continuous-time markov models assume either a discrete state space (as continuous-time bayesian networks), or a flat continuous state space (as stochastic differential equations). to address these problems, we present a new modeling class called irregular-time bayesian networks (itbns), generalizing dynamic bayesian networks, allowing substantially more compact representations, and increasing the expressivity of the temporal dynamics. in addition, a globally optimal solution is guaranteed when learning temporal systems, provided that they are fully observed at the same irregularly spaced time-points, and a semiparametric subclass of itbns is introduced to allow further adaptation to the irregular nature of the available data.","","2012-03-15","","['michael ramati', 'yuval shahar']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"360",1203.3516,"modeling events with cascades of poisson processes","cs.lg cs.ai stat.ml","we present a probabilistic model of events in continuous time in which each event triggers a poisson process of successor events. the ensemble of observed events is thereby modeled as a superposition of poisson processes. efficient inference is feasible under this model with an em algorithm. moreover, the em algorithm can be implemented as a distributed algorithm, permitting the model to be applied to very large datasets. we apply these techniques to the modeling of twitter messages and the revision history of wikipedia.","","2012-03-15","","['aleksandr simma', 'michael i. jordan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"361",1203.353,"hybrid generative/discriminative learning for automatic image annotation","cs.lg cs.cv stat.ml","automatic image annotation (aia) raises tremendous challenges to machine learning as it requires modeling of data that are both ambiguous in input and output, e.g., images containing multiple objects and labeled with multiple semantic tags. even more challenging is that the number of candidate tags is usually huge (as large as the vocabulary size) yet each image is only related to a few of them. this paper presents a hybrid generative-discriminative classifier to simultaneously address the extreme data-ambiguity and overfitting-vulnerability issues in tasks such as aia. particularly: (1) an exponential-multinomial mixture (emm) model is established to capture both the input and output ambiguity and in the meanwhile to encourage prediction sparsity; and (2) the prediction ability of the emm model is explicitly maximized through discriminative learning that integrates variational inference of graphical models and the pairwise formulation of ordinal regression. experiments show that our approach achieves both superior annotation performance and better tag scalability.","","2012-03-15","","['shuang hong yang', 'jiang bian', 'hongyuan zha']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"362",1203.3532,"learning structural changes of gaussian graphical models in controlled   experiments","cs.lg stat.ml","graphical models are widely used in scienti fic and engineering research to represent conditional independence structures between random variables. in many controlled experiments, environmental changes or external stimuli can often alter the conditional dependence between the random variables, and potentially produce significant structural changes in the corresponding graphical models. therefore, it is of great importance to be able to detect such structural changes from data, so as to gain novel insights into where and how the structural changes take place and help the system adapt to the new environment. here we report an effective learning strategy to extract structural changes in gaussian graphical model using l1-regularization based convex optimization. we discuss the properties of the problem formulation and introduce an efficient implementation by the block coordinate descent algorithm. we demonstrate the principle of the approach on a numerical simulation experiment, and we then apply the algorithm to the modeling of gene regulatory networks under different conditions and obtain promising yet biologically plausible results.","","2012-03-15","","['bai zhang', 'yue wang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"363",1203.3536,"a convex formulation for learning task relationships in multi-task   learning","cs.lg cs.ai stat.ml","multi-task learning is a learning paradigm which seeks to improve the generalization performance of a learning task with the help of some other related tasks. in this paper, we propose a regularization formulation for learning the relationships between tasks in multi-task learning. this formulation can be viewed as a novel generalization of the regularization framework for single-task learning. besides modeling positive task correlation, our method, called multi-task relationship learning (mtrl), can also describe negative task correlation and identify outlier tasks based on the same underlying principle. under this regularization framework, the objective function of mtrl is convex. for efficiency, we use an alternating method to learn the optimal model parameters for each task as well as the relationships between tasks. we study mtrl in the symmetric multi-task learning setting and then generalize it to the asymmetric setting as well. we also study the relationships between mtrl and some existing multi-task learning methods. experiments conducted on a toy problem as well as several benchmark data sets demonstrate the effectiveness of mtrl.","","2012-03-15","","['yu zhang', 'dit-yan yeung']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"364",1203.4326,"selection of tuning parameters in bridge regression models via bayesian   information criterion","stat.me stat.ml","we consider the bridge linear regression modeling, which can produce a sparse or non-sparse model. a crucial point in the model building process is the selection of adjusted parameters including a regularization parameter and a tuning parameter in bridge regression models. the choice of the adjusted parameters can be viewed as a model selection and evaluation problem. we propose a model selection criterion for evaluating bridge regression models in terms of bayesian approach. this selection criterion enables us to select the adjusted parameters objectively. we investigate the effectiveness of our proposed modeling strategy through some numerical examples.","10.1007/s00362-013-0561-7","2012-03-20","2012-04-13","['shuichi kawano']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"365",1203.4359,"bayesian joint modeling of multiple gene networks and diverse genomic   data to identify target genes of a transcription factor","stat.ap","we consider integrative modeling of multiple gene networks and diverse genomic data, including protein-dna binding, gene expression and dna sequence data, to accurately identify the regulatory target genes of a transcription factor (tf). rather than treating all the genes equally and independently a priori in existing joint modeling approaches, we incorporate the biological prior knowledge that neighboring genes on a gene network tend to be (or not to be) regulated together by a tf. a key contribution of our work is that, to maximize the use of all existing biological knowledge, we allow incorporation of multiple gene networks into joint modeling of genomic data by introducing a mixture model based on the use of multiple markov random fields (mrfs). another important contribution of our work is to allow different genomic data to be correlated and to examine the validity and effect of the independence assumption as adopted in existing methods. due to a fully bayesian approach, inference about model parameters can be carried out based on mcmc samples. application to an e. coli data set, together with simulation studies, demonstrates the utility and statistical efficiency gains with the proposed joint model.","10.1214/11-aoas502","2012-03-20","","['peng wei', 'wei pan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"366",1203.469,"bayesian nonparametric shrinkage applied to cepheid star oscillations","stat.me","bayesian nonparametric regression with dependent wavelets has dual shrinkage properties: there is shrinkage through a dependent prior put on functional differences, and shrinkage through the setting of most of the wavelet coefficients to zero through bayesian variable selection methods. the methodology can deal with unequally spaced data and is efficient because of the existence of fast moves in model space for the mcmc computation. the methodology is illustrated on the problem of modeling the oscillations of cepheid variable stars; these are a class of pulsating variable stars with the useful property that their periods of variability are strongly correlated with their absolute luminosity. once this relationship has been calibrated, knowledge of the period gives knowledge of the luminosity. this makes these stars useful as ""standard candles"" for estimating distances in the universe.","10.1214/11-sts384","2012-03-21","","['james berger', 'william h. jefferys', 'peter müller']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"367",1203.5422,"distribution free prediction bands","stat.me cs.lg math.st stat.th","we study distribution free, nonparametric prediction bands with a special focus on their finite sample behavior. first we investigate and develop different notions of finite sample coverage guarantees. then we give a new prediction band estimator by combining the idea of ""conformal prediction"" (vovk et al. 2009) with nonparametric conditional density estimation. the proposed estimator, called cops (conformal optimized prediction set), always has finite sample guarantee in a stronger sense than the original conformal prediction estimator. under regularity conditions the estimator converges to an oracle band at a minimax optimal rate. a fast approximation algorithm and a data driven method for selecting the bandwidth are developed. the method is illustrated first in simulated data. then, an application shows that the proposed method gives desirable prediction intervals in an automatic way, as compared to the classical linear regression modeling.","","2012-03-24","","['jing lei', 'larry wasserman']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"368",1203.5986,"bayesian network enhanced with structural reliability methods:   methodology","stat.ap stat.me stat.ml","we combine bayesian networks (bns) and structural reliability methods (srms) to create a new computational framework, termed enhanced bayesian network (ebn), for reliability and risk analysis of engineering structures and infrastructure. bns are efficient in representing and evaluating complex probabilistic dependence structures, as present in infrastructure and structural systems, and they facilitate bayesian updating of the model when new information becomes available. on the other hand, srms enable accurate assessment of probabilities of rare events represented by computationally demanding, physically-based models. by combining the two methods, the ebn framework provides a unified and powerful tool for efficiently computing probabilities of rare events in complex structural and infrastructure systems in which information evolves in time. strategies for modeling and efficiently analyzing the ebn are described by way of several conceptual examples. the companion paper applies the ebn methodology to example structural and infrastructure systems.","10.1061/(asce)em.1943-7889.0000173","2012-03-27","","['daniel straub', 'armen der kiureghian']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE
"369",1203.6276,"a multi-objective exploratory procedure for regression model selection","stat.co cs.ne stat.ap","variable selection is recognized as one of the most critical steps in statistical modeling. the problems encountered in engineering and social sciences are commonly characterized by over-abundance of explanatory variables, non-linearities and unknown interdependencies between the regressors. an added difficulty is that the analysts may have little or no prior knowledge on the relative importance of the variables. to provide a robust method for model selection, this paper introduces the multi-objective genetic algorithm for variable selection (moga-vs) that provides the user with an optimal set of regression models for a given data-set. the algorithm considers the regression problem as a two objective task, and explores the pareto-optimal (best subset) models by preferring those models over the other which have less number of regression coefficients and better goodness of fit. the model exploration can be performed based on in-sample or generalization error minimization. the model selection is proposed to be performed in two steps. first, we generate the frontier of pareto-optimal regression models by eliminating the dominated models without any user intervention. second, a decision making process is executed which allows the user to choose the most preferred model using visualisations and simple metrics. the method has been evaluated on a recently published real dataset on communities and crime within united states.","10.1080/10618600.2014.899236","2012-03-28","2016-07-13","['ankur sinha', 'pekka malo', 'timo kuosmanen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE
"370",1204.0286,"a two-step approach to model precipitation extremes in california based   on max-stable and marginal point processes","stat.me","in modeling spatial extremes, the dependence structure is classically inferred by assuming that block maxima derive from max-stable processes. weather stations provide daily records rather than just block maxima. the point process approach for univariate extreme value analysis, which uses more historical data and is preferred by some practitioners, does not adapt easily to the spatial setting. we propose a two-step approach with a composite likelihood that utilizes site-wise daily records in addition to block maxima. the procedure separates the estimation of marginal parameters and dependence parameters into two steps. the first step estimates the marginal parameters with an independence likelihood from the point process approach using daily records. given the marginal parameter estimates, the second step estimates the dependence parameters with a pairwise likelihood using block maxima. in a simulation study, the two-step approach was found to be more efficient than the pairwise likelihood approach using only block maxima. the method was applied to study the effect of el ni\~{n}o-southern oscillation on extreme precipitation in california with maximum daily winter precipitation from 35 sites over 55 years. using site-specific generalized extreme value models, the two-step approach led to more sites detected with the el ni\~{n}o effect, narrower confidence intervals for return levels and tighter confidence regions for risk measures of jointly defined events.","10.1214/14-aoas804","2012-04-01","2015-06-16","['hongwei shang', 'jun yan', 'xuebin zhang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"371",1204.122,"diagonal and low-rank matrix decompositions, correlation matrices, and   ellipsoid fitting","math.oc math.st stat.th","in this paper we establish links between, and new results for, three problems that are not usually considered together. the first is a matrix decomposition problem that arises in areas such as statistical modeling and signal processing: given a matrix $x$ formed as the sum of an unknown diagonal matrix and an unknown low rank positive semidefinite matrix, decompose $x$ into these constituents. the second problem we consider is to determine the facial structure of the set of correlation matrices, a convex set also known as the elliptope. this convex body, and particularly its facial structure, plays a role in applications from combinatorial optimization to mathematical finance. the third problem is a basic geometric question: given points $v_1,v_2,...,v_n\in \r^k$ (where $n > k$) determine whether there is a centered ellipsoid passing \emph{exactly} through all of the points.   we show that in a precise sense these three problems are equivalent. furthermore we establish a simple sufficient condition on a subspace $u$ that ensures any positive semidefinite matrix $l$ with column space $u$ can be recovered from $d+l$ for any diagonal matrix $d$ using a convex optimization-based heuristic known as minimum trace factor analysis. this result leads to a new understanding of the structure of rank-deficient correlation matrices and a simple condition on a set of points that ensures there is a centered ellipsoid passing through them.","10.1137/120872516","2012-04-05","","['james saunderson', 'venkat chandrasekaran', 'pablo a. parrilo', 'alan s. willsky']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"372",1204.1389,"the kumaraswamy pareto distribution","stat.me","the modeling and analysis of lifetimes is an important aspect of statistical work in a wide variety of scientific and technological fields. for the first time, the called kumaraswamy pareto distribution is introduced and studied. the new distribution can have a decreasing and upside-down bathtub failure rate function depending on the values of its parameters. it includes as special sub-models the pareto and exponentiated pareto (gupta et al. [12]) distributions. some structural properties of the proposed distribution are studied including explicit expressions for the moments and generating function. we provide the density function of the order statistics and obtain their moments. the method of maximum likelihood is used for estimating the model parameters and the observed information matrix is derived. a real data set is used to compare the new model with widely known distributions.","","2012-04-05","2012-12-04","['marcelo b. pereira', 'rodrigo b. silva', 'luz m. zea', 'gauss m. cordeiro']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"373",1204.1688,"the asymptotics of ranking algorithms","math.st cs.lg stat.ml stat.th","we consider the predictive problem of supervised ranking, where the task is to rank sets of candidate items returned in response to queries. although there exist statistical procedures that come with guarantees of consistency in this setting, these procedures require that individuals provide a complete ranking of all items, which is rarely feasible in practice. instead, individuals routinely provide partial preference information, such as pairwise comparisons of items, and more practical approaches to ranking have aimed at modeling this partial preference data directly. as we show, however, such an approach raises serious theoretical challenges. indeed, we demonstrate that many commonly used surrogate losses for pairwise comparison data do not yield consistency; surprisingly, we show inconsistency even in low-noise settings. with these negative results as motivation, we present a new approach to supervised ranking based on aggregation of partial preferences, and we develop $u$-statistic-based empirical risk minimization procedures. we present an asymptotic analysis of these new procedures, showing that they yield consistency results that parallel those available for classification. we complement our theoretical results with an experiment studying the new procedures in a large-scale web-ranking task.","10.1214/13-aos1142","2012-04-07","2013-11-26","['john c. duchi', 'lester mackey', 'michael i. jordan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"374",1204.18,"on power-law kernels, corresponding reproducing kernel hilbert space and   applications","cs.lg cs.it math.it stat.ml","the role of kernels is central to machine learning. motivated by the importance of power-law distributions in statistical modeling, in this paper, we propose the notion of power-law kernels to investigate power-laws in learning problem. we propose two power-law kernels by generalizing gaussian and laplacian kernels. this generalization is based on distributions, arising out of maximization of a generalized information measure known as nonextensive entropy that is very well studied in statistical mechanics. we prove that the proposed kernels are positive definite, and provide some insights regarding the corresponding reproducing kernel hilbert space (rkhs). we also study practical significance of both kernels in classification and regression, and present some simulation results.","","2012-04-09","2013-04-01","['debarghya ghoshdastidar', 'ambedkar dukkipati']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"375",1204.209,"consistent single- and multi-step sampling of multivariate arrival   times: a characterization of self-chaining copulas","math.pr math.st q-fin.pr stat.th","this paper deals with dependence across marginally exponentially distributed arrival times, such as default times in financial modeling or inter-failure times in reliability theory. we explore the relationship between dependence and the possibility to sample final multivariate survival in a long time-interval as a sequence of iterations of local multivariate survivals along a partition of the total time interval. we find that this is possible under a form of multivariate lack of memory that is linked to a property of the survival times copula. this property defines a ""self-chaining-copula"", and we show that this coincides with the extreme value copulas characterization. the self-chaining condition is satisfied by the gumbel-hougaard copula, a full characterization of self chaining copulas in the archimedean family, and by the marshall-olkin copula. the result has important practical implications for consistent single-step and multi-step simulation of multivariate arrival times in a way that does not destroy dependency through iterations, as happens when inconsistently iterating a gaussian copula.","","2012-04-10","2012-04-28","['damiano brigo', 'kyriakos chourdakis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"376",1204.2098,"bayesian nonstationary spatial modeling for very large datasets","stat.me stat.ap stat.co","with the proliferation of modern high-resolution measuring instruments mounted on satellites, planes, ground-based vehicles and monitoring stations, a need has arisen for statistical methods suitable for the analysis of large spatial datasets observed on large spatial domains. statistical analyses of such datasets provide two main challenges: first, traditional spatial-statistical techniques are often unable to handle large numbers of observations in a computationally feasible way. second, for large and heterogeneous spatial domains, it is often not appropriate to assume that a process of interest is stationary over the entire domain.   we address the first challenge by using a model combining a low-rank component, which allows for flexible modeling of medium-to-long-range dependence via a set of spatial basis functions, with a tapered remainder component, which allows for modeling of local dependence using a compactly supported covariance function. addressing the second challenge, we propose two extensions to this model that result in increased flexibility: first, the model is parameterized based on a nonstationary matern covariance, where the parameters vary smoothly across space. second, in our fully bayesian model, all components and parameters are considered random, including the number, locations, and shapes of the basis functions used in the low-rank component.   using simulated data and a real-world dataset of high-resolution soil measurements, we show that both extensions can result in substantial improvements over the current state-of-the-art.","10.1002/env.2200","2012-04-10","2012-12-21","['matthias katzfuss']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"377",1204.2581,"modeling relational data via latent factor blockmodel","cs.ds cs.lg stat.ml","in this paper we address the problem of modeling relational data, which appear in many applications such as social network analysis, recommender systems and bioinformatics. previous studies either consider latent feature based models but disregarding local structure in the network, or focus exclusively on capturing local structure of objects based on latent blockmodels without coupling with latent characteristics of objects. to combine the benefits of the previous work, we propose a novel model that can simultaneously incorporate the effect of latent features and covariates if any, as well as the effect of latent structure that may exist in the data. to achieve this, we model the relation graph as a function of both latent feature factors and latent cluster memberships of objects to collectively discover globally predictive intrinsic properties of objects and capture latent block structure in the network to improve prediction performance. we also develop an optimization transfer algorithm based on the generalized em-style strategy to learn the latent factors. we prove the efficacy of our proposed model through the link prediction task and cluster analysis task, and extensive experiments on the synthetic data and several real world datasets suggest that our proposed lfbm model outperforms the other state of the art approaches in the evaluated tasks.","","2012-04-11","","['sheng gao', 'ludovic denoyer', 'patrick gallinari']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"378",1204.313,"adaptive bridge regression modeling with model selection criteria","stat.me","we consider the problem of constructing an adaptive bridge regression modeling, which is a penalized procedure by imposing different weights to different coefficients in the bridge penalty term. a crucial issue in the modeling process is the choices of adjusted parameters included in the models. we treat the selection of the adjusted parameters as model selection and evaluation problems. in order to select the parameters, model selection criteria are derived from information-theoretic and bayesian approach. we conduct some numerical studies to investigate the effectiveness of our proposed modeling strategy.","","2012-04-13","2012-08-28","['shuichi kawano']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"379",1204.3524,"a euclidean likelihood estimator for bivariate tail dependence","stat.me","the spectral measure plays a key role in the statistical modeling of multivariate extremes. estimation of the spectral measure is a complex issue, given the need to obey a certain moment condition. we propose a euclidean likelihood-based estimator for the spectral measure which is simple and explicitly defined, with its expression being free of lagrange multipliers. our estimator is shown to have the same limit distribution as the maximum empirical likelihood estimator of j. h. j. einmahl and j. segers, annals of statistics 37(5b), 2953--2989 (2009). numerical experiments suggest an overall good performance and identical behavior to the maximum empirical likelihood estimator. we illustrate the method in an extreme temperature data analysis.","","2012-04-16","","['miguel de carvalho', 'boris oumow', 'johan segers', 'michał warchoł']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"380",1204.3547,"computer model calibration using the ensemble kalman filter","stat.me stat.co","the ensemble kalman filter (enkf) (evensen, 2009) has proven effective in quantifying uncertainty in a number of challenging dynamic, state estimation, or data assimilation, problems such as weather forecasting and ocean modeling. in these problems a high-dimensional state parameter is successively updated based on recurring physical observations, with the aid of a computationally demanding forward model that prop- agates the state from one time step to the next. more recently, the enkf has proven effective in history matching in the petroleum engineering community (evensen, 2009; oliver and chen, 2010). such applications typically involve estimating large numbers of parameters, describing an oil reservoir, using data from production history that accumulate over time. such history matching problems are especially challenging examples of computer model calibration since they involve a large number of model parameters as well as a computationally demanding forward model. more generally, computer model calibration combines physical observations with a computational model - a computer model - to estimate unknown parameters in the computer model. this paper explores how the enkf can be used in computer model calibration problems, comparing it to other more common approaches, considering applications in climate and cosmology.","","2012-04-16","2012-04-23","['dave higdon', 'matt pratola', 'james gattiker', 'earl lawrence', 'salman habib', 'katrin heitmann', 'steve price', 'charles jackson', 'michael tobis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"381",1204.3941,"a log-linear graphical model for inferring genetic networks from   high-throughput sequencing data","stat.ap","gaussian graphical models are often used to infer gene networks based on microarray expression data. many scientists, however, have begun using high-throughput sequencing technologies to measure gene expression. as the resulting high-dimensional count data consists of counts of sequencing reads for each gene, gaussian graphical models are not optimal for modeling gene networks based on this discrete data. we develop a novel method for estimating high-dimensional poisson graphical models, the log-linear graphical model, allowing us to infer networks based on high-throughput sequencing data. our model assumes a pair-wise markov property: conditional on all other variables, each variable is poisson. we estimate our model locally via neighborhood selection by fitting 1-norm penalized log-linear models. additionally, we develop a fast parallel algorithm, an approach we call the poisson graphical lasso, permitting us to fit our graphical model to high-dimensional genomic data sets. in simulations, we illustrate the effectiveness of our methods for recovering network structure from count data. a case study on breast cancer micrornas, a novel application of graphical models, finds known regulators of breast cancer genes and discovers novel microrna clusters and hubs that are targets for future research.","","2012-04-17","2012-05-28","['genevera i. allen', 'zhandong liu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"382",1204.4021,"kernel discriminant analysis and clustering with parsimonious gaussian   process models","stat.me stat.ap","this work presents a family of parsimonious gaussian process models which allow to build, from a finite sample, a model-based classifier in an infinite dimensional space. the proposed parsimonious models are obtained by constraining the eigen-decomposition of the gaussian processes modeling each class. this allows in particular to use non-linear mapping functions which project the observations into infinite dimensional spaces. it is also demonstrated that the building of the classifier can be directly done from the observation space through a kernel function. the proposed classification method is thus able to classify data of various types such as categorical data, functional data or networks. furthermore, it is possible to classify mixed data by combining different kernels. the methodology is as well extended to the unsupervised classification case. experimental results on various data sets demonstrate the effectiveness of the proposed method.","","2012-04-18","2012-06-15","['charles bouveyron', 'stéphane girard', 'mathieu fauvel']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"383",1204.4699,"modeling, dependence, classification, united statistical science, many   cultures","math.st stat.me stat.ml stat.th","breiman (2001) proposed to statisticians awareness of two cultures: 1. parametric modeling culture, pioneered by r.a.fisher and jerzy neyman; 2. algorithmic predictive culture, pioneered by machine learning research.   parzen (2001), as a part of discussing breiman (2001), proposed that researchers be aware of many cultures, including the focus of our research: 3. nonparametric, quantile based, information theoretic modeling. we provide a unification of many statistical methods for traditional small data sets and emerging big data sets in terms of comparison density, copula density, measure of dependence, correlation, information, new measures (called lp score comoments) that apply to long tailed distributions with out finite second order moments. a very important goal is to unify methods for discrete and continuous random variables. our research extends these methods to modern high dimensional data modeling.","","2012-04-20","2012-04-23","['emanuel parzen', 'subhadeep mukhopadhyay']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE
"384",1204.4708,"efficient hierarchical clustering for continuous data","stat.ml","we present an new sequential monte carlo sampler for coalescent based bayesian hierarchical clustering. our model is appropriate for modeling non-i.i.d. data and offers a substantial reduction of computational cost when compared to the original sampler without resorting to approximations. we also propose a quadratic complexity approximation that in practice shows almost no loss in performance compared to its counterpart. we show that as a byproduct of our formulation, we obtain a greedy algorithm that exhibits performance improvement over other greedy algorithms, particularly in small data sets. in order to exploit the correlation structure of the data, we describe how to incorporate gaussian process priors in the model as a flexible way to model non-i.i.d. data. results on artificial and real data show significant improvements over closely related approaches.","","2012-04-20","","['ricardo henao', 'joseph e. lucas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"385",1204.4927,"ehrs connect research and practice: where predictive modeling,   artificial intelligence, and clinical decision support intersect","cs.ai cs.db stat.ml","objectives: electronic health records (ehrs) are only a first step in capturing and utilizing health-related data - the challenge is turning that data into useful information. furthermore, ehrs are increasingly likely to include data relating to patient outcomes, functionality such as clinical decision support, and genetic information as well, and, as such, can be seen as repositories of increasingly valuable information about patients' health conditions and responses to treatment over time. methods: we describe a case study of 423 patients treated by centerstone within tennessee and indiana in which we utilized electronic health record data to generate predictive algorithms of individual patient treatment response. multiple models were constructed using predictor variables derived from clinical, financial and geographic data. results: for the 423 patients, 101 deteriorated, 223 improved and in 99 there was no change in clinical condition. based on modeling of various clinical indicators at baseline, the highest accuracy in predicting individual patient response ranged from 70-72% within the models tested. in terms of individual predictors, the centerstone assessment of recovery level - adult (carla) baseline score was most significant in predicting outcome over time (odds ratio 4.1 + 2.27). other variables with consistently significant impact on outcome included payer, diagnostic category, location and provision of case management services. conclusions: this approach represents a promising avenue toward reducing the current gap between research and practice across healthcare, developing data-driven clinical decision support based on real-world populations, and serving as a component of embedded clinical artificial intelligences that ""learn"" over time.","10.1016/j.hlpt.2012.03.001","2012-04-22","","['casey bennett', 'tom doub', 'rebecca selove']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"386",1204.6087,"spatial models for point and areal data using markov random fields on a   fine grid","stat.me","i consider the use of markov random fields (mrfs) on a fine grid to represent latent spatial processes when modeling point-level and areal data, including situations with spatial misalignment. point observations are related to the grid cell in which they reside, while areal observations are related to the (approximate) integral over the latent process within the area of interest. i review several approaches to specifying the neighborhood structure for constructing the mrf precision matrix, presenting results comparing these mrf representations analytically, in simulations, and in two examples. the results provide practical guidance for choosing a spatial process representation and highlight the importance of this choice. in particular, the results demonstrate that, and explain why, standard car models can behave strangely for point-level data. they show that various neighborhood weighting approaches based on higher-order neighbors that have been suggested for mrf models do not produce smooth fields, which raises doubts about their utility. finally, they indicate that an mrf that approximates a thin plate spline compares favorably to standard car models and to kriging under many circumstances.","10.1214/13-ejs791","2012-04-26","2013-04-06","['christopher j. paciorek']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"387",1204.6118,"stochastic partial differential equation based modelling of large   space-time data sets","stat.me","increasingly larger data sets of processes in space and time ask for statistical models and methods that can cope with such data. we show that the solution of a stochastic advection-diffusion partial differential equation provides a flexible model class for spatio-temporal processes which is computationally feasible also for large data sets. the gaussian process defined through the stochastic partial differential equation has in general a nonseparable covariance structure. furthermore, its parameters can be physically interpreted as explicitly modeling phenomena such as transport and diffusion that occur in many natural processes in diverse fields ranging from environmental sciences to ecology. in order to obtain computationally efficient statistical algorithms we use spectral methods to solve the stochastic partial differential equation. this has the advantage that approximation errors do not accumulate over time, and that in the spectral space the computational cost grows linearly with the dimension, the total computational costs of bayesian or frequentist inference being dominated by the fast fourier transform. the proposed model is applied to postprocessing of precipitation forecasts from a numerical weather prediction model for northern switzerland. in contrast to the raw forecasts from the numerical model, the postprocessed forecasts are calibrated and quantify prediction uncertainty. moreover, they outperform the raw forecasts, in the sense that they have a lower mean absolute error.","10.1111/rssb.12061","2012-04-27","2016-02-11","['fabio sigrist', 'hans r. künsch', 'werner a. stahel']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"388",1204.6286,"skewed multivariate birnbaum-saunders distributions","stat.me","the univariate birnbaum-saunders distribution has been used quite effectively to model times to failure for materials subject to fatigue and for modeling lifetime data. in this article, we define a skewed version of the birnbaum-saunders distribution in the multivariate setting and derive several of its properties. the proposed skewed multivariate model is an absolutely continuous distribution whose marginals are univariate birnbaum-saunders distributions. estimation of the parameters by maximum likelihood is discussed and the fisher's information matrix is determined. a skewed bivariate version for the generalized birnbaum-saunders distribution is also introduced. we provide an application to real data which illustrates the usefulness of the proposed multivariate model.","","2012-04-27","","['artur j. lemonte', 'guillermo martínez-florez', 'germán moreno-arenas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"389",1204.6491,"a selective review of group selection in high-dimensional models","math.st stat.me stat.th","grouping structures arise naturally in many statistical modeling problems. several methods have been proposed for variable selection that respect grouping structure in variables. examples include the group lasso and several concave group selection methods. in this article, we give a selective review of group selection concerning methodological developments, theoretical properties and computational algorithms. we pay particular attention to group selection methods involving concave penalties. we address both group selection and bi-level selection methods. we describe several applications of these methods in nonparametric additive models, semiparametric regression, seemingly unrelated regressions, genomic data analysis and genome wide association studies. we also highlight some issues that require further study.","10.1214/12-sts392","2012-04-29","2013-01-04","['jian huang', 'patrick breheny', 'shuangge ma']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"390",1204.6505,"marginally specified priors for nonparametric bayesian estimation","stat.me","prior specification for nonparametric bayesian inference involves the difficult task of quantifying prior knowledge about a parameter of high, often infinite, dimension. realistically, a statistician is unlikely to have informed opinions about all aspects of such a parameter, but may have real information about functionals of the parameter, such the population mean or variance. this article proposes a new framework for nonparametric bayes inference in which the prior distribution for a possibly infinite-dimensional parameter is decomposed into two parts: an informative prior on a finite set of functionals, and a nonparametric conditional prior for the parameter given the functionals. such priors can be easily constructed from standard nonparametric prior distributions in common use, and inherit the large support of the standard priors upon which they are based. additionally, posterior approximations under these informative priors can generally be made via minor adjustments to existing markov chain approximation algorithms for standard nonparametric prior distributions. we illustrate the use of such priors in the context of multivariate density estimation using dirichlet process mixture models, and in the modeling of high-dimensional sparse contingency tables.","","2012-04-29","","['david c. kessler', 'peter d. hoff', 'david b. dunson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"391",1204.6703,"a spectral algorithm for latent dirichlet allocation","cs.lg stat.ml","the problem of topic modeling can be seen as a generalization of the clustering problem, in that it posits that observations are generated due to multiple latent factors (e.g., the words in each document are generated as a mixture of several active topics, as opposed to just one). this increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic probability vectors (the distributions over words for each topic), when only the words are observed and the corresponding topics are hidden.   we provide a simple and efficient learning procedure that is guaranteed to recover the parameters for a wide class of mixture models, including the popular latent dirichlet allocation (lda) model. for lda, the procedure correctly recovers both the topic probability vectors and the prior over the topics, using only trigram statistics (i.e., third order moments, which may be estimated with documents containing just three words). the method, termed excess correlation analysis (eca), is based on a spectral decomposition of low order moments (third and fourth order) via two singular value decompositions (svds). moreover, the algorithm is scalable since the svd operations are carried out on $k\times k$ matrices, where $k$ is the number of latent factors (e.g. the number of topics), rather than in the $d$-dimensional observed space (typically $d \gg k$).","","2012-04-30","2013-01-17","['animashree anandkumar', 'dean p. foster', 'daniel hsu', 'sham m. kakade', 'yi-kai liu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"392",1205.054,"a fitness model for scholarly impact analysis","stat.ap cs.dl cs.si physics.soc-ph","we propose a model to analyze citation growth and influences of fitness (competitiveness) factors in an evolving citation network. applying the proposed method to modeling citations to papers and scholars in the infovis 2004 data, a benchmark collection about a 31-year history of information visualization, leads to findings consistent with citation distributions in general and observations of the domain in particular. fitness variables based on prior impacts and the time factor have significant influences on citation outcomes. we find considerably large effect sizes from the fitness modeling, which suggest inevitable bias in citation analysis due to these factors. while raw citation scores offer little insight into the growth of infovis, normalization of the scores by influences of time and prior fitness offers a reasonable depiction of the field's development. the analysis demonstrates the proposed model's ability to produce results consistent with observed data and to support meaningful comparison of citation scores over time.","","2012-05-02","","['weimao ke']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"393",1205.1053,"variable selection for latent dirichlet allocation","cs.lg stat.ml","in latent dirichlet allocation (lda), topics are multinomial distributions over the entire vocabulary. however, the vocabulary usually contains many words that are not relevant in forming the topics. we adopt a variable selection method widely used in statistical modeling as a dimension reduction tool and combine it with lda. in this variable selection model for lda (vslda), topics are multinomial distributions over a subset of the vocabulary, and by excluding words that are not informative for finding the latent topic structure of the corpus, vslda finds topics that are more robust and discriminative. we compare three models, vslda, lda with symmetric priors, and lda with asymmetric priors, on heldout likelihood, mcmc chain consistency, and document classification. the performance of vslda is better than symmetric lda for likelihood and classification, better than asymmetric lda for consistency and classification, and about the same in the other comparisons.","","2012-05-03","","['dongwoo kim', 'yeonseung chung', 'alice oh']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"394",1205.1107,"estimation of spatial max-stable models using threshold exceedances","stat.ap stat.ot","parametric inference for spatial max-stable processes is difficult since the related likelihoods are unavailable. a composite likelihood approach based on the bivariate distribution of block maxima has been recently proposed in the literature. however modeling block maxima is a wasteful approach provided that other information is available. moreover an approach based on block, typically annual, maxima is unable to take into account the fact that maxima occur or not simultaneously. if time series of, say, daily data are available, then estimation procedures based on exceedances of a high threshold could mitigate such problems. in this paper we focus on two approaches for composing likelihoods based on pairs of exceedances. the first one comes from the tail approximation for bivariate distribution proposed by ledford and tawn (1996) when both pairs of observations exceed the fixed threshold. the second one uses the bivariate extension (rootzen and tajvidi, 2006) of the generalized pareto distribution which allows to model exceedances when at least one of the components is over the threshold. the two approaches are compared through a simulation study according to different degrees of spatial dependency. results show that both the strength of the spatial dependencies and the threshold choice play a fundamental role in determining which is the best estimating procedure.","","2012-05-05","","['jean-noel bacro', 'carlo gaetan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE
"395",1205.1819,"a model for sequential evolution of ligands by exponential enrichment   (selex) data","stat.ap q-bio.gn","a systematic evolution of ligands by exponential enrichment (selex) experiment begins in round one with a random pool of oligonucleotides in equilibrium solution with a target. over a few rounds, oligonucleotides having a high affinity for the target are selected. data from a high throughput selex experiment consists of lists of thousands of oligonucleotides sampled after each round. thus far, selex experiments have been very good at suggesting the highest affinity oligonucleotide, but modeling lower affinity recognition site variants has been difficult. furthermore, an alignment step has always been used prior to analyzing selex data. we present a novel model, based on a biochemical parametrization of selex, which allows us to use data from all rounds to estimate the affinities of the oligonucleotides. most notably, our model also aligns the oligonucleotides. we use our model to analyze a selex experiment containing double stranded dna oligonucleotides and the transcription factor bicoid as the target. our selex model outperformed other published methods for predicting putative binding sites for bicoid as indicated by the results of an in-vivo chip-chip experiment.","10.1214/12-aoas537","2012-05-08","2012-09-27","['juli atherton', 'nathan boley', 'ben brown', 'nobuo ogawa', 'stuart m. davidson', 'michael b. eisen', 'mark d. biggin', 'peter bickel']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"396",1205.2614,"products of hidden markov models: it takes n>1 to tango","cs.lg stat.ml","products of hidden markov models(pohmms) are an interesting class of generative models which have received little attention since their introduction. this maybe in part due to their more computationally expensive gradient-based learning algorithm,and the intractability of computing the log likelihood of sequences under the model. in this paper, we demonstrate how the partition function can be estimated reliably via annealed importance sampling. we perform experiments using contrastive divergence learning on rainfall data and data captured from pairs of people dancing. our results suggest that advances in learning and evaluation for undirected graphical models and recent increases in available computing power make pohmms worth considering for complex time-series modeling tasks.","","2012-05-09","","['graham w taylor', 'geoffrey e. hinton']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"397",1205.2627,"domain knowledge uncertainty and probabilistic parameter constraints","cs.lg stat.ml","incorporating domain knowledge into the modeling process is an effective way to improve learning accuracy. however, as it is provided by humans, domain knowledge can only be specified with some degree of uncertainty. we propose to explicitly model such uncertainty through probabilistic constraints over the parameter space. in contrast to hard parameter constraints, our approach is effective also when the domain knowledge is inaccurate and generally results in superior modeling accuracy. we focus on generative and conditional modeling where the parameters are assigned a dirichlet or gaussian prior and demonstrate the framework with experiments on both synthetic and real-world data.","","2012-05-09","","['yi mao', 'guy lebanon']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"398",1205.2662,"on smoothing and inference for topic models","cs.lg stat.ml","latent dirichlet analysis, or topic modeling, is a flexible latent variable framework for modeling high-dimensional sparse count data. various learning algorithms have been developed in recent years, including collapsed gibbs sampling, variational inference, and maximum a posteriori estimation, and this variety motivates the need for careful empirical comparisons. in this paper, we highlight the close connections between these approaches. we find that the main differences are attributable to the amount of smoothing applied to the counts. when the hyperparameters are optimized, the differences in performance among the algorithms diminish significantly. the ability of these algorithms to achieve solutions of comparable accuracy gives us the freedom to select computationally efficient approaches. using the insights gained from this comparative study, we show how accurate topic models can be learned in several seconds on text corpora with thousands of documents.","","2012-05-09","","['arthur asuncion', 'max welling', 'padhraic smyth', 'yee whye teh']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"399",1205.2816,"bayesian modeling of temporal dependence in large sparse contingency   tables","stat.me","in many applications, it is of interest to study trends over time in relationships among categorical variables, such as age group, ethnicity, religious affiliation, political party and preference for particular policies. at each time point, a sample of individuals provide responses to a set of questions, with different individuals sampled at each time. in such settings, there tends to be abundant missing data and the variables being measured may change over time. at each time point, one obtains a large sparse contingency table, with the number of cells often much larger than the number of individuals being surveyed. to borrow information across time in modeling large sparse contingency tables, we propose a bayesian autoregressive tensor factorization approach. the proposed model relies on a probabilistic parafac factorization of the joint pmf characterizing the categorical data distribution at each time point, with autocorrelation included across times. efficient computational methods are developed relying on mcmc. the methods are evaluated through simulation examples and applied to social survey data.","","2012-05-12","","['tsuyoshi kunihama', 'david b. dunson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"400",1205.3347,"testing for associations between loci and environmental gradients using   latent factor mixed models","q-bio.pe stat.co","adaptation to local environments often occurs through natural selection acting on a large number of loci, each having a weak phenotypic effect. one way to detect these loci is to identify genetic polymorphisms that exhibit high correlation with environmental variables used as proxies for ecological pressures. here, we propose new algorithms based on population genetics, ecological modeling, and statistical learning techniques to screen genomes for signatures of local adaptation. implemented in the computer program ""latent factor mixed model"" (lfmm), these algorithms employ an approach in which population structure is introduced using unobserved variables. these fast and computationally efficient algorithms detect correlations between environmental and genetic variation while simultaneously inferring background levels of population structure. comparing these new algorithms with related methods provides evidence that lfmm can efficiently estimate random effects due to population history and isolation-by-distance patterns when computing gene-environment correlations, and decrease the number of false-positive associations in genome scans. we then apply these models to plant and human genetic data, identifying several genes with functions related to development that exhibit strong correlations with climatic gradients.","10.1093/molbev/mst063","2012-05-15","2013-09-26","['eric frichot', 'sean schoville', 'guillaume bouchard', 'olivier françois']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"401",1205.4697,"inference using noisy degrees: differentially private $\beta$-model and   synthetic graphs","stat.me cs.ds","the $\beta$-model of random graphs is an exponential family model with the degree sequence as a sufficient statistic. in this paper, we contribute three key results. first, we characterize conditions that lead to a quadratic time algorithm to check for the existence of mle of the $\beta$-model, and show that the mle never exists for the degree partition $\beta$-model. second, motivated by privacy problems with network data, we derive a differentially private estimator of the parameters of $\beta$-model, and show it is consistent and asymptotically normally distributed - it achieves the same rate of convergence as the nonprivate estimator. we present an efficient algorithm for the private estimator that can be used to release synthetic graphs. our techniques can also be used to release degree distributions and degree partitions accurately and privately, and to perform inference from noisy degrees arising from contexts other than privacy. we evaluate the proposed estimator on real graphs and compare it with a current algorithm for releasing degree distributions and find that it does significantly better. finally, our paper addresses shortcomings of current approaches to a fundamental problem of how to perform valid statistical inference from data released by privacy mechanisms, and lays a foundational groundwork on how to achieve optimal and private statistical inference in a principled manner by modeling the privacy mechanism; these principles should be applicable to a class of models beyond the $\beta$-model.","10.1214/15-aos1358","2012-05-21","2016-01-12","['vishesh karwa', 'aleksandra slavković']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"402",1205.484,"statistical study of asymmetry in cell lineage data","stat.ap q-bio.qm","a rigorous methodology is proposed to study cell division data consisting in several observed genealogical trees of possibly different shapes. the procedure takes into account missing observations, data from different trees, as well as the dependence structure within genealogical trees. its main new feature is the joint use of all available information from several data sets instead of single data set estimation, to avoid the drawbacks of low accuracy for estimators or low power for tests on small single-trees. the data is modeled by an asymmetric bifurcating autoregressive process and possibly missing observations are taken into account by modeling the genealogies with a two-type galton-watson process. least-squares estimators of the unknown parameters of the processes are given and symmetry tests are derived. results are applied on real data of escherichia coli division and an empirical study of the convergence rates of the estimators and power of the tests is conducted on simulated data.","","2012-05-22","2013-04-12","['benoîte de saporta', 'anne gégout petit', 'laurence marsalle']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"403",1205.5467,"transformed gaussian markov random fields and spatial modeling","stat.me","the gaussian random field (grf) and the gaussian markov random field (gmrf) have been widely used to accommodate spatial dependence under the generalized linear mixed model framework. these models have limitations rooted in the symmetry and thin tail of the gaussian distribution. we introduce a new class of random fields, termed transformed grf (tgrf), and a new class of markov random fields, termed transformed gmrf (tgmrf). they are constructed by transforming the margins of grfs and gmrfs, respectively, to desired marginal distributions to accommodate asymmetry and heavy tail as needed in practice. the gaussian copula that characterizes the dependence structure facilitates inferences and applications in modeling spatial dependence. this construction leads to new models such as gamma or beta markov fields with gaussian copulas, which can be used to model poisson intensity or bernoulli rate in a spatial generalized linear mixed model. the method is naturally implemented in a bayesian framework. we illustrate the utility of the methodology in an ecological application with spatial count data and spatial presence/absence data of some snail species, where the new models are shown to outperform the traditional spatial models. the validity of bayesian inferences and model selection are assessed through simulation studies for both spatial poisson regression and spatial bernoulli regression.","10.1016/j.spasta.2015.07.004","2012-05-24","","['marcos o. prates', 'dipak k. dey', 'michael r. willig', 'jun yan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"404",1205.5868,"sparse estimation via nonconcave penalized likelihood in a factor   analysis model","stat.me stat.co stat.ml","we consider the problem of sparse estimation in a factor analysis model. a traditional estimation procedure in use is the following two-step approach: the model is estimated by maximum likelihood method and then a rotation technique is utilized to find sparse factor loadings. however, the maximum likelihood estimates cannot be obtained when the number of variables is much larger than the number of observations. furthermore, even if the maximum likelihood estimates are available, the rotation technique does not often produce a sufficiently sparse solution. in order to handle these problems, this paper introduces a penalized likelihood procedure that imposes a nonconvex penalty on the factor loadings. we show that the penalized likelihood procedure can be viewed as a generalization of the traditional two-step approach, and the proposed methodology can produce sparser solutions than the rotation technique. a new algorithm via the em algorithm along with coordinate descent is introduced to compute the entire solution path, which permits the application to a wide variety of convex and nonconvex penalties. monte carlo simulations are conducted to investigate the performance of our modeling strategy. a real data example is also given to illustrate our procedure.","","2012-05-26","2013-03-15","['kei hirose', 'michio yamamoto']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE
"405",1205.6439,"a mathematical reformulation of the reference price","stat.ap stat.co","reference prices have long been studied in applied economics and business research. one of the classic formulations of the reference price is in terms of an iterative function of past prices. there are a number of limitations of such a formulation, however. such limitations include burdensome computational time to estimate parameters, an inability to truly account for customer heterogeneity, and an estimation procedure that implies a misspecified model. managerial recommendations based on inferences from such a model can be quite misleading. we mathematically reformulate the reference price by developing a closed-form expansion that addresses the aforementioned issues, enabling one to elicit truly meaningful managerial advice from the model. we estimate our model on a real world data set to illustrate the efficacy of our approach. our work is not only useful from a modeling perspective, but also has important behavioral and managerial implications, which modelers and non-modelers alike would find useful.","","2012-05-29","","['kevin d. dayaratna', 'p. k. kannan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE
"406",1206.0613,"factor modeling for high-dimensional time series: inference for the   number of factors","math.st stat.th","this paper deals with the factor modeling for high-dimensional time series based on a dimension-reduction viewpoint. under stationary settings, the inference is simple in the sense that both the number of factors and the factor loadings are estimated in terms of an eigenanalysis for a nonnegative definite matrix, and is therefore applicable when the dimension of time series is on the order of a few thousands. asymptotic properties of the proposed method are investigated under two settings: (i) the sample size goes to infinity while the dimension of time series is fixed; and (ii) both the sample size and the dimension of time series go to infinity together. in particular, our estimators for zero-eigenvalues enjoy faster convergence (or slower divergence) rates, hence making the estimation for the number of factors easier. in particular, when the sample size and the dimension of time series go to infinity together, the estimators for the eigenvalues are no longer consistent. however, our estimator for the number of the factors, which is based on the ratios of the estimated eigenvalues, still works fine. furthermore, this estimation shows the so-called ""blessing of dimensionality"" property in the sense that the performance of the estimation may improve when the dimension of time series increases. a two-step procedure is investigated when the factors are of different degrees of strength. numerical illustration with both simulated and real data is also reported.","10.1214/12-aos970","2012-06-04","","['clifford lam', 'qiwei yao']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"407",1206.1493,"effect of solar-terrestrial phenomena on solar cell's efficiency","stat.ap","it is assumed that the solar cell efficiency of pv device is closely related to the solar irradiance, considered the solar parameter global solar irradiance (g) and the meteorological parameters like daily data of earth skin temperature (e), average temperature (t), relative humidity (h) and dew frost point (d), for the coastal city karachi and a non-coastal city jacobabad, k and j is used as a subscripts for parameters of karachi and jacobabad respectively. all variables used here are dependent on the location (latitude and longitude) of our stations except g. to employ arima modeling, the first eighteen years data is used for modeling and forecast is done for the last five years data. in most cases results show good correlation among monthly actual and monthly forecasted values of all the predictors. next, multiple linear regression is employed to the data obtained by arima modeling and models for mean monthly observed g values are constructed. for each station, two equations are constructed the r2 values are above 93% for each model, showing adequacy of the fit. our computations show that solar cell efficiency can be increased if better modeling for meteorological predictors governs the process.","","2012-06-07","","['kashif bin zaheer', 'waseem ahmed ansari', 'syed mohammad murshid raza']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"408",1206.1773,"sparse component separation for accurate cmb map estimation","astro-ph.co astro-ph.im stat.ap","the cosmological microwave background (cmb) is of premier importance for the cosmologists to study the birth of our universe. unfortunately, most cmb experiments such as cobe, wmap or planck do not provide a direct measure of the cosmological signal; cmb is mixed up with galactic foregrounds and point sources. for the sake of scientific exploitation, measuring the cmb requires extracting several different astrophysical components (cmb, sunyaev-zel'dovich clusters, galactic dust) form multi-wavelength observations. mathematically speaking, the problem of disentangling the cmb map from the galactic foregrounds amounts to a component or source separation problem. in the field of cmb studies, a very large range of source separation methods have been applied which all differ from each other in the way they model the data and the criteria they rely on to separate components. two main difficulties are i) the instrument's beam varies across frequencies and ii) the emission laws of most astrophysical components vary across pixels. this paper aims at introducing a very accurate modeling of cmb data, based on sparsity, accounting for beams variability across frequencies as well as spatial variations of the components' spectral characteristics. based on this new sparse modeling of the data, a sparsity-based component separation method coined local-generalized morphological component analysis (l-gmca) is described. extensive numerical experiments have been carried out with simulated planck data. these experiments show the high efficiency of the proposed component separation methods to estimate a clean cmb map with a very low foreground contamination, which makes l-gmca of prime interest for cmb studies.","10.1051/0004-6361/201219781","2012-06-08","","['j. bobin', 'j. -l. starck', 'f. sureau', 's. basak']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"409",1206.1874,"multivariate bernoulli distribution","stat.ap math.st stat.ml stat.th","in this paper, we consider the multivariate bernoulli distribution as a model to estimate the structure of graphs with binary nodes. this distribution is discussed in the framework of the exponential family, and its statistical properties regarding independence of the nodes are demonstrated. importantly the model can estimate not only the main effects and pairwise interactions among the nodes but also is capable of modeling higher order interactions, allowing for the existence of complex clique effects. we compare the multivariate bernoulli model with existing graphical inference models - the ising model and the multivariate gaussian model, where only the pairwise interactions are considered. on the other hand, the multivariate bernoulli distribution has an interesting property in that independence and uncorrelatedness of the component random variables are equivalent. both the marginal and conditional distributions of a subset of variables in the multivariate bernoulli distribution still follow the multivariate bernoulli distribution. furthermore, the multivariate bernoulli logistic model is developed under generalized linear model theory by utilizing the canonical link function in order to include covariate information on the nodes, edges and cliques. we also consider variable selection techniques such as lasso in the logistic model to impose sparsity structure on the graph. finally, we discuss extending the smoothing spline anova approach to the multivariate bernoulli logistic model to enable estimation of non-linear effects of the predictor variables.","10.3150/12-bejsp10","2012-06-08","2013-11-12","['bin dai', 'shilin ding', 'grace wahba']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE
"410",1206.2696,"flexible variable selection for recovering sparsity in nonadditive   nonparametric models","stat.me","variable selection for recovering sparsity in nonadditive nonparametric models has been challenging. this problem becomes even more difficult due to complications in modeling unknown interaction terms among high dimensional variables. there is currently no variable selection method to overcome these limitations. hence, in this paper we propose a variable selection approach that is developed by connecting a kernel machine with the nonparametric multiple regression model. the advantages of our approach are that it can: (1) recover the sparsity, (2) automatically model unknown and complicated interactions, (3) connect with several existing approaches including linear nonnegative garrote, kernel learning and automatic relevant determinants (ard), and (4) provide flexibility for both additive and nonadditive nonparametric models. our approach may be viewed as a nonlinear version of a nonnegative garrote method. we model the smoothing function by a least squares kernel machine and construct the nonnegative garrote objective function as the function of the similarity matrix. since the multiple regression similarity matrix can be written as an additive form of univariate similarity matrices corresponding to input variables, applying a sparse scale parameter on each univariate similarity matrix can reveal its relevance to the response variable. we also derive the asymptotic properties of our approach, and show that it provides a square root consistent estimator of the scale parameters. furthermore, we prove that sparsistency is satisfied with consistent initial kernel function coefficients under certain conditions and give the necessary and sufficient conditions for sparsistency. an efficient coordinate descent/backfitting algorithm is developed. a resampling procedure for our variable selection methodology is also proposed to improve power.","","2012-06-12","","['zaili fang', 'inyoung kim', 'patrick schaumont']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"411",1206.3249,"projected subgradient methods for learning sparse gaussians","cs.lg stat.ml","gaussian markov random fields (gmrfs) are useful in a broad range of applications. in this paper we tackle the problem of learning a sparse gmrf in a high-dimensional space. our approach uses the l1-norm as a regularization on the inverse covariance matrix. we utilize a novel projected gradient method, which is faster than previous methods in practice and equal to the best performing of these in asymptotic complexity. we also extend the l1-regularized objective to the problem of sparsifying entire blocks within the inverse covariance matrix. our methods generalize fairly easily to this case, while other methods do not. we demonstrate that our extensions give better generalization performance on two real domains--biological network analysis and a 2d-shape modeling image task.","","2012-06-13","","['john duchi', 'stephen gould', 'daphne koller']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"412",1206.3252,"convex point estimation using undirected bayesian transfer hierarchies","cs.lg stat.ml","when related learning tasks are naturally arranged in a hierarchy, an appealing approach for coping with scarcity of instances is that of transfer learning using a hierarchical bayes framework. as fully bayesian computations can be difficult and computationally demanding, it is often desirable to use posterior point estimates that facilitate (relatively) efficient prediction. however, the hierarchical bayes framework does not always lend itself naturally to this maximum aposteriori goal. in this work we propose an undirected reformulation of hierarchical bayes that relies on priors in the form of similarity measures. we introduce the notion of ""degree of transfer"" weights on components of these similarity measures, and show how they can be automatically learned within a joint probabilistic framework. importantly, our reformulation results in a convex objective for many learning problems, thus facilitating optimal posterior point estimation using standard optimization techniques. in addition, we no longer require proper priors, allowing for flexible and straightforward specification of joint distributions over transfer hierarchies. we show that our framework is effective for learning models that are part of transfer hierarchies for two real-life tasks: object shape modeling using gaussian density estimation and document classification.","","2012-06-13","","['gal elidan', 'ben packer', 'geremy heitz', 'daphne koller']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"413",1206.3254,"latent topic models for hypertext","cs.ir cs.cl cs.lg stat.ml","latent topic models have been successfully applied as an unsupervised topic discovery technique in large document collections. with the proliferation of hypertext document collection such as the internet, there has also been great interest in extending these approaches to hypertext [6, 9]. these approaches typically model links in an analogous fashion to how they model words - the document-link co-occurrence matrix is modeled in the same way that the document-word co-occurrence matrix is modeled in standard topic models. in this paper we present a probabilistic generative model for hypertext document collections that explicitly models the generation of links. specifically, links from a word w to a document d depend directly on how frequent the topic of w is in d, in addition to the in-degree of d. we show how to perform em learning on this model efficiently. by not modeling links as analogous to words, we end up using far fewer free parameters and obtain better link prediction results.","","2012-06-13","","['amit gruber', 'michal rosen-zvi', 'yair weiss']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"414",1206.3279,"the phylogenetic indian buffet process: a non-exchangeable nonparametric   prior for latent features","cs.lg stat.ml","nonparametric bayesian models are often based on the assumption that the objects being modeled are exchangeable. while appropriate in some applications (e.g., bag-of-words models for documents), exchangeability is sometimes assumed simply for computational reasons; non-exchangeable models might be a better choice for applications based on subject matter. drawing on ideas from graphical models and phylogenetics, we describe a non-exchangeable prior for a class of nonparametric latent feature models that is nearly as efficient computationally as its exchangeable counterpart. our model is applicable to the general setting in which the dependencies between objects can be expressed using a tree, where edge lengths indicate the strength of relationships. we demonstrate an application to modeling probabilistic choice.","","2012-06-13","","['kurt t. miller', 'thomas griffiths', 'michael i. jordan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"415",1206.3421,"linear latent variable models: the lava-package","stat.co","an r package for specifying and estimating linear latent variable models is presented. the philosophy of the implementation is to separate the model specification from the actual data, which leads to a dynamic and easy way of modeling complex hierarchical structures. several advanced features are implemented including robust standard errors for clustered correlated data, multigroup analyses, non-linear parameter constraints, inference with incomplete data, maximum likelihood estimation with censored and binary observations, and instrumental variable estimators. in addition an extensive simulation interface covering a broad range of non-linear generalized structural equation models is described. the model and software are demonstrated in data of measurements of the serotonin transporter in the human brain.","10.1007/s00180-012-0344-y","2012-06-15","","['klaus k. holst', 'esben budtz-jørgensen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"416",1206.3627,"posterior contraction in sparse bayesian factor models for massive   covariance matrices","math.st stat.th","sparse bayesian factor models are routinely implemented for parsimonious dependence modeling and dimensionality reduction in high-dimensional applications. we provide theoretical understanding of such bayesian procedures in terms of posterior convergence rates in inferring high-dimensional covariance matrices where the dimension can be larger than the sample size. under relevant sparsity assumptions on the true covariance matrix, we show that commonly-used point mass mixture priors on the factor loadings lead to consistent estimation in the operator norm even when $p\gg n$. one of our major contributions is to develop a new class of continuous shrinkage priors and provide insights into their concentration around sparse vectors. using such priors for the factor loadings, we obtain similar rate of convergence as obtained with point mass mixture priors. to obtain the convergence rates, we construct test functions to separate points in the space of high-dimensional covariance matrices using insights from random matrix theory; the tools developed may be of independent interest. we also derive minimax rates and show that the bayesian posterior rates of convergence coincide with the minimax rates upto a $\sqrt{\log n}$ term.","10.1214/14-aos1215","2012-06-15","2014-06-02","['debdeep pati', 'anirban bhattacharya', 'natesh s. pillai', 'david dunson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"417",1206.372,"description of the emod-hiv model v0.7","q-bio.qm q-bio.pe stat.ap","the expansion of tools against hiv transmission has brought increased interest in epidemiological models that can predict the impact of these interventions. the emod-hiv model was recently compared to eleven other independently developed mathematical models of hiv transmission to determine the extent to which they agree about the potential impact of expanded use of antiretroviral therapy in south africa. here we describe in detail the modeling methodology used to produce the results in this comparison, which we term emod-hiv v0.7. we include a discussion of the structure and a full list of model parameters. we also discuss the architecture of the model, and its potential utility in comparing structural assumptions within a single modeling framework.","","2012-06-16","","['anna bershteyn', 'daniel j. klein', 'edward wenger', 'philip a. eckhoff']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"418",1206.4278,"commentary on bayesian coincidence assessment (cross-matching)","astro-ph.im stat.ap","this paper is an invited commentary on tamas budavari's presentation, ""on statistical cross-identification in astronomy,"" for the statistical challenges in modern astronomy v conference held at pennsylvania state university in june 2011. i begin with a brief review of previous work on probabilistic (bayesian) assessment of directional and spatio-temporal coincidences in astronomy (e.g., cross-matching or cross-identification of objects across multiple catalogs). then i discuss an open issue in the recent innovative work of budavari and his colleagues on large-scale probabilistic cross-identification: how to assign prior probabilities that play an important role in the analysis. with a simple toy problem, i show how bayesian multilevel modeling (hierarchical bayes) provides a principled framework that justifies and generalizes pragmatic rules of thumb that have been successfully used by budavari's team to assign priors.","","2012-06-19","","['thomas j. loredo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"419",1206.46,"bayesian nonexhaustive learning for online discovery and modeling of   emerging classes","cs.lg stat.ml","we present a framework for online inference in the presence of a nonexhaustively defined set of classes that incorporates supervised classification with class discovery and modeling. a dirichlet process prior (dpp) model defined over class distributions ensures that both known and unknown class distributions originate according to a common base distribution. in an attempt to automatically discover potentially interesting class formations, the prior model is coupled with a suitably chosen data model, and sequential monte carlo sampling is used to perform online inference. our research is driven by a biodetection application, where a new class of pathogen may suddenly appear, and the rapid increase in the number of samples originating from this class indicates the onset of an outbreak.","","2012-06-18","","['murat dundar', 'ferit akova', 'alan qi', 'bartek rajwa']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"420",1206.4601,"convex multitask learning with flexible task clusters","cs.lg stat.ml","traditionally, multitask learning (mtl) assumes that all the tasks are related. this can lead to negative transfer when tasks are indeed incoherent. recently, a number of approaches have been proposed that alleviate this problem by discovering the underlying task clusters or relationships. however, they are limited to modeling these relationships at the task level, which may be restrictive in some applications. in this paper, we propose a novel mtl formulation that captures task relationships at the feature-level. depending on the interactions among tasks and features, the proposed method construct different task clusters for different features, without even the need of pre-specifying the number of clusters. computationally, the proposed formulation is strongly convex, and can be efficiently solved by accelerated proximal methods. experiments are performed on a number of synthetic and real-world data sets. under various degrees of task relationships, the accuracy of the proposed method is consistently among the best. moreover, the feature-specific task clusters obtained agree with the known/plausible task structures of the data.","","2012-06-18","","['wenliang zhong', 'james kwok']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"421",1206.4616,"a hierarchical dirichlet process model with multiple levels of   clustering for human eeg seizure modeling","stat.ap cs.lg stat.ml","driven by the multi-level structure of human intracranial electroencephalogram (ieeg) recordings of epileptic seizures, we introduce a new variant of a hierarchical dirichlet process---the multi-level clustering hierarchical dirichlet process (mlc-hdp)---that simultaneously clusters datasets on multiple levels. our seizure dataset contains brain activity recorded in typically more than a hundred individual channels for each seizure of each patient. the mlc-hdp model clusters over channels-types, seizure-types, and patient-types simultaneously. we describe this model and its implementation in detail. we also present the results of a simulation study comparing the mlc-hdp to a similar model, the nested dirichlet process and finally demonstrate the mlc-hdp's use in modeling seizures across multiple patients. we find the mlc-hdp's clustering to be comparable to independent human physician clusterings. to our knowledge, the mlc-hdp model is the first in the epilepsy literature capable of clustering seizures within and between patients.","","2012-06-18","","['drausin wulsin', 'shane jensen', 'brian litt']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"422",1206.4645,"ensemble methods for convex regression with applications to geometric   programming based circuit design","cs.lg cs.na stat.me stat.ml","convex regression is a promising area for bridging statistical estimation and deterministic convex optimization. new piecewise linear convex regression methods are fast and scalable, but can have instability when used to approximate constraints or objective functions for optimization. ensemble methods, like bagging, smearing and random partitioning, can alleviate this problem and maintain the theoretical properties of the underlying estimator. we empirically examine the performance of ensemble methods for prediction and optimization, and then apply them to device modeling and constraint approximation for geometric programming based circuit design.","","2012-06-18","","['lauren hannah', 'david dunson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"423",1206.4649,"learning efficient structured sparse models","cs.lg cs.cv stat.ml","we present a comprehensive framework for structured sparse coding and modeling extending the recent ideas of using learnable fast regressors to approximate exact sparse codes. for this purpose, we develop a novel block-coordinate proximal splitting method for the iterative solution of hierarchical sparse coding problems, and show an efficient feed forward architecture derived from its iteration. this architecture faithfully approximates the exact structured sparse codes with a fraction of the complexity of the standard optimization methods. we also show that by using different training objective functions, learnable sparse encoders are no longer restricted to be mere approximants of the exact sparse code for a pre-given dictionary, as in earlier formulations, but can be rather used as full-featured sparse encoders or even modelers. a simple implementation shows several orders of magnitude speedup compared to the state-of-the-art at minimal performance degradation, making the proposed framework suitable for real time and large-scale applications.","","2012-06-18","","['alex bronstein', 'pablo sprechmann', 'guillermo sapiro']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"424",1206.4658,"dirichlet process with mixed random measures: a nonparametric topic   model for labeled data","cs.lg stat.ml","we describe a nonparametric topic model for labeled data. the model uses a mixture of random measures (mrm) as a base distribution of the dirichlet process (dp) of the hdp framework, so we call it the dp-mrm. to model labeled data, we define a dp distributed random measure for each label, and the resulting model generates an unbounded number of topics for each label. we apply dp-mrm on single-labeled and multi-labeled corpora of documents and compare the performance on label prediction with medlda, lda-svm, and labeled-lda. we further enhance the model by incorporating ddcrp and modeling multi-labeled images for image segmentation and object labeling, comparing the performance with ncuts and rddcrp.","","2012-06-18","","['dongwoo kim', 'suin kim', 'alice oh']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"425",1206.4671,"dependent hierarchical normalized random measures for dynamic topic   modeling","cs.lg stat.ml","we develop dependent hierarchical normalized random measures and apply them to dynamic topic modeling. the dependency arises via superposition, subsampling and point transition on the underlying poisson processes of these measures. the measures used include normalised generalised gamma processes that demonstrate power law properties, unlike dirichlet processes used previously in dynamic topic modeling. inference for the model includes adapting a recently developed slice sampler to directly manipulate the underlying poisson process. experiments performed on news, blogs, academic and twitter collections demonstrate the technique gives superior perplexity over a number of previous models.","","2012-06-18","","['changyou chen', 'nan ding', 'wray buntine']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"426",1206.4685,"sparse-gev: sparse latent space model for multivariate extreme value   time serie modeling","stat.me cs.lg stat.ap","in many applications of time series models, such as climate analysis and social media analysis, we are often interested in extreme events, such as heatwave, wind gust, and burst of topics. these time series data usually exhibit a heavy-tailed distribution rather than a gaussian distribution. this poses great challenges to existing approaches due to the significantly different assumptions on the data distributions and the lack of sufficient past data on extreme events. in this paper, we propose the sparse-gev model, a latent state model based on the theory of extreme value modeling to automatically learn sparse temporal dependence and make predictions. our model is theoretically significant because it is among the first models to learn sparse temporal dependencies among multivariate extreme value time series. we demonstrate the superior performance of our algorithm to the state-of-art methods, including granger causality, copula approach, and transfer entropy, on one synthetic dataset, one climate dataset and two twitter datasets.","","2012-06-18","","['yan liu', 'taha bahadori', 'hongfei li']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"427",1206.5036,"estimating densities with non-parametric exponential families","stat.ml cs.lg","we propose a novel approach for density estimation with exponential families for the case when the true density may not fall within the chosen family. our approach augments the sufficient statistics with features designed to accumulate probability mass in the neighborhood of the observed points, resulting in a non-parametric model similar to kernel density estimators. we show that under mild conditions, the resulting model uses only the sufficient statistics if the density is within the chosen exponential family, and asymptotically, it approximates densities outside of the chosen exponential family. using the proposed approach, we modify the exponential random graph model, commonly used for modeling small-size graph distributions, to address the well-known issue of model degeneracy.","","2012-06-21","2012-09-06","['lin yuan', 'sergey kirshner', 'robert givan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"428",1206.5102,"hidden markov models with mixtures as emission distributions","stat.ml cs.lg stat.co","in unsupervised classification, hidden markov models (hmm) are used to account for a neighborhood structure between observations. the emission distributions are often supposed to belong to some parametric family. in this paper, a semiparametric modeling where the emission distributions are a mixture of parametric distributions is proposed to get a higher flexibility. we show that the classical em algorithm can be adapted to infer the model parameters. for the initialisation step, starting from a large number of components, a hierarchical method to combine them into the hidden states is proposed. three likelihood-based criteria to select the components to be combined are discussed. to estimate the number of hidden states, bic-like criteria are derived. a simulation study is carried out both to determine the best combination between the merging criteria and the model selection criteria and to evaluate the accuracy of classification. the proposed method is also illustrated using a biological dataset from the model plant arabidopsis thaliana. a r package hmmmix is freely available on the cran.","","2012-06-22","","['stevenn volant', 'caroline bérard', 'marie-laure martin-magniette', 'stéphane robin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE
"429",1206.5241,"shift-invariance sparse coding for audio classification","cs.lg stat.ml","sparse coding is an unsupervised learning algorithm that learns a succinct high-level representation of the inputs given only unlabeled data; it represents each input as a sparse linear combination of a set of basis functions. originally applied to modeling the human visual cortex, sparse coding has also been shown to be useful for self-taught learning, in which the goal is to solve a supervised classification task given access to additional unlabeled data drawn from different classes than that in the supervised learning problem. shift-invariant sparse coding (sisc) is an extension of sparse coding which reconstructs a (usually time-series) input using all of the basis functions in all possible shifts. in this paper, we present an efficient algorithm for learning sisc bases. our method is based on iteratively solving two large convex optimization problems: the first, which computes the linear coefficients, is an l1-regularized linear least squares problem with potentially hundreds of thousands of variables. existing methods typically use a heuristic to select a small subset of the variables to optimize, but we present a way to efficiently compute the exact solution. the second, which solves for bases, is a constrained linear least squares problem. by optimizing over complex-valued variables in the fourier domain, we reduce the coupling between the different variables, allowing the problem to be solved efficiently. we show that sisc's learned high-level representations of speech and music provide useful features for classification tasks within those domains. when applied to classification, under certain conditions the learned features outperform state of the art spectral and cepstral features.","","2012-06-20","","['roger grosse', 'rajat raina', 'helen kwong', 'andrew y. ng']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"430",1206.5248,"statistical translation, heat kernels and expected distances","cs.lg cs.cv cs.ir stat.ml","high dimensional structured data such as text and images is often poorly understood and misrepresented in statistical modeling. the standard histogram representation suffers from high variance and performs poorly in general. we explore novel connections between statistical translation, heat kernels on manifolds and graphs, and expected distances. these connections provide a new framework for unsupervised metric learning for text documents. experiments indicate that the resulting distances are generally superior to their more standard counterparts.","","2012-06-20","","['joshua dillon', 'yi mao', 'guy lebanon', 'jian zhang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"431",1206.5278,"fast nonparametric conditional density estimation","stat.me cs.lg stat.ml","conditional density estimation generalizes regression by modeling a full density f(yjx) rather than only the expected value e(yjx). this is important for many tasks, including handling multi-modality and generating prediction intervals. though fundamental and widely applicable, nonparametric conditional density estimators have received relatively little attention from statisticians and little or none from the machine learning community. none of that work has been applied to greater than bivariate data, presumably due to the computational difficulty of data-driven bandwidth selection. we describe the double kernel conditional density estimator and derive fast dual-tree-based algorithms for bandwidth selection using a maximum likelihood criterion. these techniques give speedups of up to 3.8 million in our experiments, and enable the first applications to previously intractable large multivariate datasets, including a redshift prediction problem from the sloan digital sky survey.","","2012-06-20","","['michael p. holmes', 'alexander g. gray', 'charles lee isbell']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"432",1206.529,"imitation learning with a value-based prior","cs.lg cs.ai stat.ml","the goal of imitation learning is for an apprentice to learn how to behave in a stochastic environment by observing a mentor demonstrating the correct behavior. accurate prior knowledge about the correct behavior can reduce the need for demonstrations from the mentor. we present a novel approach to encoding prior knowledge about the correct behavior, where we assume that this prior knowledge takes the form of a markov decision process (mdp) that is used by the apprentice as a rough and imperfect model of the mentor's behavior. specifically, taking a bayesian approach, we treat the value of a policy in this modeling mdp as the log prior probability of the policy. in other words, we assume a priori that the mentor's behavior is likely to be a high value policy in the modeling mdp, though quite possibly different from the optimal policy. we describe an efficient algorithm that, given a modeling mdp and a set of demonstrations by a mentor, provably converges to a stationary point of the log posterior of the mentor's policy, where the posterior is computed with respect to the ""value based"" prior. we also present empirical evidence that this prior does in fact speed learning of the mentor's policy, and is an improvement in our experiments over similar previous methods.","","2012-06-20","","['umar syed', 'robert e. schapire']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"433",1206.5754,"bayesian modeling with gaussian processes using the gpstuff toolbox","stat.ml cs.ai cs.ms","gaussian processes (gp) are powerful tools for probabilistic modeling purposes. they can be used to define prior distributions over latent functions in hierarchical bayesian models. the prior over functions is defined implicitly by the mean and covariance function, which determine the smoothness and variability of the function. the inference can then be conducted directly in the function space by evaluating or approximating the posterior process. despite their attractive theoretical properties gps provide practical challenges in their implementation. gpstuff is a versatile collection of computational tools for gp models compatible with linux and windows matlab and octave. it includes, among others, various inference methods, sparse approximations and tools for model assessment. in this work, we review these tools and demonstrate the use of gpstuff in several models.","","2012-06-25","2015-07-15","['jarno vanhatalo', 'jaakko riihimäki', 'jouni hartikainen', 'pasi jylänki', 'ville tolvanen', 'aki vehtari']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"434",1206.5862,"cluster and feature modeling from combinatorial stochastic processes","math.st stat.me stat.th","one of the focal points of the modern literature on bayesian nonparametrics has been the problem of clustering, or partitioning, where each data point is modeled as being associated with one and only one of some collection of groups called clusters or partition blocks. underlying these bayesian nonparametric models are a set of interrelated stochastic processes, most notably the dirichlet process and the chinese restaurant process. in this paper we provide a formal development of an analogous problem, called feature modeling, for associating data points with arbitrary nonnegative integer numbers of groups, now called features or topics. we review the existing combinatorial stochastic process representations for the clustering problem and develop analogous representations for the feature modeling problem. these representations include the beta process and the indian buffet process as well as new representations that provide insight into the connections between these processes. we thereby bring the same level of completeness to the treatment of bayesian nonparametric feature modeling that has previously been achieved for bayesian nonparametric clustering.","10.1214/13-sts434","2012-06-25","2013-10-01","['tamara broderick', 'michael i. jordan', 'jim pitman']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"435",1206.6392,"modeling temporal dependencies in high-dimensional sequences:   application to polyphonic music generation and transcription","cs.lg cs.sd stat.ml","we investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. we introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. we show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.","","2012-06-27","","['nicolas boulanger-lewandowski', 'yoshua bengio', 'pascal vincent']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"436",1206.6396,"joint optimization and variable selection of high-dimensional gaussian   processes","cs.lg stat.ml","maximizing high-dimensional, non-convex functions through noisy observations is a notoriously hard problem, but one that arises in many applications. in this paper, we tackle this challenge by modeling the unknown function as a sample from a high-dimensional gaussian process (gp) distribution. assuming that the unknown function only depends on few relevant variables, we show that it is possible to perform joint variable selection and gp optimization. we provide strong performance guarantees for our algorithm, bounding the sample complexity of variable selection, and as well as providing cumulative regret bounds. we further provide empirical evidence on the effectiveness of our algorithm on several benchmark optimization problems.","","2012-06-27","","['bo chen', 'rui castro', 'andreas krause']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"437",1206.6438,"information-theoretical learning of discriminative clusters for   unsupervised domain adaptation","cs.lg stat.ml","we study the problem of unsupervised domain adaptation, which aims to adapt classifiers trained on a labeled source domain to an unlabeled target domain. many existing approaches first learn domain-invariant features and then construct classifiers with them. we propose a novel approach that jointly learn the both. specifically, while the method identifies a feature space where data in the source and the target domains are similarly distributed, it also learns the feature space discriminatively, optimizing an information-theoretic metric as an proxy to the expected misclassification error on the target domain. we show how this optimization can be effectively carried out with simple gradient-based methods and how hyperparameters can be cross-validated without demanding any labeled data from the target domain. empirical studies on benchmark tasks of object recognition and sentiment analysis validated our modeling assumptions and demonstrated significant improvement of our method over competing ones in classification accuracies.","","2012-06-27","","['yuan shi', 'fei sha']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"438",1206.6462,"learning object arrangements in 3d scenes using human context","cs.lg cs.cv cs.ro stat.ml","we consider the problem of learning object arrangements in a 3d scene. the key idea here is to learn how objects relate to human poses based on their affordances, ease of use and reachability. in contrast to modeling object-object relationships, modeling human-object relationships scales linearly in the number of objects. we design appropriate density functions based on 3d spatial features to capture this. we learn the distribution of human poses in a scene using a variant of the dirichlet process mixture model that allows sharing of the density function parameters across the same object types. then we can reason about arrangements of the objects in the room based on these meaningful human poses. in our extensive experiments on 20 different rooms with a total of 47 objects, our algorithm predicted correct placements with an average error of 1.6 meters from ground truth. in arranging five real scenes, it received a score of 4.3/5 compared to 3.7 for the best baseline method.","","2012-06-27","","['yun jiang', 'marcus lim', 'ashutosh saxena']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"439",1206.6479,"the landmark selection method for multiple output prediction","cs.lg stat.ml","conditional modeling x \to y is a central problem in machine learning. a substantial research effort is devoted to such modeling when x is high dimensional. we consider, instead, the case of a high dimensional y, where x is either low dimensional or high dimensional. our approach is based on selecting a small subset y_l of the dimensions of y, and proceed by modeling (i) x \to y_l and (ii) y_l \to y. composing these two models, we obtain a conditional model x \to y that possesses convenient statistical properties. multi-label classification and multivariate regression experiments on several datasets show that this model outperforms the one vs. all approach as well as several sophisticated multiple output prediction methods.","","2012-06-27","","['krishnakumar balasubramanian', 'guy lebanon']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"440",1206.6482,"modeling images using transformed indian buffet processes","cs.cv cs.lg stat.ml","latent feature models are attractive for image modeling, since images generally contain multiple objects. however, many latent feature models ignore that objects can appear at different locations or require pre-segmentation of images. while the transformed indian buffet process (tibp) provides a method for modeling transformation-invariant features in unsegmented binary images, its current form is inappropriate for real images because of its computational cost and modeling assumptions. we combine the tibp with likelihoods appropriate for real images and develop an efficient inference, using the cross-correlation between images and features, that is theoretically and empirically faster than existing inference techniques. our method discovers reasonable components and achieve effective image reconstruction in natural images.","","2012-06-27","","['ke zhai', 'yuening hu', 'sinead williamson', 'jordan boyd-graber']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"441",1206.6484,"apprenticeship learning for model parameters of partially observable   environments","cs.lg cs.ai stat.ml","we consider apprenticeship learning, i.e., having an agent learn a task by observing an expert demonstrating the task in a partially observable environment when the model of the environment is uncertain. this setting is useful in applications where the explicit modeling of the environment is difficult, such as a dialogue system. we show that we can extract information about the environment model by inferring action selection process behind the demonstration, under the assumption that the expert is choosing optimal actions based on knowledge of the true model of the target environment. proposed algorithms can achieve more accurate estimates of pomdp parameters and better policies from a short demonstration, compared to methods that learns only from the reaction from the environment.","","2012-06-27","","['takaki makino', 'johane takeuchi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"442",1206.6488,"the nonparanormal skeptic","stat.me cs.lg stat.ml","we propose a semiparametric approach, named nonparanormal skeptic, for estimating high dimensional undirected graphical models. in terms of modeling, we consider the nonparanormal family proposed by liu et al (2009). in terms of estimation, we exploit nonparametric rank-based correlation coefficient estimators including the spearman's rho and kendall's tau. in high dimensional settings, we prove that the nonparanormal skeptic achieves the optimal parametric rate of convergence in both graph and parameter estimation. this result suggests that the nonparanormal graphical models are a safe replacement of the gaussian graphical models, even when the data are gaussian.","","2012-06-27","","['han liu', 'fang han', 'ming yuan', 'john lafferty', 'larry wasserman']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"443",1206.6519,"a permutation approach to testing interactions in many dimensions","stat.ml stat.co stat.me","to date, testing interactions in high dimensions has been a challenging task. existing methods often have issues with sensitivity to modeling assumptions and heavily asymptotic nominal p-values. to help alleviate these issues, we propose a permutation-based method for testing marginal interactions with a binary response. our method searches for pairwise correlations which differ between classes. in this manuscript, we compare our method on real and simulated data to the standard approach of running many pairwise logistic models. on simulated data our method finds more significant interactions at a lower false discovery rate (especially in the presence of main effects). on real genomic data, although there is no gold standard, our method finds apparent signal and tells a believable story, while logistic regression does not. we also give asymptotic consistency results under not too restrictive assumptions.","","2012-06-27","","['noah simon', 'robert tibshirani']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE
"444",1206.6653,"bayesian hierarchical rule modeling for predicting medical conditions","stat.ap","we propose a statistical modeling technique, called the hierarchical association rule model (harm), that predicts a patient's possible future medical conditions given the patient's current and past history of reported conditions. the core of our technique is a bayesian hierarchical model for selecting predictive association rules (such as ""condition 1 and condition 2 $\rightarrow$ condition 3"") from a large set of candidate rules. because this method ""borrows strength"" using the conditions of many similar patients, it is able to provide predictions specialized to any given patient, even when little information about the patient's history of conditions is available.","10.1214/11-aoas522","2012-06-28","","['tyler h. mccormick', 'cynthia rudin', 'david madigan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"445",1206.6666,"analyzing establishment nonresponse using an interpretable regression   tree model with linked administrative data","stat.ap","to gain insight into how characteristics of an establishment are associated with nonresponse, a recursive partitioning algorithm is applied to the occupational employment statistics may 2006 survey data to build a regression tree. the tree models an establishment's propensity to respond to the survey given certain establishment characteristics. it provides mutually exclusive cells based on the characteristics with homogeneous response propensities. this makes it easy to identify interpretable associations between the characteristic variables and an establishment's propensity to respond, something not easily done using a logistic regression propensity model. we test the model obtained using the may data against data from the november 2006 occupational employment statistics survey. testing the model on a disjoint set of establishment data with a very large sample size $(n=179,360)$ offers evidence that the regression tree model accurately describes the association between the establishment characteristics and the response propensity for the oes survey. the accuracy of this modeling approach is compared to that of logistic regression through simulation. this representation is then used along with frame-level administrative wage data linked to sample data to investigate the possibility of nonresponse bias. we show that without proper adjustments the nonresponse does pose a risk of bias and is possibly nonignorable.","10.1214/11-aoas521","2012-06-28","","['polly phipps', 'daniell toth']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"446",1206.6845,"gibbs sampling for (coupled) infinite mixture models in the stick   breaking representation","stat.me cs.lg stat.ml","nonparametric bayesian approaches to clustering, information retrieval, language modeling and object recognition have recently shown great promise as a new paradigm for unsupervised data analysis. most contributions have focused on the dirichlet process mixture models or extensions thereof for which efficient gibbs samplers exist. in this paper we explore gibbs samplers for infinite complexity mixture models in the stick breaking representation. the advantage of this representation is improved modeling flexibility. for instance, one can design the prior distribution over cluster sizes or couple multiple infinite mixture models (e.g. over time) at the level of their parameters (i.e. the dependent dirichlet process model). however, gibbs samplers for infinite mixture models (as recently introduced in the statistics literature) seem to mix poorly over cluster labels. among others issues, this can have the adverse effect that labels for the same cluster in coupled mixture models are mixed up. we introduce additional moves in these samplers to improve mixing over cluster labels and to bring clusters into correspondence. an application to modeling of storm trajectories is used to illustrate these ideas.","","2012-06-27","","['ian porteous', 'alexander t. ihler', 'padhraic smyth', 'max welling']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"447",1206.6873,"variable noise and dimensionality reduction for sparse gaussian   processes","cs.lg stat.ml","the sparse pseudo-input gaussian process (spgp) is a new approximation method for speeding up gp regression in the case of a large number of data points n. the approximation is controlled by the gradient optimization of a small set of m `pseudo-inputs', thereby reducing complexity from n^3 to nm^2. one limitation of the spgp is that this optimization space becomes impractically big for high dimensional data sets. this paper addresses this limitation by performing automatic dimensionality reduction. a projection of the input space to a low dimensional space is learned in a supervised manner, alongside the pseudo-inputs, which now live in this reduced space. the paper also investigates the suitability of the spgp for modeling data with input-dependent noise. a further extension of the model is made to make it even more powerful in this regard - we learn an uncertainty parameter for each pseudo-input. the combination of sparsity, reduced dimension, and input-dependent noise makes it possible to apply gps to much larger and more complex data sets than was previously practical. we demonstrate the benefits of these methods on several synthetic and real world problems.","","2012-06-27","","['edward snelson', 'zoubin ghahramani']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"448",1207.0188,"model-based clustering of large networks","stat.co cs.si physics.soc-ph stat.ap","we describe a network clustering framework, based on finite mixture models, that can be applied to discrete-valued networks with hundreds of thousands of nodes and billions of edge variables. relative to other recent model-based clustering work for networks, we introduce a more flexible modeling framework, improve the variational-approximation estimation algorithm, discuss and implement standard error estimation via a parametric bootstrap approach, and apply these methods to much larger data sets than those seen elsewhere in the literature. the more flexible framework is achieved through introducing novel parameterizations of the model, giving varying degrees of parsimony, using exponential family models whose structure may be exploited in various theoretical and algorithmic ways. the algorithms are based on variational generalized em algorithms, where the e-steps are augmented by a minorization-maximization (mm) idea. the bootstrapped standard error estimates are based on an efficient monte carlo network simulation idea. last, we demonstrate the usefulness of the model-based clustering framework by applying it to a discrete-valued network with more than 131,000 nodes and 17 billion edge variables.","10.1214/12-aoas617","2012-07-01","2013-12-10","['duy q. vu', 'david r. hunter', 'michael schweinberger']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE
"449",1207.0242,"pc algorithm for gaussian copula graphical models","math.st stat.th","the pc algorithm uses conditional independence tests for model selection in graphical modeling with acyclic directed graphs. in gaussian models, tests of conditional independence are typically based on pearson correlations, and high-dimensional consistency results have been obtained for the pc algorithm in this setting. we prove that high-dimensional consistency carries over to the broader class of gaussian copula or \textit{nonparanormal} models when using rank-based measures of correlation. for graphs with bounded degree, our result is as strong as prior gaussian results. in simulations, the `rank pc' algorithm works as well as the `pearson pc' algorithm for normal data and considerably better for non-normal gaussian copula data, all the while incurring a negligible increase of computation time. simulations with contaminated data show that rank correlations can also perform better than other robust estimates considered in previous work when the underlying distribution does not belong to the nonparanormal family.","","2012-07-01","","['naftali harris', 'mathias drton']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"450",1207.052,"sparse vector autoregressive modeling","stat.ap stat.co","the vector autoregressive (var) model has been widely used for modeling temporal dependence in a multivariate time series. for large (and even moderate) dimensions, the number of ar coefficients can be prohibitively large, resulting in noisy estimates, unstable predictions and difficult-to-interpret temporal dependence. to overcome such drawbacks, we propose a 2-stage approach for fitting sparse var (svar) models in which many of the ar coefficients are zero. the first stage selects non-zero ar coefficients based on an estimate of the partial spectral coherence (psc) together with the use of bic. the psc is useful for quantifying the conditional relationship between marginal series in a multivariate process. a refinement second stage is then applied to further reduce the number of parameters. the performance of this 2-stage approach is illustrated with simulation results. the 2-stage approach is also applied to two real data examples: the first is the google flu trends data and the second is a time series of concentration levels of air pollutants.","","2012-07-02","","['richard a. davis', 'pengfei zang', 'tian zheng']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE
"451",1207.0939,"flexible mixture modeling with the polynomial gaussian cluster-weighted   model","stat.me stat.co","in the mixture modeling frame, this paper presents the polynomial gaussian cluster-weighted model (cwm). it extends the linear gaussian cwm, for bivariate data, in a twofold way. firstly, it allows for possible nonlinear dependencies in the mixture components by considering a polynomial regression. secondly, it is not restricted to be used for model-based clustering only being contextualized in the most general model-based classification framework. maximum likelihood parameter estimates are derived using the em algorithm and model selection is carried out using the bayesian information criterion (bic) and the integrated completed likelihood (icl). the paper also investigates the conditions under which the posterior probabilities of component-membership from a polynomial gaussian cwm coincide with those of other well-established mixture-models which are related to it. with respect to these models, the polynomial gaussian cwm has shown to give excellent clustering and classification results when applied to the artificial and real data considered in the paper.","","2012-07-04","","['antonio punzo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"452",1207.1221,"robust bayesian inference of networks using dirichlet t-distributions","stat.me","bayesian graphical modeling provides an appealing way to obtain uncertainty estimates when inferring network structures, and much recent progress has been made for gaussian models. these models have been used extensively in applications to gene expression data, even in cases where there appears to be significant deviations from the gaussian model. for more robust inferences, it is natural to consider extensions to t-distribution models. we argue that the classical multivariate t-distribution, defined using a single latent gamma random variable to rescale a gaussian random vector, is of little use in highly multivariate settings, and propose other, more flexible t-distributions. using an independent gamma-divisor for each component of the random vector defines what we term the alternative t-distribution. the associated model allows one to extract information from highly multivariate data even when most experiments contain outliers for some of their measurements. however, the use of this alternative model comes at increased computational cost and imposes constraints on the achievable correlation structures, raising the need for a compromise between the classical and alternative models. to this end we propose the use of dirichlet processes for adaptive clustering of the latent gamma-scalars, each of which may then divide a group of latent gaussian variables. dirichlet processes are commonly used to cluster independent observations; here they are used instead to cluster the dependent components of a single observation. the resulting dirichlet t-distribution interpolates naturally between the two extreme cases of the classical and alternative t-distributions and combines more appealing modeling of the multivariate dependence structure with favorable computational properties.","","2012-07-05","","['michael finegold', 'mathias drton']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"453",1207.1423,"mining associated text and images with dual-wing harmoniums","cs.lg cs.db stat.ml","we propose a multi-wing harmonium model for mining multimedia data that extends and improves on earlier models based on two-layer random fields, which capture bidirectional dependencies between hidden topic aspects and observed inputs. this model can be viewed as an undirected counterpart of the two-layer directed models such as lda for similar tasks, but bears significant difference in inference/learning cost tradeoffs, latent topic representations, and topic mixing mechanisms. in particular, our model facilitates efficient inference and robust topic mixing, and potentially provides high flexibilities in modeling the latent topic spaces. a contrastive divergence and a variational algorithm are derived for learning. we specialized our model to a dual-wing harmonium for captioned images, incorporating a multivariate poisson for word-counts and a multivariate gaussian for color histogram. we present empirical results on the applications of this model to classification, retrieval and image annotation on news video collections, and we report an extensive comparison with various extant models.","","2012-07-04","","['eric p. xing', 'rong yan', 'alexander g. hauptmann']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"454",1207.1963,"bayesian subset simulation: a kriging-based subset simulation algorithm   for the estimation of small probabilities of failure","stat.co","the estimation of small probabilities of failure from computer simulations is a classical problem in engineering, and the subset simulation algorithm proposed by au & beck (prob. eng. mech., 2001) has become one of the most popular method to solve it. subset simulation has been shown to provide significant savings in the number of simulations to achieve a given accuracy of estimation, with respect to many other monte carlo approaches. the number of simulations remains still quite high however, and this method can be impractical for applications where an expensive-to-evaluate computer model is involved. we propose a new algorithm, called bayesian subset simulation, that takes the best from the subset simulation algorithm and from sequential bayesian methods based on kriging (also known as gaussian process modeling). the performance of this new algorithm is illustrated using a test case from the literature. we are able to report promising results. in addition, we provide a numerical study of the statistical properties of the estimator.","","2012-07-09","","['ling li', 'julien bect', 'emmanuel vazquez']",1,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"455",1207.2296,"extremal t processes: elliptical domain of attraction and a spectral   representation","stat.me stat.ap stat.ot","the extremal t process was proposed in the literature for modeling spatial extremes within a copula framework based on the extreme value limit of elliptical t distributions (davison, padoan and ribatet (2012)). a major drawback of this max-stable model was the lack of a spectral representation such that for instance direct simulation was infeasible. the main contribution of this note is to propose such a spectral construction for the extremal t process. interestingly, the extremal gaussian process introduced by schlather (2002) appears as a special case. we further highlight the role of the extremal t process as the maximum attractor for processes with finite-dimensional elliptical distributions. all results naturally also hold within the multivariate domain.","10.1016/j.jmva.2013.08.008","2012-07-10","2013-03-25","['thomas opitz']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE
"456",1207.2378,"parametric and nonparametric tests for speckled imagery","stat.co cs.gr","synthetic aperture radar (sar) has a pivotal role as a remote imaging method. obtained by means of coherent illumination, sar images are contaminated with speckle noise. the statistical modeling of such contamination is well described according with the multiplicative model and its implied g0 distribution. the understanding of sar imagery and scene element identification is an important objective in the field. in particular, reliable image contrast tools are sought. aiming the proposition of new tools for evaluating sar image contrast, we investigated new methods based on stochastic divergence. we propose several divergence measures specifically tailored for g0 distributed data. we also introduce a nonparametric approach based on the kolmogorov-smirnov distance for g0 data. we devised and assessed tests based on such measures, and their performances were quantified according to their test sizes and powers. using monte carlo simulation, we present a robustness analysis of test statistics and of maximum likelihood estimators for several degrees of innovative contamination. it was identified that the proposed tests based on triangular and arithmetic-geometric measures outperformed the kolmogorov-smirnov methodology.","10.1007/s10044-011-0249-3","2012-07-10","","['renato j. cintra', 'abraão d. c. nascimento', 'alejandro c. frery']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"457",1207.3106,"maximum likelihood estimation of gaussian cluster weighted models and   relationships with mixtures of regression","stat.co","cluster-weighted modeling (cwm) is a mixture approach for modeling the joint probability of a response variable and a set of explanatory variables. the parameters are estimated by means of the expectation-maximization algorithm according to the maximum likelihood approach. under gaussian assumptions, we analyse the complete-data likelihood function of cluster weighted models. further, under suitable hypotheses we show that the maximization of the likelihood function of gaussian cluster weighted models leads to the same parameter estimates of finite mixtures of regression and finite mixtures of regression with concomitant variables. in this sense, the latter ones can be considered as nested models of gaussian cluster weighted models.","","2012-07-12","2013-08-08","['salvatore ingrassia', 'simona c. minotti']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"458",1207.3288,"integrating resource selection information with spatial   capture-recapture","q-bio.qm stat.ap","understanding space usage and resource selection is a primary focus of many studies of animal populations. usually, such studies are based on location data obtained from telemetry, and resource selection functions (rsf) are used for inference. another important focus of wildlife research is estimation and modeling population size and density. recently developed spatial capture-recapture (scr) models accomplish this objective using individual encounter history data with auxiliary spatial information on location of capture. scr models include encounter probability functions that are intuitively related to rsfs, but to date, no one has extended scr models to allow for explicit inference about space usage and resource selection. we develop a statistical framework for jointly modeling space usage, resource selection, and population density by integrating scr data, such as from camera traps, mist-nets, or conventional catch-traps, with resource selection data from telemetered individuals. we provide a framework for estimation based on marginal likelihood, wherein we estimate simultaneously the parameters of the scr and rsf models.   our method leads to increases in precision for estimating population density and parameters of ordinary scr models. importantly, we also find that scr models alone can estimate parameters of resource selection functions and, as such, scr methods can be used as the sole source for studying space-usage; however, precision will be higher when telemetry data are available. finally, we find that scr models using standard symmetric and stationary encounter probability models produce biased estimates of density when animal space usage is related to a landscape covariate. therefore, it is important that space usage be taken into consideration, if possible, in studies focused on estimating density using capture-recapture methods.","","2012-07-13","","['j. andrew royle', 'richard b. chandler']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"459",1207.3438,"mahnmf: manhattan non-negative matrix factorization","stat.ml cs.lg cs.na","non-negative matrix factorization (nmf) approximates a non-negative matrix $x$ by a product of two non-negative low-rank factor matrices $w$ and $h$. nmf and its extensions minimize either the kullback-leibler divergence or the euclidean distance between $x$ and $w^t h$ to model the poisson noise or the gaussian noise. in practice, when the noise distribution is heavy tailed, they cannot perform well. this paper presents manhattan nmf (mahnmf) which minimizes the manhattan distance between $x$ and $w^t h$ for modeling the heavy tailed laplacian noise. similar to sparse and low-rank matrix decompositions, mahnmf robustly estimates the low-rank part and the sparse part of a non-negative matrix and thus performs effectively when data are contaminated by outliers. we extend mahnmf for various practical applications by developing box-constrained mahnmf, manifold regularized mahnmf, group sparse mahnmf, elastic net inducing mahnmf, and symmetric mahnmf. the major contribution of this paper lies in two fast optimization algorithms for mahnmf and its extensions: the rank-one residual iteration (rri) method and nesterov's smoothing method. in particular, by approximating the residual matrix by the outer product of one row of w and one row of $h$ in mahnmf, we develop an rri method to iteratively update each variable of $w$ and $h$ in a closed form solution. although rri is efficient for small scale mahnmf and some of its extensions, it is neither scalable to large scale matrices nor flexible enough to optimize all mahnmf extensions. since the objective functions of mahnmf and its extensions are neither convex nor smooth, we apply nesterov's smoothing method to recursively optimize one factor matrix with another matrix fixed. by setting the smoothing parameter inversely proportional to the iteration number, we improve the approximation accuracy iteratively for both mahnmf and its extensions.","","2012-07-14","","['naiyang guan', 'dacheng tao', 'zhigang luo', 'john shawe-taylor']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"460",1207.4085,"a point-process response model for spike trains from single neurons in   neural circuits under optogenetic stimulation","stat.me q-bio.nc stat.ap stat.co","optogenetics is a new tool to study neuronal circuits that have been genetically modified to allow stimulation by flashes of light. we study recordings from single neurons within neural circuits under optogenetic stimulation. the data from these experiments present a statistical challenge of modeling a high frequency point process (neuronal spikes) while the input is another high frequency point process (light flashes). we further develop a generalized linear model approach to model the relationships between two point processes, employing additive point-process response functions. the resulting model, point-process responses for optogenetics (pro), provides explicit nonlinear transformations to link the input point process with the output one. such response functions may provide important and interpretable scientific insights into the properties of the biophysical process that governs neural spiking in response to optogenetic stimulation. we validate and compare the pro model using a real dataset and simulations, and our model yields a superior area-under-the- curve value as high as 93% for predicting every future spike. for our experiment on the recurrent layer v circuit in the prefrontal cortex, the pro model provides evidence that neurons integrate their inputs in a sophisticated manner. another use of the model is that it enables understanding how neural circuits are altered under various disease conditions and/or experimental conditions by comparing the pro parameters.","10.1002/sim.6742","2012-07-17","2016-12-21","['xi luo', 'steven gee', 'vikaas s. sohal', 'dylan s. small']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"461",1207.4112,"algebraic statistics in model selection","cs.lg stat.ml","we develop the necessary theory in computational algebraic geometry to place bayesian networks into the realm of algebraic statistics. we present an algebra{statistics dictionary focused on statistical modeling. in particular, we link the notion of effiective dimension of a bayesian network with the notion of algebraic dimension of a variety. we also obtain the independence and non{independence constraints on the distributions over the observable variables implied by a bayesian network with hidden variables, via a generating set of an ideal of polynomials associated to the network. these results extend previous work on the subject. finally, the relevance of these results for model selection is discussed.","","2012-07-11","","['luis david garcia']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"462",1207.4122,"bayesian biosurveillance of disease outbreaks","stat.ap cs.ai cs.ce","early, reliable detection of disease outbreaks is a critical problem today. this paper reports an investigation of the use of causal bayesian networks to model spatio-temporal patterns of a non-contagious disease (respiratory anthrax infection) in a population of people. the number of parameters in such a network can become enormous, if not carefully managed. also, inference needs to be performed in real time as population data stream in. we describe techniques we have applied to address both the modeling and inference challenges. a key contribution of this paper is the explication of assumptions and techniques that are sufficient to allow the scaling of bayesian network modeling and inference to millions of nodes for real-time surveillance applications. the results reported here provide a proof-of-concept that bayesian networks can serve as the foundation of a system that effectively performs bayesian biosurveillance of disease outbreaks.","","2012-07-11","","['gregory f. cooper', 'denver dash', 'john levander', 'weng-keen wong', 'william hogan', 'michael wagner']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"463",1207.4142,"conditional chow-liu tree structures for modeling discrete-valued vector   time series","cs.lg stat.ml","we consider the problem of modeling discrete-valued vector time series data using extensions of chow-liu tree models to capture both dependencies across time and dependencies across variables. conditional chow-liu tree models are introduced, as an extension to standard chow-liu trees, for modeling conditional rather than joint densities. we describe learning algorithms for such models and show how they can be used to learn parsimonious representations for the output distributions in hidden markov models. these models are applied to the important problem of simulating and forecasting daily precipitation occurrence for networks of rain stations. to demonstrate the effectiveness of the models, we compare their performance versus a number of alternatives using historical precipitation data from southwestern australia and the western united states. we illustrate how the structure and parameters of the models can be used to provide an improved meteorological interpretation of such data.","","2012-07-11","","['sergey kirshner', 'padhraic smyth', 'andrew robertson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"464",1207.4143,"modeling waveform shapes with random eects segmental hidden markov   models","stat.ap cs.ce","in this paper we describe a general probabilistic framework for modeling waveforms such as heartbeats from ecg data. the model is based on segmental hidden markov models (as used in speech recognition) with the addition of random effects to the generative model. the random effects component of the model handles shape variability across different waveforms within a general class of waveforms of similar shape. we show that this probabilistic model provides a unified framework for learning these models from sets of waveform data as well as parsing, classification, and prediction of new waveforms. we derive a computationally efficient em algorithm to fit the model on multiple waveforms, and introduce a scoring method that evaluates a test waveform based on its shape. results on two real-world data sets demonstrate that the random effects methodology leads to improved accuracy (compared to alternative approaches) on classification and segmentation of real-world waveforms.","","2012-07-11","","['seyoung kim', 'padhraic smyth', 'stefan luther']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"465",1207.4148,"dynamical systems trees","cs.lg stat.ml","we propose dynamical systems trees (dsts) as a flexible class of models for describing multiple processes that interact via a hierarchy of aggregating parent chains. dsts extend kalman filters, hidden markov models and nonlinear dynamical systems to an interactive group scenario. various individual processes interact as communities and sub-communities in a tree structure that is unrolled in time. to accommodate nonlinear temporal activity, each individual leaf process is modeled as a dynamical system containing discrete and/or continuous hidden states with discrete and/or gaussian emissions. subsequent higher level parent processes act like hidden markov models and mediate the interaction between leaf processes or between other parent processes in the hierarchy. aggregator chains are parents of child processes that they combine and mediate, yielding a compact overall parameterization. we provide tractable inference and learning algorithms for arbitrary dst topologies via an efficient structured mean-field algorithm. the diverse applicability of dsts is demonstrated by experiments on gene expression data and by modeling group behavior in the setting of an american football game.","","2012-07-11","","['andrew howard', 'tony s. jebara']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"466",1207.5578,"studies in astronomical time series analysis. vi. bayesian block   representations","astro-ph.im math.st stat.th","this paper addresses the problem of detecting and characterizing local variability in time series and other forms of sequential data. the goal is to identify and characterize statistically significant variations, at the same time suppressing the inevitable corrupting observational errors. we present a simple nonparametric modeling technique and an algorithm implementing it - an improved and generalized version of bayesian blocks (scargle 1998) - that finds the optimal segmentation of the data in the observation interval. the structure of the algorithm allows it to be used in either a real-time trigger mode, or a retrospective mode. maximum likelihood or marginal posterior functions to measure model fitness are presented for events, binned counts, and measurements at arbitrary times with known error distributions. problems addressed include those connected with data gaps, variable exposure, extension to piecewise linear and piecewise exponential representations, multi-variate time series data, analysis of variance, data on the circle, other data modes, and dispersed data. simulations provide evidence that the detection efficiency for weak signals is close to a theoretical asymptotic limit derived by (arias-castro, donoho and huo 2003). in the spirit of reproducible research (donoho et al. 2008) all of the code and data necessary to reproduce all of the figures in this paper are included as auxiliary material.","10.1088/0004-637x/764/2/167","2012-07-23","2012-08-06","['jeffrey d. scargle', 'jay p. norris', 'brad jackson', 'james chiang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"467",1207.5649,"statistical significance of the netflix challenge","stat.me","inspired by the legacy of the netflix contest, we provide an overview of what has been learned---from our own efforts, and those of others---concerning the problems of collaborative filtering and recommender systems. the data set consists of about 100 million movie ratings (from 1 to 5 stars) involving some 480 thousand users and some 18 thousand movies; the associated ratings matrix is about 99% sparse. the goal is to predict ratings that users will give to movies; systems which can do this accurately have significant commercial applications, particularly on the world wide web. we discuss, in some detail, approaches to ""baseline"" modeling, singular value decomposition (svd), as well as knn (nearest neighbor) and neural network models; temporal effects, cross-validation issues, ensemble methods and other considerations are discussed as well. we compare existing models in a search for new models, and also discuss the mission-critical issues of penalization and parameter shrinkage which arise when the dimensions of a parameter space reaches into the millions. although much work on such problems has been carried out by the computer science and machine learning communities, our goal here is to address a statistical audience, and to provide a primarily statistical treatment of the lessons that have been learned from this remarkable set of data.","10.1214/11-sts368","2012-07-24","","['andrey feuerverger', 'yu he', 'shashi khatri']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"468",1207.5947,"functional additive mixed models","stat.me stat.ap","we propose an extensive framework for additive regression models for correlated functional responses, allowing for multiple partially nested or crossed functional random effects with flexible correlation structures for, e.g., spatial, temporal, or longitudinal functional data. additionally, our framework includes linear and nonlinear effects of functional and scalar covariates that may vary smoothly over the index of the functional response. it accommodates densely or sparsely observed functional responses and predictors which may be observed with additional error and includes both spline-based and functional principal component-based terms. estimation and inference in this framework is based on standard additive mixed models, allowing us to take advantage of established methods and robust, flexible algorithms. we provide easy-to-use open source software in the pffr() function for the r-package refund. simulations show that the proposed method recovers relevant effects reliably, handles small sample sizes well and also scales to larger data sets. applications with spatially and longitudinally observed functional data demonstrate the flexibility in modeling and interpretability of results of our approach.","","2012-07-25","2013-11-25","['fabian scheipl', 'ana-maria staicu', 'sonja greven']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"469",1207.6083,"determinantal point processes for machine learning","stat.ml cs.ir cs.lg","determinantal point processes (dpps) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. in contrast to traditional structured models like markov random fields, which become intractable and hard to approximate in the presence of negative correlations, dpps offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. we provide a gentle introduction to dpps, focusing on the intuitions, algorithms, and extensions that are most relevant to the machine learning community, and show how dpps can be applied to real-world applications like finding diverse sets of high-quality search results, building informative summaries by selecting diverse sentences from documents, modeling non-overlapping human poses in images or video, and automatically building timelines of important news stories.","10.1561/2200000044","2012-07-25","2013-01-10","['alex kulesza', 'ben taskar']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"470",1207.6228,"a class of measure-valued markov chains and bayesian nonparametrics","math.st stat.th","measure-valued markov chains have raised interest in bayesian nonparametrics since the seminal paper by (math. proc. cambridge philos. soc. 105 (1989) 579--585) where a markov chain having the law of the dirichlet process as unique invariant measure has been introduced. in the present paper, we propose and investigate a new class of measure-valued markov chains defined via exchangeable sequences of random variables. asymptotic properties for this new class are derived and applications related to bayesian nonparametric mixture modeling, and to a generalization of the markov chain proposed by (math. proc. cambridge philos. soc. 105 (1989) 579--585), are discussed. these results and their applications highlight once again the interplay between bayesian nonparametrics and the theory of measure-valued markov chains.","10.3150/11-bej356","2012-07-26","","['stefano favaro', 'alessandra guglielmi', 'stephen g. walker']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"471",1207.695,"finite-sample equivalence in statistical models for presence-only data","stat.ap","statistical modeling of presence-only data has attracted much recent attention in the ecological literature, leading to a proliferation of methods, including the inhomogeneous poisson process (ipp) model, maximum entropy (maxent) modeling of species distributions and logistic regression models. several recent articles have shown the close relationships between these methods. we explain why the ipp intensity function is a more natural object of inference in presence-only studies than occurrence probability (which is only defined with reference to quadrat size), and why presence-only data only allows estimation of relative, and not absolute intensity of species occurrence. all three of the above techniques amount to parametric density estimation under the same exponential family model (in the case of the ipp, the fitted density is multiplied by the number of presence records to obtain a fitted intensity). we show that ipp and maxent give the exact same estimate for this density, but logistic regression in general yields a different estimate in finite samples. when the model is misspecified - as it practically always is - logistic regression and the ipp may have substantially different asymptotic limits with large data sets. we propose ``infinitely weighted logistic regression,'' which is exactly equivalent to the ipp in finite samples. consequently, many already-implemented methods extending logistic regression can also extend the maxent and ipp models in directly analogous ways using this technique.","10.1214/13-aoas667","2012-07-30","2014-01-08","['william fithian', 'trevor hastie']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"472",1207.7306,"hierarchical models for relational event sequences","stat.me","interaction within small groups can often be represented as a sequence of events, where each event involves a sender and a recipient. recent methods for modeling network data in continuous time model the rate at which individuals interact conditioned on the previous history of events as well as actor covariates. we present a hierarchical extension for modeling multiple such sequences, facilitating inferences about event-level dynamics and their variation across sequences. the hierarchical approach allows one to share information across sequences in a principled manner---we illustrate the efficacy of such sharing through a set of prediction experiments. after discussing methods for adequacy checking and model selection for this class of models, the method is illustrated with an analysis of high school classroom dynamics.","","2012-07-31","","['christopher dubois', 'carter t. butts', 'daniel mcfarland', 'padhraic smyth']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"473",1208.0121,"exponential-family random network models","stat.me","random graphs, where the connections between nodes are considered random variables, have wide applicability in the social sciences. exponential-family random graph models (ergm) have shown themselves to be a useful class of models for representing com- plex social phenomena. we generalize ergm by also modeling nodal attributes as random variates, thus creating a random model of the full network, which we call exponential-family random network models (ernm). we demonstrate how this framework allows a new formu- lation for logistic regression in network data. we develop likelihood-based inference for the model and an mcmc algorithm to implement it. this new model formulation is used to analyze a peer social network from the national lon- gitudinal study of adolescent health. we model the relationship between substance use and friendship relations, and show how the results differ from the standard use of logistic regression on network data.","","2012-08-01","","['ian fellows', 'mark s. handcock']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"474",1208.0402,"multidimensional membership mixture models","cs.lg stat.ml","we present the multidimensional membership mixture (m3) models where every dimension of the membership represents an independent mixture model and each data point is generated from the selected mixture components jointly. this is helpful when the data has a certain shared structure. for example, three unique means and three unique variances can effectively form a gaussian mixture model with nine components, while requiring only six parameters to fully describe it. in this paper, we present three instantiations of m3 models (together with the learning and inference algorithms): infinite, finite, and hybrid, depending on whether the number of mixtures is fixed or not. they are built upon dirichlet process mixture models, latent dirichlet allocation, and a combination respectively. we then consider two applications: topic modeling and learning 3d object arrangements. our experiments show that our m3 models achieve better performance using fewer topics than many classic topic models. we also observe that topics from the different dimensions of m3 models are meaningful and orthogonal to each other.","","2012-08-02","","['yun jiang', 'marcus lim', 'ashutosh saxena']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"475",1208.3035,"on the future of astrostatistics: statistical foundations and   statistical practice","astro-ph.im stat.ap","this paper summarizes a presentation for a panel discussion on ""the future of astrostatistics"" held at the statistical challenges in modern astronomy v conference at pennsylvania state university in june 2011. i argue that the emerging needs of astrostatistics may both motivate and benefit from fundamental developments in statistics. i highlight some recent work within statistics on fundamental topics relevant to astrostatistical practice, including the bayesian/frequentist debate (and ideas for a synthesis), multilevel models, and multiple testing. as an important direction for future work in statistics, i emphasize that astronomers need a statistical framework that explicitly supports unfolding chains of discovery, with acquisition, cataloging, and modeling of data not seen as isolated tasks, but rather as parts of an ongoing, integrated sequence of analyses, with information and uncertainty propagating forward and backward through the chain. a prototypical example is surveying of astronomical populations, where source detection, demographic modeling, and the design of survey instruments and strategies all interact.","10.1007/978-1-4614-3520-4_42","2012-08-15","","['thomas j. loredo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"476",1208.3036,"bayesian astrostatistics: a backward look to the future","astro-ph.im physics.data-an stat.ap","this perspective chapter briefly surveys: (1) past growth in the use of bayesian methods in astrophysics; (2) current misconceptions about both frequentist and bayesian statistical inference that hinder wider adoption of bayesian methods by astronomers; and (3) multilevel (hierarchical) bayesian modeling as a major future direction for research in bayesian astrostatistics, exemplified in part by presentations at the first isi invited session on astrostatistics, commemorated in this volume. it closes with an intentionally provocative recommendation for astronomical survey data reporting, motivated by the multilevel bayesian perspective on modeling cosmic populations: that astronomers cease producing catalogs of estimated fluxes and other source properties from surveys. instead, summaries of likelihood functions (or marginal likelihood functions) for source properties should be reported (not posterior probability density functions), including nontrivial summaries (not simply upper limits) for candidate objects that do not pass traditional detection thresholds.","10.1007/978-1-4614-3508-2_2","2012-08-15","2012-08-29","['thomas j. loredo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"477",1208.3378,"statistical modeling of spatial extremes","stat.me","the areal modeling of the extremes of a natural process such as rainfall or temperature is important in environmental statistics; for example, understanding extreme areal rainfall is crucial in flood protection. this article reviews recent progress in the statistical modeling of spatial extremes, starting with sketches of the necessary elements of extreme value statistics and geostatistics. the main types of statistical models thus far proposed, based on latent variables, on copulas and on spatial max-stable processes, are described and then are compared by application to a data set on rainfall in switzerland. whereas latent variable modeling allows a better fit to marginal distributions, it fits the joint distributions of extremes poorly, so appropriately-chosen copula or max-stable models seem essential for successful spatial modeling of extremes.","10.1214/11-sts376","2012-08-16","","['a. c. davison', 's. a. padoan', 'm. ribatet']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"478",1208.3524,"power-law distributions in binned empirical data","physics.data-an physics.soc-ph stat.ap stat.me","many man-made and natural phenomena, including the intensity of earthquakes, population of cities and size of international wars, are believed to follow power-law distributions. the accurate identification of power-law patterns has significant consequences for correctly understanding and modeling complex systems. however, statistical evidence for or against the power-law hypothesis is complicated by large fluctuations in the empirical distribution's tail, and these are worsened when information is lost from binning the data. we adapt the statistically principled framework for testing the power-law hypothesis, developed by clauset, shalizi and newman, to the case of binned data. this approach includes maximum-likelihood fitting, a hypothesis test based on the kolmogorov--smirnov goodness-of-fit statistic and likelihood ratio tests for comparing against alternative explanations. we evaluate the effectiveness of these methods on synthetic binned data with known structure, quantify the loss of statistical power due to binning, and apply the methods to twelve real-world binned data sets with heavy-tailed patterns.","10.1214/13-aoas710","2012-08-16","2014-04-14","['yogesh virkar', 'aaron clauset']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"479",1208.3559,"discussion of ""statistical modeling of spatial extremes"" by a. c.   davison, s. a. padoan and m. ribatet","stat.me","discussion of ""statistical modeling of spatial extremes"" by a. c. davison, s. a. padoan and m. ribatet [arxiv:1208.3378].","10.1214/12-sts376a","2012-08-17","","['d. cooley', 's. r. sain']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"480",1208.3571,"nonparametric inference for max-stable dependence","stat.me","discussion of ""statistical modeling of spatial extremes"" by a. c. davison, s. a. padoan and m. ribatet [arxiv:1208.3378].","10.1214/12-sts376c","2012-08-17","","['johan segers']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"481",1208.3574,"discussion of ""statistical modeling of spatial extremes"" by a. c.   davison, s. a. padoan and m. ribatet","stat.me","discussion of ""statistical modeling of spatial extremes"" by a. c. davison, s. a. padoan and m. ribatet [arxiv:1208.3378].","10.1214/12-sts376d","2012-08-17","","['benjamin shaby', 'brian j. reich']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"482",1208.3575,"discussion of ""statistical modeling of spatial extremes"" by a. c.   davison, s. a. padoan and m. ribatet","stat.me","discussion of ""statistical modeling of spatial extremes"" by a. c. davison, s. a. padoan and m. ribatet [arxiv:1208.3378].","10.1214/12-sts376b","2012-08-17","","['darmesah gabda', 'ross towe', 'jennifer wadsworth', 'jonathan tawn']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"483",1208.3577,"rejoinder to ""statistical modeling of spatial extremes""","stat.me","rejoinder to ""statistical modeling of spatial extremes"" by a. c. davison, s. a. padoan and m. ribatet [arxiv:1208.3378].","10.1214/12-sts376rej","2012-08-17","","['a. c. davison', 's. a. padoan', 'm. ribatet']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"484",1208.4285,"mark-recapture with multiple non-invasive marks","stat.me stat.ap","non-invasive marks, including pigmentation patterns, acquired scars,and genetic mark- ers, are often used to identify individuals in mark-recapture experiments. if animals in a population can be identified from multiple, non-invasive marks then some individuals may be counted twice in the observed data. analyzing the observed histories without accounting for these errors will provide incorrect inference about the population dynamics. previous approaches to this problem include modeling data from only one mark and combining estimators obtained from each mark separately assuming that they are independent. motivated by the analysis of data from the ecocean online whale shark (rhincodon typus) catalog, we describe a bayesian method to analyze data from multiple, non-invasive marks that is based on the latent-multinomial model of link et al. (2010). further to this, we describe a simplification of the markov chain monte carlo algorithm of link et al. (2010) that leads to more efficient computation. we present results from the analysis of the ecocean whale shark data and from simulation studies comparing our method with the previous approaches.","","2012-08-21","2013-03-08","['simon j. bonner', 'jason a. holmberg']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"485",1208.4411,"a non-parametric mixture model for topic modeling over time","stat.ml","a single, stationary topic model such as latent dirichlet allocation is inappropriate for modeling corpora that span long time periods, as the popularity of topics is likely to change over time. a number of models that incorporate time have been proposed, but in general they either exhibit limited forms of temporal variation, or require computationally expensive inference methods. in this paper we propose non-parametric topics over time (nptot), a model for time-varying topics that allows an unbounded number of topics and exible distribution over the temporal variations in those topics' popularity. we develop a collapsed gibbs sampler for the proposed model and compare against existing models on synthetic and real document sets.","","2012-08-21","","['avinava dubey', 'ahmed hefny', 'sinead williamson', 'eric p. xing']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"486",1208.5062,"changepoint detection for high-dimensional time series with missing data","stat.ml cs.lg","this paper describes a novel approach to change-point detection when the observed high-dimensional data may have missing elements. the performance of classical methods for change-point detection typically scales poorly with the dimensionality of the data, so that a large number of observations are collected after the true change-point before it can be reliably detected. furthermore, missing components in the observed data handicap conventional approaches. the proposed method addresses these challenges by modeling the dynamic distribution underlying the data as lying close to a time-varying low-dimensional submanifold embedded within the ambient observation space. specifically, streaming data is used to track a submanifold approximation, measure deviations from this approximation, and calculate a series of statistics of the deviations for detecting when the underlying manifold has changed in a sharp or unexpected manner. the approach described in this paper leverages several recent results in the field of high-dimensional data analysis, including subspace tracking with missing data, multiscale analysis techniques for point clouds, online optimization, and change-point detection performance analysis. simulations and experiments highlight the robustness and efficacy of the proposed approach in detecting an abrupt change in an otherwise slowly varying low-dimensional manifold.","10.1109/jstsp.2012.2234082","2012-08-24","2012-12-07","['yao xie', 'jiaji huang', 'rebecca willett']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"487",1208.5384,"local quantile regression","math.st stat.th","quantile regression is a technique to estimate conditional quantile curves. it provides a comprehensive picture of a response contingent on explanatory variables. in a flexible modeling framework, a specific form of the conditional quantile curve is not a priori fixed. % indeed, the majority of applications do not per se require specific functional forms. this motivates a local parametric rather than a global fixed model fitting approach. a nonparametric smoothing estimator of the conditional quantile curve requires to balance between local curvature and stochastic variability. in this paper, we suggest a local model selection technique that provides an adaptive estimator of the conditional quantile regression curve at each design point. theoretical results claim that the proposed adaptive procedure performs as good as an oracle which would minimize the local estimation risk for the problem at hand. we illustrate the performance of the procedure by an extensive simulation study and consider a couple of applications: to tail dependence analysis for the hong kong stock market and to analysis of the distributions of the risk factors of temperature dynamics.","","2012-08-27","2012-08-30","['vladimir spokoiny', 'weining wang', 'wolfgang karl härdle']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"488",1209.1119,"augment-and-conquer negative binomial processes","stat.ml stat.me","by developing data augmentation methods unique to the negative binomial (nb) distribution, we unite seemingly disjoint count and mixture models under the nb process framework. we develop fundamental properties of the models and derive efficient gibbs sampling inference. we show that the gamma-nb process can be reduced to the hierarchical dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. a variety of nb processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the nb dispersion and probability parameters.","","2012-09-05","2013-02-15","['mingyuan zhou', 'lawrence carin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"489",1209.1145,"restricting exchangeable nonparametric distributions","stat.me stat.ml","distributions over exchangeable matrices with infinitely many columns, such as the indian buffet process, are useful in constructing nonparametric latent variable models. however, the distribution implied by such models over the number of features exhibited by each data point may be poorly- suited for many modeling tasks. in this paper, we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models. such models allow us to specify the distribution over the number of features per data point, and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution.","","2012-09-05","","['sinead williamson', 'zoubin ghahramani', 'steven n. maceachern', 'eric p. xing']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"490",1209.1302,"weighted bootstrap in garch models","math.st stat.th","garch models are useful tools in the investigation of phenomena, where volatility changes are prominent features, like most financial data. the parameter estimation via quasi maximum likelihood (qmle) and its properties are by now well understood. however, there is a gap between practical applications and the theory, as in reality there are usually not enough observations for the limit results to be valid approximations. we try to fill this gap by this paper, where the properties of a recent bootstrap methodology in the context of garch modeling are revealed. the results are promising as it turns out that this remarkably simple method has essentially the same limit distribution, as the original estimatorwith the advantage of easy confidence interval construction, as it is demonstrated in the paper.   the finite-sample properties of the suggested estimators are investigated through a simulation study, which ensures that the results are practically applicable for sample sizes as low as a thousand. on the other hand, the results are not 100% accurate until sample size reaches 100 thousands - but it is shown that this property is not a feature of our bootstrap procedure only, as it is shared by the original qmle, too.","","2012-09-06","","['lászló varga', 'andrás zempléni']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"491",1209.1341,"polygenic modeling with bayesian sparse linear mixed models","q-bio.qm q-bio.gn stat.ap stat.me","both linear mixed models (lmms) and sparse regression models are widely used in genetics applications, including, recently, polygenic modeling in genome-wide association studies. these two approaches make very different assumptions, so are expected to perform well in different situations. however, in practice, for a given data set one typically does not know which assumptions will be more accurate. motivated by this, we consider a hybrid of the two, which we refer to as a ""bayesian sparse linear mixed model"" (bslmm) that includes both these models as special cases. we address several key computational and statistical issues that arise when applying bslmm, including appropriate prior specification for the hyper-parameters, and a novel markov chain monte carlo algorithm for posterior inference. we apply bslmm and compare it with other methods for two polygenic modeling applications: estimating the proportion of variance in phenotypes explained (pve) by available genotypes, and phenotype (or breeding value) prediction. for pve estimation, we demonstrate that bslmm combines the advantages of both standard lmms and sparse regression modeling. for phenotype prediction it considerably outperforms either of the other two methods, as well as several other large-scale regression methods previously suggested for this problem. software implementing our method is freely available from http://stephenslab.uchicago.edu/software.html","","2012-09-06","2012-11-14","['xiang zhou', 'peter carbonetto', 'matthew stephens']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"492",1209.323,"link prediction in graphs with autoregressive features","stat.ml","in the paper, we consider the problem of link prediction in time-evolving graphs. we assume that certain graph features, such as the node degree, follow a vector autoregressive (var) model and we propose to use this information to improve the accuracy of prediction. our strategy involves a joint optimization procedure over the space of adjacency matrices and var matrices which takes into account both sparsity and low rank properties of the matrices. oracle inequalities are derived and illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank property. the estimate is computed efficiently using proximal methods through a generalized forward-backward agorithm.","","2012-09-14","","['emile richard', 'stephane gaiffas', 'nicolas vayatis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"493",1209.3442,"negative binomial process count and mixture modeling","stat.me stat.ml","the seemingly disjoint problems of count and mixture modeling are united under the negative binomial (nb) process. a gamma process is employed to model the rate measure of a poisson process, whose normalization provides a random probability measure for mixture modeling and whose marginalization leads to an nb process for count modeling. a draw from the nb process consists of a poisson distributed finite number of distinct atoms, each of which is associated with a logarithmic distributed number of data samples. we reveal relationships between various count- and mixture-modeling distributions and construct a poisson-logarithmic bivariate distribution that connects the nb and chinese restaurant table distributions. fundamental properties of the models are developed, and we derive efficient bayesian inference. it is shown that with augmentation and normalization, the nb process and gamma-nb process can be reduced to the dirichlet process and hierarchical dirichlet process, respectively. these relationships highlight theoretical, structural and computational advantages of the nb process. a variety of nb processes, including the beta-geometric, beta-nb, marked-beta-nb, marked-gamma-nb and zero-inflated-nb processes, with distinct sharing mechanisms, are also constructed. these models are applied to topic modeling, with connections made to existing algorithms under poisson factor analysis. example results show the importance of inferring both the nb dispersion and probability parameters.","","2012-09-15","2013-10-12","['mingyuan zhou', 'lawrence carin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"494",1209.5183,"modeling left-truncated and right-censored survival data with   longitudinal covariates","math.st stat.th","there is a surge in medical follow-up studies that include longitudinal covariates in the modeling of survival data. so far, the focus has been largely on right-censored survival data. we consider survival data that are subject to both left truncation and right censoring. left truncation is well known to produce biased sample. the sampling bias issue has been resolved in the literature for the case which involves baseline or time-varying covariates that are observable. the problem remains open, however, for the important case where longitudinal covariates are present in survival models. a joint likelihood approach has been shown in the literature to provide an effective way to overcome those difficulties for right-censored data, but this approach faces substantial additional challenges in the presence of left truncation. here we thus propose an alternative likelihood to overcome these difficulties and show that the regression coefficient in the survival component can be estimated unbiasedly and efficiently. issues about the bias for the longitudinal component are discussed. the new approach is illustrated numerically through simulations and data from a multi-center aids cohort study.","10.1214/12-aos996","2012-09-24","","['yu-ru su', 'jane-ling wang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"495",1209.5776,"distflow ode: modeling, analyzing and controlling long distribution   feeder","math.oc physics.soc-ph stat.ap","we consider a linear feeder connecting multiple distributed loads and generators to the sub-station. voltage is controlled directly at the sub-station, however, voltage down the line shifts up or down, in particular depending on if the feeder operates in the power export regime or power import regime. starting from this finite element description of the feeder, assuming that the consumption/generation is distributed heterogeneously along the feeder, and following the asymptotic homogenization approach, we derive simple low-parametric ode model of the feeder. we also explain how the homogeneous ode modeling is generalized to account for other distributed effects, e.g. for inverter based and voltage dependent control of reactive power. the resulting system of the distflow-odes, relating homogenized voltage to flows of real and reactive power along the lines, admits computationally efficient analysis in terms of the minimal number of the feeder line ""media"" parameters, such as the ratio of the inductance-to-resistance densities. exploring the space of the media and control parameters allows us to test and juxtapose different measures of the system performance, in particular expressed in terms of the voltage drop along the feeder, power import/export from the feeder line as the whole, power losses within the feeder, and critical (with respect to possible voltage collapse) length of the feeder. our most surprising funding relates to performance of a feeder rich on photovoltaic (pv) systems during a sunny day. we observe that if the feeder is sufficiently long the distflow-odes may have multiple stable solutions. the multiplicity may mean troubles for successful recovery of the feeder after a very short, few periods long, fault at the head of the line.","10.1109/cdc.2012.6426054","2012-09-25","","['danhua wang', 'konstantin turitsyn', 'michael chertkov']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"496",1209.6344,"dependence structure of spatial extremes using threshold approach","stat.me","the analysis of spatial extremes requires the joint modeling of a spatial process at a large number of stations and max-stable processes have been developed as a class of stochastic processes suitable for studying spatial extremes. spatial dependence structure in the extreme value analysis can be measured by max-stable processes. however, there have been few works on the threshold approach of max-stable processes. we propose a threshold version of max-stable process estimation and we apply the pairwise composite likelihood method by padoan et al. (2010) to estimate spatial dependence parameters. it is of interest to establish limit behavior of the estimates based on the settings of increasing domain asymptotics with stochastic sampling design. two different types of asymptotic normality are drawn under the second-order regular variation condition for the distribution satisfying the domain of attraction. the theoretical property of dependence parameter estimators in limiting sense is implemented by simulation and a choice of optimal threshold is discussed in this paper.","","2012-09-27","","['soyoung jeon', 'richard l. smith']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"497",1210.03,"semiparametric zero-inflated modeling in multi-ethnic study of   atherosclerosis (mesa)","stat.ap","we analyze the agatston score of coronary artery calcium (cac) from the multi-ethnic study of atherosclerosis (mesa) using the semiparametric zero-inflated modeling approach, where the observed cac scores from this cohort consist of high frequency of zeroes and continuously distributed positive values. both partially constrained and unconstrained models are considered to investigate the underlying biological processes of cac development from zero to positive, and from small amount to large amount. different from existing studies, a model selection procedure based on likelihood cross-validation is adopted to identify the optimal model, which is justified by comparative monte carlo studies. a shrinkaged version of cubic regression spline is used for model estimation and variable selection simultaneously. when applying the proposed methods to the mesa data analysis, we show that the two biological mechanisms influencing the initiation of cac and the magnitude of cac when it is positive are better characterized by an unconstrained zero-inflated normal model. our results are significantly different from those in published studies, and may provide further insights into the biological mechanisms underlying cac development in humans. this highly flexible statistical framework can be applied to zero-inflated data analyses in other areas.","10.1214/11-aoas534","2012-10-01","","['hai liu', 'shuangge ma', 'richard kronmal', 'kung-sik chan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"498",1210.1928,"information fusion in multi-task gaussian processes","stat.ml cs.ai cs.lg","this paper evaluates heterogeneous information fusion using multi-task gaussian processes in the context of geological resource modeling. specifically, it empirically demonstrates that information integration across heterogeneous information sources leads to superior estimates of all the quantities being modeled, compared to modeling them individually. multi-task gaussian processes provide a powerful approach for simultaneous modeling of multiple quantities of interest while taking correlations between these quantities into consideration. experiments are performed on large scale real sensor data.","","2012-10-06","2013-09-04","['shrihari vasudevan', 'arman melkumyan', 'steven scheding']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"499",1210.2022,"locally adaptive factor processes for multivariate time series","stat.ap stat.ml","in modeling multivariate time series, it is important to allow time-varying smoothness in the mean and covariance process. in particular, there may be certain time intervals exhibiting rapid changes and others in which changes are slow. if such time-varying smoothness is not accounted for, one can obtain misleading inferences and predictions, with over-smoothing across erratic time intervals and under-smoothing across times exhibiting slow variation. this can lead to mis-calibration of predictive intervals, which can be substantially too narrow or wide depending on the time. we propose a locally adaptive factor process for characterizing multivariate mean-covariance changes in continuous time, allowing locally varying smoothness in both the mean and covariance matrix. this process is constructed utilizing latent dictionary functions evolving in time through nested gaussian processes and linearly related to the observed data with a sparse mapping. using a differential equation representation, we bypass usual computational bottlenecks in obtaining mcmc and online algorithms for approximate bayesian inference. the performance is assessed in simulations and illustrated in a financial application.","","2012-10-07","2013-06-21","['daniele durante', 'bruno scarpa', 'david b. dunson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"500",1210.3851,"an introduction to particle integration methods: with applications to   risk and insurance","q-fin.cp math.st q-fin.rm stat.th","interacting particle methods are increasingly used to sample from complex and high-dimensional distributions. these stochastic particle integration techniques can be interpreted as an universal acceptance-rejection sequential particle sampler equipped with adaptive and interacting recycling mechanisms. practically, the particles evolve randomly around the space independently and to each particle is associated a positive potential function. periodically, particles with high potentials duplicate at the expense of low potential particle which die. this natural genetic type selection scheme appears in numerous applications in applied probability, physics, bayesian statistics, signal processing, biology, and information engineering. it is the intention of this paper to introduce them to risk modeling. from a purely mathematical point of view, these stochastic samplers can be interpreted as feynman-kac particle integration methods. these functional models are natural mathematical extensions of the traditional change of probability measures, commonly used to design an importance sampling strategy. in this article, we provide a brief introduction to the stochastic modeling and the theoretical analysis of these particle algorithms. then we conclude with an illustration of a subset of such methods to resolve important risk measure and capital estimation in risk and insurance modelling.","","2012-10-14","2012-10-29","['p. del moral', 'g. w. peters', 'ch. vergé']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"501",1210.4846,"variational dual-tree framework for large-scale transition matrix   approximation","cs.lg stat.ml","in recent years, non-parametric methods utilizing random walks on graphs have been used to solve a wide range of machine learning problems, but in their simplest form they do not scale well due to the quadratic complexity. in this paper, a new dual-tree based variational approach for approximating the transition matrix and efficiently performing the random walk is proposed. the approach exploits a connection between kernel density estimation, mixture modeling, and random walk on graphs in an optimization of the transition matrix for the data graph that ties together edge transitions probabilities that are similar. compared to the de facto standard approximation method based on k-nearestneighbors, we demonstrate order of magnitudes speedup without sacrificing accuracy for label propagation tasks on benchmark data sets in semi-supervised learning.","","2012-10-16","","['saeed amizadeh', 'bo thiesson', 'milos hauskrecht']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"502",1210.485,"markov determinantal point processes","cs.lg cs.ir stat.ml","a determinantal point process (dpp) is a random process useful for modeling the combinatorial problem of subset selection. in particular, dpps encourage a random subset y to contain a diverse set of items selected from a base set y. for example, we might use a dpp to display a set of news headlines that are relevant to a user's interests while covering a variety of topics. suppose, however, that we are asked to sequentially select multiple diverse sets of items, for example, displaying new headlines day-by-day. we might want these sets to be diverse not just individually but also through time, offering headlines today that are unlike the ones shown yesterday. in this paper, we construct a markov dpp (m-dpp) that models a sequence of random sets {yt}. the proposed m-dpp defines a stationary process that maintains dpp margins. crucially, the induced union process zt = yt u yt-1 is also marginally dpp-distributed. jointly, these properties imply that the sequence of random sets are encouraged to be diverse both at a given time step as well as across time steps. we describe an exact, efficient sampling procedure, and a method for incrementally learning a quality measure over items in the base set y based on external preferences. we apply the m-dpp to the task of sequentially displaying diverse and relevant news articles to a user with topic preferences.","","2012-10-16","","['raja hafiz affandi', 'alex kulesza', 'emily b. fox']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"503",1210.4855,"a slice sampler for restricted hierarchical beta process with   applications to shared subspace learning","cs.lg cs.cv stat.ml","hierarchical beta process has found interesting applications in recent years. in this paper we present a modified hierarchical beta process prior with applications to hierarchical modeling of multiple data sources. the novel use of the prior over a hierarchical factor model allows factors to be shared across different sources. we derive a slice sampler for this model, enabling tractable inference even when the likelihood and the prior over parameters are non-conjugate. this allows the application of the model in much wider contexts without restrictions. we present two different data generative models a linear gaussiangaussian model for real valued data and a linear poisson-gamma model for count data. encouraging transfer learning results are shown for two real world applications text modeling and content based image retrieval.","","2012-10-16","","['sunil kumar gupta', 'dinh q. phung', 'svetha venkatesh']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"504",1210.4864,"graph-coupled hmms for modeling the spread of infection","cs.si physics.soc-ph stat.ap","we develop graph-coupled hidden markov models (gchmms) for modeling the spread of infectious disease locally within a social network. unlike most previous research in epidemiology, which typically models the spread of infection at the level of entire populations, we successfully leverage mobile phone data collected from 84 people over an extended period of time to model the spread of infection on an individual level. our model, the gchmm, is an extension of widely-used coupled hidden markov models (chmms), which allow dependencies between state transitions across multiple hidden markov models (hmms), to situations in which those dependencies are captured through the structure of a graph, or to social networks that may change over time. the benefit of making infection predictions on an individual level is enormous, as it allows people to receive more personalized and relevant health advice.","","2012-10-16","","['wen dong', 'alex pentland', 'katherine a. heller']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"505",1210.4905,"latent composite likelihood learning for the structured canonical   correlation model","stat.ml cs.lg","latent variable models are used to estimate variables of interest quantities which are observable only up to some measurement error. in many studies, such variables are known but not precisely quantifiable (such as ""job satisfaction"" in social sciences and marketing, ""analytical ability"" in educational testing, or ""inflation"" in economics). this leads to the development of measurement instruments to record noisy indirect evidence for such unobserved variables such as surveys, tests and price indexes. in such problems, there are postulated latent variables and a given measurement model. at the same time, other unantecipated latent variables can add further unmeasured confounding to the observed variables. the problem is how to deal with unantecipated latents variables. in this paper, we provide a method loosely inspired by canonical correlation that makes use of background information concerning the ""known"" latent variables. given a partially specified structure, it provides a structure learning approach to detect ""unknown unknowns,"" the confounding effect of potentially infinitely many other latent variables. this is done without explicitly modeling such extra latent factors. because of the special structure of the problem, we are able to exploit a new variation of composite likelihood fitting to efficiently learn this structure. validation is provided with experiments in synthetic data and the analysis of a large survey done with a sample of over 100,000 staff members of the national health service of the united kingdom.","","2012-10-16","","['ricardo silva']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"506",1210.5371,"bayesian structure learning in sparse gaussian graphical models","stat.me","decoding complex relationships among large numbers of variables with relatively few observations is one of the crucial issues in science. one approach to this problem is gaussian graphical modeling, which describes conditional independence of variables through the presence or absence of edges in the underlying graph. in this paper, we introduce a novel and efficient bayesian framework for gaussian graphical model determination which is a trans-dimensional markov chain monte carlo (mcmc) approach based on a continuous-time birth-death process. we cover the theory and computational details of the method. it is easy to implement and computationally feasible for high-dimensional graphs. we show our method outperforms alternative bayesian approaches in terms of convergence, mixing in the graph space and computing time. unlike frequentist approaches, it gives a principled and, in practice, sensible approach for structure learning. we illustrate the efficiency of the method on a broad range of simulated data. we then apply the method on large-scale real applications from human and mammary gland gene expression studies to show its empirical usefulness. in addition, we implemented the method in the r package bdgraph which is freely available at http://cran.r-project.org/package=bdgraph","10.1214/14-ba889","2012-10-19","2019-04-25","['a. mohammadi', 'e. c. wit']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"507",1210.55,"modeling with copulas and vines in estimation of distribution algorithms","cs.ne stat.me","the aim of this work is studying the use of copulas and vines in the optimization with estimation of distribution algorithms (edas). two edas are built around the multivariate product and normal copulas, and other two are based on pair-copula decomposition of vine models. empirically we study the effect of both marginal distributions and dependence structure separately, and show that both aspects play a crucial role in the success of the optimization. the results show that the use of copulas and vines opens new opportunities to a more appropriate modeling of search distributions in edas.","","2012-10-19","","['marta soto', 'yasser gonzález-fernández', 'alberto ochoa']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"508",1210.6044,"multistable binary decision making on networks","physics.soc-ph cond-mat.stat-mech cs.si stat.ap","we propose a simple model for a binary decision making process on a graph, motivated by modeling social decision making with cooperative individuals. the model is similar to a random field ising model or fiber bundle model, but with key differences on heterogeneous networks. for many types of disorder and interactions between the nodes, we predict discontinuous phase transitions with mean field theory which are largely independent of network structure. we show how these phase transitions can also be understood by studying microscopic avalanches, and describe how network structure enhances fluctuations in the distribution of avalanches. we suggest theoretically the existence of a ""glassy"" spectrum of equilibria associated with a typical phase, even on infinite graphs, so long as the first moment of the degree distribution is finite. this behavior implies that the model is robust against noise below a certain scale, and also that phase transitions can switch from discontinuous to continuous on networks with too few edges. numerical simulations suggest that our theory is accurate.","10.1103/physreve.87.032806","2012-10-22","2013-03-14","['andrew lucas', 'ching hua lee']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"509",1210.6321,"high quality topic extraction from business news explains abnormal   financial market volatility","stat.ml cs.lg cs.si physics.soc-ph q-fin.st","understanding the mutual relationships between information flows and social activity in society today is one of the cornerstones of the social sciences. in financial economics, the key issue in this regard is understanding and quantifying how news of all possible types (geopolitical, environmental, social, financial, economic, etc.) affect trading and the pricing of firms in organized stock markets. in this article, we seek to address this issue by performing an analysis of more than 24 million news records provided by thompson reuters and of their relationship with trading activity for 206 major stocks in the s&p us stock index. we show that the whole landscape of news that affect stock price movements can be automatically summarized via simple regularized regressions between trading activity and news information pieces decomposed, with the help of simple topic modeling techniques, into their ""thematic"" features. using these methods, we are able to estimate and quantify the impacts of news on trading. we introduce network-based visualization techniques to represent the whole landscape of news information associated with a basket of stocks. the examination of the words that are representative of the topic distributions confirms that our method is able to extract the significant pieces of information influencing the stock market. our results show that one of the most puzzling stylized fact in financial economies, namely that at certain times trading volumes appear to be ""abnormally large,"" can be partially explained by the flow of news. in this sense, our results prove that there is no ""excess trading,"" when restricting to times when news are genuinely novel and provide relevant financial information.","10.1371/journal.pone.0064846","2012-10-23","2013-03-23","['ryohei hisano', 'didier sornette', 'takayuki mizuno', 'takaaki ohnishi', 'tsutomu watanabe']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"510",1210.6738,"nested hierarchical dirichlet processes","stat.ml cs.lg","we develop a nested hierarchical dirichlet process (nhdp) for hierarchical topic modeling. the nhdp is a generalization of the nested chinese restaurant process (ncrp) that allows each word to follow its own path to a topic node according to a document-specific distribution on a shared tree. this alleviates the rigid, single-path formulation of the ncrp, allowing a document to more easily express thematic borrowings as a random effect. we derive a stochastic variational inference algorithm for the model, in addition to a greedy subtree selection method for each document, which allows for efficient inference using massive collections of text documents. we demonstrate our algorithm on 1.8 million documents from the new york times and 3.3 million documents from wikipedia.","10.1109/tpami.2014.2318728","2012-10-25","2014-05-02","['john paisley', 'chong wang', 'david m. blei', 'michael i. jordan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"511",1211.1171,"generalized linear gaussian cluster-weighted modeling","stat.me stat.ap stat.co","cluster-weighted modeling (cwm) is a flexible mixture approach for modeling the joint probability of data coming from a heterogeneous population as a weighted sum of the products of marginal distributions and conditional distributions. in this paper, we introduce a wide family of cluster weighted models in which the conditional distributions are assumed to belong to the exponential family with canonical links which will be referred to as generalized linear gaussian cluster weighted models. moreover, we show that, in a suitable sense, mixtures of generalized linear models can be considered as nested in generalized linear gaussian cluster weighted models. the proposal is illustrated through many numerical studies based on both simulated and real data sets.","","2012-11-06","2012-12-19","['salvatore ingrassia', 'simona c. minotti', 'antonio punzo', 'giorgio vittadini']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"512",1211.1208,"generalized fiducial inference for normal linear mixed models","stat.me","while linear mixed modeling methods are foundational concepts introduced in any statistical education, adequate general methods for interval estimation involving models with more than a few variance components are lacking, especially in the unbalanced setting. generalized fiducial inference provides a possible framework that accommodates this absence of methodology. under the fabric of generalized fiducial inference along with sequential monte carlo methods, we present an approach for interval estimation for both balanced and unbalanced gaussian linear mixed models. we compare the proposed method to classical and bayesian results in the literature in a simulation study of two-fold nested models and two-factor crossed designs with an interaction term. the proposed method is found to be competitive or better when evaluated based on frequentist criteria of empirical coverage and average length of confidence intervals for small sample sizes. a matlab implementation of the proposed algorithm is available from the authors.","10.1214/12-aos1030","2012-11-06","","['jessi cisewski', 'jan hannig']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"513",1211.131,"dynamic modeling in health research as a framework for developing   statistical applications free of misuse of statistics","stat.me","we introduce a novel framework for developing statistical applications in health research, based on dynamic modeling of the investigated processes. we formulate the principles of dynamic modeling in health research, which are coherent to those in other fields of research. dynamic models explicitly describe causal relations which are to be adequately accounted in statistical methods, making them free of misuse of statistics and statistical fallacy. we propose the dynamic model of population health describing temporal changes in health indicators, having nature of state variables. the dynamic regression method was developed as statistical method for the identification of the model. this method evaluates cohort trends for state variables at each age and calendar year. the method is illustrated by evaluating cohort trends for the body mass index for men, using survey data collected in the years 1982, 1987, 1992, in north karelia, finland.","","2012-11-06","2012-11-15","['vladislav moltchanov']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"514",1211.1992,"continuous-time discrete-space models for animal movement","stat.ap","the processes influencing animal movement and resource selection are complex and varied. past efforts to model behavioral changes over time used bayesian statistical models with variable parameter space, such as reversible-jump markov chain monte carlo approaches, which are computationally demanding and inaccessible to many practitioners. we present a continuous-time discrete-space (ctds) model of animal movement that can be fit using standard generalized linear modeling (glm) methods. this ctds approach allows for the joint modeling of location-based as well as directional drivers of movement. changing behavior over time is modeled using a varying-coefficient framework which maintains the computational simplicity of a glm approach, and variable selection is accomplished using a group lasso penalty. we apply our approach to a study of two mountain lions (puma concolor) in colorado, usa.","10.1214/14-aoas803","2012-11-08","2015-05-28","['ephraim m. hanks', 'mevin b. hooten', 'mat w. alldredge']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"515",1211.219,"efficient monte carlo methods for multi-dimensional learning with   classifier chains","cs.lg stat.co stat.ml","multi-dimensional classification (mdc) is the supervised learning problem where an instance is associated with multiple classes, rather than with a single class, as in traditional classification problems. since these classes are often strongly correlated, modeling the dependencies between them allows mdc methods to improve their performance - at the expense of an increased computational cost. in this paper we focus on the classifier chains (cc) approach for modeling dependencies, one of the most popular and highest- performing methods for multi-label classification (mlc), a particular case of mdc which involves only binary classes (i.e., labels). the original cc algorithm makes a greedy approximation, and is fast but tends to propagate errors along the chain. here we present novel monte carlo schemes, both for finding a good chain sequence and performing efficient inference. our algorithms remain tractable for high-dimensional data sets and obtain the best predictive performance across several real data sets.","10.1016/j.patcog.2013.10.006","2012-11-09","2013-09-07","['jesse read', 'luca martino', 'david luengo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE
"516",1211.3792,"modeling repairs of systems with a bathtub-shaped failure rate function","stat.ap","most of the reliability literature on modeling the effect of repairs on systems assumes the failure rate functions are monotonically increasing. for systems with non-monotonic failure rate functions, most models deal with minimal repairs (which do not affect the working condition of the system) or replacements (which return the working condition to that of a new and identical system). we explore a new approach to model repairs of a system with a non-monotonic failure rate function; in particular, we consider systems with a bathtub-shaped failure rate function. we propose a repair model specified in terms of modifications to the virtual age function of the system, while preserving the usual definitions of the types of repair (minimal, imperfect and perfect repairs) and distinguishing between perfect repair and replacement. in addition, we provide a numerical illustration of the proposed repair model.","","2012-11-15","","['sima varnosafaderani', 'stefanka chukova']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"517",1211.4372,"a framework for uplink intercell interference modeling with   channel-based scheduling","math.st cs.it math.it stat.th","this paper presents a novel framework for modeling the uplink intercell interference (ici) in a multiuser cellular network. the proposed framework assists in quantifying the impact of various fading channel models and state-of-the-art scheduling schemes on the uplink ici. firstly, we derive a semianalytical expression for the distribution of the location of the scheduled user in a given cell considering a wide range of scheduling schemes. based on this, we derive the distribution and moment generating function (mgf) of the uplink ici considering a single interfering cell. consequently, we determine the mgf of the cumulative ici observed from all interfering cells and derive explicit mgf expressions for three typical fading models. finally, we utilize the obtained expressions to evaluate important network performance metrics such as the outage probability, ergodic capacity, and average fairness numerically. monte-carlo simulation results are provided to demonstrate the efficacy of the derived analytical expressions.","","2012-11-19","","['hina tabassum', 'ferkan yilmaz', 'zaher dawy', 'mohamed-slim alouini']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"518",1211.441,"mixture gaussian process conditional heteroscedasticity","cs.lg stat.ml","generalized autoregressive conditional heteroscedasticity (garch) models have long been considered as one of the most successful families of approaches for volatility modeling in financial return series. in this paper, we propose an alternative approach based on methodologies widely used in the field of statistical machine learning. specifically, we propose a novel nonparametric bayesian mixture of gaussian process regression models, each component of which models the noise variance process that contaminates the observed data as a separate latent gaussian process driven by the observed data. this way, we essentially obtain a mixture gaussian process conditional heteroscedasticity (mgpch) model for volatility modeling in financial return series. we impose a nonparametric prior with power-law nature over the distribution of the model mixture components, namely the pitman-yor process prior, to allow for better capturing modeled data distributions with heavy tails and skewness. finally, we provide a copula- based approach for obtaining a predictive posterior for the covariances over the asset returns modeled by means of a postulated mgpch model. we evaluate the efficacy of our approach in a number of benchmark scenarios, and compare its performance to state-of-the-art methodologies.","","2012-11-19","2013-01-25","['emmanouil a. platanios', 'sotirios p. chatzis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"519",1211.4601,"smoothing dynamic systems with state-dependent covariance matrices","math.oc stat.co stat.ml","kalman filtering and smoothing algorithms are used in many areas, including tracking and navigation, medical applications, and financial trend filtering. one of the basic assumptions required to apply the kalman smoothing framework is that error covariance matrices are known and given. in this paper, we study a general class of inference problems where covariance matrices can depend functionally on unknown parameters. in the kalman framework, this allows modeling situations where covariance matrices may depend functionally on the state sequence being estimated. we present an extended formulation and generalized gauss-newton (ggn) algorithm for inference in this context. when applied to dynamic systems inference, we show the algorithm can be implemented to preserve the computational efficiency of the classic kalman smoother. the new approach is illustrated with a synthetic numerical example.","","2012-11-19","2014-03-20","['aleksandr y. aravkin', 'james v. burke']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE
"520",1211.5018,"single and multiple index functional regression models with   nonparametric link","math.st stat.th","fully nonparametric methods for regression from functional data have poor accuracy from a statistical viewpoint, reflecting the fact that their convergence rates are slower than nonparametric rates for the estimation of high-dimensional functions. this difficulty has led to an emphasis on the so-called functional linear model, which is much more flexible than common linear models in finite dimension, but nevertheless imposes structural constraints on the relationship between predictors and responses. recent advances have extended the linear approach by using it in conjunction with link functions, and by considering multiple indices, but the flexibility of this technique is still limited. for example, the link may be modeled parametrically or on a grid only, or may be constrained by an assumption such as monotonicity; multiple indices have been modeled by making finite-dimensional assumptions. in this paper we introduce a new technique for estimating the link function nonparametrically, and we suggest an approach to multi-index modeling using adaptively defined linear projections of functional data. we show that our methods enable prediction with polynomial convergence rates. the finite sample performance of our methods is studied in simulations, and is illustrated by an application to a functional regression problem.","10.1214/11-aos882","2012-11-21","","['dong chen', 'peter hall', 'hans-georg müller']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"521",1211.5422,"asymptotics for a bayesian nonparametric estimator of species variety","math.st stat.th","in bayesian nonparametric inference, random discrete probability measures are commonly used as priors within hierarchical mixture models for density estimation and for inference on the clustering of the data. recently, it has been shown that they can also be exploited in species sampling problems: indeed they are natural tools for modeling the random proportions of species within a population thus allowing for inference on various quantities of statistical interest. for applications that involve large samples, the exact evaluation of the corresponding estimators becomes impracticable and, therefore, asymptotic approximations are sought. in the present paper, we study the limiting behaviour of the number of new species to be observed from further sampling, conditional on observed data, assuming the observations are exchangeable and directed by a normalized generalized gamma process prior. such an asymptotic study highlights a connection between the normalized generalized gamma process and the two-parameter poisson-dirichlet process that was previously known only in the unconditional case.","10.3150/11-bej371","2012-11-23","","['stefano favaro', 'antonio lijoi', 'igor prünster']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"522",1211.5687,"texture modeling with convolutional spike-and-slab rbms and deep   extensions","cs.lg stat.ml","we apply the spike-and-slab restricted boltzmann machine (ssrbm) to texture modeling. the ssrbm with tiled-convolution weight sharing (tssrbm) achieves or surpasses the state-of-the-art on texture synthesis and inpainting by parametric models. we also develop a novel rbm model with a spike-and-slab visible layer and binary variables in the hidden layer. this model is designed to be stacked on top of the tssrbm. we show the resulting deep belief network (dbn) is a powerful generative model that improves on single-layer models and is capable of modeling not only single high-resolution and challenging textures but also multiple textures.","","2012-11-24","","['heng luo', 'pierre luc carrier', 'aaron courville', 'yoshua bengio']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"523",1211.5706,"data augmentation for hierarchical capture-recapture models","stat.me","capture-recapture studies are widely used to obtain information about abundance (population size or density) of animal populations. a common design is that in which multiple distinct populations are sampled, and the research objective is modeling variation in population size $n_{s}; s=1,2,...,s$ among the populations such as estimating a treatment effect or some other source of variation related to landscape structure. the problem is naturally resolved using hierarchical models. we provide a bayesian formulation of such models using data augmentation which preserves the individual encounter histories in the model and, as such, is amenable to modeling individual effects. we formulate the model by conditioning on the total population size among all populations. in this case, the abundance model can be formulated as a multinomial model that allocates individuals among sites. mcmc is easily carried out by the introduction of a categorical individual effect, $g_{i}$, which partitions the total population size. the prior distribution for the latent variable $g$ is derived from the model assumed for the population sizes $n_{s}$.","","2012-11-24","","['j. andrew royle', 'sarah j. converse', 'william a. link']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"524",1211.7103,"a variational approach to modeling slow processes in stochastic   dynamical systems","math-ph math.mp math.st physics.chem-ph stat.th","the slow processes of metastable stochastic dynamical systems are difficult to access by direct numerical simulation due the sampling problem. here, we suggest an approach for modeling the slow parts of markov processes by approximating the dominant eigenfunctions and eigenvalues of the propagator. to this end, a variational principle is derived that is based on the maximization of a rayleigh coefficient. it is shown that this rayleigh coefficient can be estimated from statistical observables that can be obtained from short distributed simulations starting from different parts of state space. the approach forms a basis for the development of adaptive and efficient computational algorithms for simulating and analyzing metastable markov processes while avoiding the sampling problem. since any stochastic process with finite memory can be transformed into a markov process, the approach is applicable to a wide range of processes relevant for modeling complex real-world phenomena.","","2012-11-29","","['frank noé', 'feliks nüske']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"525",1212.087,"errors-in-variables beta regression models","stat.me","beta regression models provide an adequate approach for modeling continuous outcomes limited to the interval (0,1). this paper deals with an extension of beta regression models that allow for explanatory variables to be measured with error. the structural approach, in which the covariates measured with error are assumed to be random variables, is employed. three estimation methods are presented, namely maximum likelihood, maximum pseudo-likelihood and regression calibration. monte carlo simulations are used to evaluate the performance of the proposed estimators and the na\""ive estimator. also, a residual analysis for beta regression models with measurement errors is proposed. the results are illustrated in a real data set.","","2012-12-04","2013-04-10","['jalmar m. f. carrasco', 'silvia l. p. ferrari', 'reinaldo b. arellano-valle']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"526",1212.0873,"parallel coordinate descent methods for big data optimization","math.oc cs.ai stat.ml","in this work we show that randomized (block) coordinate descent methods can be accelerated by parallelization when applied to the problem of minimizing the sum of a partially separable smooth convex function and a simple separable convex function. the theoretical speedup, as compared to the serial method, and referring to the number of iterations needed to approximately solve the problem with high probability, is a simple expression depending on the number of parallel processors and a natural and easily computable measure of separability of the smooth component of the objective function. in the worst case, when no degree of separability is present, there may be no speedup; in the best case, when the problem is separable, the speedup is equal to the number of processors. our analysis also works in the mode when the number of blocks being updated at each iteration is random, which allows for modeling situations with busy or unreliable processors. we show that our algorithm is able to solve a lasso problem involving a matrix with 20 billion nonzeros in 2 hours on a large memory node with 24 cores.","","2012-12-04","2013-11-25","['peter richtárik', 'martin takáč']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"527",1212.144,"an introduction to solving for quantities of interest in finite-state   semi-markov processes","stat.ap stat.me","semi-markov processes (smps) provide a rich framework for many real-world problems. however, due to difficulty implementing practical solutions they are rarely used with their full capability. the theory of smps is quite mature but was mainly developed at a time when computational resources were not widely available. with the exception of some of the simplest cases, solutions to smps are inherently numerical, and smps have been underutilized by practitioners because of difficulty implementing the theory in applications. this paper demonstrates the theory and computational methods needed to implement smp models in practical settings. methods are illustrated with an application modeling the movement of coronary patients in a hospital. our aim is to allow practitioners to use richer smp models without being burdened with the rigorous mathematical theory.","","2012-12-06","2014-09-18","['richard l. warr', 'david h. collins']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"528",1212.1791,"generative models for functional data using phase and amplitude   separation","stat.co math.st stat.th","constructing generative models for functional observations is an important task in statistical functional analysis. in general, functional data contains both phase (or x or horizontal) and amplitude (or y or vertical) variability. tradi- tional methods often ignore the phase variability and focus solely on the amplitude variation, using cross-sectional techniques such as fpca for dimensional reduction and data modeling. ignoring phase variability leads to a loss of structure in the data and inefficiency in data models. this paper presents an approach that relies on separating the phase (x-axis) and amplitude (y-axis), then modeling these components using joint distributions. this separation, in turn, is performed using a technique called elastic shape analysis of curves that involves a new mathematical representation of functional data. then, using individual fpcas, one each for phase and amplitude components, while respecting the nonlinear geometry of the phase representation space; impose joint probability models on principal coefficients of these components. these ideas are demonstrated using random sampling, for models estimated from simulated and real datasets, and show their superiority over models that ignore phase-amplitude separation. furthermore, the generative models are applied to classification of functional data and achieve high performance in applications involv- ing sonar signals of underwater objects, handwritten signatures, and periodic body movements recorded by smart phones.","10.1016/j.csda.2012.12.001","2012-12-08","2012-12-18","['j. derek tucker', 'wei wu', 'anuj srivastava']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE
"529",1212.2393,"simulating the continuation of a time series in r","stat.co stat.ap","the simulation of the continuation of a given time series is useful for many practical applications. but no standard procedure for this task is suggested in the literature. it is therefore demonstrated how to use the seasonal arima process to simulate the continuation of an observed time series. the r-code presented uses well-known modeling procedures for arima models and conditional simulation of a sarima model with known parameters. a small example demonstrates the correctness and practical relevance of the new idea.","","2012-12-11","","['halis sak', 'wolfgang hörmann']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE
"530",1212.2652,"testing second order dynamics for autoregressive processes in presence   of time-varying variance","stat.me","the volatility modeling for autoregressive univariate time series is considered. a benchmark approach is the stationary arch model of engle (1982). motivated by real data evidence, processes with non constant unconditional variance and arch effects have been recently introduced. we take into account such possible non stationarity and propose simple testing procedures for arch effects. adaptive mcleod and li's portmanteau and arch-lm tests for checking for second order dynamics are provided. the standard versions of these tests, commonly used by practitioners, suppose constant unconditional variance. we prove the failure of these standard tests with time-varying unconditional variance. the theoretical results are illustrated by mean of simulated and real data.","","2012-12-11","","['valentin patilea', 'hamdi raïssi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"531",1212.2991,"accelerating inference: towards a full language, compiler and hardware   stack","cs.se cs.ai stat.ml","we introduce dimple, a fully open-source api for probabilistic modeling. dimple allows the user to specify probabilistic models in the form of graphical models, bayesian networks, or factor graphs, and performs inference (by automatically deriving an inference engine from a variety of algorithms) on the model. dimple also serves as a compiler for gp5, a hardware accelerator for inference.","","2012-12-12","","['shawn hershey', 'jeff bernstein', 'bill bradley', 'andrew schweitzer', 'noah stein', 'theo weber', 'ben vigoda']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"532",1212.2995,"a simple method for detecting interactions between a treatment and a   large number of covariates","stat.me","we consider a setting in which we have a treatment and a large number of covariates for a set of observations, and wish to model their relationship with an outcome of interest. we propose a simple method for modeling interactions between the treatment and covariates. the idea is to modify the covariate in a simple way, and then fit a standard model using the modified covariates and no main effects. we show that coupled with an efficiency augmentation procedure, this method produces valid inferences in a variety of settings. it can be useful for personalized medicine: determining from a large set of biomarkers the subset of patients that can potentially benefit from a treatment. we apply the method to both simulated datasets and gene expression studies of cancer. the modified data can be used for other purposes, for example large scale hypothesis testing for determining which of a set of covariates interact with a treatment variable.","","2012-12-12","","['lu tian', 'ash alizadeh', 'andrew gentles', 'robert tibshirani']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"533",1212.3967,"compartmental analysis of renal physiology using nuclear medicine data   and statistical optimization","math.na physics.med-ph q-bio.to stat.ap","this paper describes a general approach to the compartmental modeling of nuclear data based on spectral analysis and statistical optimization. we utilize the renal physiology as test case and validate the method against both synthetic data and real measurements acquired during two micro-pet experiments with murine models.","","2012-12-17","","['sara garbarino', 'giacomo caviglia', 'massimo brignone', 'michela massollo', 'gianmario sambuceti', 'michele piana']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"534",1212.4786,"a statistical framework for joint eqtl analysis in multiple tissues","q-bio.qm q-bio.gn stat.ap","mapping expression quantitative trait loci (eqtls) represents a powerful and widely-adopted approach to identifying putative regulatory variants and linking them to specific genes. up to now eqtl studies have been conducted in a relatively narrow range of tissues or cell types. however, understanding the biology of organismal phenotypes will involve understanding regulation in multiple tissues, and ongoing studies are collecting eqtl data in dozens of cell types. here we present a statistical framework for powerfully detecting eqtls in multiple tissues or cell types (or, more generally, multiple subgroups). the framework explicitly models the potential for each eqtl to be active in some tissues and inactive in others. by modeling the sharing of active eqtls among tissues this framework increases power to detect eqtls that are present in more than one tissue compared with ""tissue-by-tissue"" analyses that examine each tissue separately. conversely, by modeling the inactivity of eqtls in some tissues, the framework allows the proportion of eqtls shared across different tissues to be formally estimated as parameters of a model, addressing the difficulties of accounting for incomplete power when comparing overlaps of eqtls identified by tissue-by-tissue analyses. applying our framework to re-analyze data from transformed b cells, t cells and fibroblasts we find that it substantially increases power compared with tissue-by-tissue analysis, identifying 63% more genes with eqtls (at fdr=0.05). further the results suggest that, in contrast to previous analyses of the same data, the majority of eqtls detectable in these data are shared among all three tissues.","10.1371/journal.pgen.1003486","2012-12-19","","['timothée flutre', 'xiaoquan wen', 'jonathan pritchard', 'matthew stephens']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"535",1212.5049,"a partial least squares algorithm handling ordinal variables also in   presence of a small number of categories","stat.me stat.ap","the partial least squares (pls) is a popular modeling technique commonly used in social sciences. the traditional pls algorithm deals with variables measured on interval scales while data are often collected on ordinal scales: a reformulation of the algorithm, named ordinal pls (opls), is introduced, which properly deals with ordinal variables. an application to customer satisfaction data and some simulations are also presented. the technique seems to perform better than the traditional pls when the number of categories of the items in the questionnaire is small (4 or 5) which is typical in the most common practical situations.","","2012-12-20","","['gabriele cantaluppi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"536",1212.509,"bayesian analysis of multivariate stochastic volatility with skew   distribution","stat.me stat.ap","multivariate stochastic volatility models with skew distributions are proposed. exploiting cholesky stochastic volatility modeling, univariate stochastic volatility processes with leverage effect and generalized hyperbolic skew t-distributions are embedded to multivariate analysis with time-varying correlations. bayesian prior works allow this approach to provide parsimonious skew structure and to easily scale up for high-dimensional problem. analyses of daily stock returns are illustrated. empirical results show that the time-varying correlations and the sparse skew structure contribute to improved prediction performance and var forecasts.","","2012-12-20","","['jouchi nakajima']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"537",1212.518,"growth curve based on scale mixtures of skew-normal distributions to   model the age-length relationship of cardinalfish (epigonus crassicaudus)","stat.ap","our article presents a robust and flexible statistical modeling for the growth curve associated to the age-length relationship of cardinalfish (epigonus crassicaudus). specifically, we consider a non-linear regression model, in which the error distribution allows heteroscedasticity and belongs to the family of scale mixture of the skewnormal (smsn) distributions, thus eliminating the need to transform the dependent variable into many data sets. the smsn is a tractable and flexible class of asymmetric heavy-tailed distributions that are useful for robust inference when the normality assumption for error distribution is questionable. two well-known important members of this class are the proper skew-normal and skew-t distributions. in this work emphasis is given to the skew-t model. however, the proposed methodology can be adapted for each of the smsn models with some basic changes. the present work is motivated by previous analysis about of cardinalfish age, in which a maximum age of 15 years has been determined. therefore, in this study we carry out the mentioned methodology over a data set that include a long-range of ages based on an otolith sample where the determined longevity is higher than 54 years.","10.1016/j.fishres.2013.05.002","2012-12-20","","['javier e. contreras-reyes', 'reinaldo b. arellano-valle']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"538",1212.5423,"topic extraction and bundling of related scientific articles","cs.ir cs.dl stat.ml","automatic classification of scientific articles based on common characteristics is an interesting problem with many applications in digital library and information retrieval systems. properly organized articles can be useful for automatic generation of taxonomies in scientific writings, textual summarization, efficient information retrieval etc. generating article bundles from a large number of input articles, based on the associated features of the articles is tedious and computationally expensive task. in this report we propose an automatic two-step approach for topic extraction and bundling of related articles from a set of scientific articles in real-time. for topic extraction, we make use of latent dirichlet allocation (lda) topic modeling techniques and for bundling, we make use of hierarchical agglomerative clustering techniques.   we run experiments to validate our bundling semantics and compare it with existing models in use. we make use of an online crowdsourcing marketplace provided by amazon called amazon mechanical turk to carry out experiments. we explain our experimental setup and empirical results in detail and show that our method is advantageous over existing ones.","","2012-12-21","2015-05-01","['shameem a puthiya parambath']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"539",1212.5615,"beta-linear failure rate distribution and its applications","stat.me","we introduce in this paper a new four-parameter generalized version of the linear failure rate (lfr) distribution which is called beta-linear failure rate (blfr) distribution. the new distribution is quite flexible and can be used effectively in modeling survival data and reliability problems. it can have a constant, decreasing, increasing, upside-down bathtub (unimodal) and bathtub-shaped failure rate function depending on its parameters. it includes some well-known lifetime distributions as special submodels. we provide a comprehensive account of the mathematical properties of the new distributions. in particular, a closed-form expressions for the density, cumulative distribution and hazard rate function of the blfr is given. also, the $r$th order moment of this distribution is derived. we discuss maximum likelihood estimation of the unknown parameters of the new model for complete sample and obtain an expression for fishers information matrix. in the end, to show the flexibility of this distribution and illustrative purposes, an application using a real data set is presented.","","2012-12-21","","['ali akbar jafari', 'eisa mahmoudi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"540",1212.6232,"high-dimensional sparse additive hazards regression","stat.me math.st stat.th","high-dimensional sparse modeling with censored survival data is of great practical importance, as exemplified by modern applications in high-throughput genomic data analysis and credit risk analysis. in this article, we propose a class of regularization methods for simultaneous variable selection and estimation in the additive hazards model, by combining the nonconcave penalized likelihood approach and the pseudoscore method. in a high-dimensional setting where the dimensionality can grow fast, polynomially or nonpolynomially, with the sample size, we establish the weak oracle property and oracle property under mild, interpretable conditions, thus providing strong performance guarantees for the proposed methodology. moreover, we show that the regularity conditions required by the $l_1$ method are substantially relaxed by a certain class of sparsity-inducing concave penalties. as a result, concave penalties such as the smoothly clipped absolute deviation (scad), minimax concave penalty (mcp), and smooth integration of counting and absolute deviation (sica) can significantly improve on the $l_1$ method and yield sparser models with better prediction performance. we present a coordinate descent algorithm for efficient implementation and rigorously investigate its convergence properties. the practical utility and effectiveness of the proposed methods are demonstrated by simulation studies and a real data example.","10.1080/01621459.2012.746068","2012-12-26","","['wei lin', 'jinchi lv']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"541",1301.0586,"staged mixture modelling and boosting","cs.lg stat.ml","in this paper, we introduce and evaluate a data-driven staged mixture modeling technique for building density, regression, and classification models. our basic approach is to sequentially add components to a finite mixture model using the structural expectation maximization (sem) algorithm. we show that our technique is qualitatively similar to boosting. this correspondence is a natural byproduct of the fact that we use the sem algorithm to sequentially fit the mixture model. finally, in our experimental evaluation, we demonstrate the effectiveness of our approach on a variety of prediction and density estimation tasks using real-world data.","","2012-12-12","","['christopher meek', 'bo thiesson', 'david heckerman']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"542",1301.0599,"advances in boosting (invited talk)","cs.lg stat.ml","boosting is a general method of generating many simple classification rules and combining them into a single, highly accurate rule. in this talk, i will review the adaboost boosting algorithm and some of its underlying theory, and then look at how this theory has helped us to face some of the challenges of applying adaboost in two domains: in the first of these, we used boosting for predicting and modeling the uncertainty of prices in complicated, interacting auctions. the second application was to the classification of caller utterances in a telephone spoken-dialogue system where we faced two challenges: the need to incorporate prior knowledge to compensate for initially insufficient data; and a later need to filter the large stream of unlabeled examples being collected to select the ones whose labels are likely to be most informative.","","2012-12-12","","['robert e. schapire']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"543",1301.0601,"reinforcement learning with partially known world dynamics","cs.lg stat.ml","reinforcement learning would enjoy better success on real-world problems if domain knowledge could be imparted to the algorithm by the modelers. most problems have both hidden state and unknown dynamics. partially observable markov decision processes (pomdps) allow for the modeling of both. unfortunately, they do not provide a natural framework in which to specify knowledge about the domain dynamics. the designer must either admit to knowing nothing about the dynamics or completely specify the dynamics (thereby turning it into a planning problem). we propose a new framework called a partially known markov decision process (pkmdp) which allows the designer to specify known dynamics while still leaving portions of the environment s dynamics unknown.the model represents not only the environment dynamics but also the agents knowledge of the dynamics. we present a reinforcement learning algorithm for this model based on importance sampling. the algorithm incorporates planning based on the known dynamics and learning about the unknown dynamics. our results clearly demonstrate the ability to add domain knowledge and the resulting benefits for learning.","","2012-12-12","","['christian r. shelton']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"544",1301.0604,"discriminative probabilistic models for relational data","cs.lg cs.ai stat.ml","in many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent. for example, in hypertext classification, the labels of linked pages are highly correlated. a standard approach is to classify each entity independently, ignoring the correlations between them. recently, probabilistic relational models, a relational version of bayesian networks, were used to define a joint probabilistic model for a collection of related entities. in this paper, we present an alternative framework that builds on (conditional) markov networks and addresses two limitations of the previous approach. first, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models. second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy. we show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities. we provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies.","","2012-12-12","","['ben taskar', 'pieter abbeel', 'daphne koller']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"545",1301.0802,"borrowing strengh in hierarchical bayes: posterior concentration of the   dirichlet base measure","math.st cs.lg math.pr stat.th","this paper studies posterior concentration behavior of the base probability measure of a dirichlet measure, given observations associated with the sampled dirichlet processes, as the number of observations tends to infinity. the base measure itself is endowed with another dirichlet prior, a construction known as the hierarchical dirichlet processes (teh et al. [j. amer. statist. assoc. 101 (2006) 1566-1581]). convergence rates are established in transportation distances (i.e., wasserstein metrics) under various conditions on the geometry of the support of the true base measure. as a consequence of the theory, we demonstrate the benefit of ""borrowing strength"" in the inference of multiple groups of data - a powerful insight often invoked to motivate hierarchical modeling. in certain settings, the gain in efficiency due to the latent hierarchy can be dramatic, improving from a standard nonparametric rate to a parametric rate of convergence. tools developed include transportation distances for nonparametric bayesian hierarchies of random measures, the existence of tests for dirichlet measures, and geometric properties of the support of dirichlet measures.","10.3150/15-bej703","2013-01-04","2016-03-24","['xuanlong nguyen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"546",1301.1178,"modeling high energy cosmic rays mass composition data via mixtures of   multivariate skew-t distributions","astro-ph.he astro-ph.im stat.ap","we consider multivariate skew-t distributions for modeling composition data of high energy cosmic rays. the model has been validated with simulated data for different primary nuclei and hadronic models focusing on the depth of maximum xmax and number of muons n{\mu} observables. further, we consider mixtures of multivariate skew-t distributions for cosmic ray mass composition determination and event-by-event classification. with respect to other approaches in the field, it is based on analytical calculations and allows to incorporate different sets of constraints provided by the present hadronic models. we present some applications to simulated data sets generated with different nuclear abundances assumptions. as it does not fully rely on the hadronic model predictions, the method is particularly suited to the current experimental scenario in which evidences of discrepancies of the measured data with respect to the models have been reported for some shower observables, such as the number of muons at ground level.","","2013-01-07","","['s. riggi', 's. ingrassia']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"547",1301.1505,"maximum likelihood estimation in constrained parameter spaces for   mixtures of factor analyzers","stat.me","mixtures of factor analyzers are becoming more and more popular in the area of model based clustering of high-dimensional data. according to the likelihood approach in data modeling, it is well known that the unconstrained log-likelihood function may present spurious maxima and singularities and this is due to specific patterns of the estimated covariance structure, when their determinant approaches 0. to reduce such drawbacks, in this paper we introduce a procedure for the parameter estimation of mixtures of factor analyzers, which maximizes the likelihood function in a constrained parameter space. we then analyze and measure its performance, compared to the usual non-constrained approach, via some simulations and applications to real data sets.","","2013-01-08","","['francesca greselin', 'salvatore ingrassia']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"548",1301.153,"a hierarchical max-stable spatial model for extreme precipitation","stat.ap","extreme environmental phenomena such as major precipitation events manifestly exhibit spatial dependence. max-stable processes are a class of asymptotically-justified models that are capable of representing spatial dependence among extreme values. while these models satisfy modeling requirements, they are limited in their utility because their corresponding joint likelihoods are unknown for more than a trivial number of spatial locations, preventing, in particular, bayesian analyses. in this paper, we propose a new random effects model to account for spatial dependence. we show that our specification of the random effect distribution leads to a max-stable process that has the popular gaussian extreme value process (gevp) as a limiting case. the proposed model is used to analyze the yearly maximum precipitation from a regional climate model.","10.1214/12-aoas591","2013-01-08","","['brian j. reich', 'benjamin a. shaby']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"549",1301.2194,"network-based clustering with mixtures of l1-penalized gaussian   graphical models: an empirical investigation","stat.ml cs.lg stat.me","in many applications, multivariate samples may harbor previously unrecognized heterogeneity at the level of conditional independence or network structure. for example, in cancer biology, disease subtypes may differ with respect to subtype-specific interplay between molecular components. then, both subtype discovery and estimation of subtype-specific networks present important and related challenges. to enable such analyses, we put forward a mixture model whose components are sparse gaussian graphical models. this brings together model-based clustering and graphical modeling to permit simultaneous estimation of cluster assignments and cluster-specific networks. we carry out estimation within an l1-penalized framework, and investigate several specific penalization regimes. we present empirical results on simulated data and provide general recommendations for the formulation and use of mixtures of l1-penalized gaussian graphical models.","","2013-01-10","","['steven m. hill', 'sach mukherjee']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"550",1301.2316,"cross-covariance modelling via dags with hidden variables","cs.lg stat.ml","dag models with hidden variables present many difficulties that are not present when all nodes are observed. in particular, fully observed dag models are identified and correspond to well-defined sets ofdistributions, whereas this is not true if nodes are unobserved. inthis paper we characterize exactly the set of distributions given by a class of one-dimensional gaussian latent variable models. these models relate two blocks of observed variables, modeling only the cross-covariance matrix. we describe the relation of this model to the singular value decomposition of the cross-covariance matrix. we show that, although the model is underidentified, useful information may be extracted. we further consider an alternative parametrization in which one latent variable is associated with each block. our analysis leads to some novel covariance equivalence results for gaussian hidden variable models.","","2013-01-10","","['jacob a. wegelin', 'thomas s. richardson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"551",1301.2411,"assessing transient carryover effects in recurrent event processes, with   application to chronic health conditions","stat.ap","in some settings involving recurrent events, the occurrence of one event may produce a temporary increase in the event intensity; we refer to this phenomenon as a transient carryover effect. this paper provides models and tests for carryover effect. motivation for our work comes from events associated with chronic health conditions, and we consider two studies involving asthma attacks in children in some detail. we consider how carryover effects can be modeled and assessed, and note some difficulties in the context of heterogeneous groups of individuals. we give a simple intuitive test for no carryover effect and examine its properties. in addition, we demonstrate the need for detailed modeling in trying to deconstruct the dynamics of recurrent events.","10.1214/12-aoas560","2013-01-11","","['candemir çiğşar', 'jerald f. lawless']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"552",1301.2435,"toxicity profiling of engineered nanomaterials via multivariate   dose-response surface modeling","stat.ap","new generation in vitro high-throughput screening (hts) assays for the assessment of engineered nanomaterials provide an opportunity to learn how these particles interact at the cellular level, particularly in relation to injury pathways. these types of assays are often characterized by small sample sizes, high measurement error and high dimensionality, as multiple cytotoxicity outcomes are measured across an array of doses and durations of exposure. in this paper we propose a probability model for the toxicity profiling of engineered nanomaterials. a hierarchical structure is used to account for the multivariate nature of the data by modeling dependence between outcomes and thereby combining information across cytotoxicity pathways. in this framework we are able to provide a flexible surface-response model that provides inference and generalizations of various classical risk assessment parameters. we discuss applications of this model to data on eight nanoparticles evaluated in relation to four cytotoxicity parameters.","10.1214/12-aoas563","2013-01-11","","['trina patel', 'donatello telesca', 'saji george', 'andré e. nel']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"553",1301.3027,"semi-parametric robust event detection for massive time-domain databases","stat.ap astro-ph.im","the detection and analysis of events within massive collections of time-series has become an extremely important task for time-domain astronomy. in particular, many scientific investigations (e.g. the analysis of microlensing and other transients) begin with the detection of isolated events in irregularly-sampled series with both non-linear trends and non-gaussian noise. we outline a semi-parametric, robust, parallel method for identifying variability and isolated events at multiple scales in the presence of the above complications. this approach harnesses the power of bayesian modeling while maintaining much of the speed and scalability of more ad-hoc machine learning approaches. we also contrast this work with event detection methods from other fields, highlighting the unique challenges posed by astronomical surveys. finally, we present results from the application of this method to 87.2 million eros-2 sources, where we have obtained a greater than 100-fold reduction in candidates for certain types of phenomena while creating high-quality features for subsequent analyses.","","2013-01-14","2013-01-19","['alexander w blocker', 'pavlos protopapas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"554",1301.3192,"matrix approximation under local low-rank assumption","cs.lg stat.ml","matrix approximation is a common tool in machine learning for building accurate prediction models for recommendation systems, text mining, and computer vision. a prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. we propose a new matrix approximation model where we assume instead that the matrix is only locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. we analyze the accuracy of the proposed local low-rank modeling. our experiments show improvements in prediction accuracy in recommendation tasks.","","2013-01-14","","['joonseok lee', 'seungyeon kim', 'guy lebanon', 'yoram singer']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"555",1301.357,"a nested hdp for hierarchical topic models","stat.ml","we develop a nested hierarchical dirichlet process (nhdp) for hierarchical topic modeling. the nhdp is a generalization of the nested chinese restaurant process (ncrp) that allows each word to follow its own path to a topic node according to a document-specific distribution on a shared tree. this alleviates the rigid, single-path formulation of the ncrp, allowing a document to more easily express thematic borrowings as a random effect. we demonstrate our algorithm on 1.8 million documents from the new york times.","","2013-01-15","","['john paisley', 'chong wang', 'david blei', 'michael i. jordan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"556",1301.3602,"fourier transform methods for pathwise covariance estimation in the   presence of jumps","math.st stat.th","we provide a new non-parametric fourier procedure to estimate the trajectory of the instantaneous covariance process (from discrete observations of a multidimensional price process) in the presence of jumps extending the seminal work malliavin and mancino~\cite{mm:02, mm:09}. our approach relies on a modification of (classical) jump-robust estimators of integrated realized covariance to estimate the fourier coefficients of the covariance trajectory. using fourier-f\'ejer inversion we reconstruct the path of the instantaneous covariance. we prove consistency and central limit theorem (clt) and in particular that the asymptotic estimator variance is smaller by a factor $ 2/3$ in comparison to classical local estimators.   the procedure is robust enough to allow for an iteration and we can show theoretically and empirically how to estimate the integrated realized covariance of the instantaneous stochastic covariance process. we apply these techniques to robust calibration problems for multivariate modeling in finance, i.e., the selection of a pricing measure by using time series and derivatives' price information simultaneously.","","2013-01-16","2014-06-20","['christa cuchiero', 'josef teichmann']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"557",1301.3617,"sequential bayesian inference in hidden markov stochastic kinetic models   with application to detection and response to seasonal epidemics","stat.co math.pr math.st stat.th","we study sequential bayesian inference in stochastic kinetic models with latent factors. assuming continuous observation of all the reactions, our focus is on joint inference of the unknown reaction rates and the dynamic latent states, modeled as a hidden markov factor. using insights from nonlinear filtering of continuous-time jump markov processes we develop a novel sequential monte carlo algorithm for this purpose. our approach applies the ideas of particle learning to minimize particle degeneracy and exploit the analytical jump markov structure. a motivating application of our methods is modeling of seasonal infectious disease outbreaks represented through a compartmental epidemic model. we demonstrate inference in such models with several numerical illustrations and also discuss predictive analysis of epidemic countermeasures using sequential bayes estimates.","10.1007/s11222-013-9419-z","2013-01-16","","['junjing lin', 'michael ludkovski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE
"558",1301.3851,"minimum message length clustering using gibbs sampling","cs.lg stat.ml","the k-mean and em algorithms are popular in clustering and mixture modeling, due to their simplicity and ease of implementation. however, they have several significant limitations. both coverage to a local optimum of their respective objective functions (ignoring the uncertainty in the model space), require the apriori specification of the number of classes/clsuters, and are inconsistent. in this work we overcome these limitations by using the minimum message length (mml) principle and a variation to the k-means/em observation assignment and parameter calculation scheme. we maintain the simplicity of these approaches while constructing a bayesian mixture modeling tool that samples/searches the model space using a markov chain monte carlo (mcmc) sampler known as a gibbs sampler. gibbs sampling allows us to visit each model according to its posterior probability. therefore, if the model space is multi-modal we will visit all models and not get stuck in local optima. we call our approach multiple chains at equilibrium (mce) mml sampling.","","2013-01-16","","['ian davidson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"559",1301.3852,"mix-nets: factored mixtures of gaussians in bayesian networks with mixed   continuous and discrete variables","cs.lg cs.ai stat.ml","recently developed techniques have made it possible to quickly learn accurate probability density functions from data in low-dimensional continuous space. in particular, mixtures of gaussians can be fitted to data very quickly using an accelerated em algorithm that employs multiresolution kd-trees (moore, 1999). in this paper, we propose a kind of bayesian networks in which low-dimensional mixtures of gaussians over different subsets of the domain's variables are combined into a coherent joint probability model over the entire domain. the network is also capable of modeling complex dependencies between discrete variables and continuous variables without requiring discretization of the continuous variables. we present efficient heuristic algorithms for automatically learning these networks from data, and perform comparative experiments illustrated how well these networks model real scientific data and synthetic data. we also briefly discuss some possible improvements to the networks, as well as possible applications.","","2013-01-16","","['scott davies', 'andrew moore']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"560",1301.3854,"learning graphical models of images, videos and their spatial   transformations","cs.cv cs.lg stat.ml","mixtures of gaussians, factor analyzers (probabilistic pca) and hidden markov models are staples of static and dynamic data modeling and image and video modeling in particular. we show how topographic transformations in the input, such as translation and shearing in images, can be accounted for in these models by including a discrete transformation variable. the resulting models perform clustering, dimensionality reduction and time-series analysis in a way that is invariant to transformations in the input. using the em algorithm, these transformation-invariant models can be fit to static data and time series. we give results on filtering microscopy images, face and facial pose clustering, handwritten digit modeling and recognition, video clustering, object tracking, and removal of distractions from video sequences.","","2013-01-16","","['brendan j. frey', 'nebojsa jojic']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"561",1301.3877,"the anchors hierachy: using the triangle inequality to survive high   dimensional data","cs.lg cs.ds stat.ml","this paper is about metric data structures in high-dimensional or non-euclidean space that permit cached sufficient statistics accelerations of learning algorithms.   it has recently been shown that for less than about 10 dimensions, decorating kd-trees with additional ""cached sufficient statistics"" such as first and second moments and contingency tables can provide satisfying acceleration for a very wide range of statistical learning tasks such as kernel regression, locally weighted regression, k-means clustering, mixture modeling and bayes net learning.   in this paper, we begin by defining the anchors hierarchy - a fast data structure and algorithm for localizing data based only on a triangle-inequality-obeying distance metric. we show how this, in its own right, gives a fast and effective clustering of data. but more importantly we show how it can produce a well-balanced structure similar to a ball-tree (omohundro, 1991) or a kind of metric tree (uhlmann, 1991; ciaccia, patella, & zezula, 1997) in a way that is neither ""top-down"" nor ""bottom-up"" but instead ""middle-out"". we then show how this structure, decorated with cached sufficient statistics, allows a wide variety of statistical learning algorithms to be accelerated even in thousands of dimensions.","","2013-01-16","","['andrew moore']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"562",1301.4566,"sparse/robust estimation and kalman smoothing with nonsmooth log-concave   densities: modeling, computation, and theory","stat.ml math.oc math.st stat.th","we introduce a class of quadratic support (qs) functions, many of which play a crucial role in a variety of applications, including machine learning, robust statistical inference, sparsity promotion, and kalman smoothing. well known examples include the l2, huber, l1 and vapnik losses. we build on a dual representation for qs functions using convex analysis, revealing the structure necessary for a qs function to be interpreted as the negative log of a probability density, and providing the foundation for statistical interpretation and analysis of qs loss functions. for a subclass of qs functions called piecewise linear quadratic (plq) penalties, we also develop efficient numerical estimation schemes. these components form a flexible statistical modeling framework for a variety of learning applications, together with a toolbox of efficient numerical methods for inference. in particular, for plq densities, interior point (ip) methods can be used. ip methods solve nonsmooth optimization problems by working directly with smooth systems of equations characterizing their optimality. the efficiency of the ip approach depends on the structure of particular applications. we consider the class of dynamic inverse problems using kalman smoothing, where the aim is to reconstruct the state of a dynamical system with known process and measurement models starting from noisy output samples. in the classical case, gaussian errors are assumed in the process and measurement models. the extended framework allows arbitrary plq densities to be used, and the proposed ip approach solves the generalized kalman smoothing problem while maintaining the linear complexity in the size of the time series, just as in the gaussian case. this extends the computational efficiency of classic algorithms to a much broader nonsmooth setting, and includes many recently proposed robust and sparse smoothers as special cases.","","2013-01-19","2013-05-02","['aleksandr y. aravkin', 'james v. burke', 'gianluigi pillonetto']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"563",1301.495,"bayesian conditional tensor factorizations for high-dimensional   classification","stat.me","in many application areas, data are collected on a categorical response and high-dimensional categorical predictors, with the goals being to build a parsimonious model for classification while doing inferences on the important predictors. in settings such as genomics, there can be complex interactions among the predictors. by using a carefully-structured tucker factorization, we define a model that can characterize any conditional probability, while facilitating variable selection and modeling of higher-order interactions. following a bayesian approach, we propose a markov chain monte carlo algorithm for posterior computation accommodating uncertainty in the predictors to be included. under near sparsity assumptions, the posterior distribution for the conditional probability is shown to achieve close to the parametric rate of contraction even in ultra high-dimensional settings. the methods are illustrated using simulation examples and biomedical applications.","","2013-01-21","","['yun yang', 'david b. dunson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"564",1301.5129,"a bayesian non-parametric approach to asymmetric dynamic conditional   correlation model with application to portfolio selection","q-fin.pm stat.ap","we propose a bayesian non-parametric approach for modeling the distribution of multiple returns. in particular, we use an asymmetric dynamic conditional correlation (adcc) model to estimate the time-varying correlations of financial returns where the individual volatilities are driven by gjr-garch models. the adcc-gjr-garch model takes into consideration the asymmetries in individual assets' volatilities, as well as in the correlations. the errors are modeled using a dirichlet location-scale mixture of multivariate gaussian distributions allowing for a great flexibility in the return distribution in terms of skewness and kurtosis. model estimation and prediction are developed using mcmc methods based on slice sampling techniques. we carry out a simulation study to illustrate the flexibility of the proposed approach. we find that the proposed dpm model is able to adapt to several frequently used distribution models and also accurately estimates the posterior distribution of the volatilities of the returns, without assuming any underlying distribution. finally, we present a financial application using apple and nasdaq industrial index data to solve a portfolio allocation problem. we find that imposing a restrictive parametric distribution can result into underestimation of the portfolio variance, whereas dpm model is able to overcome this problem.","10.1016/j.csda.2014.12.005","2013-01-22","2014-01-31","['audrone virbickaite', 'm. concepción ausín', 'pedro galeano']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"565",1301.5579,"random intersection graph process","math.pr math.co math.st stat.th","we introduce a random intersection graph process aimed at modeling sparse evolving affiliation networks that admit tunable (power law) degree distribution and assortativity and clustering coefficients. we show the asymptotic degree distribution and provide explicit asymptotic formulas for assortativity and clustering coefficients.","","2013-01-23","","['mindaugas bloznelis', 'michal karonski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"566",1301.5686,"transfer topic modeling with ease and scalability","cs.cl cs.lg stat.ml","the increasing volume of short texts generated on social media sites, such as twitter or facebook, creates a great demand for effective and efficient topic modeling approaches. while latent dirichlet allocation (lda) can be applied, it is not optimal due to its weakness in handling short texts with fast-changing topics and scalability concerns. in this paper, we propose a transfer learning approach that utilizes abundant labeled documents from other domains (such as yahoo! news or wikipedia) to improve topic modeling, with better model fitting and result interpretation. specifically, we develop transfer hierarchical lda (thlda) model, which incorporates the label information from other domains via informative priors. in addition, we develop a parallel implementation of our model for large-scale applications. we demonstrate the effectiveness of our thlda model on both a microblogging dataset and standard text collections including ap and rcv1 datasets.","","2013-01-23","2013-01-26","['jeon-hyung kang', 'jun ma', 'yan liu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"567",1301.645,"recursive pathways to marginal likelihood estimation with   prior-sensitivity analysis","stat.me astro-ph.im","we investigate the utility to computational bayesian analyses of a particular family of recursive marginal likelihood estimators characterized by the (equivalent) algorithms known as ""biased sampling"" or ""reverse logistic regression"" in the statistics literature and ""the density of states"" in physics. through a pair of numerical examples (including mixture modeling of the well-known galaxy data set) we highlight the remarkable diversity of sampling schemes amenable to such recursive normalization, as well as the notable efficiency of the resulting pseudo-mixture distributions for gauging prior sensitivity in the bayesian model selection context. our key theoretical contributions are to introduce a novel heuristic (""thermodynamic integration via importance sampling"") for qualifying the role of the bridging sequence in this procedure and to reveal various connections between these recursive estimators and the nested sampling technique.","10.1214/13-sts465","2013-01-28","2014-10-15","['ewan cameron', 'anthony pettitt']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"568",1301.6722,"bayes nets in educational assessment: where do the numbers come from?","cs.ai cs.cy stat.ap","as observations and student models become complex, educational assessments that exploit advances in technology and cognitive psychology can outstrip familiar testing models and analytic methods. within the portal conceptual framework for assessment design, bayesian inference networks (bins) record beliefs about students' knowledge and skills, in light of what they say and do. joining evidence model bin fragments- which contain observable variables and pointers to student model variables - to the student model allows one to update belief about knowledge and skills as observations arrive. markov chain monte carlo (mcmc) techniques can estimate the required conditional probabilities from empirical data, supplemented by expert judgment or substantive theory. details for the special cases of item response theory (irt) and multivariate latent class modeling are given, with a numerical example of the latter.","","2013-01-23","","['robert mislevy', 'russell almond', 'duanli yan', 'linda s. steinberg']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"569",1301.6727,"learning bayesian networks with restricted causal interactions","cs.ai cs.lg stat.ml","a major problem for the learning of bayesian networks (bns) is the exponential number of parameters needed for conditional probability tables. recent research reduces this complexity by modeling local structure in the probability tables. we examine the use of log-linear local models. while log-linear models in this context are not new (whittaker, 1990; buntine, 1991; neal, 1992; heckerman and meek, 1997), for structure learning they are generally subsumed under a naive bayes model. we describe an alternative interpretation, and use a minimum message length (mml) (wallace, 1987) metric for structure learning of networks exhibiting causal independence, which we term first-order networks (fons). we also investigate local model selection on a node-by-node basis.","","2013-01-23","","['julian r. neil', 'chris s. wallace', 'kevin b. korb']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"570",1302.0115,"a bayesian nonparametric approach to modeling market share dynamics","math.st math.pr stat.me stat.th","we propose a flexible stochastic framework for modeling the market share dynamics over time in a multiple markets setting, where firms interact within and between markets. firms undergo stochastic idiosyncratic shocks, which contract their shares, and compete to consolidate their position by acquiring new ones in both the market where they operate and in new markets. the model parameters can meaningfully account for phenomena such as barriers to entry and exit, fixed and sunk costs, costs of expanding to new sectors with different technologies and competitive advantage among firms. the construction is obtained in a bayesian framework by means of a collection of nonparametric hierarchical mixtures, which induce the dependence between markets and provide a generalization of the blackwell-macqueen p\'{o}lya urn scheme, which in turn is used to generate a partially exchangeable dynamical particle system. a markov chain monte carlo algorithm is provided for simulating trajectories of the system, by means of which we perform a simulation study for transitions to different economic regimes. moreover, it is shown that the infinite-dimensional properties of the system, when appropriately transformed and rescaled, are those of a collection of interacting fleming-viot diffusions.","10.3150/11-bej392","2013-02-01","","['igor prünster', 'matteo ruggiero']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"571",1302.3979,"gaussian process vine copulas for multivariate dependence","stat.me stat.ml","copulas allow to learn marginal distributions separately from the multivariate dependence structure (copula) that links them together into a density function. vine factorizations ease the learning of high-dimensional copulas by constructing a hierarchy of conditional bivariate copulas. however, to simplify inference, it is common to assume that each of these conditional bivariate copulas is independent from its conditioning variables. in this paper, we relax this assumption by discovering the latent functions that specify the shape of a conditional copula given its conditioning variables we learn these functions by following a bayesian approach based on sparse gaussian processes with expectation propagation for scalable, approximate inference. experiments on real-world datasets show that, when modeling all conditional dependencies, we obtain better estimates of the underlying copula of the data.","","2013-02-16","","['david lopez-paz', 'josé miguel hernández-lobato', 'zoubin ghahramani']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"572",1302.4211,"multivariate varying coefficient model for functional responses","math.st stat.th","motivated by recent work studying massive imaging data in the neuroimaging literature, we propose multivariate varying coefficient models (mvcm) for modeling the relation between multiple functional responses and a set of covariates. we develop several statistical inference procedures for mvcm and systematically study their theoretical properties. we first establish the weak convergence of the local linear estimate of coefficient functions, as well as its asymptotic bias and variance, and then we derive asymptotic bias and mean integrated squared error of smoothed individual functions and their uniform convergence rate. we establish the uniform convergence rate of the estimated covariance function of the individual functions and its associated eigenvalue and eigenfunctions. we propose a global test for linear hypotheses of varying coefficient functions, and derive its asymptotic distribution under the null hypothesis. we also propose a simultaneous confidence band for each individual effect curve. we conduct monte carlo simulation to examine the finite-sample performance of the proposed procedures. we apply mvcm to investigate the development of white matter diffusivities along the genu tract of the corpus callosum in a clinical study of neurodevelopment.","10.1214/12-aos1045","2013-02-18","","['hongtu zhu', 'runze li', 'linglong kong']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"573",1302.4631,"intelligent compaction and quality assurance of roller measurement   values utilizing backfitting and multiresolution scale space analysis","stat.ap","modern earthwork compaction rollers collect location and compaction information as they traverse a compaction site. these roller measurement values present a challenging spatio-temporal statistical problem that requires careful implementation of a proper stochastic model and estimation procedure. heersink and furrer (2013) proposed a sequential, spatial mixed-effects model and a sequential, spatial backfitting routine for estimation of the modeling terms for such data. the estimated fields produced from this backfitting procedure are analyzed using a multiresolution scale space analysis developed by holmstrom et al. (2011). this image analysis is proposed as a viable solution to improved intelligent compaction and quality assurance of the compaction process.","","2013-02-19","2013-03-20","['daniel k. heersink', 'reinhard furrer', 'mike a. mooney']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"574",1302.4964,"estimating continuous distributions in bayesian classifiers","cs.lg cs.ai stat.ml","when modeling a probability distribution with a bayesian network, we are faced with the problem of how to handle continuous variables. most previous work has either solved the problem by discretizing, or assumed that the data are generated by a single gaussian. in this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation. for a naive bayesian classifier, we present experimental results on a variety of natural and artificial domains, comparing two methods of density estimation: assuming normality and modeling each conditional distribution with a single gaussian; and using nonparametric kernel density estimation. we observe large reductions in error on several natural and artificial data sets, which suggests that kernel estimation is a useful tool for learning bayesian models.","","2013-02-20","","['george h. john', 'pat langley']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"575",1302.5449,"nonparametric basis pursuit via sparse kernel-based learning","cs.lg cs.cv cs.it math.it stat.ml","signal processing tasks as fundamental as sampling, reconstruction, minimum mean-square error interpolation and prediction can be viewed under the prism of reproducing kernel hilbert spaces. endowing this vantage point with contemporary advances in sparsity-aware modeling and processing, promotes the nonparametric basis pursuit advocated in this paper as the overarching framework for the confluence of kernel-based learning (kbl) approaches leveraging sparse linear regression, nuclear-norm regularization, and dictionary learning. the novel sparse kbl toolbox goes beyond translating sparse parametric approaches to their nonparametric counterparts, to incorporate new possibilities such as multi-kernel selection and matrix smoothing. the impact of sparse kbl to signal processing applications is illustrated through test cases from cognitive radio sensing, microarray data imputation, and network traffic prediction.","","2013-02-21","","['juan andres bazerque', 'georgios b. giannakis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"576",1302.5475,"estimation of oblique structure via penalized likelihood factor analysis","stat.me stat.co","we consider the problem of sparse estimation via a lasso-type penalized likelihood procedure in a factor analysis model. typically, the model estimation is done under the assumption that the common factors are orthogonal (uncorrelated). however, the lasso-type penalization method based on the orthogonal model can often estimate a completely different model from that with the true factor structure when the common factors are correlated. in order to overcome this problem, we propose to incorporate a factor correlation into the model, and estimate the factor correlation along with parameters included in the orthogonal model by maximum penalized likelihood procedure. an entire solution path is computed by the em algorithm with coordinate descent, which permits the application to a wide variety of convex and nonconvex penalties. the proposed method can provide sufficiently sparse solutions, and be applied to the data where the number of variables is larger than the number of observations. monte carlo simulations are conducted to investigate the effectiveness of our modeling strategies. the results show that the lasso-type penalization based on the orthogonal model cannot often approximate the true factor structure, whereas our approach performs well in various situations. the usefulness of the proposed procedure is also illustrated through the analysis of real data.","","2013-02-21","","['kei hirose', 'michio yamamoto']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"577",1302.5847,"characterizing branching processes from sampled data","stat.ap","branching processes model the evolution of populations of agents that randomly generate offsprings. these processes, more patently galton-watson processes, are widely used to model biological, social, cognitive, and technological phenomena, such as the diffusion of ideas, knowledge, chain letters, viruses, and the evolution of humans through their y-chromosome dna or mitochondrial rna. a practical challenge of modeling real phenomena using a galton-watson process is the offspring distribution, which must be measured from the population. in most cases, however, directly measuring the offspring distribution is unrealistic due to lack of resources or the death of agents. so far, researchers have relied on informed guesses to guide their choice of offspring distribution. in this work we propose two methods to estimate the offspring distribution from real sampled data. using a small sampled fraction of the agents and instrumented with the identity of the ancestors of the sampled agents, we show that accurate offspring distribution estimates can be obtained by sampling as little as 14% of the population.","","2013-02-23","","['fabricio murai', 'bruno ribeiro', 'don towsley', 'krista gile']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"578",1302.6427,"hypothesis testing for validation and certification","stat.me math.pr","we develop a hypothesis testing framework for the formulation of the problems of 1) the validation of a simulation model and 2) using modeling to certify the performance of a physical system. these results are used to solve the extrapolative validation and certification problems, namely problems where the regime of interest is different than the regime for which we have experimental data. we use concentration of measure theory to develop the tests and analyze their errors. this work was stimulated by the work of lucas, owhadi, and ortiz where a rigorous method of validation and certification is described and tested. in a remark we describe the connection between the two approaches. moreover, as mentioned in that work these results have important implications in the quantification of margins and uncertainties (qmu) framework. in particular, in a remark we describe how it provides a rigorous interpretation of the notion of confidence and new notions of margins and uncertainties which allow this interpretation. since certain concentration parameters used in the above tests may be unkown, we furthermore show, in the last half of the paper, how to derive equally powerful tests which estimate them from sample data, thus replacing the assumption of the values of the concentration parameters with weaker assumptions. this paper is an essentially exact copy of one dated april 10, 2010.","","2013-02-26","","['clint scovel', 'ingo steinwart']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"579",1302.6498,"parameter estimation for multivariate generalized gaussian distributions","stat.ap","due to its heavy-tailed and fully parametric form, the multivariate generalized gaussian distribution (mggd) has been receiving much attention for modeling extreme events in signal and image processing applications. considering the estimation issue of the mggd parameters, the main contribution of this paper is to prove that the maximum likelihood estimator (mle) of the scatter matrix exists and is unique up to a scalar factor, for a given shape parameter \beta\in(0,1). moreover, an estimation algorithm based on a newton-raphson recursion is proposed for computing the mle of mggd parameters. various experiments conducted on synthetic and real data are presented to illustrate the theoretical derivations in terms of number of iterations and number of samples for different values of the shape parameter. the main conclusion of this work is that the parameters of mggds can be estimated using the maximum likelihood principle with good performance.","10.1109/tsp.2013.2282909","2013-02-26","2017-02-24","['f. pascal', 'l. bombrun', 'j. y. tourneret', 'y. berthoumieu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"580",1302.6613,"an introductory study on time series modeling and forecasting","cs.lg stat.ml","time series modeling and forecasting has fundamental importance to various practical domains. thus a lot of active research works is going on in this subject during several years. many important models have been proposed in literature for improving the accuracy and effectiveness of time series forecasting. the aim of this dissertation work is to present a concise description of some popular time series forecasting models used in practice, with their salient features. in this thesis, we have described three important classes of time series models, viz. the stochastic, neural networks and svm based models, together with their inherent forecasting strengths and weaknesses. we have also discussed about the basic issues related to time series modeling, such as stationarity, parsimony, overfitting, etc. our discussion about different time series models is supported by giving the experimental forecast results, performed on six real time series datasets. while fitting a model to a dataset, special care is taken to select the most parsimonious one. to evaluate forecast accuracy as well as to compare among different models fitted to a time series, we have used the five performance measures, viz. mse, mad, rmse, mape and theil's u-statistics. for each of the six datasets, we have shown the obtained forecast diagram which graphically depicts the closeness between the original and forecasted observations. to have authenticity as well as clarity in our discussion about time series modeling and forecasting, we have taken the help of various published research works from reputed journals and some standard books.","","2013-02-26","","['ratnadip adhikari', 'r. k. agrawal']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"581",1302.7088,"continuous-time infinite dynamic topic models","cs.ir stat.ap stat.ml","topic models are probabilistic models for discovering topical themes in collections of documents. in real world applications, these models provide us with the means of organizing what would otherwise be unstructured collections. they can help us cluster a huge collection into different topics or find a subset of the collection that resembles the topical theme found in an article at hand.   the first wave of topic models developed were able to discover the prevailing topics in a big collection of documents spanning a period of time. it was later realized that these time-invariant models were not capable of modeling 1) the time varying number of topics they discover and 2) the time changing structure of these topics. few models were developed to address this two deficiencies. the online-hierarchical dirichlet process models the documents with a time varying number of topics. it varies the structure of the topics over time as well. however, it relies on document order, not timestamps to evolve the model over time. the continuous-time dynamic topic model evolves topic structure in continuous-time. however, it uses a fixed number of topics over time.   in this dissertation, i present a model, the continuous-time infinite dynamic topic model, that combines the advantages of these two models 1) the online-hierarchical dirichlet process, and 2) the continuous-time dynamic topic model. more specifically, the model i present is a probabilistic topic model that does the following: 1) it changes the number of topics over continuous time, and 2) it changes the topic structure over continuous-time.   i compared the model i developed with the two other models with different setting values. the results obtained were favorable to my model and showed the need for having a model that has a continuous-time varying number of topics and topic structure.","","2013-02-28","","['wesam elshamy']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"582",1303.0383,"local gaussian process approximation for large computer experiments","stat.me stat.co","we provide a new approach to approximate emulation of large computer experiments. by focusing expressly on desirable properties of the predictive equations, we derive a family of local sequential design schemes that dynamically define the support of a gaussian process predictor based on a local subset of the data. we further derive expressions for fast sequential updating of all needed quantities as the local designs are built-up iteratively. then we show how independent application of our local design strategy across the elements of a vast predictive grid facilitates a trivially parallel implementation. the end result is a global predictor able to take advantage of modern multicore architectures, while at the same time allowing for a nonstationary modeling feature as a bonus. we demonstrate our method on two examples utilizing designs sized in the thousands, and tens of thousands of data points. comparisons are made to the method of compactly supported covariances.","","2013-03-02","2014-10-10","['robert b. gramacy', 'daniel w. apley']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"583",1303.0417,"on the convergence of the irls algorithm in non-local patch regression","cs.cv stat.ml","recently, it was demonstrated in [cs2012,cs2013] that the robustness of the classical non-local means (nlm) algorithm [bcm2005] can be improved by incorporating $\ell^p (0 < p \leq 2)$ regression into the nlm framework. this general optimization framework, called non-local patch regression (nlpr), contains nlm as a special case. denoising results on synthetic and natural images show that nlpr consistently performs better than nlm beyond a moderate noise level, and significantly so when $p$ is close to zero. an iteratively reweighted least-squares (irls) algorithm was proposed for solving the regression problem in nlpr, where the nlm output was used to initialize the iterations. based on exhaustive numerical experiments, we observe that the irls algorithm is globally convergent (for arbitrary initialization) in the convex regime $1 \leq p \leq 2$, and locally convergent (fails very rarely using nlm initialization) in the non-convex regime $0 < p < 1$. in this letter, we adapt the ""majorize-minimize"" framework introduced in [voss1980] to explain these observations.   [cs2012] chaudhury et al. (2012), ""non-local euclidean medians,"" ieee signal processing letters.   [cs2013] chaudhury et al. (2013), ""non-local patch regression: robust image denoising in patch space,"" ieee icassp.   [bcm2005] buades et al. (2005), ""a review of image denoising algorithms, with a new one,"" multiscale modeling and simulation.   [voss1980] voss et al. (1980), ""linear convergence of generalized weiszfeld's method,"" computing.","10.1109/lsp.2013.2268248","2013-03-02","2013-04-29","['kunal n. chaudhury']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"584",1303.0449,"bayesian learning of joint distributions of objects","stat.me","there is increasing interest in broad application areas in defining flexible joint models for data having a variety of measurement scales, while also allowing data of complex types, such as functions, images and documents. we consider a general framework for nonparametric bayes joint modeling through mixture models that incorporate dependence across data types through a joint mixing measure. the mixing measure is assigned a novel infinite tensor factorization (itf) prior that allows flexible dependence in cluster allocation across data types. the itf prior is formulated as a tensor product of stick-breaking processes. focusing on a convenient special case corresponding to a parafac factorization, we provide basic theory justifying the flexibility of the proposed prior and resulting asymptotic properties. focusing on itf mixtures of product kernels, we develop a new gibbs sampling algorithm for routine implementation relying on slice sampling. the methods are compared with alternative joint mixture models based on dirichlet processes and related approaches through simulations and real data applications.","","2013-03-02","","['anjishnu banerjee', 'jared murray', 'david b. dunson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"585",1303.0742,"multivariate temporal dictionary learning for eeg","cs.lg q-bio.nc stat.ml","this article addresses the issue of representing electroencephalographic (eeg) signals in an efficient way. while classical approaches use a fixed gabor dictionary to analyze eeg signals, this article proposes a data-driven method to obtain an adapted dictionary. to reach an efficient dictionary learning, appropriate spatial and temporal modeling is required. inter-channels links are taken into account in the spatial multivariate model, and shift-invariance is used for the temporal model. multivariate learned kernels are informative (a few atoms code plentiful energy) and interpretable (the atoms can have a physiological meaning). using real eeg data, the proposed method is shown to outperform the classical multichannel matching pursuit used with a gabor dictionary, as measured by the representative power of the learned dictionary and its spatial flexibility. moreover, dictionary learning can capture interpretable patterns: this ability is illustrated on real data, learning a p300 evoked potential.","","2013-03-04","","['quentin barthélemy', 'cédric gouy-pailler', 'yoann isaac', 'antoine souloumiac', 'anthony larue', 'jérôme i. mars']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"586",1303.1312,"a fast iterative bayesian inference algorithm for sparse channel   estimation","stat.ml cs.it math.it","in this paper, we present a bayesian channel estimation algorithm for multicarrier receivers based on pilot symbol observations. the inherent sparse nature of wireless multipath channels is exploited by modeling the prior distribution of multipath components' gains with a hierarchical representation of the bessel k probability density function; a highly efficient, fast iterative bayesian inference method is then applied to the proposed model. the resulting estimator outperforms other state-of-the-art bayesian and non-bayesian estimators, either by yielding lower mean squared estimation error or by attaining the same accuracy with improved convergence rate, as shown in our numerical evaluation.","","2013-03-06","","['niels lovmand pedersen', 'carles navarro manchón bernard henri fleury']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"587",1303.2108,"classification of segments in polsar imagery by minimum stochastic   distances between wishart distributions","cs.cv stat.ap","a new classifier for polarimetric sar (polsar) images is proposed and assessed in this paper. its input consists of segments, and each one is assigned the class which minimizes a stochastic distance. assuming the complex wishart model, several stochastic distances are obtained from the h-phi family of divergences, and they are employed to derive hypothesis test statistics that are also used in the classification process. this article also presents, as a novelty, analytic expressions for the test statistics based on the following stochastic distances between complex wishart models: kullback-leibler, bhattacharyya, hellinger, r\'enyi, and chi-square; also, the test statistic based on the bhattacharyya distance between multivariate gaussian distributions is presented. the classifier performance is evaluated using simulated and real polsar data. the simulated data are based on the complex wishart model, aiming at the analysis of the proposal well controlled data. the real data refer to the complex l-band image, acquired during the 1994 sir-c mission. the results of the proposed classifier are compared with those obtained by a wishart per-pixel/contextual classifier, and we show the better performance of the region-based classification. the influence of the statistical modeling is assessed by comparing the results using the bhattacharyya distance between multivariate gaussian distributions for amplitude data. the results with simulated data indicate that the proposed classification method has a very good performance when the data follow the wishart model. the proposed classifier also performs better than the per-pixel/contextual classifier and the bhattacharyya gaussian distance using sir-c polsar data.","10.1109/jstars.2013.2248132","2013-03-11","","['wagner barreto da silva', 'corina da costa freitas', ""sidnei joão siqueira sant'anna"", 'alejandro c. frery']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"588",1303.2797,"combining dynamic predictions from joint models for longitudinal and   time-to-event data using bayesian model averaging","stat.ap stat.co stat.me","the joint modeling of longitudinal and time-to-event data is an active area of statistics research that has received a lot of attention in the recent years. more recently, a new and attractive application of this type of models has been to obtain individualized predictions of survival probabilities and/or of future longitudinal responses. the advantageous feature of these predictions is that they are dynamically updated as extra longitudinal responses are collected for the subjects of interest, providing real time risk assessment using all recorded information. the aim of this paper is two-fold. first, to highlight the importance of modeling the association structure between the longitudinal and event time responses that can greatly influence the derived predictions, and second, to illustrate how we can improve the accuracy of the derived predictions by suitably combining joint models with different association structures. the second goal is achieved using bayesian model averaging, which, in this setting, has the very intriguing feature that the model weights are not fixed but they are rather subject- and time-dependent, implying that at different follow-up times predictions for the same subject may be based on different models.","","2013-03-12","","['dimitris rizopoulos', 'laura a. hatfield', 'bradley p. carlin', 'johanna j. m. takkenberg']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"589",1303.2912,"integrated pre-processing for bayesian nonlinear system identification   with gaussian processes","cs.ai cs.ro cs.sy stat.ml","we introduce gp-fnarx: a new model for nonlinear system identification based on a nonlinear autoregressive exogenous model (narx) with filtered regressors (f) where the nonlinear regression problem is tackled using sparse gaussian processes (gp). we integrate data pre-processing with system identification into a fully automated procedure that goes from raw data to an identified model. both pre-processing parameters and gp hyper-parameters are tuned by maximizing the marginal likelihood of the probabilistic model. we obtain a bayesian model of the system's dynamics which is able to report its uncertainty in regions where the data is scarce. the automated approach, the modeling of uncertainty and its relatively low computational cost make of gp-fnarx a good candidate for applications in robotics and adaptive control.","","2013-03-12","2013-09-17","['roger frigola', 'carl edward rasmussen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"590",1303.2915,"modeling us housing prices by spatial dynamic structural equation models","stat.ap","this article proposes a spatial dynamic structural equation model for the analysis of housing prices at the state level in the usa. the study contributes to the existing literature by extending the use of dynamic factor models to the econometric analysis of multivariate lattice data. one of the main advantages of our model formulation is that by modeling the spatial variation via spatially structured factor loadings, we entertain the possibility of identifying similarity ""regions"" that share common time series components. the factor loadings are modeled as conditionally independent multivariate gaussian markov random fields, while the common components are modeled by latent dynamic factors. the general model is proposed in a state-space formulation where both stationary and nonstationary autoregressive distributed-lag processes for the latent factors are considered. for the latent factors which exhibit a common trend, and hence are cointegrated, an error correction specification of the (vector) autoregressive distributed-lag process is proposed. full probabilistic inference for the model parameters is facilitated by adapting standard markov chain monte carlo (mcmc) algorithms for dynamic linear models to our model formulation. the fit of the model is discussed for a data set of 48 states for which we model the relationship between housing prices and the macroeconomy, using state level unemployment and per capita personal income.","10.1214/12-aoas613","2013-03-12","2013-12-20","['pasquale valentini', 'luigi ippoliti', 'lara fontanella']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"591",1303.3128,"estimation stability with cross validation (escv)","stat.me stat.ml","cross-validation (cv) is often used to select the regularization parameter in high dimensional problems. however, when applied to the sparse modeling method lasso, cv leads to models that are unstable in high-dimensions, and consequently not suited for reliable interpretation. in this paper, we propose a model-free criterion escv based on a new estimation stability (es) metric and cv. our proposed escv finds a locally es-optimal model smaller than the cv choice so that the it fits the data and also enjoys estimation stability property. we demonstrate that escv is an effective alternative to cv at a similar easily parallelizable computational cost. in particular, we compare the two approaches with respect to several performance measures when applied to the lasso on both simulated and real data sets. for dependent predictors common in practice, our main finding is that, escv cuts down false positive rates often by a large margin, while sacrificing little of true positive rates. escv usually outperforms cv in terms of parameter estimation while giving similar performance as cv in terms of prediction. for the two real data sets from neuroscience and cell biology, the models found by escv are less than half of the model sizes by cv. judged based on subject knowledge, they are more plausible than those by cv as well. we also discuss some regularization parameter alignment issues that come up in both approaches.","","2013-03-13","2015-10-27","['chinghway lim', 'bin yu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"592",1303.3216,"jointly interventional and observational data: estimation of   interventional markov equivalence classes of directed acyclic graphs","math.st stat.me stat.th","in many applications we have both observational and (randomized) interventional data. we propose a gaussian likelihood framework for joint modeling of such different data-types, based on global parameters consisting of a directed acyclic graph (dag) and correponding edge weights and error variances. thanks to the global nature of the parameters, maximum likelihood estimation is reasonable with only one or few data points per intervention. we prove consistency of the bic criterion for estimating the interventional markov equivalence class of dags which is smaller than the observational analogue due to increased partial identifiability from interventional data. such an improvement in identifiability has immediate implications for tighter bounds for inferring causal effects. besides methodology and theoretical derivations, we present empirical results from real and simulated data.","10.1111/rssb.12071","2013-03-13","","['alain hauser', 'peter bühlmann']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"593",1303.3594,"a multiple filter test for the detection of rate changes in renewal   processes with varying variance","stat.ap math.st stat.th","nonstationarity of the event rate is a persistent problem in modeling time series of events, such as neuronal spike trains. motivated by a variety of patterns in neurophysiological spike train recordings, we define a general class of renewal processes. this class is used to test the null hypothesis of stationary rate versus a wide alternative of renewal processes with finitely many rate changes (change points). our test extends ideas from the filtered derivative approach by using multiple moving windows simultaneously. to adjust the rejection threshold of the test, we use a gaussian process, which emerges as the limit of the filtered derivative process. we also develop a multiple filter algorithm, which can be used when the null hypothesis is rejected in order to estimate the number and location of change points. we analyze the benefits of multiple filtering and its increased detection probability as compared to a single window approach. application to spike trains recorded from dopamine midbrain neurons in anesthetized mice illustrates the relevance of the proposed techniques as preprocessing steps for methods that assume rate stationarity. in over 70% of all analyzed spike trains classified as rate nonstationary, different change points were detected by different window sizes.","10.1214/14-aoas782","2013-03-14","2015-01-16","['michael messer', 'marietta kirchner', 'julia schiemann', 'jochen roeper', 'ralph neininger', 'gaby schneider']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE
"594",1303.3664,"topic discovery through data dependent and random projections","stat.ml cs.lg","we present algorithms for topic modeling based on the geometry of cross-document word-frequency patterns. this perspective gains significance under the so called separability condition. this is a condition on existence of novel-words that are unique to each topic. we present a suite of highly efficient algorithms based on data-dependent and random projections of word-frequency patterns to identify novel words and associated topics. we will also discuss the statistical guarantees of the data-dependent projections method based on two mild assumptions on the prior density of topic document matrix. our key insight here is that the maximum and minimum values of cross-document frequency patterns projected along any direction are associated with novel words. while our sample complexity bounds for topic recovery are similar to the state-of-art, the computational complexity of our random projection scheme scales linearly with the number of documents and the number of words per document. we present several experiments on synthetic and real-world datasets to demonstrate qualitative and quantitative merits of our scheme.","","2013-03-14","2013-03-18","['weicong ding', 'mohammad h. rohban', 'prakash ishwar', 'venkatesh saligrama']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"595",1303.4385,"modeling a sensor to improve its efficacy","physics.ins-det astro-ph.im stat.ml","robots rely on sensors to provide them with information about their surroundings. however, high-quality sensors can be extremely expensive and cost-prohibitive. thus many robotic systems must make due with lower-quality sensors. here we demonstrate via a case study how modeling a sensor can improve its efficacy when employed within a bayesian inferential framework. as a test bed we employ a robotic arm that is designed to autonomously take its own measurements using an inexpensive lego light sensor to estimate the position and radius of a white circle on a black field. the light sensor integrates the light arriving from a spatially distributed region within its field of view weighted by its spatial sensitivity function (ssf). we demonstrate that by incorporating an accurate model of the light sensor ssf into the likelihood function of a bayesian inference engine, an autonomous system can make improved inferences about its surroundings. the method presented here is data-based, fairly general, and made with plug-and play in mind so that it could be implemented in similar problems.","","2013-03-18","","['n. k. malakar', 'd. gladkov', 'k. h. knuth']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"596",1303.4431,"generalized thompson sampling for sequential decision-making and causal   inference","cs.ai stat.ml","recently, it has been shown how sampling actions from the predictive distribution over the optimal action-sometimes called thompson sampling-can be applied to solve sequential adaptive control problems, when the optimal policy is known for each possible environment. the predictive distribution can then be constructed by a bayesian superposition of the optimal policies weighted by their posterior probability that is updated by bayesian inference and causal calculus. here we discuss three important features of this approach. first, we discuss in how far such thompson sampling can be regarded as a natural consequence of the bayesian modeling of policy uncertainty. second, we show how thompson sampling can be used to study interactions between multiple adaptive agents, thus, opening up an avenue of game-theoretic analysis. third, we show how thompson sampling can be applied to infer causal relationships when interacting with an environment in a sequential fashion. in summary, our results suggest that thompson sampling might not merely be a useful heuristic, but a principled method to address problems of adaptive sequential decision-making and causal inference.","10.1186/2194-3206-2-2","2013-03-18","","['pedro a. ortega', 'daniel a. braun']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"597",1303.4871,"estimation of the lead-lag parameter from non-synchronous data","math.st stat.th","we propose a simple continuous time model for modeling the lead-lag effect between two financial assets. a two-dimensional process $(x_t,y_t)$ reproduces a lead-lag effect if, for some time shift $\vartheta\in \mathbb{r}$, the process $(x_t,y_{t+\vartheta})$ is a semi-martingale with respect to a certain filtration. the value of the time shift $\vartheta$ is the lead-lag parameter. depending on the underlying filtration, the standard no-arbitrage case is obtained for $\vartheta=0$. we study the problem of estimating the unknown parameter $\vartheta\in \mathbb{r}$, given randomly sampled non-synchronous data from $(x_t)$ and $(y_t)$. by applying a certain contrast optimization based on a modified version of the hayashi-yoshida covariation estimator, we obtain a consistent estimator of the lead-lag parameter, together with an explicit rate of convergence governed by the sparsity of the sampling design.","10.3150/11-bej407","2013-03-20","","['m. hoffmann', 'm. rosenbaum', 'n. yoshida']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"598",1303.4911,"weighted estimation of the dependence function for an extreme-value   distribution","math.st stat.th","bivariate extreme-value distributions have been used in modeling extremes in environmental sciences and risk management. an important issue is estimating the dependence function, such as the pickands dependence function. some estimators for the pickands dependence function have been studied by assuming that the marginals are known. recently, genest and segers [ann. statist. 37 (2009) 2990-3022] derived the asymptotic distributions of those proposed estimators with marginal distributions replaced by the empirical distributions. in this article, we propose a class of weighted estimators including those of genest and segers (2009) as special cases. we propose a jackknife empirical likelihood method for constructing confidence intervals for the pickands dependence function, which avoids estimating the complicated asymptotic variance. a simulation study demonstrates the effectiveness of our proposed jackknife empirical likelihood method.","10.3150/11-bej409","2013-03-20","","['liang peng', 'linyi qian', 'jingping yang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"599",1303.5588,"robust and trend following student's t kalman smoothers","math.oc math.na stat.ap stat.ml","we present a kalman smoothing framework based on modeling errors using the heavy tailed student's t distribution, along with algorithms, convergence theory, open-source general implementation, and several important applications. the computational effort per iteration grows linearly with the length of the time series, and all smoothers allow nonlinear process and measurement models.   robust smoothers form an important subclass of smoothers within this framework. these smoothers work in situations where measurements are highly contaminated by noise or include data unexplained by the forward model. highly robust smoothers are developed by modeling measurement errors using the student's t distribution, and outperform the recently proposed l1-laplace smoother in extreme situations with data containing 20% or more outliers.   a second special application we consider in detail allows tracking sudden changes in the state. it is developed by modeling process noise using the student's t distribution, and the resulting smoother can track sudden changes in the state.   these features can be used separately or in tandem, and we present a general smoother algorithm and open source implementation, together with convergence analysis that covers a wide range of smoothers. a key ingredient of our approach is a technique to deal with the non-convexity of the student's t loss function. numerical results for linear and nonlinear models illustrate the performance of the new smoothers for robust and tracking applications, as well as for mixed problems that have both types of features.","","2013-03-22","","['aleksandr y. aravkin', 'james v. burke', 'gianluigi pillonetto']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"600",1303.5913,"a diffusion process on riemannian manifold for visual tracking","cs.cv cs.lg cs.ro stat.ml","robust visual tracking for long video sequences is a research area that has many important applications. the main challenges include how the target image can be modeled and how this model can be updated. in this paper, we model the target using a covariance descriptor, as this descriptor is robust to problems such as pixel-pixel misalignment, pose and illumination changes, that commonly occur in visual tracking. we model the changes in the template using a generative process. we introduce a new dynamical model for the template update using a random walk on the riemannian manifold where the covariance descriptors lie in. this is done using log-transformed space of the manifold to free the constraints imposed inherently by positive semidefinite matrices. modeling template variations and poses kinetics together in the state space enables us to jointly quantify the uncertainties relating to the kinematic states and the template in a principled way. finally, the sequential inference of the posterior distribution of the kinematic states and the template is done using a particle filter. our results shows that this principled approach can be robust to changes in illumination, poses and spatial affine transformation. in the experiments, our method outperformed the current state-of-the-art algorithm - the incremental principal component analysis method, particularly when a target underwent fast poses changes and also maintained a comparable performance in stable target tracking cases.","","2013-03-24","","['marcus chen', 'cham tat jen', 'pang sze kim', 'alvina goh']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"601",1303.6349,"measures of serial extremal dependence and their estimation","math.st stat.th","the goal of this paper is two-fold: 1. we review classical and recent measures of serial extremal dependence in a strictly stationary time series as well as their estimation. 2. we discuss recent concepts of heavy-tailed time series, including regular variation and max-stable processes. serial extremal dependence is typically characterized by clusters of exceedances of high thresholds in the series. we start by discussing the notion of extremal index of a univariate sequence, i.e. the reciprocal of the expected cluster size, which has attracted major attention in the extremal value literature. then we continue by introducing the extremogram which is an asymptotic auto-correlation function for sequences of extremal events in a time series. in this context, we discuss regular variation of a time series. this notion has been useful for describing serial extremal dependence and heavy tails in a strictly stationary sequence. we briefly discuss the tail process coined by basrak and segers to describe the dependence structure of regularly varying sequences in a probabilistic way. max-stable processes with frechet marginals are an important class of reg- ularly varying sequences. recently, this class has attracted attention for modeling and statistical purposes. we apply the extremogram to max-stable processes. finally, we discuss estimation of the extremogram both in the time and frequency domains.","","2013-03-25","","['richard a. davis', 'thomas mikosch', 'yuwei zhao']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"602",1303.6668,"spatial fay-herriot models for small area estimation with functional   covariates","stat.me","the fay-herriot (fh) model is widely used in small area estimation and uses auxiliary information to reduce estimation variance at undersampled locations. we extend the type of covariate information used in the fh model to include functional covariates, such as social-media search loads or remote-sensing images (e.g., in crop-yield surveys). the inclusion of these functional covariates is facilitated through a two-stage dimension-reduction approach that includes a karhunen-lo\`{e}ve expansion followed by stochastic search variable selection. additionally, the importance of modeling spatial autocorrelation has recently been recognized in the fh model; our model utilizes the intrinsic conditional autoregressive class of spatial models in addition to functional covariates. we demonstrate the effectiveness of our approach through simulation and analysis of data from the american community survey. we use google trends searches over time as functional covariates to analyze relative changes in rates of percent household spanish-speaking in the eastern half of the united states.","","2013-03-26","2014-05-09","['aaron t. porter', 'scott h. holan', 'christopher k. wikle', 'noel cressie']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"603",1303.669,"parameter estimation for fractional birth and fractional death processes","math.st math.pr stat.th","the fractional birth and the fractional death processes are more desirable in practice than their classical counterparts as they naturally provide greater flexibility in modeling growing and decreasing systems. in this paper, we propose formal parameter estimation procedures for the fractional yule, the fractional linear death, and the fractional sublinear death processes. the methods use all available data possible, are computationally simple and asymptotically unbiased. the procedures exploited the natural structure of the random inter-birth and inter-death times that are known to be independent but are not identically distributed. we also showed how these methods can be applied to certain models with more general birth and death rates. the computational tests showed favorable results for our proposed methods even with relatively small sample sizes. the proposed methods are also illustrated using the branching times of the plethodontid salamanders data of \cite{hal79}.","10.1007/s11222-012-9365-1","2013-03-26","","['dexter o. cahoy', 'federico polito']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"604",1303.67,"two general methods for population pharmacokinetic modeling:   non-parametric adaptive grid and non-parametric bayesian","q-bio.qm q-bio.gn stat.ap stat.me","population pharmacokinetic (pk) modeling methods can be statistically classified as either parametric or nonparametric (np). each classification can be divided into maximum likelihood (ml) or bayesian (b) approaches. in this paper we discuss the nonparametric case using both maximum likelihood and bayesian approaches. we present two nonparametric methods for estimating the unknown joint population distribution of model parameter values in a pharmacokinetic/pharmacodynamic (pk/pd) dataset. the first method is the np adaptive grid (npag). the second is the np bayesian (npb) algorithm with a stick-breaking process to construct a dirichlet prior. our objective is to compare the performance of these two methods using a simulated pk/pd dataset. our results showed excellent performance of npag and npb in a realistically simulated pk study. this simulation allowed us to have benchmarks in the form of the true population parameters to compare with the estimates produced by the two methods, while incorporating challenges like unbalanced sample times and sample numbers as well as the ability to include the covariate of patient weight. we conclude that both npml and npb can be used in realistic pk/pd population analysis problems. the advantages of one versus the other are discussed in the paper. npag and npb are implemented in r and freely available for download within the pmetrics package from www.lapk.org.","10.1007/s10928-013-9302-8","2013-03-26","","['tatiana tatarinova', 'michael neely', 'jay bartroff', 'michael van guilder', 'walter yamada', 'david bayard', 'roger jelliffe', 'robert leary', 'alyona chubatiuk', 'alan schumitzky']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"605",1303.675,"sequential testing over multiple stages and performance analysis of data   fusion","stat.ml cs.lg","we describe a methodology for modeling the performance of decision-level data fusion between different sensor configurations, implemented as part of the jieddo analytic decision engine (jade). we first discuss a bayesian network formulation of classical probabilistic data fusion, which allows elementary fusion structures to be stacked and analyzed efficiently. we then present an extension of the wald sequential test for combining the outputs of the bayesian network over time. we discuss an algorithm to compute its performance statistics and illustrate the approach on some examples. this variant of the sequential test involves multiple, distinct stages, where the evidence accumulated from each stage is carried over into the next one, and is motivated by a need to keep certain sensors in the network inactive unless triggered by other sensors.","10.1117/12.2017754","2013-03-27","","['gaurav thakur']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"606",1303.705,"quantile models with endogeneity","stat.ap econ.em","in this article, we review quantile models with endogeneity. we focus on models that achieve identification through the use of instrumental variables and discuss conditions under which partial and point identification are obtained. we discuss key conditions, which include monotonicity and full-rank-type conditions, in detail. in providing this review, we update the identification results of chernozhukov and hansen (2005, econometrica). we illustrate the modeling assumptions through economically motivated examples. we also briefly review the literature on estimation and inference.   key words: identification, treatment effects, structural models, instrumental variables","10.1146/annurev-economics-080511-110952","2013-03-28","","['victor chernozhukov', 'christian hansen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"607",1303.7264,"scalable text and link analysis with mixed-topic link models","cs.lg cs.ir cs.si physics.data-an stat.ml","many data sets contain rich information about objects, as well as pairwise relations between them. for instance, in networks of websites, scientific papers, and other documents, each node has content consisting of a collection of words, as well as hyperlinks or citations to other nodes. in order to perform inference on such data sets, and make predictions and recommendations, it is useful to have models that are able to capture the processes which generate the text at each node and the links between them. in this paper, we combine classic ideas in topic modeling with a variant of the mixed-membership block model recently developed in the statistical physics community. the resulting model has the advantage that its parameters, including the mixture of topics of each document and the resulting overlapping communities, can be inferred with a simple and scalable expectation-maximization algorithm. we test our model on three data sets, performing unsupervised topic classification and link prediction. for both tasks, our model outperforms several existing state-of-the-art methods, achieving higher accuracy with significantly less computation, analyzing a data set with 1.3 million words and 44 thousand links in a few minutes.","10.1145/2487575.2487693","2013-03-28","","['yaojia zhu', 'xiaoran yan', 'lise getoor', 'cristopher moore']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"608",1303.7286,"on the symmetrical kullback-leibler jeffreys centroids","cs.it cs.lg math.it stat.ml","due to the success of the bag-of-word modeling paradigm, clustering histograms has become an important ingredient of modern information processing. clustering histograms can be performed using the celebrated $k$-means centroid-based algorithm. from the viewpoint of applications, it is usually required to deal with symmetric distances. in this letter, we consider the jeffreys divergence that symmetrizes the kullback-leibler divergence, and investigate the computation of jeffreys centroids. we first prove that the jeffreys centroid can be expressed analytically using the lambert $w$ function for positive histograms. we then show how to obtain a fast guaranteed approximation when dealing with frequency histograms. finally, we conclude with some remarks on the $k$-means histogram clustering.","10.1109/lsp.2013.2260538","2013-03-28","2014-01-22","['frank nielsen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"609",1304.015,"fitting bivariate mixed-type data via the generalized linear exponential   cluster-weighted model","stat.me stat.co","the cluster-weighted model (cwm) is a mixture model with random covariates which allows for flexible clustering and density estimation of a random vector composed by a response variable and by a set of covariates. in this class of models, the generalized linear exponential cwm is here introduced especially for modeling bivariate data of mixed-type. its natural counterpart, in the family of latent class models, is also defined. maximum likelihood parameter estimates are derived using the em algorithm and model selection is carried out using the bayesian information criterion (bic). artificial and real data are finally considered to exemplify and appreciate the proposed model.","","2013-03-30","2013-08-05","['salvatore ingrassia', 'antonio punzo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"610",1304.0503,"nonparametric likelihood based estimation of linear filters for point   processes","stat.co","we consider models for multivariate point processes where the intensity is given nonparametrically in terms of functions in a reproducing kernel hilbert space. the likelihood function involves a time integral and is consequently not given in terms of a finite number of kernel evaluations. the main result is a representation of the gradient of the log-likelihood, which we use to derive computable approximations of the log-likelihood and the gradient by time discretization. these approximations are then used to minimize the approximate penalized log-likelihood. for time and memory efficiency the implementation relies crucially on the use of sparse matrices. as an illustration we consider neuron network modeling, and we use this example to investigate how the computational costs of the approximations depend on the resolution of the time discretization. the implementation is available in the r package ppstat.","","2013-04-01","2014-02-12","['niels richard hansen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"611",1304.0542,"multichannel electrophysiological spike sorting via joint dictionary   learning & mixture modeling","q-bio.qm stat.ap","we propose a construction for joint feature learning and clustering of multichannel extracellular electrophysiological data across multiple recording periods for action potential detection and discrimination (""spike sorting""). our construction improves over the previous state-of-the art principally in four ways. first, via sharing information across channels, we can better distinguish between single-unit spikes and artifacts. second, our proposed ""focused mixture model"" (fmm) elegantly deals with units appearing, disappearing, or reappearing over multiple recording days, an important consideration for any chronic experiment. third, by jointly learning features and clusters, we improve performance over previous attempts that proceeded via a two-stage (""frequentist"") learning process. fourth, by directly modeling spike rate, we improve detection of sparsely spiking neurons. moreover, our bayesian construction seamlessly handles missing data. we present state-of-the-art performance without requiring manually tuning of many hyper-parameters on both a public dataset with partial ground truth and a new experimental dataset.","","2013-04-02","2013-08-04","['david e. carlson', 'joshua t. vogelstein', 'qisong wu', 'wenzhao lian', 'mingyuan zhou', 'colin r. stoetzner', 'daryl kipke', 'douglas weber', 'david b. dunson', 'lawrence carin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"612",1304.0861,"shape invariant model approach for functional data analysis in   uncertainty and sensitivity studies","stat.ap","dynamic simulators model systems evolving over time. often, it operates iteratively over fixed number of time-steps. the output of such simulator can be considered as time series or discrete functional outputs. metamodeling is an e ective method to approximate demanding computer codes. numerous metamodeling techniques are developed for simulators with a single output. standard approach to model a dynamic simulator uses the same method also for multi-time series outputs: the metamodel is evaluated independently at every time step. this can be computationally demanding in case of large number of time steps. in some cases, simulator outputs for di erent combinations of input parameters have quite similar behaviour. in this paper, we propose an application of shape invariant model approach to model dynamic simulators. this model assumes a common pattern shape curve and curve-specific di erences in amplitude and timing are modelled with linear transformations. we provide an e cient algorithm of transformation parameters estimation and subsequent prediction algorithm. the method was tested with a co2 storage reservoir case using an industrial commercial simulator and compared with a standard single step approach. the method provides satisfactory predictivity and it does not depend on the number of involved time steps.","","2013-04-03","","['ekaterina sergienko', 'fabrice gamboa', 'daniel busby']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"613",1304.1598,"a new distribution-random limit normal distribution","math.st stat.th","this paper introduces a new distribution to improve tail risk modeling. based on the classical normal distribution, we define a new distribution by a series of heat equations. then, we use market data to verify our model.","","2013-04-04","","['xiaolin gong', 'shuzhen yang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"614",1304.1834,"detecting the structure of haplotypes, local ancestry and excessive   local european ancestry in mexicans","q-bio.qm q-bio.pe stat.ap","we present a two-layer hidden markov model to detect structure of haplotypes for unrelated individuals. this allows modeling two scales of linkage disequilibrium (one within a group of haplotypes and one between groups), thereby taking advantage of rich haplotype information to infer local ancestry for admixed individuals. our method outperforms competing state-of-art methods, particularly for regions of small ancestral track lengths. applying our method to mexican samples in hapmap3, we found five coding regions, ranging from $0.3 -1.3$ megabase (mb) in lengths, that exhibit excessive european ancestry (average dosage > 1.6). a particular interesting region of 1.1mb (with average dosage 1.95) locates on chromosome 2p23 that harbors two genes, pxdn and myt1l, both of which are associated with autism and schizophrenia. in light of the low prevalence of autism in hispanics, this region warrants special attention. we confirmed our findings using mexican samples from the 1000 genomes project. a software package implementing methods described in the paper is freely available at \url{http://bcm.edu/cnrc/mcmcmc}","","2013-04-05","","['yongtao guan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"615",1304.1875,"nonlinear unmixing of hyperspectral images: models and algorithms","physics.data-an stat.ap stat.me stat.ml","when considering the problem of unmixing hyperspectral images, most of the literature in the geoscience and image processing areas relies on the widely used linear mixing model (lmm). however, the lmm may be not valid and other nonlinear models need to be considered, for instance, when there are multi-scattering effects or intimate interactions. consequently, over the last few years, several significant contributions have been proposed to overcome the limitations inherent in the lmm. in this paper, we present an overview of recent advances in nonlinear unmixing modeling.","10.1109/msp.2013.2279274","2013-04-06","2013-07-18","['nicolas dobigeon', 'jean-yves tourneret', 'cédric richard', 'josé c. m. bermudez', 'stephen mclaughlin', 'alfred o. hero']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE
"616",1304.2024,"a general framework for interacting bayes-optimally with self-interested   agents using arbitrary parametric model and model prior","cs.lg cs.ai cs.ma stat.ml","recent advances in bayesian reinforcement learning (brl) have shown that bayes-optimality is theoretically achievable by modeling the environment's latent dynamics using flat-dirichlet-multinomial (fdm) prior. in self-interested multi-agent environments, the transition dynamics are mainly controlled by the other agent's stochastic behavior for which fdm's independence and modeling assumptions do not hold. as a result, fdm does not allow the other agent's behavior to be generalized across different states nor specified using prior domain knowledge. to overcome these practical limitations of fdm, we propose a generalization of brl to integrate the general class of parametric models and model priors, thus allowing practitioners' domain knowledge to be exploited to produce a fine-grained and compact representation of the other agent's behavior. empirical evaluation shows that our approach outperforms existing multi-agent reinforcement learning algorithms.","","2013-04-07","2014-03-16","['trong nghia hoang', 'kian hsiang low']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"617",1304.2302,"clustercluster: parallel markov chain monte carlo for dirichlet process   mixtures","stat.ml cs.dc cs.lg","the dirichlet process (dp) is a fundamental mathematical tool for bayesian nonparametric modeling, and is widely used in tasks such as density estimation, natural language processing, and time series modeling. although mcmc inference methods for the dp often provide a gold standard in terms asymptotic accuracy, they can be computationally expensive and are not obviously parallelizable. we propose a reparameterization of the dirichlet process that induces conditional independencies between the atoms that form the random measure. this conditional independence enables many of the markov chain transition operators for dp inference to be simulated in parallel across multiple cores. applied to mixture modeling, our approach enables the dirichlet process to simultaneously learn clusters that describe the data and superclusters that define the granularity of parallelization. unlike previous approaches, our technique does not require alteration of the model and leaves the true posterior distribution invariant. it also naturally lends itself to a distributed software implementation in terms of map-reduce, which we test in cluster configurations of over 50 machines and 100 cores. we present experiments exploring the parallel efficiency and convergence properties of our approach on both synthetic and real-world data, including runs on 1mm data vectors in 256 dimensions.","","2013-04-08","","['dan lovell', 'jonathan malmaud', 'ryan p. adams', 'vikash k. mansinghka']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"618",1304.3608,"individual differences in structural equation model parameters","stat.me","individuals may differ in their parameter values. this article discusses a three-step method of studying such differences by calculating and then modeling ""individual parameter contributions"", making the study of heterogeneity in arbitrary structural equation model parameters as technically challenging as performing linear regression. the proposed approach allows for a contribution to the study of differential error variances in survey methodology that would have been difficult to make otherwise.","","2013-04-12","","['daniel leonard oberski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"619",1304.3637,"integrating influenza antigenic dynamics with molecular evolution","q-bio.pe q-bio.qm stat.ap","influenza viruses undergo continual antigenic evolution allowing mutant viruses to evade host immunity acquired to previous virus strains. antigenic phenotype is often assessed through pairwise measurement of cross-reactivity between influenza strains using the hemagglutination inhibition (hi) assay. here, we extend previous approaches to antigenic cartography, and simultaneously characterize antigenic and genetic evolution by modeling the diffusion of antigenic phenotype over a shared virus phylogeny. using hi data from influenza lineages a/h3n2, a/h1n1, b/victoria and b/yamagata, we determine patterns of antigenic drift across viral lineages, showing that a/h3n2 evolves faster and in a more punctuated fashion than other influenza lineages. we also show that year-to-year antigenic drift appears to drive incidence patterns within each influenza lineage. this work makes possible substantial future advances in investigating the dynamics of influenza and other antigenically-variable pathogens by providing a model that intimately combines molecular and antigenic evolution.","","2013-04-12","2013-12-19","['trevor bedford', 'marc a. suchard', 'philippe lemey', 'gytis dudas', 'victoria gregory', 'alan j. hay', 'john w. mccauley', 'colin a. russell', 'derek j. smith', 'andrew rambaut']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"620",1304.3673,"bayesian analysis of matrix data with rstiefel","stat.co stat.me","we illustrate the use of the r-package ""rstiefel"" for matrix-variate data analysis in the context of two examples. the first example considers estimation of a reduced-rank mean matrix in the presence of normally distributed noise. the second example considers the modeling of a social network of friendships among teenagers. bayesian estimation for these models requires the ability to simulate from the matrix-variate von mises-fisher distributions and the matrix-variate bingham distributions on the stiefel manifold.","","2013-04-12","","['peter d. hoff']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"621",1304.4056,"variable selection for general index models via sliced inverse   regression","stat.me","variable selection, also known as feature selection in machine learning, plays an important role in modeling high dimensional data and is key to data-driven scientific discoveries. we consider here the problem of detecting influential variables under the general index model, in which the response is dependent of predictors through an unknown function of one or more linear combinations of them. instead of building a predictive model of the response given combinations of predictors, we model the conditional distribution of predictors given the response. this inverse modeling perspective motivates us to propose a stepwise procedure based on likelihood-ratio tests, which is effective and computationally efficient in identifying important variables without specifying a parametric relationship between predictors and the response. for example, the proposed procedure is able to detect variables with pairwise, three-way or even higher-order interactions among $p$ predictors with a computational time of $o(p)$ instead of $o(p^k)$ (with $k$ being the highest order of interactions). its excellent empirical performance in comparison with existing methods is demonstrated through simulation studies as well as real data examples. consistency of the variable selection procedure when both the number of predictors and the sample size go to infinity is established.","10.1214/14-aos1233","2013-04-15","2014-09-23","['bo jiang', 'jun s. liu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"622",1304.4445,"modeling temporal gradients in regionally aggregated california asthma   hospitalization data","stat.ap","advances in geographical information systems (gis) have led to the enormous recent burgeoning of spatial-temporal databases and associated statistical modeling. here we depart from the rather rich literature in space-time modeling by considering the setting where space is discrete (e.g., aggregated data over regions), but time is continuous. our major objective in this application is to carry out inference on gradients of a temporal process in our data set of monthly county level asthma hospitalization rates in the state of california, while at the same time accounting for spatial similarities of the temporal process across neighboring counties. use of continuous time models here allows inference at a finer resolution than at which the data are sampled. rather than use parametric forms to model time, we opt for a more flexible stochastic process embedded within a dynamic markov random field framework. through the matrix-valued covariance function we can ensure that the temporal process realizations are mean square differentiable, and may thus carry out inference on temporal gradients in a posterior predictive fashion. we use this approach to evaluate temporal gradients where we are concerned with temporal changes in the residual and fitted rate curves after accounting for seasonality, spatiotemporal ozone levels and several spatially-resolved important sociodemographic covariates.","10.1214/12-aoas600","2013-04-16","","['harrison quick', 'sudipto banerjee', 'bradley p. carlin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"623",1304.4929,"a new method to obtain risk neutral probability, without stochastic   calculus and price modeling, confirms the universal validity of   black-scholes-merton formula and volatility's role","q-fin.pr stat.ap","a new method is proposed to obtain the risk neutral probability of share prices without stochastic calculus and price modeling, via an embedding of the price return modeling problem in le cam's statistical experiments framework. strategies-probabilities $p_{t_0,n}$ and $p_{t,n}$ are thus determined and used, respectively,for the trader selling the share's european call option at time $t_0$ and for the buyer who may exercise it in the future, at $t; \ n$ increases with the number of share's transactions in $[t_0,t].$ when the transaction times are dense in $[t_0,t]$ it is shown, with mild conditions, that under each of these probabilities $\log \frac{s_t}{s_{t_0}}$ has infinitely divisible distribution and in particular normal distribution for ""calm"" share; $s_t$ is the share's price at time $t.$ the price of the share's call is the limit of the expected values of the call's payoff under the translated $p_{t_0,n}.$ it coincides for ""calm"" share prices with the black-scholes-merton formula with variance not necessarily proportional to $(t-t_0),$ thus confirming formula's universal validity without model assumptions. additional results clarify volatility's role in the transaction and the behaviors of the trader and the buyer. traders may use the pricing formulae after estimation of the unknown parameters.","","2013-04-17","2014-11-18","['yannis g. yatracos']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"624",1304.5156,"option pricing, bayes risks and applications","q-fin.pr stat.ap","a statistical decision problem is hidden in the core of option pricing. a simple form for the price c of a european call option is obtained via the minimum bayes risk, r_b, of a 2-parameter estimation problem, thus justifying calling c bayes (b-)price. the result provides new insight in option pricing, among others obtaining c for some stock-price models using the underlying probability instead of the risk neutral probability and giving r_b an economic interpretation. when logarithmic stock prices follow brownian motion, discrete normal mixture and hyperbolic levy motion the obtained b-prices are ""fair"" prices. a new expression for the price of american call option is also obtained and statistical modeling of r_b can be used when pricing european and american call options.","","2013-04-18","","['yannis g. yatracos']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"625",1304.5401,"sparse integrative clustering of multiple omics data sets","stat.ap","high resolution microarrays and second-generation sequencing platforms are powerful tools to investigate genome-wide alterations in dna copy number, methylation and gene expression associated with a disease. an integrated genomic profiling approach measures multiple omics data types simultaneously in the same set of biological samples. such approach renders an integrated data resolution that would not be available with any single data type. in this study, we use penalized latent variable regression methods for joint modeling of multiple omics data types to identify common latent variables that can be used to cluster patient samples into biologically and clinically relevant disease subtypes. we consider lasso [j. roy. statist. soc. ser. b 58 (1996) 267-288], elastic net [j. r. stat. soc. ser. b stat. methodol. 67 (2005) 301-320] and fused lasso [j. r. stat. soc. ser. b stat. methodol. 67 (2005) 91-108] methods to induce sparsity in the coefficient vectors, revealing important genomic features that have significant contributions to the latent variables. an iterative ridge regression is used to compute the sparse coefficient vectors. in model selection, a uniform design [monographs on statistics and applied probability (1994) chapman & hall] is used to seek ""experimental"" points that scattered uniformly across the search domain for efficient sampling of tuning parameter combinations. we compared our method to sparse singular value decomposition (svd) and penalized gaussian mixture model (gmm) using both real and simulated data sets. the proposed method is applied to integrate genomic, epigenomic and transcriptomic data for subtype analysis in breast and lung cancer data sets.","10.1214/12-aoas578","2013-04-19","","['ronglai shen', 'sijian wang', 'qianxing mo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"626",1304.5943,"on the conditional distributions of low-dimensional projections from   high-dimensional data","math.st stat.th","we study the conditional distribution of low-dimensional projections from high-dimensional data, where the conditioning is on other low-dimensional projections. to fix ideas, consider a random d-vector z that has a lebesgue density and that is standardized so that $\mathbb{e}z=0$ and $\mathbb{e}zz'=i_d$. moreover, consider two projections defined by unit-vectors $\alpha$ and $\beta$, namely a response $y=\alpha'z$ and an explanatory variable $x=\beta'z$. it has long been known that the conditional mean of y given x is approximately linear in x$ under some regularity conditions; cf. hall and li [ann. statist. 21 (1993) 867-889]. however, a corresponding result for the conditional variance has not been available so far. we here show that the conditional variance of y given x is approximately constant in x (again, under some regularity conditions). these results hold uniformly in $\alpha$ and for most $\beta$'s, provided only that the dimension of z is large. in that sense, we see that most linear submodels of a high-dimensional overall model are approximately correct. our findings provide new insights in a variety of modeling scenarios. we discuss several examples, including sliced inverse regression, sliced average variance estimation, generalized linear models under potential link violation, and sparse linear modeling.","10.1214/12-aos1081","2013-04-22","","['hannes leeb']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"627",1304.5974,"dynamic stochastic blockmodels: statistical models for time-evolving   networks","cs.si cs.lg physics.soc-ph stat.me","significant efforts have gone into the development of statistical models for analyzing data in the form of networks, such as social networks. most existing work has focused on modeling static networks, which represent either a single time snapshot or an aggregate view over time. there has been recent interest in statistical modeling of dynamic networks, which are observed at multiple points in time and offer a richer representation of many complex phenomena. in this paper, we propose a state-space model for dynamic networks that extends the well-known stochastic blockmodel for static networks to the dynamic setting. we then propose a procedure to fit the model using a modification of the extended kalman filter augmented with a local search. we apply the procedure to analyze a dynamic social network of email communication.","10.1007/978-3-642-37210-0_22","2013-04-22","","['kevin s. xu', 'alfred o. hero']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"628",1304.8038,"assessment of long-range correlation in animal behaviour time series:   the temporal pattern of locomotor activity of japanese quail (coturnix   coturnix) and mosquito larva (culex quinquefasciatus)","stat.ap q-bio.qm","the aim of this study was to evaluate the performance of a classical method of fractal analysis, detrended fluctuation analysis (dfa), in the analysis of the dynamics of animal behavior time series. in order to correctly use dfa to assess the presence of long-range correlation, previous authors using statistical model systems have stated that different aspects should be taken into account such as: 1) the establishment by hypothesis testing of the absence of short term correlation, 2) an accurate estimation of a straight line in the log-log plot of the fluctuation function, 3) the elimination of artificial crossovers in the fluctuation function, and 4) the length of the time series. taking into consideration these factors, herein we evaluated the presence of long-range correlation in the temporal pattern of locomotor activity of japanese quail ({\sl coturnix coturnix}) and mosquito larva ({\sl culex quinquefasciatus}). in our study, modeling the data with the general arfima model, we rejected the hypothesis of short range correlations (d=0) in all cases. we also observed that dfa was able to distinguish between the artificial crossover observed in the temporal pattern of locomotion of japanese quail, and the crossovers in the correlation behavior observed in mosquito larvae locomotion. although the test duration can slightly influence the parameter estimation, no qualitative differences were observed between different test durations.","10.1016/j.physa.2013.08.017","2013-04-30","","['jackelyn m. kembro', 'ana georgina flesia', 'raquel m. gleiser', 'maría a. perillo', 'raúl h. marín']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"629",1305.0889,"model-based dose finding under model uncertainty using general   parametric models","stat.me","statistical methodology for the design and analysis of clinical phase ii dose response studies, with related software implementation, are well developed for the case of a normally distributed, homoscedastic response considered for a single timepoint in parallel group study designs. in practice, however, binary, count, or time-to-event endpoints are often used, typically measured repeatedly over time and sometimes in more complex settings like crossover study designs. in this paper we develop an overarching methodology to perform efficient multiple comparisons and modeling for dose finding, under uncertainty about the dose-response shape, using general parametric models. the framework described here is quite general and covers dose finding using generalized non-linear models, linear and non-linear mixed effects models, cox proportional hazards (ph) models, etc. in addition to the core framework, we also develop a general purpose methodology to fit dose response data in a computationally and statistically efficient way. several examples, using a variety of different statistical models, illustrate the breadth of applicability of the results. for the analyses we developed the r add-on package dosefinding, which provides a convenient interface to the general approach adopted here.","10.1002/sim.6052","2013-05-04","2013-05-11","['josé pinheiro', 'björn bornkamp', 'ekkehard glimm', 'frank bretz']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"630",1305.1385,"functional and parametric estimation in a semi- and nonparametric model   with application to mass-spectrometry data","stat.me math.st stat.ap stat.th","motivated by modeling and analysis of mass-spectrometry data, a semi- and nonparametric model is proposed that consists of a linear parametric component for individual location and scale and a nonparametric regression function for the common shape. a multi-step approach is developed that simultaneously estimates the parametric components and the nonparametric function. under certain regularity conditions, it is shown that the resulting estimators is consistent and asymptotic normal for the parametric part and achieve the optimal rate of convergence for the nonparametric part when the bandwidth is suitably chosen. simulation results are presented to demonstrate the effectiveness and finite-sample performance of the method. the method is also applied to a seldi-tof mass spectrometry data set from a study of liver cancer patients.","","2013-05-06","","['weiping ma', 'yang feng', 'kani chen', 'zhiliang ying']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE
"631",1305.3095,"estimation of frequency modulations on wideband signals; applications to   audio signal analysis","math.st stat.th","the problem of joint estimation of power spectrum and modulation from realizations of frequency modulated stationary wideband signals is considered. the study is motivated by some specific signal classes from which departures to stationarity can carry relevant information and has to be estimated. the estimation procedure is based upon explicit modeling of the signal as a wideband stationary gaussian signal, transformed by time-dependent, smooth frequency modulation. under such assumptions, an approximate expression for the second order statistics of the transformed signal's gabor transform is obtained, which leads to an approximate maximum likelihood estimation procedure. the proposed approach is validated on numerical simulations.","","2013-05-14","","['harold omer', 'bruno torrésani']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"632",1305.3492,"approximation of epidemic models by diffusion processes and their   statistical inference","stat.me","multidimensional continuous-time markov jump processes $(z(t))$ on $\mathbb{z}^p$ form a usual set-up for modeling $sir$-like epidemics. however, when facing incomplete epidemic data, inference based on $(z(t))$ is not easy to be achieved. here, we start building a new framework for the estimation of key parameters of epidemic models based on statistics of diffusion processes approximating $(z(t))$. first, \previous results on the approximation of density-dependent $sir$-like models by diffusion processes with small diffusion coefficient $\frac{1}{\sqrt{n}}$, where $n$ is the population size, are generalized to non-autonomous systems. second, our previous inference results on discretely observed diffusion processes with small diffusion coefficient are extended to time-dependent diffusions. consistent and asymptotically gaussian estimates are obtained for a fixed number $n$ of observations, which corresponds to the epidemic context, and for $n\rightarrow \infty$. a correction term, which yields better estimates non asymptotically, is also included. finally, performances and robustness of our estimators with respect to various parameters such as $r_0$ (the basic reproduction number), $n$, $n$ are investigated on simulations. two models, $sir$ and $sirs$, corresponding to single and recurrent outbreaks, respectively, are used to simulate data. the findings indicate that our estimators have good asymptotic properties and behave noticeably well for realistic numbers of observations and population sizes. this study lays the foundations of a generic inference method currently under extension to incompletely observed epidemic data. indeed, contrary to the majority of current inference techniques for partially observed processes, which necessitates computer intensive simulations, our method being mostly an analytical approach requires only the classical optimization steps.","","2013-05-15","2013-12-31","['romain guy', 'catherine larédo', 'elisabeta vergu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"633",1305.3616,"modeling information propagation with survival theory","cs.si cs.ds physics.soc-ph stat.ml","networks provide a skeleton for the spread of contagions, like, information, ideas, behaviors and diseases. many times networks over which contagions diffuse are unobserved and need to be inferred. here we apply survival theory to develop general additive and multiplicative risk models under which the network inference problems can be solved efficiently by exploiting their convexity. our additive risk model generalizes several existing network inference models. we show all these models are particular cases of our more general model. our multiplicative model allows for modeling scenarios in which a node can either increase or decrease the risk of activation of another node, in contrast with previous approaches, which consider only positive risk increments. we evaluate the performance of our network inference algorithms on large synthetic and real cascade datasets, and show that our models are able to predict the length and duration of cascades in real data.","","2013-05-15","","['manuel gomez rodriguez', 'jure leskovec', 'bernhard schoelkopf']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"634",1305.4268,"dynamic covariance models for multivariate financial time series","stat.me stat.ml","the accurate prediction of time-changing covariances is an important problem in the modeling of multivariate financial data. however, some of the most popular models suffer from a) overfitting problems and multiple local optima, b) failure to capture shifts in market conditions and c) large computational costs. to address these problems we introduce a novel dynamic model for time-changing covariances. over-fitting and local optima are avoided by following a bayesian approach instead of computing point estimates. changes in market conditions are captured by assuming a diffusion process in parameter values, and finally computationally efficient and scalable inference is performed using particle filters. experiments with financial data show excellent performance of the proposed method with respect to current standard models.","","2013-05-18","2013-06-02","['yue wu', 'josé miguel hernández-lobato', 'zoubin ghahramani']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"635",1305.5306,"a supervised neural autoregressive topic model for simultaneous image   classification and annotation","cs.cv cs.lg stat.ml","topic modeling based on latent dirichlet allocation (lda) has been a framework of choice to perform scene recognition and annotation. recently, a new type of topic model called the document neural autoregressive distribution estimator (docnade) was proposed and demonstrated state-of-the-art performance for document modeling. in this work, we show how to successfully apply and extend this model to the context of visual scene modeling. specifically, we propose supdocnade, a supervised extension of docnade, that increases the discriminative power of the hidden topic features by incorporating label information into the training objective of the model. we also describe how to leverage information about the spatial position of the visual words and how to embed additional image annotations, so as to simultaneously perform image classification and annotation. we test our model on the scene15, labelme and uiuc-sports datasets and show that it compares favorably to other topic models such as the supervised variant of lda.","","2013-05-22","","['yin zheng', 'yu-jin zhang', 'hugo larochelle']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"636",1305.5734,"characterizing a database of sequential behaviors with latent dirichlet   hidden markov models","stat.ml cs.lg","this paper proposes a generative model, the latent dirichlet hidden markov models (ldhmm), for characterizing a database of sequential behaviors (sequences). ldhmms posit that each sequence is generated by an underlying markov chain process, which are controlled by the corresponding parameters (i.e., the initial state vector, transition matrix and the emission matrix). these sequence-level latent parameters for each sequence are modeled as latent dirichlet random variables and parameterized by a set of deterministic database-level hyper-parameters. through this way, we expect to model the sequence in two levels: the database level by deterministic hyper-parameters and the sequence-level by latent parameters. to learn the deterministic hyper-parameters and approximate posteriors of parameters in ldhmms, we propose an iterative algorithm under the variational em framework, which consists of e and m steps. we examine two different schemes, the fully-factorized and partially-factorized forms, for the framework, based on different assumptions. we present empirical results of behavior modeling and sequence classification on three real-world data sets, and compare them to other related models. the experimental results prove that the proposed ldhmms produce better generalization performance in terms of log-likelihood and deliver competitive results on the sequence classification problem.","","2013-05-24","","['yin song', 'longbing cao', 'xuhui fan', 'wei cao', 'jian zhang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"637",1305.7344,"joint modeling and registration of cell populations in cohorts of   high-dimensional flow cytometric data","stat.ml","in systems biomedicine, an experimenter encounters different potential sources of variation in data such as individual samples, multiple experimental conditions, and multi-variable network-level responses. in multiparametric cytometry, which is often used for analyzing patient samples, such issues are critical. while computational methods can identify cell populations in individual samples, without the ability to automatically match them across samples, it is difficult to compare and characterize the populations in typical experiments, such as those responding to various stimulations or distinctive of particular patients or time-points, especially when there are many samples. joint clustering and matching (jcm) is a multi-level framework for simultaneous modeling and registration of populations across a cohort. jcm models every population with a robust multivariate probability distribution. simultaneously, jcm fits a random-effects model to construct an overall batch template -- used for registering populations across samples, and classifying new samples. by tackling systems-level variation, jcm supports practical biomedical applications involving large cohorts.","10.1371/journal.pone.0100334","2013-05-31","","['saumyadipta pyne', 'kui wang', 'jonathan irish', 'pablo tamayo', 'marc-danie nazaire', 'tarn duong', 'sharon lee', 'shu-kay ng', 'david hafler', 'ronald levy', 'garry nolan', 'jill mesirov', 'geoffrey j. mclachlan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"638",1306.0364,"on moment indeterminacy of the benini income distribution","stat.ot math.pr","the benini distribution is a lognormal-like distribution generalizing the pareto distribution. like the pareto and the lognormal distributions it was originally proposed for modeling economic size distributions, notably the size distribution of personal income. this paper explores a probabilistic property of the benini distribution, showing that it is not determined by the sequence of its moments although all the moments are finite. it also provides explicit examples of distributions possessing the same set of moments. related distributions are briefly explored.","10.1007/s00362-013-0535-9","2013-06-03","","['christian kleiber']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE
"639",1306.0963,"inferring robot task plans from human team meetings: a generative   modeling approach with logic-based prior","cs.ai cs.cl cs.ro stat.ml","we aim to reduce the burden of programming and deploying autonomous systems to work in concert with people in time-critical domains, such as military field operations and disaster response. deployment plans for these operations are frequently negotiated on-the-fly by teams of human planners. a human operator then translates the agreed upon plan into machine instructions for the robots. we present an algorithm that reduces this translation burden by inferring the final plan from a processed form of the human team's planning conversation. our approach combines probabilistic generative modeling with logical plan validation used to compute a highly structured prior over possible plans. this hybrid approach enables us to overcome the challenge of performing inference over the large solution space with only a small amount of noisy data from the team planning session. we validate the algorithm through human subject experimentation and show we are able to infer a human team's final plan with 83% accuracy on average. we also describe a robot demonstration in which two people plan and execute a first-response collaborative task with a pr2 robot. to the best of our knowledge, this is the first work that integrates a logical planning technique within a generative model to perform plan inference.","","2013-06-04","","['been kim', 'caleb m. chacha', 'julie shah']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"640",1306.1678,"a discrete time event-history approach to informative drop-out in   multivariate latent markov models with covariates","math.st stat.th","latent markov (lm) models represent an important tool of analysis of longitudinal data when response variables are affected by time-varying unobserved heterogeneity, which is accounted for by a hidden markov chain. in order to avoid bias when using a model of this type in the presence of informative drop-out, we propose an event-history (eh) extension of the lm approach that may be used with multivariate longitudinal data, in which one or more outcomes of a different nature are observed at each time occasion. the eh component of the resulting model is referred to the interval-censored drop-out, and bias in lm modeling is avoided by correlated random effects, included in the different model components, which follow a common markov chain. in order to perform maximum likelihood estimation of the proposed model by the expectation-maximization algorithm, we extend the usual backward-forward recursions of baum and welch. the algorithm has the same complexity of the one adopted in cases of non-informative drop-out. standard errors for the parameter estimates are derived by using the oakes' identity. we illustrate the proposed approach through an application based on data coming from a medical study about primary biliary cirrhosis in which there are two outcomes of interest, the first of which is continuous and the second is binary.","","2013-06-07","","['francesco bartolucci', 'alessio farcomeni']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"641",1306.2685,"flexible sampling of discrete data correlations without the marginal   distributions","stat.ml cs.lg stat.co","learning the joint dependence of discrete variables is a fundamental problem in machine learning, with many applications including prediction, clustering and dimensionality reduction. more recently, the framework of copula modeling has gained popularity due to its modular parametrization of joint distributions. among other properties, copulas provide a recipe for combining flexible models for univariate marginal distributions with parametric families suitable for potentially high dimensional dependence structures. more radically, the extended rank likelihood approach of hoff (2007) bypasses learning marginal models completely when such information is ancillary to the learning task at hand as in, e.g., standard dimensionality reduction problems or copula parameter estimation. the main idea is to represent data by their observable rank statistics, ignoring any other information from the marginals. inference is typically done in a bayesian framework with gaussian copulas, and it is complicated by the fact this implies sampling within a space where the number of constraints increases quadratically with the number of data points. the result is slow mixing when using off-the-shelf gibbs sampling. we present an efficient algorithm based on recent advances on constrained hamiltonian markov chain monte carlo that is simple to implement and does not require paying for a quadratic cost in sample size.","","2013-06-11","2013-11-14","['alfredo kalaitzis', 'ricardo silva']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE
"642",1306.2733,"copula mixed-membership stochastic blockmodel for intra-subgroup   correlations","cs.lg stat.ml","the \emph{mixed-membership stochastic blockmodel (mmsb)} is a popular framework for modeling social network relationships. it can fully exploit each individual node's participation (or membership) in a social structure. despite its powerful representations, this model makes an assumption that the distributions of relational membership indicators between two nodes are independent. under many social network settings, however, it is possible that certain known subgroups of people may have high or low correlations in terms of their membership categories towards each other, and such prior information should be incorporated into the model. to this end, we introduce a \emph{copula mixed-membership stochastic blockmodel (cmmsb)} where an individual copula function is employed to jointly model the membership pairs of those nodes within the subgroup of interest. the model enables the use of various copula functions to suit the scenario, while maintaining the membership's marginal distribution, as needed, for modeling membership indicators with other nodes outside of the subgroup of interest. we describe the proposed model and its inference algorithm in detail for both the finite and infinite cases. in the experiment section, we compare our algorithms with other popular models in terms of link prediction, using both synthetic and real world data.","","2013-06-12","2013-10-06","['xuhui fan', 'longbing cao', 'richard yi da xu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"643",1306.3014,"mixtures of spatial spline regressions","stat.me stat.co","we present an extension of the functional data analysis framework for univariate functions to the analysis of surfaces: functions of two variables. the spatial spline regression (ssr) approach developed can be used to model surfaces that are sampled over a rectangular domain. furthermore, combining ssr with linear mixed effects models (lmm) allows for the analysis of populations of surfaces, and combining the joint ssr-lmm method with finite mixture models allows for the analysis of populations of surfaces with sub-family structures. through the mixtures of spatial splines regressions (mssr) approach developed, we present methodologies for clustering surfaces into sub-families, and for performing surface-based discriminant analysis. the effectiveness of our methodologies, as well as the modeling capabilities of the ssr model are assessed through an application to handwritten character recognition.","","2013-06-12","2013-06-13","['hien d. nguyen', 'geoffrey j. mclachlan', 'ian a. wood']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"644",1306.3033,"copula-type estimators for flexible multivariate density modeling using   mixtures","stat.me","copulas are popular as models for multivariate dependence because they allow the marginal densities and the joint dependence to be modeled separately. however, they usually require that the transformation from uniform marginals to the marginals of the joint dependence structure is known. this can only be done for a restricted set of copulas, e.g. a normal copula. our article introduces copula-type estimators for flexible multivariate density estimation which also allow the marginal densities to be modeled separately from the joint dependence, as in copula modeling, but overcomes the lack of flexibility of most popular copula estimators. an iterative scheme is proposed for estimating copula-type estimators and its usefulness is demonstrated through simulation and real examples. the joint dependence is is modeled by mixture of normals and mixture of normals factor analyzers models, and mixture of t and mixture of t factor analyzers models. we develop efficient variational bayes algorithms for fitting these in which model selection is performed automatically. based on these mixture models, we construct four classes of copula-type densities which are far more flexible than current popular copula densities, and outperform them in simulation and several real data sets.","","2013-06-13","","['minh-ngoc tran', 'paolo giordani', 'xiuyan mun', 'robert kohn', 'mike pitt']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"645",1306.3392,"unsupervised deconvolution of dynamic imaging reveals intratumor   vascular heterogeneity","q-bio.qm stat.ml","intratumor heterogeneity is often manifested by vascular compartments with distinct pharmacokinetics that cannot be resolved directly by in vivo dynamic imaging. we developed tissue-specific compartment modeling (tscm), an unsupervised computational method of deconvolving dynamic imaging series from heterogeneous tumors that can improve vascular phenotyping in many biological contexts. applying tscm to dynamic contrast-enhanced mri of breast cancers revealed characteristic intratumor vascular heterogeneity and therapeutic responses that were otherwise undetectable.","10.1371/journal.pone.0112143","2013-06-14","2014-09-04","['li chen', 'peter l. choyke', 'niya wang', 'robert clarke', 'zaver m. bhujwalla', 'elizabeth m. c. hillman', 'yue wang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"646",1306.4041,"bayesian monotone regression using gaussian process projection","stat.me","shape constrained regression analysis has applications in dose-response modeling, environmental risk assessment, disease screening and many other areas. incorporating the shape constraints can improve estimation efficiency and avoid implausible results. we propose two novel methods focusing on bayesian monotone curve and surface estimation using gaussian process projections. the first projects samples from an unconstrained prior, while the second projects samples from the gaussian process posterior. theory is developed on continuity of the projection, posterior consistency and rates of contraction. the second approach is shown to have an empirical bayes justification and to lead to simple computation with good performance in finite samples. our projection approach can be applied in other constrained function estimation problems including in multivariate settings.","","2013-06-17","","['lizhen lin', 'david b. dunson']",1,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"647",1306.4052,"target localization in wireless sensor networks using error correcting   codes","stat.ap","in this work, we consider the task of target localization using quantized data in wireless sensor networks (wsns). we propose an energy efficient localization scheme by modeling it as an iterative classification problem. we design coding based iterative approaches for target localization where at every iteration, the fusion center (fc) solves an m-ary hypothesis testing problem and decides the region of interest (roi) for the next iteration. the coding based iterative approach works well even in the presence of byzantine (malicious) sensors in the network. we further consider the effect of non-ideal channels. we suggest the use of soft-decision decoding to compensate for the loss due to the presence of fading channels between the local sensors and the fc. we evaluate the performance of the proposed schemes in terms of the byzantine fault tolerance capability and probability of detection of the target region. we also present performance bounds which help us in designing the system. we provide asymptotic analysis of the proposed schemes and show that the schemes achieve perfect region detection irrespective of the noise variance when the number of sensors tends to infinity. our numerical results show that the proposed schemes provide a similar performance in terms of mean square error (mse) as compared to the traditional maximum likelihood estimation (mle) but are computationally much more efficient and are resilient to errors due to byzantines and non-ideal channels.","10.1109/tit.2013.2289859","2013-06-17","2013-11-03","['aditya vempaty', 'yunghsiang s. han', 'pramod k. varshney']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"648",1306.4231,"flexible multivariate marginal models for analyzing multivariate   longitudinal data, with applications in r","stat.me","most of the available multivariate statistical models dictate on fitting different parameters for the covariate effects on each multiple responses. this might be unnecessary and inefficient for some cases. in this article, we propose a modeling framework for multivariate marginal models to analyze multivariate longitudinal data which provides flexible model building strategies. we show that the model handles several response families such as binomial, count and continuous. we illustrate the model on the mother's stress and children's morbidity data set. a simulation study is conducted to examine the parameter estimates. an r package mmm2 is proposed to fit the model.","","2013-06-18","2013-11-01","['ozgur asar', 'ozlem ilk']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"649",1306.4529,"conditional least squares and copulae in claims reserving for a single   line of business","stat.ap math.pr math.st stat.me stat.th","one of the main goals in non-life insurance is to estimate the claims reserve distribution. a generalized time series model, that allows for modeling the conditional mean and variance of the claim amounts, is proposed for the claims development. on contrary to the classical stochastic reserving techniques, the number of model parameters does not depend on the number of development periods, which leads to a more precise forecasting.   moreover, the time series innovations for the consecutive claims are not considered to be independent anymore. conditional least squares are used for model parameter estimation and consistency of such estimate is proved. copula approach is used for modeling the dependence structure, which improves the precision of the reserve distribution estimate as well.   real data examples are provided as an illustration of the potential benefits of the presented approach.","","2013-06-19","","['michal pešta', 'ostap okhrin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE
"650",1306.4657,"finite state space non parametric hidden markov models are in general   identifiable","stat.me","in this paper, we prove that finite state space non parametric hidden markov models are identifiable as soon as the transition matrix of the latent markov chain has full rank and the emission probability distributions are linearly independent. we then propose several non parametric likelihood based estimation methods, which we apply to models used in applications. we finally show on examples that the use of non parametric modeling and estimation may improve the classification performances.","","2013-06-19","","['elisabeth gassiat', 'alice cleynen', 'stéphane robin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"651",1306.4708,"testing and modeling dependencies between a network and nodal attributes","stat.me","network analysis is often focused on characterizing the dependencies between network relations and node-level attributes. potential relationships are typically explored by modeling the network as a function of the nodal attributes or by modeling the attributes as a function of the network. these methods require specification of the exact nature of the association between the network and attributes, reduce the network data to a small number of summary statistics, and are unable provide predictions simultaneously for missing attribute and network information. existing methods that model the attributes and network jointly also assume the data are fully observed. in this article we introduce a unified approach to analysis that addresses these shortcomings. we use a latent variable model to obtain a low dimensional representation of the network in terms of node-specific network factors and use a test of dependence between the network factors and attributes as a surrogate for a test of dependence between the network and attributes. we propose a formal testing procedure to determine if dependencies exists between the network factors and attributes. we also introduce a joint model for the network and attributes, for use if the test rejects, that can capture a variety of dependence patterns and be used to make inference and predictions for missing observations.","","2013-06-19","","['bailey k. fosdick', 'peter d. hoff']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"652",1306.4734,"real-time semiparametric regression for distributed data sets","stat.me","this paper proposes a method for semiparametric regression analysis of large-scale data which are distributed over multiple hosts. this enables modeling of nonlinear relationships and both the batch approach, where analysis starts after all data have been collected, and the real-time setting are addressed. the methodology is extended to operate in evolving environments, where it can no longer be assumed that model parameters remain constant over time. two areas of application for the methodology are presented: regression modeling when there are multiple data owners and regression modeling within the mapreduce framework. a website, realtime-semiparametric-regression.net, illustrates the use of the proposed method on united states domestic airline data in real-time.","","2013-06-18","","['jan luts']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"653",1306.5583,"vsmc: parallel sequential monte carlo in c++","stat.co cs.ms","sequential monte carlo is a family of algorithms for sampling from a sequence of distributions. some of these algorithms, such as particle filters, are widely used in the physics and signal processing researches. more recent developments have established their application in more general inference problems such as bayesian modeling.   these algorithms have attracted considerable attentions in recent years as they admit natural and scalable parallelizations. however, these algorithms are perceived to be difficult to implement. in addition, parallel programming is often unfamiliar to many researchers though conceptually appealing, especially for sequential monte carlo related fields.   a c++ template library is presented for the purpose of implementing general sequential monte carlo algorithms on parallel hardware. two examples are presented: a simple particle filter and a classic bayesian modeling problem.","","2013-06-24","","['yan zhou']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"654",1306.5993,"frequency-domain stochastic modeling of stationary bivariate or   complex-valued signals","stat.me stat.ap stat.co stat.ml","there are three equivalent ways of representing two jointly observed real-valued signals: as a bivariate vector signal, as a single complex-valued signal, or as two analytic signals known as the rotary components. each representation has unique advantages depending on the system of interest and the application goals. in this paper we provide a joint framework for all three representations in the context of frequency-domain stochastic modeling. this framework allows us to extend many established statistical procedures for bivariate vector time series to complex-valued and rotary representations. these include procedures for parametrically modeling signal coherence, estimating model parameters using the whittle likelihood, performing semi-parametric modeling, and choosing between classes of nested models using model choice. we also provide a new method of testing for impropriety in complex-valued signals, which tests for noncircular or anisotropic second-order statistical structure when the signal is represented in the complex plane. finally, we demonstrate the usefulness of our methodology in capturing the anisotropic structure of signals observed from fluid dynamic simulations of turbulence.","","2013-06-25","2017-03-15","['adam m. sykulski', 'sofia c. olhede', 'jonathan m. lilly', 'jeffrey j. early']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,TRUE,FALSE,FALSE
"655",1306.6103,"a semiparametric bayesian model for detecting synchrony among multiple   neurons","stat.ap q-bio.nc","we propose a scalable semiparametric bayesian model to capture dependencies among multiple neurons by detecting their co-firing (possibly with some lag time) patterns over time. after discretizing time so there is at most one spike at each interval, the resulting sequence of 1's (spike) and 0's (silence) for each neuron is modeled using the logistic function of a continuous latent variable with a gaussian process prior. for multiple neurons, the corresponding marginal distributions are coupled to their joint probability distribution using a parametric copula model. the advantages of our approach are as follows: the nonparametric component (i.e., the gaussian process model) provides a flexible framework for modeling the underlying firing rates; the parametric component (i.e., the copula model) allows us to make inference regarding both contemporaneous and lagged relationships among neurons; using the copula model, we construct multivariate probabilistic models by separating the modeling of univariate marginal distributions from the modeling of dependence structure among variables; our method is easy to implement using a computationally efficient sampling algorithm that can be easily extended to high dimensional problems. using simulated data, we show that our approach could correctly capture temporal dependencies in firing rates and identify synchronous neurons. we also apply our model to spike train data obtained from prefrontal cortical areas in rat's brain.","10.1162/neco_a_00631","2013-06-25","2014-03-03","['babak shahbaba', 'bo zhou', 'shiwei lan', 'hernando ombao', 'david moorman', 'sam behseta']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"656",1306.6111,"understanding the predictive power of computational mechanics and echo   state networks in social media","cs.si cs.lg physics.soc-ph stat.ap stat.ml","there is a large amount of interest in understanding users of social media in order to predict their behavior in this space. despite this interest, user predictability in social media is not well-understood. to examine this question, we consider a network of fifteen thousand users on twitter over a seven week period. we apply two contrasting modeling paradigms: computational mechanics and echo state networks. both methods attempt to model the behavior of users on the basis of their past behavior. we demonstrate that the behavior of users on twitter can be well-modeled as processes with self-feedback. we find that the two modeling approaches perform very similarly for most users, but that they differ in performance on a small subset of the users. by exploring the properties of these performance-differentiated users, we highlight the challenges faced in applying predictive models to dynamic social data.","","2013-06-25","2013-08-23","['david darmon', 'jared sylvester', 'michelle girvan', 'william rand']",1,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"657",1306.6482,"traffic data reconstruction based on markov random field modeling","stat.ml cond-mat.dis-nn cs.lg","we consider the traffic data reconstruction problem. suppose we have the traffic data of an entire city that are incomplete because some road data are unobserved. the problem is to reconstruct the unobserved parts of the data. in this paper, we propose a new method to reconstruct incomplete traffic data collected from various traffic sensors. our approach is based on markov random field modeling of road traffic. the reconstruction is achieved by using mean-field method and a machine learning method. we numerically verify the performance of our method using realistic simulated traffic data for the real road network of sendai, japan.","10.1088/0266-5611/30/2/025003","2013-06-27","","['shun kataoka', 'muneki yasuda', 'cyril furtlehner', 'kazuyuki tanaka']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"658",1307.017,"mixture regression for observational data, with application to   functional regression models","stat.me","in a regression analysis, suppose we suspect that there are several heterogeneous groups in the population that a sample represents. mixture regression models have been applied to address such problems. by modeling the conditional distribution of the response given the covariate as a mixture, the sample can be clustered into groups and the individual regression models for the groups can be estimated simultaneously. this approach treats the covariate as deterministic so that the covariate carries no information as to which group the subject is likely to belong to. although this assumption may be reasonable in experiments where the covariate is completely determined by the experimenter, in observational data the covariate may behave differently across the groups. thus the model should also incorporate the heterogeneity of the covariate, which allows us to estimate the membership of the subject from the covariate.   in this paper, we consider a mixture regression model where the joint distribution of the response and the covariate is modeled as a mixture. given a new observation of the covariate, this approach allows us to compute the posterior probabilities that the subject belongs to each group. using these posterior probabilities, the prediction of the response can adaptively use the covariate. we introduce an inference procedure for this approach and show its properties concerning estimation and prediction. the model is explored for the functional covariate as well as the multivariate covariate. we present a real-data example where our approach outperforms the traditional approach, using the well-analyzed berkeley growth study data.","","2013-06-29","","['toshiya hoshikawa']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"659",1307.0293,"a direct estimation of high dimensional stationary vector   autoregressions","stat.ml","the vector autoregressive (var) model is a powerful tool in modeling complex time series and has been exploited in many fields. however, fitting high dimensional var model poses some unique challenges: on one hand, the dimensionality, caused by modeling a large number of time series and higher order autoregressive processes, is usually much higher than the time series length; on the other hand, the temporal dependence structure in the var model gives rise to extra theoretical challenges. in high dimensions, one popular approach is to assume the transition matrix is sparse and fit the var model using the ""least squares"" method with a lasso-type penalty. in this manuscript, we propose an alternative way in estimating the var model. the main idea is, via exploiting the temporal dependence structure, to formulate the estimating problem into a linear program. there is instant advantage for the proposed approach over the lasso-type estimators: the estimation equation can be decomposed into multiple sub-equations and accordingly can be efficiently solved in a parallel fashion. in addition, our method brings new theoretical insights into the var model analysis. so far the theoretical results developed in high dimensions (e.g., song and bickel (2011) and kock and callot (2012)) mainly pose assumptions on the design matrix of the formulated regression problems. such conditions are indirect about the transition matrices and not transparent. in contrast, our results show that the operator norm of the transition matrices plays an important role in estimation accuracy. we provide explicit rates of convergence for both estimation and prediction. in addition, we provide thorough experiments on both synthetic and real-world equity data to show that there are empirical advantages of our method over the lasso-type estimators in both parameter estimation and forecasting.","","2013-07-01","2014-10-28","['fang han', 'huanran lu', 'han liu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"660",1307.1164,"statistical inference for stochastic differential equations with memory","stat.me stat.ap","in this paper we construct a framework for doing statistical inference for discretely observed stochastic differential equations (sdes) where the driving noise has 'memory'. classical sde models for inference assume the driving noise to be brownian motion, or ""white noise"", thus implying a markov assumption. we focus on the case when the driving noise is a fractional brownian motion, which is a common continuous-time modeling device for capturing long-range memory. since the likelihood is intractable, we proceed via data augmentation, adapting a familiar discretization and missing data approach developed for the white noise case. in addition to the other sde parameters, we take the hurst index to be unknown and estimate it from the data. posterior sampling is performed via a hybrid monte carlo algorithm on both the parameters and the missing data simultaneously so as to improve mixing. we point out that, due to the long-range correlations of the driving noise, careful discretization of the underlying sde is necessary for valid inference. our approach can be adapted to other types of rough-path driving processes such as gaussian ""colored"" noise. the methodology is used to estimate the evolution of the memory parameter in us short-term interest rates.","","2013-07-03","","['martin lysy', 'natesh s. pillai']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"661",1307.1402,"spatial modelling of temperature and humidity using systems of   stochastic partial differential equations","stat.ap","this work is motivated by constructing a weather simulator for precipitation. temperature and humidity are two of the most important driving forces of precipitation, and the strategy is to have a stochastic model for temperature and humidity, and use a deterministic model to go from these variables to precipitation. temperature and humidity are empirically positively correlated. generally speaking, if variables are empirically dependent, then multivariate models should be considered. in this work we model humidity and temperature in southern norway. we want to construct bivariate gaussian random fields (grfs) based on this dataset. the aim of our work is to use the bivariate grfs to capture both the dependence structure between humidity and temperature as well as their spatial dependencies. one important feature for the dataset is that the humidity and temperature are not necessarily observed at the same locations. both univariate and bivariate spatial models are fitted and compared. for modeling and inference the spde approach for univariate models and the systems of spdes approach for multivariate models have been used.   to evaluate the performance of the difference between the univariate and bivariate models, we compare predictive performance using some commonly used scoring rules: mean absolute error, mean-square error and continuous ranked probability score. the results illustrate that we can capture strong positive correlation between the temperature and the humidity. furthermore, the results also agree with the physical or empirical knowledge. at the end, we conclude that using the bivariate grfs to model this dataset is superior to the approach with independent univariate grfs both when evaluating point predictions and for quantifying prediction uncertainty.","","2013-07-04","2015-05-26","['xiangping hu', 'ingelin steinsland', 'daniel simpson', 'sara martino', 'håvard rue']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"662",1307.2302,"the blessing of transitivity in sparse and stochastic networks","stat.ml","the interaction between transitivity and sparsity, two common features in empirical networks, implies that there are local regions of large sparse networks that are dense. we call this the blessing of transitivity and it has consequences for both modeling and inference. extant research suggests that statistical inference for the stochastic blockmodel is more difficult when the edges are sparse. however, this conclusion is confounded by the fact that the asymptotic limit in all of the previous studies is not merely sparse, but also non-transitive. to retain transitivity, the blocks cannot grow faster than the expected degree. thus, in sparse models, the blocks must remain asymptotically small. \n previous algorithmic research demonstrates that small ""local"" clusters are more amenable to computation, visualization, and interpretation when compared to ""global"" graph partitions. this paper provides the first statistical results that demonstrate how these small transitive clusters are also more amenable to statistical estimation. theorem 2 shows that a ""local"" clustering algorithm can, with high probability, detect a transitive stochastic block of a fixed size (e.g. 30 nodes) embedded in a large graph. the only constraint on the ambient graph is that it is large and sparse--it could be generated at random or by an adversary--suggesting a theoretical explanation for the robust empirical performance of local clustering algorithms.","","2013-07-08","2013-08-01","['karl rohe', 'tai qin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"663",1307.3617,"mcmc learning","cs.lg stat.ml","the theory of learning under the uniform distribution is rich and deep, with connections to cryptography, computational complexity, and the analysis of boolean functions to name a few areas. this theory however is very limited due to the fact that the uniform distribution and the corresponding fourier basis are rarely encountered as a statistical model.   a family of distributions that vastly generalizes the uniform distribution on the boolean cube is that of distributions represented by markov random fields (mrf). markov random fields are one of the main tools for modeling high dimensional data in many areas of statistics and machine learning.   in this paper we initiate the investigation of extending central ideas, methods and algorithms from the theory of learning under the uniform distribution to the setup of learning concepts given examples from mrf distributions. in particular, our results establish a novel connection between properties of mcmc sampling of mrfs and learning under the mrf distribution.","","2013-07-13","2015-06-12","['varun kanade', 'elchanan mossel']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"664",1307.5476,"bootstrapped pivots for sample and population means and distribution   functions","math.st stat.th","in this paper we introduce the concept of bootstrapped pivots for the sample and the population means. this is in contrast to the classical method of constructing bootstrapped confidence intervals for the population mean via estimating the cutoff points via drawing a number of bootstrap sub-samples. we show that this new method leads to constructing asymptotic confidence intervals with significantly smaller error in comparison to both of the traditional t-intervals and the classical bootstrapped confidence intervals. the approach taken in this paper relates naturally to super-population modeling, as well as to estimating empirical and theoretical distributions.","","2013-07-20","","['miklos csorgo', 'masoud m nasari']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"665",1307.5601,"kinetic energy plus penalty functions for sparse estimation","stat.ml","in this paper we propose and study a family of sparsity-inducing penalty functions. since the penalty functions are related to the kinetic energy in special relativity, we call them \emph{kinetic energy plus} (kep) functions. we construct the kep function by using the concave conjugate of a $\chi^2$-distance function and present several novel insights into the kep function with $q=1$. in particular, we derive a thresholding operator based on the kep function, and prove its mathematical properties and asymptotic properties in sparsity modeling. moreover, we show that a coordinate descent algorithm is especially appropriate for the kep function. additionally, we discuss the relationship of kep with the penalty functions $\ell_{1/2}$ and mcp. the theoretical and empirical analysis validates that the kep function is effective and efficient in high-dimensional data modeling.","","2013-07-22","2014-07-06","['zhihua zhang', 'shibo zhao', 'zebang shen', 'shuchang zhou']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"666",1307.6616,"does generalization performance of $l^q$ regularization learning depend   on $q$? a negative example","cs.lg stat.ml","$l^q$-regularization has been demonstrated to be an attractive technique in machine learning and statistical modeling. it attempts to improve the generalization (prediction) capability of a machine (model) through appropriately shrinking its coefficients. the shape of a $l^q$ estimator differs in varying choices of the regularization order $q$. in particular, $l^1$ leads to the lasso estimate, while $l^{2}$ corresponds to the smooth ridge regression. this makes the order $q$ a potential tuning parameter in applications. to facilitate the use of $l^{q}$-regularization, we intend to seek for a modeling strategy where an elaborative selection on $q$ is avoidable. in this spirit, we place our investigation within a general framework of $l^{q}$-regularized kernel learning under a sample dependent hypothesis space (sdhs). for a designated class of kernel functions, we show that all $l^{q}$ estimators for $0< q < \infty$ attain similar generalization error bounds. these estimated bounds are almost optimal in the sense that up to a logarithmic factor, the upper and lower bounds are asymptotically identical. this finding tentatively reveals that, in some modeling contexts, the choice of $q$ might not have a strong impact in terms of the generalization capability. from this perspective, $q$ can be arbitrarily specified, or specified merely by other no generalization criteria like smoothness, computational complexity, sparsity, etc..","","2013-07-24","","['shaobo lin', 'chen xu', 'jingshan zeng', 'jian fang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"667",1307.7028,"infinite mixtures of multivariate gaussian processes","cs.lg stat.ml","this paper presents a new model called infinite mixtures of multivariate gaussian processes, which can be used to learn vector-valued functions and applied to multitask learning. as an extension of the single multivariate gaussian process, the mixture model has the advantages of modeling multimodal data and alleviating the computationally cubic complexity of the multivariate gaussian process. a dirichlet process prior is adopted to allow the (possibly infinite) number of mixture components to be automatically inferred from training data, and markov chain monte carlo sampling techniques are used for parameter and latent variable inference. preliminary experimental results on multivariate regression show the feasibility of the proposed model.","","2013-07-26","","['shiliang sun']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"668",1307.7949,"limiting approach to generalized gamma bessel model via fractional   calculus and its applications in various disciplines","math-ph math.mp math.st physics.data-an stat.th","the essentials of fractional calculus according to different approaches that can be useful for our applications in the theory of probability and stochastic processes are established. in addition to this, from this fractional integral one can list out almost all the extended densities for the pathway parameter $q < 1$ and $q \rightarrow 1$. here we bring out the idea of thicker or thinner tailed models associated with a gamma type distribution as a limiting case of pathway operator. applications of this extended gamma model in statistical mechanics, input-output models, and solar spectral irradiance modeling etc are established.","","2013-07-30","","['nicy sebastian']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"669",1307.8046,"joint estimation of causal effects from observational and intervention   gene expression data","stat.ap","background: inference of gene regulatory networks from transcriptomic data has been a wide research area in recent years. proposed methods are mainly based on the use of graphical gaussian models for observational wild-type data and provide undirected graphs that are not able to accurately highlight the causal relationships among genes. in the present work, we seek to improve estimation of causal effects among genes by jointly modeling observational transcriptomic data with intervention data obtained by performing knock-outs or knock-downs on a subset of genes. by examining the impact of such expression perturbations on other genes, a more accurate reflection of regulatory relationships may be obtained than through the use of wild-type data alone. results: using the framework of gaussian bayesian networks, we propose a markov chain monte carlo algorithm with a mallows model and an analytical likelihood maximization to sample from the posterior distribution of causal node orderings, and in turn, to estimate causal effects. the main advantage of the proposed algorithm over previously proposed methods is that it has the flexibility to accommodate any kind of intervention design, including partial or multiple knock-out experiments. methods were compared on simulated data as well as data from the dream 2007 challenge. conclusions: the simulation study confirmed the impossibility of estimating causal orderings of genes with observation data only. the proposed algorithm was found, in most cases, to perform better than the previously proposed methods in terms of accuracy for the estimation of causal effects. in addition, multiple knock-outs proved to bring valuable additional information compared to single knock-outs. the choice of optimal intervention design therefore appears to be a crucial aspect for causal inference and an interesting challenge for future research.","","2013-07-30","","['andrea rau', 'florence jaffrézic', 'grégory nuel']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"670",1307.8229,"posterior contraction rates of the phylogenetic indian buffet processes","stat.ml math.st q-bio.qm stat.ap stat.th","by expressing prior distributions as general stochastic processes, nonparametric bayesian methods provide a flexible way to incorporate prior knowledge and constrain the latent structure in statistical inference. the indian buffet process (ibp) is such an example that can be used to define a prior distribution on infinite binary features, where the exchangeability among subjects is assumed. the phylogenetic indian buffet process (pibp), a derivative of ibp, enables the modeling of non-exchangeability among subjects through a stochastic process on a rooted tree, which is similar to that used in phylogenetics, to describe relationships among the subjects. in this paper, we study the theoretical properties of ibp and pibp under a binary factor model. we establish the posterior contraction rates for both ibp and pibp and substantiate the theoretical results through simulation studies. this is the first work addressing the frequentist property of the posterior behaviors of ibp and pibp. we also demonstrated its practical usefulness by applying pibp prior to a real data example arising in the field of cancer genomics where the exchangeability among subjects is violated.","","2013-07-31","2015-05-19","['mengjie chen', 'chao gao', 'hongyu zhao']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE
"671",1308.0641,"united statistical algorithm, small and big data: future of statistician","math.st stat.me stat.ml stat.th","this article provides the role of big idea statisticians in future of big data science. we describe the `united statistical algorithms' framework for comprehensive unification of traditional and novel statistical methods for modeling small data and big data, especially mixed data (discrete, continuous).","","2013-08-02","","['emanuel parzen', 'subhadeep mukhopadhyay']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE
"672",1308.0642,"nonlinear time series modeling: a unified perspective, algorithm, and   application","math.st stat.ap stat.me stat.ml stat.th","a new comprehensive approach to nonlinear time series analysis and modeling is developed in the present paper. we introduce novel data-specific mid-distribution based legendre polynomial (lp) like nonlinear transformations of the original time series y(t) that enables us to adapt all the existing stationary linear gaussian time series modeling strategy and made it applicable for non-gaussian and nonlinear processes in a robust fashion. the emphasis of the present paper is on empirical time series modeling via the algorithm lptime. we demonstrate the effectiveness of our theoretical framework using daily s&p 500 return data between jan/2/1963 - dec/31/2009. our proposed lptime algorithm systematically discovers all the `stylized facts' of the financial time series automatically all at once, which were previously noted by many researchers one at a time.","","2013-08-02","2017-12-23","['subhadeep mukhopadhyay', 'emanuel parzen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,TRUE
"673",1308.0774,"efficient data augmentation in dynamic models for binary and count data","stat.co","dynamic linear models with gaussian observations and gaussian states lead to closed-form formulas for posterior simulation. however, these closed-form formulas break down when the response or state evolution ceases to be gaussian. dynamic, generalized linear models exemplify a class of models for which this is the case, and include, amongst other models, dynamic binomial logistic regression and dynamic negative binomial regression. finding and appraising posterior simulation techniques for these models is important since modeling temporally correlated categories or counts is useful in a variety of disciplines, including ecology, economics, epidemiology, medicine, and neuroscience. in this paper, we present one such technique, p\'olya-gamma data augmentation, and compare it against two competing methods. we find that the p\'olya-gamma approach works well for dynamic logistic regression and for dynamic negative binomial regression when the count sizes are small. supplementary files are provided for replicating the benchmarks.","","2013-08-03","2013-09-19","['jesse windle', 'carlos m. carvalho', 'james g. scott', 'liang sun']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"674",1308.241,"collective mind: cleaning up the research and experimentation mess in   computer engineering using crowdsourcing, big data and machine learning","cs.se cs.hc stat.ml","software and hardware co-design and optimization of hpc systems has become intolerably complex, ad-hoc, time consuming and error prone due to enormous number of available design and optimization choices, complex interactions between all software and hardware components, and multiple strict requirements placed on performance, power consumption, size, reliability and cost. we present our novel long-term holistic and practical solution to this problem based on customizable, plugin-based, schema-free, heterogeneous, open-source collective mind repository and infrastructure with unified web interfaces and on-line advise system. this collaborative framework distributes analysis and multi-objective off-line and on-line auto-tuning of computer systems among many participants while utilizing any available smart phone, tablet, laptop, cluster or data center, and continuously observing, classifying and modeling their realistic behavior. any unexpected behavior is analyzed using shared data mining and predictive modeling plugins or exposed to the community at ctuning.org for collaborative explanation, top-down complexity reduction, incremental problem decomposition and detection of correlating program, architecture or run-time properties (features). gradually increasing optimization knowledge helps to continuously improve optimization heuristics of any compiler, predict optimizations for new programs or suggest efficient run-time (online) tuning and adaptation strategies depending on end-user requirements. we decided to share all our past research artifacts including hundreds of codelets, numerical applications, data sets, models, universal experimental analysis and auto-tuning pipelines, self-tuning machine learning based meta compiler, and unified statistical analysis and machine learning plugins in a public repository to initiate systematic, reproducible and collaborative research, development and experimentation with a new publication model where experiments and techniques are validated, ranked and improved by the community.","","2013-08-11","","['grigori fursin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"675",1308.243,"nondetection sampling bias in marked presence-only data","q-bio.pe stat.ap","1. species distribution models (sdm) are tools used to determine environmental features that influence the geographic distribution of species' abundance and have been used to analyze presence-only records. analysis of presence-only records may require correction for nondetection sampling bias to yield reliable conclusions. in addition, individuals of some species of animals may be highly aggregated and standard sdms ignore environmental features that may influence aggregation behavior. 2. we contend that nondetection sampling bias can be treated as missing data. statistical theory and corrective methods are well developed for missing data, but have been ignored in the literature on sdms. we developed a marked inhomogeneous poisson point process model that accounted for nondetection and aggregation behavior in animals and tested our methods on simulated data. 3. correcting for nondetection sampling bias requires estimates of the probability of detection which must be obtained from auxiliary data, as presence-only data do not contain information about the detection mechanism. weighted likelihood methods can be used to correct for nondetection if estimates of the probability of detection are available. we used an inhomogeneous poisson point process model to model group abundance, a zero-truncated generalized linear model to model group size, and combined these two models to describe the distribution of abundance. our methods performed well on simulated data when nondetection was accounted for and poorly when detection was ignored. 4. we recommend researchers consider the effects of nondetection sampling bias when modeling species distributions using presence-only data. if information about the detection process is available, we recommend researchers explore the effects of nondetection and, when warranted, correct the bias using our methods.","10.1002/ece3.887","2013-08-11","2013-12-04","['trevor hefley', 'andrew tyre', 'david baasch', 'erin blankenship']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"676",1308.2719,"learning interactions through hierarchical group-lasso regularization","stat.me","we introduce a method for learning pairwise interactions in a manner that satisfies strong hierarchy: whenever an interaction is estimated to be nonzero, both its associated main effects are also included in the model. we motivate our approach by modeling pairwise interactions for categorical variables with arbitrary numbers of levels, and then show how we can accommodate continuous variables and mixtures thereof. our approach allows us to dispense with explicitly applying constraints on the main effects and interactions for identifiability, which results in interpretable interaction models. we compare our method with existing approaches on both simulated and real data, including a genome wide association study, all using our r package glinternet.","","2013-08-12","","['michael lim', 'trevor hastie']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"677",1308.2853,"when are overcomplete topic models identifiable? uniqueness of tensor   tucker decompositions with structured sparsity","cs.lg cs.ir math.na math.st stat.ml stat.th","overcomplete latent representations have been very popular for unsupervised feature learning in recent years. in this paper, we specify which overcomplete models can be identified given observable moments of a certain order. we consider probabilistic admixture or topic models in the overcomplete regime, where the number of latent topics can greatly exceed the size of the observed word vocabulary. while general overcomplete topic models are not identifiable, we establish generic identifiability under a constraint, referred to as topic persistence. our sufficient conditions for identifiability involve a novel set of ""higher order"" expansion conditions on the topic-word matrix or the population structure of the model. this set of higher-order expansion conditions allow for overcomplete models, and require the existence of a perfect matching from latent topics to higher order observed words. we establish that random structured topic models are identifiable w.h.p. in the overcomplete regime. our identifiability results allows for general (non-degenerate) distributions for modeling the topic proportions, and thus, we can handle arbitrarily correlated topics in our framework. our identifiability results imply uniqueness of a class of tensor decompositions with structured sparsity which is contained in the class of tucker decompositions, but is more general than the candecomp/parafac (cp) decomposition.","","2013-08-13","","['animashree anandkumar', 'daniel hsu', 'majid janzamin', 'sham kakade']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"678",1308.3467,"improved likelihood inference in generalized linear models","stat.me","we address the issue of performing testing inference in generalized linear models when the sample size is small. this class of models provides a straightforward way of modeling normal and non-normal data and has been widely used in several practical situations. the likelihood ratio, wald and score statistics, and the recently proposed gradient statistic provide the basis for testing inference on the parameters in these models. we focus on the small-sample case, where the reference chi-squared distribution gives a poor approximation to the true null distribution of these test statistics. we derive a general bartlett-type correction factor in matrix notation for the gradient test which reduces the size distortion of the test, and numerically compare the proposed test with the usual likelihood ratio, wald, score and gradient tests, and with the bartlett-corrected likelihood ratio and score tests. our simulation results suggest that the corrected test we propose can be an interesting alternative to the other tests since it leads to very accurate inference even for very small samples. we also present an empirical application for illustrative purposes.","","2013-08-15","","['tiago m. vargas', 'silvia l. p. ferrari', 'artur j. lemonte']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"679",1308.3506,"computational rationalization: the inverse equilibrium problem","cs.gt cs.lg stat.ml","modeling the purposeful behavior of imperfect agents from a small number of observations is a challenging task. when restricted to the single-agent decision-theoretic setting, inverse optimal control techniques assume that observed behavior is an approximately optimal solution to an unknown decision problem. these techniques learn a utility function that explains the example behavior and can then be used to accurately predict or imitate future behavior in similar observed or unobserved situations.   in this work, we consider similar tasks in competitive and cooperative multi-agent domains. here, unlike single-agent settings, a player cannot myopically maximize its reward; it must speculate on how the other agents may act to influence the game's outcome. employing the game-theoretic notion of regret and the principle of maximum entropy, we introduce a technique for predicting and generalizing behavior.","","2013-08-15","","['kevin waugh', 'brian d. ziebart', 'j. andrew bagnell']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"680",1308.4211,"flexible low-rank statistical modeling with side information","stat.ml","we propose a general framework for reduced-rank modeling of matrix-valued data. by applying a generalized nuclear norm penalty we can directly model low-dimensional latent variables associated with rows and columns. our framework flexibly incorporates row and column features, smoothing kernels, and other sources of side information by penalizing deviations from the row and column models. moreover, a large class of these models can be estimated scalably using convex optimization. the computational bottleneck in each case is one singular value decomposition per iteration of a large but easy-to-apply matrix. our framework generalizes traditional convex matrix completion and multi-task learning methods as well as maximum a posteriori estimation under a large class of popular hierarchical bayesian models.","","2013-08-19","2017-08-22","['william fithian', 'rahul mazumder']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"681",1308.4747,"joint modeling of multiple time series via the beta process with   application to motion capture segmentation","stat.me stat.ml","we propose a bayesian nonparametric approach to the problem of jointly modeling multiple related time series. our model discovers a latent set of dynamical behaviors shared among the sequences, and segments each time series into regions defined by a subset of these behaviors. using a beta process prior, the size of the behavior set and the sharing pattern are both inferred from data. we develop markov chain monte carlo (mcmc) methods based on the indian buffet process representation of the predictive distribution of the beta process. our mcmc inference algorithm efficiently adds and removes behaviors via novel split-merge moves as well as data-driven birth and death proposals, avoiding the need to consider a truncated model. we demonstrate promising results on unsupervised segmentation of human motion capture data.","10.1214/14-aoas742","2013-08-21","2014-11-13","['emily b. fox', 'michael c. hughes', 'erik b. sudderth', 'michael i. jordan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"682",1309.0423,"nonparametric inference in hidden markov models using p-splines","stat.me","hidden markov models (hmms) are flexible time series models in which the distributions of the observations depend on unobserved serially correlated states. the state-dependent distributions in hmms are usually taken from some class of parametrically specified distributions. the choice of this class can be difficult, and an unfortunate choice can have serious consequences for example on state estimates, on forecasts and generally on the resulting model complexity and interpretation, in particular with respect to the number of states. we develop a novel approach for estimating the state-dependent distributions of an hmm in a nonparametric way, which is based on the idea of representing the corresponding densities as linear combinations of a large number of standardized b-spline basis functions, imposing a penalty term on non-smoothness in order to maintain a good balance between goodness-of-fit and smoothness. we illustrate the nonparametric modeling approach in a real data application concerned with vertical speeds of a diving beaked whale, demonstrating that compared to parametric counterparts it can lead to models that are more parsimonious in terms of the number of states yet fit the data equally well.","","2013-09-02","2014-06-17","['roland langrock', 'thomas kneib', 'alexander sohn', 'stacy deruiter']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"683",1309.0579,"modeling bimodal discrete data using conway-maxwell-poisson mixture   models","stat.me","bimodal truncated count distributions are frequently observed in aggregate survey data and in user ratings when respondents are mixed in their opinion. they also arise in censored count data, where the highest category might create an additional mode. modeling bimodal behavior in discrete data is useful for various purposes, from comparing shapes of different samples (or survey questions) to predicting future ratings by new raters. the poisson distribution is the most common distribution for fitting count data and can be modified to achieve mixtures of truncated poisson distributions. however, it is suitable only for modeling equi-dispersed distributions and is limited in its ability to capture bimodality. the conway-maxwell-poisson (cmp) distribution is a two-parameter generalization of the poisson distribution that allows for over- and under-dispersion. in this work, we propose a mixture of cmps for capturing a wide range of truncated discrete data, which can exhibit unimodal and bimodal behavior. we present methods for estimating the parameters of a mixture of two cmp distributions using an em approach. our approach introduces a special two-step optimization within the m step to estimate multiple parameters. we examine computational and theoretical issues. the methods are illustrated for modeling ordered rating data as well as truncated count data, using simulated and real examples.","","2013-09-02","2014-01-23","['pragya sur', 'galit shmueli', 'smarajit bose', 'paromita dubey']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"684",1309.0609,"coherent prior distributions in univariate finite mixture and   markov-switching models","math.st stat.me stat.th","finite mixture and markov-switching models generalize and, therefore, nest specifications featuring only one component. while specifying priors in the two: the general (mixture) model and its special (single-component) case, it may be desirable to ensure that the prior assumptions introduced into both structures are coherent in the sense that the prior distribution in the nested model amounts to the conditional prior in the mixture model under relevant parametric restriction. the study provides the rudiments of setting coherent priors in bayesian univariate finite mixture and markov-switching models. once some primary results are delivered, we derive specific conditions for coherence in the case of three types of continuous priors commonly engaged in bayesian modeling: the normal, inverse gamma, and gamma distributions. further, we study the consequences of introducing additional constraints into the mixture model's prior (such as the ones enforcing identifiability or some sort of regularity, e.g. second-order stationarity) on the coherence conditions. finally, the methodology is illustrated through a discussion of setting coherent priors for a class of markov-switching ar(2) models.","","2013-09-03","","['łukasz kwiatkowski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"685",1309.0781,"an exploratory data survey of drug name incidence and prevalence from   the fda's adverse event reporting system, 2004 to 2012q2","cs.ce stat.ap","drug names, population level surveillance and the fda's adverse event reporting system: an exploratory data survey of drug name incidence and prevalence, 2004-2012q2 purpose: to count and monitor the drug names reported in the publicly available version of the federal adverse event reporting system (faers) from 2004 to 2012q2 in a maximized sensitivity relational model. methods: data mining and data modeling was conducted and event based summary statistics with plots were created from over nine continuous years of continuous faers data. results: this faers model contains 344,452 individual drug names and 432,541,994 count references which occurred across 4,148,761 human subjects in the 34 quarter study period. plots for the top 100 scoring drug name references are reported by year and quarter; the top 100 drug names contain 143,384,240 references or 33% of all drug name references over 34 quarters of continuous faers data. conclusions: while faers contains many drugs and adverse event reports, its data pertains to very few of them. drug name incidence lends timely and effective surveillance of large populations of averse event reports and does not require the cause of the ae, nor its validity to be known to detect a mass poisoning.","","2013-08-30","","['nick williams']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"686",1309.0787,"online tensor methods for learning latent variable models","cs.lg cs.dc cs.si stat.ml","we introduce an online tensor decomposition based approach for two latent variable modeling problems namely, (1) community detection, in which we learn the latent communities that the social actors in social networks belong to, and (2) topic modeling, in which we infer hidden topics of text articles. we consider decomposition of moment tensors using stochastic gradient descent. we conduct optimization of multilinear operations in sgd and avoid directly forming the tensors, to save computational and storage costs. we present optimized algorithm in two platforms. our gpu-based implementation exploits the parallelism of simd architectures to allow for maximum speed-up by a careful optimization of storage and data transfer, whereas our cpu-based implementation uses efficient sparse matrix computations and is suitable for large sparse datasets. for the community detection problem, we demonstrate accuracy and computational efficiency on facebook, yelp and dblp datasets, and for the topic modeling problem, we also demonstrate good performance on the new york times dataset. we compare our results to the state-of-the-art algorithms such as the variational method, and report a gain of accuracy and a gain of several orders of magnitude in the execution time.","","2013-09-03","2015-10-03","['furong huang', 'u. n. niranjan', 'mohammad umar hakeem', 'animashree anandkumar']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"687",1309.2074,"learning transformations for clustering and classification","cs.cv cs.lg stat.ml","a low-rank transformation learning framework for subspace clustering and classification is here proposed. many high-dimensional data, such as face images and motion sequences, approximately lie in a union of low-dimensional subspaces. the corresponding subspace clustering problem has been extensively studied in the literature to partition such high-dimensional data into clusters corresponding to their underlying low-dimensional subspaces. however, low-dimensional intrinsic structures are often violated for real-world observations, as they can be corrupted by errors or deviate from ideal models. we propose to address this by learning a linear transformation on subspaces using matrix rank, via its convex surrogate nuclear norm, as the optimization criteria. the learned linear transformation restores a low-rank structure for data from the same subspace, and, at the same time, forces a a maximally separated structure for data from different subspaces. in this way, we reduce variations within subspaces, and increase separation between subspaces for a more robust subspace clustering. this proposed learned robust subspace clustering framework significantly enhances the performance of existing subspace clustering methods. basic theoretical results here presented help to further support the underlying framework. to exploit the low-rank structures of the transformed subspaces, we further introduce a fast subspace clustering technique, which efficiently combines robust pca with sparse modeling. when class labels are present at the training stage, we show this low-rank transformation framework also significantly enhances classification performance. extensive experiments using public datasets are presented, showing that the proposed approach significantly outperforms state-of-the-art methods for subspace clustering and classification.","","2013-09-09","2014-03-09","['qiang qiu', 'guillermo sapiro']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"688",1309.3075,"inferring heterogeneous evolutionary processes through time: from   sequence substitution to phylogeography","q-bio.pe stat.co","molecular phylogenetic and phylogeographic reconstructions generally assume time-homogeneous substitution processes. motivated by computational convenience, this assumption sacrifices biological realism and offers little opportunity to uncover the temporal dynamics in evolutionary histories. here, we extend and generalize an evolutionary approach that relaxes the time-homogeneous process assumption by allowing the specification of different infinitesimal substitution rate matrices across different time intervals, called epochs, along the evolutionary history. we focus on an epoch model implementation in a bayesian inference framework that offers great modeling flexibility in drawing inference about any discrete data type characterized as a continuous-time markov chain, including phylogeographic traits. to alleviate the computational burden that the additional temporal heterogeneity imposes, we adopt a massively parallel approach that achieves both fine- and coarse-grain parallelization of the computations across branches that accommodate epoch transitions, making extensive use of graphics processing units. through synthetic examples, we assess model performance in recovering evolutionary parameters from data generated according to different evolutionary scenarios that comprise different numbers of epochs for both nucleotide and codon substitution processes. we illustrate the usefulness of our inference framework in two different applications to empirical data sets: the selection dynamics on within-host hiv populations throughout infection and the seasonality of global influenza circulation. in both cases, our epoch model captures key features of temporal heterogeneity that remained difficult to test using ad hoc procedures.","","2013-09-12","","['filip bielejec', 'philippe lemey', 'guy baele', 'andrew rambaut', 'marc a suchard']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"689",1309.3197,"inducing honest reporting without observing outcomes: an application to   the peer-review process","cs.ma cs.ai cs.dl math.st stat.th","when eliciting opinions from a group of experts, traditional devices used to promote honest reporting assume that there is an observable future outcome. in practice, however, this assumption is not always reasonable. in this paper, we propose a scoring method built on strictly proper scoring rules to induce honest reporting without assuming observable outcomes. our method provides scores based on pairwise comparisons between the reports made by each pair of experts in the group. for ease of exposition, we introduce our scoring method by illustrating its application to the peer-review process. in order to do so, we start by modeling the peer-review process using a bayesian model where the uncertainty regarding the quality of the manuscript is taken into account. thereafter, we introduce our scoring method to evaluate the reported reviews. under the assumptions that reviewers are bayesian decision-makers and that they cannot influence the reviews of other reviewers, we show that risk-neutral reviewers strictly maximize their expected scores by honestly disclosing their reviews. we also show how the group's scores can be used to find a consensual review. experimental results show that encouraging honest reporting through the proposed scoring method creates more accurate reviews than the traditional peer-review process.","","2013-09-12","2013-10-22","['arthur carvalho', 'stanko dimitrov', 'kate larson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"690",1309.3533,"mixed membership models for time series","stat.me cs.lg stat.ml","in this article we discuss some of the consequences of the mixed membership perspective on time series analysis. in its most abstract form, a mixed membership model aims to associate an individual entity with some set of attributes based on a collection of observed data. although much of the literature on mixed membership models considers the setting in which exchangeable collections of data are associated with each member of a set of entities, it is equally natural to consider problems in which an entire time series is viewed as an entity and the goal is to characterize the time series in terms of a set of underlying dynamic attributes or ""dynamic regimes"". indeed, this perspective is already present in the classical hidden markov model, where the dynamic regimes are referred to as ""states"", and the collection of states realized in a sample path of the underlying process can be viewed as a mixed membership characterization of the observed time series. our goal here is to review some of the richer modeling possibilities for time series that are provided by recent developments in the mixed membership framework.","","2013-09-13","","['emily b. fox', 'michael i. jordan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"691",1309.3802,"monotone function estimation for computer experiments","stat.me","in statistical modeling of computer experiments sometimes prior information is available about the underlying function. for example, the physical system simulated by the computer code may be known to be monotone with respect to some or all inputs. we develop a bayesian approach to gaussian process modelling capable of incorporating monotonicity information for computer model emulation. markov chain monte carlo methods are used to sample from the posterior distribution of the process given the simulator output and monotonicity information. the performance of the proposed approach in terms of predictive accuracy and uncertainty quantification is demonstrated in a number of simulated examples as well as a real queueing system application.","","2013-09-15","2014-06-14","['shirin golchi', 'derek r. bingham', 'hugh chipman', 'david a. campbell']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"692",1309.4306,"sparsity based poisson denoising with dictionary learning","cs.cv stat.ml","the problem of poisson denoising appears in various imaging applications, such as low-light photography, medical imaging and microscopy. in cases of high snr, several transformations exist so as to convert the poisson noise into an additive i.i.d. gaussian noise, for which many effective algorithms are available. however, in a low snr regime, these transformations are significantly less accurate, and a strategy that relies directly on the true noise statistics is required. a recent work by salmon et al. took this route, proposing a patch-based exponential image representation model based on gmm (gaussian mixture model), leading to state-of-the-art results. in this paper, we propose to harness sparse-representation modeling to the image patches, adopting the same exponential idea. our scheme uses a greedy pursuit with boot-strapping based stopping condition and dictionary learning within the denoising process. the reconstruction performance of the proposed scheme is competitive with leading methods in high snr, and achieving state-of-the-art results in cases of low snr.","10.1109/tip.2014.2362057","2013-09-17","2014-10-14","['raja giryes', 'michael elad']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"693",1309.5337,"change point analysis of histone modifications reveals epigenetic blocks   linking to physical domains","q-bio.gn q-bio.qm stat.ap","histone modification is a vital epigenetic mechanism for transcriptional control in eukaryotes. high-throughput techniques have enabled whole-genome analysis of histone modifications in recent years. however, most studies assume one combination of histone modification invariantly translates to one transcriptional output regardless of local chromatin environment. in this study we hypothesize that, the genome is organized into local domains that manifest similar enrichment pattern of histone modification, which leads to orchestrated regulation of expression of genes with relevant bio- logical functions. we propose a multivariate bayesian change point (bcp) model to segment the drosophila melanogaster genome into consecutive blocks on the basis of combinatorial patterns of histone marks. by modeling the sparse distribution of histone marks across the chromosome with a zero-inflated gaussian mixture, our partitions capture local blocks that manifest relatively homogeneous enrichment pattern of histone modifications. we further characterized blocks by their transcription levels, distribution of genes, degree of co-regulation and go enrichment. our results demonstrate that these blocks, although inferred merely from histone modifications, reveal strong relevance with physical domains, which suggest their important roles in chromatin organization and coordinated gene regulation.","","2013-09-20","2014-05-09","['mengjie chen', 'haifan lin', 'hongyu zhao']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"694",1309.6111,"bayesian hierarchical modeling of extreme hourly precipitation in norway","stat.ap","spatial maps of extreme precipitation are a critical component of flood estimation in hydrological modeling, as well as in the planning and design of important infrastructure. this is particularly relevant in countries such as norway that have a high density of hydrological power generating facilities and are exposed to significant risk of infrastructure damage due to flooding. in this work, we estimate a spatially coherent map of the distribution of extreme hourly precipitation in norway, in terms of return levels, by linking generalized extreme value (gev) distributions with latent gaussian fields in a bayesian hierarchical model. generalized linear models on the parameters of the gev distribution are able to incorporate location-specific geographic and meteorological information and thereby accommodate these effects on extreme precipitation. a gaussian field on the gev parameters captures additional unexplained spatial heterogeneity and overcomes the sparse grid on which observations are collected. we conduct an extensive analysis of the factors that affect the gev parameters and show that our combination is able to appropriately characterize both the spatial variability of the distribution of extreme hourly precipitation in norway, and the associated uncertainty in these estimates.","","2013-09-24","2014-05-26","['anita v. dyrrdal', 'alex lenkoski', 'thordis l. thorarinsdottir', 'frode stordal']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"695",1309.6702,"statistical paleoclimate reconstructions via markov random fields","stat.ap","understanding centennial scale climate variability requires data sets that are accurate, long, continuous and of broad spatial coverage. since instrumental measurements are generally only available after 1850, temperature fields must be reconstructed using paleoclimate archives, known as proxies. various climate field reconstructions (cfr) methods have been proposed to relate past temperature to such proxy networks. in this work, we propose a new cfr method, called graphem, based on gaussian markov random fields embedded within an em algorithm. gaussian markov random fields provide a natural and flexible framework for modeling high-dimensional spatial fields. at the same time, they provide the parameter reduction necessary for obtaining precise and well-conditioned estimates of the covariance structure, even in the sample-starved setting common in paleoclimate applications. in this paper, we propose and compare the performance of different methods to estimate the graphical structure of climate fields, and demonstrate how the graphem algorithm can be used to reconstruct past climate variations. the performance of graphem is compared to the widely used cfr method regem with regularization via truncated total least squares, using synthetic data. our results show that graphem can yield significant improvements, with uniform gains over space, and far better risk properties. we demonstrate that the spatial structure of temperature fields can be well estimated by graphs where each neighbor is only connected to a few geographically close neighbors, and that the increase in performance is directly related to recovering the underlying sparsity in the covariance of the spatial field. our work demonstrates how significant improvements can be made in climate reconstruction methods by better modeling the covariance structure of the climate field.","10.1214/14-aoas794","2013-09-25","2015-06-02","['dominique guillot', 'bala rajaratnam', 'julien emile-geay']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"696",1309.6811,"generative multiple-instance learning models for quantitative   electromyography","cs.lg stat.ml","we present a comprehensive study of the use of generative modeling approaches for multiple-instance learning (mil) problems. in mil a learner receives training instances grouped together into bags with labels for the bags only (which might not be correct for the comprised instances). our work was motivated by the task of facilitating the diagnosis of neuromuscular disorders using sets of motor unit potential trains (mupts) detected within a muscle which can be cast as a mil problem. our approach leads to a state-of-the-art solution to the problem of muscle classification. by introducing and analyzing generative models for mil in a general framework and examining a variety of model structures and components, our work also serves as a methodological guide to modelling mil tasks. we evaluate our proposed methods both on mupt datasets and on the musk1 dataset, one of the most widely used benchmarks for mil.","","2013-09-26","","['tameem adel', 'benn smith', 'ruth urner', 'daniel stashuk', 'daniel j. lizotte']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"697",1309.6849,"cyclic causal discovery from continuous equilibrium data","cs.lg cs.ai stat.ml","we propose a method for learning cyclic causal models from a combination of observational and interventional equilibrium data. novel aspects of the proposed method are its ability to work with continuous data (without assuming linearity) and to deal with feedback loops. within the context of biochemical reactions, we also propose a novel way of modeling interventions that modify the activity of compounds instead of their abundance. for computational reasons, we approximate the nonlinear causal mechanisms by (coupled) local linearizations, one for each experimental condition. we apply the method to reconstruct a cellular signaling network from the flow cytometry data measured by sachs et al. (2005). we show that our method finds evidence in the data for feedback loops and that it gives a more accurate quantitative description of the data at comparable model complexity.","","2013-09-26","","['joris mooij', 'tom heskes']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"698",1309.6863,"sparse nested markov models with log-linear parameters","cs.lg cs.ai stat.ml","hidden variables are ubiquitous in practical data analysis, and therefore modeling marginal densities and doing inference with the resulting models is an important problem in statistics, machine learning, and causal inference. recently, a new type of graphical model, called the nested markov model, was developed which captures equality constraints found in marginals of directed acyclic graph (dag) models. some of these constraints, such as the so called `verma constraint', strictly generalize conditional independence. to make modeling and inference with nested markov models practical, it is necessary to limit the number of parameters in the model, while still correctly capturing the constraints in the marginal of a dag model. placing such limits is similar in spirit to sparsity methods for undirected graphical models, and regression models. in this paper, we give a log-linear parameterization which allows sparse modeling with nested markov models. we illustrate the advantages of this parameterization with a simulation study.","","2013-09-26","","['ilya shpitser', 'robin j. evans', 'thomas s. richardson', 'james m. robins']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"699",1309.6865,"modeling documents with deep boltzmann machines","cs.lg cs.ir stat.ml","we introduce a deep boltzmann machine model suitable for modeling and extracting latent semantic representations from a large unstructured collection of documents. we overcome the apparent difficulty of training a dbm with judicious parameter tying. this parameter tying enables an efficient pretraining algorithm and a state initialization scheme that aids inference. the model can be trained just as efficiently as a standard restricted boltzmann machine. our experiments show that the model assigns better log probability to unseen data than the replicated softmax model. features extracted from our model outperform lda, replicated softmax, and docnade models on document retrieval and document classification tasks.","","2013-09-26","","['nitish srivastava', 'ruslan r salakhutdinov', 'geoffrey e. hinton']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"700",1309.6874,"integrating document clustering and topic modeling","cs.lg cs.cl cs.ir stat.ml","document clustering and topic modeling are two closely related tasks which can mutually benefit each other. topic modeling can project documents into a topic space which facilitates effective document clustering. cluster labels discovered by document clustering can be incorporated into topic models to extract local topics specific to each cluster and global topics shared by all clusters. in this paper, we propose a multi-grain clustering topic model (mgctm) which integrates document clustering and topic modeling into a unified framework and jointly performs the two tasks to achieve the overall best performance. our model tightly couples two components: a mixture component used for discovering latent groups in document collection and a topic model component used for mining multi-grain topics including local topics specific to each cluster and global topics shared across clusters.we employ variational inference to approximate the posterior of hidden variables and learn model parameters. experiments on two datasets demonstrate the effectiveness of our model.","","2013-09-26","","['pengtao xie', 'eric p. xing']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"701",1310.0424,"waste not, want not: why rarefying microbiome data is inadmissible","q-bio.qm q-bio.gn stat.ap","the interpretation of count data originating from the current generation of dna sequencing platforms requires special attention. in particular, the per-sample library sizes often vary by orders of magnitude from the same sequencing run, and the counts are overdispersed relative to a simple poisson model these challenges can be addressed using an appropriate mixture model that simultaneously accounts for library size differences and biological variability. this approach is already well-characterized and implemented for rna-seq data in r packages such as edger and deseq.   we use statistical theory, extensive simulations, and empirical data to show that variance stabilizing normalization using a mixture model like the negative binomial is appropriate for microbiome count data. in simulations detecting differential abundance, normalization procedures based on a gamma-poisson mixture model provided systematic improvement in performance over crude proportions or rarefied counts -- both of which led to a high rate of false positives. in simulations evaluating clustering accuracy, we found that the rarefying procedure discarded samples that were nevertheless accurately clustered by alternative methods, and that the choice of minimum library size threshold was critical in some settings, but with an optimum that is unknown in practice. techniques that use variance stabilizing transformations by modeling microbiome count data with a mixture distribution, such as those implemented in edger and deseq, substantially improved upon techniques that attempt to normalize by rarefying or crude proportions. based on these results and well-established statistical theory, we advocate that investigators avoid rarefying altogether. we have provided microbiome-specific extensions to these tools in the r package, phyloseq.","10.1371/journal.pcbi.1003531","2013-10-01","2013-12-12","['paul j. mcmurdie', 'susan holmes']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"702",1310.074,"pseudo-marginal bayesian inference for gaussian processes","stat.ml cs.lg stat.me","the main challenges that arise when adopting gaussian process priors in probabilistic modeling are how to carry out exact bayesian inference and how to account for uncertainty on model parameters when making model-based predictions on out-of-sample data. using probit regression as an illustrative working example, this paper presents a general and effective methodology based on the pseudo-marginal approach to markov chain monte carlo that efficiently addresses both of these issues. the results presented in this paper show improvements over existing sampling methods to simulate from the posterior distribution over the parameters defining the covariance function of the gaussian process prior. this is particularly important as it offers a powerful tool to carry out full bayesian inference of gaussian process based hierarchic statistical models in general. the results also demonstrate that monte carlo based integration of all model parameters is actually feasible in this class of models providing a superior quantification of uncertainty in predictions. extensive comparisons with respect to state-of-the-art probabilistic classifiers confirm this assertion.","","2013-10-02","2014-04-07","['maurizio filippone', 'mark girolami']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"703",1310.1404,"sequential monte carlo bandits","stat.ml cs.lg stat.me","in this paper we propose a flexible and efficient framework for handling multi-armed bandits, combining sequential monte carlo algorithms with hierarchical bayesian modeling techniques. the framework naturally encompasses restless bandits, contextual bandits, and other bandit variants under a single inferential model. despite the model's generality, we propose efficient monte carlo algorithms to make inference scalable, based on recent developments in sequential monte carlo methods. through two simulation studies, the framework is shown to outperform other empirical methods, while also naturally scaling to more complex problems for which existing approaches can not cope. additionally, we successfully apply our framework to online video-based advertising recommendation, and show its increased efficacy as compared to current state of the art bandit algorithms.","","2013-10-04","","['michael cherkassky', 'luke bornn']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"704",1310.1628,"modeling and forecasting electricity spot prices: a functional data   perspective","stat.ap","classical time series models have serious difficulties in modeling and forecasting the enormous fluctuations of electricity spot prices. markov regime switch models belong to the most often used models in the electricity literature. these models try to capture the fluctuations of electricity spot prices by using different regimes, each with its own mean and covariance structure. usually one regime is dedicated to moderate prices and another is dedicated to high prices. however, these models show poor performance and there is no theoretical justification for this kind of classification. the merit order model, the most important micro-economic pricing model for electricity spot prices, however, suggests a continuum of mean levels with a functional dependence on electricity demand. we propose a new statistical perspective on modeling and forecasting electricity spot prices that accounts for the merit order model. in a first step, the functional relation between electricity spot prices and electricity demand is modeled by daily price-demand functions. in a second step, we parameterize the series of daily price-demand functions using a functional factor model. the power of this new perspective is demonstrated by a forecast study that compares our functional factor model with two established classical time series models as well as two alternative functional data models.","10.1214/13-aoas652","2013-10-06","2013-11-28","['dominik liebl']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"705",1310.2557,"srcek: a continuous embedding of the channel selection problem for   weighted pls modeling","stat.ap","srcek, is a technique for selecting useful channels for affine modeling of a response by pls. the technique embeds the discrete channel selection problem into the continuous space of predictor preweighting, then employs a quasi-newton (or other) optimization algorithm to optimize the preweighting vector. once the weighting vector has been optimized, the magnitudes of the weights indicate the relative importance of each channel. the relative importances are used to construct n different models, the kth consisting of the k most important channels. the different models are then compared by means of cross validation or an information criterion (e.g. bic), allowing automatic selection of a `good' subset of the channels. the analytical jacobian of the pls regression vector with respect to the predictor weighting is derived to facilitate optimization of the latter. this formulation exploits the reduced rank of the predictor matrix to gain some speedup when the number of observations is fewer than the number of predictors (the usual case for e.g. ir spectroscopy). the method compares favourably with predictor selection techniques surveyed by forina et. al.","","2013-10-09","","['steven e. pav']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"706",1310.2598,"statistical mechanics of inference","stat.me","statistical modeling often involves identifying an optimal estimate to some underlying probability distribution known to satisfy some given constraints. i show here that choosing as estimate the centroid, or center of mass, of the set consistent with the constraints formally minimizes an objective measure of the expected error. further, i obtain a useful approximation to this point, valid in the thermodynamic limit, that immediately provides much information relating to the full solution set's geometry. for weak constraints, the centroid is close to the popular maximum entropy solution, whereas for strong constraints the two are far apart. because of this, centroid inference is often substantially more accurate. the results i present allow for its straightforward application.","","2013-10-08","","['jonathan landy']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"707",1310.2627,"a sparse and adaptive prior for time-dependent model parameters","stat.ml cs.ai cs.lg","we consider the scenario where the parameters of a probabilistic model are expected to vary over time. we construct a novel prior distribution that promotes sparsity and adapts the strength of correlation between parameters at successive timesteps, based on the data. we derive approximate variational inference procedures for learning and prediction with this prior. we test the approach on two tasks: forecasting financial quantities from relevant text, and modeling language contingent on time-varying financial measurements.","","2013-10-09","2015-11-07","['dani yogatama', 'bryan r. routledge', 'noah a. smith']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"708",1310.3223,"sparse median graphs estimation in a high dimensional semiparametric   model","stat.ap","in this manuscript a unified framework for conducting inference on complex aggregated data in high dimensional settings is proposed. the data are assumed to be a collection of multiple non-gaussian realizations with underlying undirected graphical structures. utilizing the concept of median graphs in summarizing the commonality across these graphical structures, a novel semiparametric approach to modeling such complex aggregated data is provided along with robust estimation of the median graph, which is assumed to be sparse. the estimator is proved to be consistent in graph recovery and an upper bound on the rate of convergence is given. experiments on both synthetic and real datasets are conducted to illustrate the empirical usefulness of the proposed models and methods.","","2013-10-11","","['fang han', 'han liu', 'brian caffo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"709",1310.4195,"bayesian low rank and sparse covariance matrix decomposition","stat.me","we consider the problem of estimating high-dimensional covariance matrices of a particular structure, which is a summation of low rank and sparse matrices. this covariance structure has a wide range of applications including factor analysis and random effects models. we propose a bayesian method of estimating the covariance matrices by representing the covariance model in the form of a factor model with unknown number of latent factors. we introduce binary indicators for factor selection and rank estimation for the low rank component combined with a bayesian lasso method for the sparse component estimation. simulation studies show that our method can recover the rank as well as the sparsity of the two components respectively. we further extend our method to a graphical factor model where the graphical model of the residuals as well as selecting the number of factors is of interest. we employ a hyper-inverse wishart prior for modeling decomposable graphs of the residuals, and a bayesian graphical lasso selection method for unrestricted graphs. we show through simulations that the extended models can recover both the number of latent factors and the graphical model of the residuals successfully when the sample size is sufficient relative to the dimension.","","2013-10-15","","['lin zhang', 'abhra sarkar', 'bani k. mallick']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"710",1310.4461,"scoring dynamics across professional team sports: tempo, balance and   predictability","stat.ap cs.cy physics.data-an physics.soc-ph","despite growing interest in quantifying and modeling the scoring dynamics within professional sports games, relative little is known about what patterns or principles, if any, cut across different sports. using a comprehensive data set of scoring events in nearly a dozen consecutive seasons of college and professional (american) football, professional hockey, and professional basketball, we identify several common patterns in scoring dynamics. across these sports, scoring tempo---when scoring events occur---closely follows a common poisson process, with a sport-specific rate. similarly, scoring balance---how often a team wins an event---follows a common bernoulli process, with a parameter that effectively varies with the size of the lead. combining these processes within a generative model of gameplay, we find they both reproduce the observed dynamics in all four sports and accurately predict game outcomes. these results demonstrate common dynamical patterns underlying within-game scoring dynamics across professional team sports, and suggest specific mechanisms for driving them. we close with a brief discussion of the implications of our results for several popular hypotheses about sports dynamics.","10.1140/epjds29","2013-10-16","2014-03-20","['sears merritt', 'aaron clauset']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"711",1310.5951,"a tractable state-space model for symmetric positive-definite matrices","stat.me","bayesian analysis of state-space models includes computing the posterior distribution of the system's parameters as well as filtering, smoothing, and predicting the system's latent states. when the latent states wander around $\mathbb{r}^n$ there are several well-known modeling components and computational tools that may be profitably combined to achieve these tasks. however, there are scenarios, like tracking an object in a video or tracking a covariance matrix of financial assets returns, when the latent states are restricted to a curve within $\mathbb{r}^n$ and these models and tools do not immediately apply. within this constrained setting, most work has focused on filtering and less attention has been paid to the other aspects of bayesian state-space inference, which tend to be more challenging. to that end, we present a state-space model whose latent states take values on the manifold of symmetric positive-definite matrices and for which one may easily compute the posterior distribution of the latent states and the system's parameters, in addition to filtered distributions and one-step ahead predictions. deploying the model within the context of finance, we show how one can use realized covariance matrices as data to predict latent time-varying covariance matrices. this approach out-performs factor stochastic volatility.","","2013-10-22","2013-12-22","['jesse windle', 'carlos m. carvalho']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"712",1310.6904,"probabilistic forecasts of solar irradiance by stochastic differential   equations","stat.ap","probabilistic forecasts of renewable energy production provide users with valuable information about the uncertainty associated with the expected generation. current state-of-the-art forecasts for solar irradiance have focused on producing reliable \emph{point} forecasts. the additional information included in probabilistic forecasts may be paramount for decision makers to efficiently make use of this uncertain and variable generation. in this paper, a stochastic differential equation (sde) framework for modeling the uncertainty associated with the solar irradiance point forecast is proposed. this modeling approach allows for characterizing both the interdependence structure of prediction errors of short-term solar irradiance and their predictive distribution. a series of different sde models are fitted to a training set and subsequently evaluated on a one-year test set. the final model proposed is defined on a bounded and time-varying state space with zero probability almost surely of events outside this space.","","2013-10-25","","['emil b. iversen', 'juan m. morales', 'jan k. møller', 'henrik madsen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"713",1310.7643,"advection-dispersion across interfaces","math.st stat.me stat.th","this article concerns a systemic manifestation of small scale interfacial heterogeneities in large scale quantities of interest to a variety of diverse applications spanning the earth, biological and ecological sciences. beginning with formulations in terms of partial differential equations governing the conservative, advective-dispersive transport of mass concentrations in divergence form, the specific interfacial heterogeneities are introduced in terms of (spatial) discontinuities in the diffusion coefficient across a lower-dimensional hypersurface. a pathway to an equivalent stochastic formulation is then developed with special attention to the interfacial effects in various functionals such as first passage times, occupation times and local times. that an appreciable theory is achievable within a framework of applications involving one-dimensional models having piecewise constant coefficients greatly facilitates our goal of a gentle introduction to some rather dramatic mathematical consequences of interfacial effects that can be used to predict structure and to inform modeling.","10.1214/13-sts442","2013-10-28","2013-12-23","['jorge m. ramirez', 'enrique a. thomann', 'edward c. waymire']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"714",1310.7679,"structured optimal transmission control in network-coded two-way relay   channels","cs.sy stat.ml","this paper considers a transmission control problem in network-coded two-way relay channels (nc-twrc), where the relay buffers random symbol arrivals from two users, and the channels are assumed to be fading. the problem is modeled by a discounted infinite horizon markov decision process (mdp). the objective is to find a transmission control policy that minimizes the symbol delay, buffer overflow and transmission power consumption and error rate simultaneously and in the long run. by using the concepts of submodularity, multimodularity and l-natural convexity, we study the structure of the optimal policy searched by dynamic programming (dp) algorithm. we show that the optimal transmission policy is nondecreasing in queue occupancies or/and channel states under certain conditions such as the chosen values of parameters in the mdp model, channel modeling method, modulation scheme and the preservation of stochastic dominance in the transitions of system states. the results derived in this paper can be used to relieve the high complexity of dp and facilitate real-time control.","","2013-10-29","","['ni ding', 'parastoo sadeghi', 'rodney a. kennedy']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"715",1310.8192,"spbayes for large univariate and multivariate point-referenced   spatio-temporal data models","stat.co stat.me","in this paper we detail the reformulation and rewrite of core functions in the spbayes r package. these efforts have focused on improving computational efficiency, flexibility, and usability for point-referenced data models. attention is given to algorithm and computing developments that result in improved sampler convergence rate and efficiency by reducing parameter space; decreased sampler run-time by avoiding expensive matrix computations, and; increased scalability to large datasets by implementing a class of predictive process models that attempt to overcome computational hurdles by representing spatial processes in terms of lower-dimensional realizations. beyond these general computational improvements for existing model functions, we detail new functions for modeling data indexed in both space and time. these new functions implement a class of dynamic spatio-temporal models for settings where space is viewed as continuous and time is taken as discrete.","","2013-10-30","","['andrew o. finley', 'sudipto banerjee', 'alan e. gelfand']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"716",1310.8604,"modeling catastrophic deaths using evt with a microsimulation approach   to reinsurance pricing","q-fin.rm stat.ap","recently, a marked poisson process (mpp) model for life catastrophe risk was proposed in [6]. we provide a justification and further support for the model by considering more general poisson point processes in the context of extreme value theory (evt), and basing the choice of model on statistical tests and model comparisons. a case study examining accidental deaths in the finnish population is provided.   we further extend the applicability of the catastrophe risk model by considering small and big accidents separately; the resulting combined mpp model can flexibly capture the whole range of accidental death counts. using the proposed model, we present a simulation framework for pricing (life) catastrophe reinsurance, based on modeling the underlying policies at individual contract level. the accidents are first simulated at population level, and their effect on a specific insurance company is then determined by explicitly simulating the resulting insured deaths. the proposed microsimulation approach can potentially lead to more accurate results than the traditional methods, and to a better view of risk, as it can make use of all the information available to the re/insurer and can explicitly accommodate even complex re/insurance terms and product features. as an example we price several excess reinsurance contracts. the proposed simulation model is also suitable for solvency assessment.","","2013-10-31","","['matias leppisaari']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"717",1311.027,"there is a var beyond usual approximations","stat.me q-fin.rm","basel ii and solvency 2 both use the value-at-risk (var) as the risk measure to compute the capital requirements. in practice, to calibrate the var, a normal approximation is often chosen for the unknown distribution of the yearly log returns of financial assets. this is usually justified by the use of the central limit theorem (clt), when assuming aggregation of independent and identically distributed (iid) observations in the portfolio model. such a choice of modeling, in particular using light tail distributions, has proven during the crisis of 2008/2009 to be an inadequate approximation when dealing with the presence of extreme returns; as a consequence, it leads to a gross underestimation of the risks. the main objective of our study is to obtain the most accurate evaluations of the aggregated risks distribution and risk measures when working on financial or insurance data under the presence of heavy tail and to provide practical solutions for accurately estimating high quantiles of aggregated risks. we explore a new method, called normex, to handle this problem numerically as well as theoretically, based on properties of upper order statistics. normex provides accurate results, only weakly dependent upon the sample size and the tail index. we compare it with existing methods.","","2013-11-01","","['marie kratz']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"718",1311.0416,"structured functional regression models for high-dimensional spatial   spectroscopy data","stat.ap","modeling and analysis of spectroscopy data is an active area of research with applications to chemistry and biology. this paper focuses on analyzing raman spectra obtained from a bone fracture healing experiment, although the functional regression model for predicting a scalar response from high-dimensional tensors can be applied to any spectroscopy data. the regression model is built on a sparse functional representation of the spectra, and accommodates multiple spatial dimensions. we apply our models to the task of predicting bone-mineral-density (bmd), an important indicator of fracture healing, from raman spectra, in both the in vivo and ex vivo settings of the bone fracture healing experiment. to illustrate the general applicability of the method, we also use it to predict lipoprotein concentrations from spectra obtained by nuclear magnetic resonance (nmr) spectroscopy.","","2013-11-02","","['arash a. amini', 'elizaveta levina', 'kerby a. shedden']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"719",1311.0562,"lp mixed data science : outline of theory","math.st stat.me stat.th","this article presents the theoretical foundation of a new frontier of research-`lp mixed data science'-that simultaneously extends and integrates the practice of traditional and novel statistical methods for nonparametric exploratory data modeling, and is applicable to the teaching and training of statistics.   statistics journals have great difficulty accepting papers unlike those previously published. for statisticians with new big ideas a practical strategy is to publish them in many small applied studies which enables one to provide references to work of others. this essay outlines the many concepts, new theory, and important algorithms of our new culture of statistical science called lp mixed data science. it provides comprehensive solutions to problems of data analysis and nonparametric modeling of many variables that are continuous or discrete, which does not yet have a large literature. it develops a new modeling approach to nonparametric estimation of the multivariate copula density. we discuss the theory which we believe is very elegant (and can provide a framework for united statistical algorithms, for traditional small data methods and big data methods).","","2013-11-03","2013-11-06","['emanuel parzen', 'subhadeep mukhopadhyay']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"720",1311.0907,"bayesian nonparametric inference on the stiefel manifold","stat.co","the stiefel manifold $v_{p,d}$ is the space of all $d \times p$ orthonormal matrices, with the $d-1$ hypersphere and the space of all orthogonal matrices constituting special cases. in modeling data lying on the stiefel manifold, parametric distributions such as the matrix langevin distribution are often used; however, model misspecification is a concern and it is desirable to have nonparametric alternatives. current nonparametric methods are fr\'echet mean based. we take a fully generative nonparametric approach, which relies on mixing parametric kernels such as the matrix langevin. the proposed kernel mixtures can approximate a large class of distributions on the stiefel manifold, and we develop theory showing posterior consistency. while there exists work developing general posterior consistency results, extending these results to this particular manifold requires substantial new theory. posterior inference is illustrated on a real-world dataset of near-earth objects.","","2013-11-04","2014-07-03","['lizhen lin', 'vinayak rao', 'david b. dunson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"721",1311.1027,"perfect simulation of determinantal point processes","math.pr math.st stat.th","determinantal point processes (dpp) serve as a practicable modeling for many applications of repulsive point processes. a known approach for simulation was proposed in \cite{hough(2006)}, which generate the desired distribution point wise through rejection sampling. unfortunately, the size of rejection could be very large. in this paper, we investigate the application of perfect simulation via coupling from the past (cftp) on dpp. we give a general framework for perfect simulation on dpp model. it is shown that the limiting sequence of the time-to-coalescence of the coupling is bounded by $k|\lambda|\log k|\lambda|$. an application is given to the stationary models in dpp.","","2013-11-05","","['laurent decreusefond', 'ian flint', 'kah choon low']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"722",1311.1039,"maximum penalized likelihood estimation in semiparametric   capture-recapture models","stat.ap q-bio.qm stat.me","we discuss the semiparametric modeling of mark-recapture-recovery data where the temporal and/or individual variation of model parameters is explained via covariates. typically, in such analyses a fixed (or mixed) effects parametric model is specified for the relationship between the model parameters and the covariates of interest. in this paper, we discuss the modeling of the relationship via the use of penalized splines, to allow for considerably more flexible functional forms. corresponding models can be fitted via numerical maximum penalized likelihood estimation, employing cross-validation to choose the smoothing parameters in a data-driven way. our contribution builds on and extends the existing literature, providing a unified inferential framework for semiparametric mark-recapture-recovery models for open populations, where the interest typically lies in the estimation of survival probabilities. the approach is applied to two real datasets, corresponding to grey herons (ardea cinerea), where we model the survival probability as a function of environmental condition (a time-varying global covariate), and soay sheep (ovis aries), where we model the survival probability as a function of individual weight (a time-varying individual-specific covariate). the proposed semiparametric approach is compared to a standard parametric (logistic) regression and new interesting underlying dynamics are observed in both cases.","","2013-11-05","2015-05-20","['théo michelot', 'roland langrock', 'thomas kneib', 'ruth king']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"723",1311.1292,"a generalized savage-dickey ratio","stat.me astro-ph.im","in this brief research note i present a generalized version of the savage-dickey density ratio for representation of the bayes factor (or marginal likelihood ratio) of nested statistical models; the new version takes the form of a radon-nikodym derivative and is thus applicable to a wider family of probability spaces than the original (restricted to those admitting an ordinary lebesgue density). a derivation is given following the measure-theoretic construction of marin & robert (2010), and the equivalent estimator is demonstrated in application to a distributional modeling problem.","","2013-11-06","","['ewan cameron']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"724",1311.14,"the propensity score estimation in the presence of length-biased   sampling: a nonparametric adjustment approach","math.st stat.th","the pervasive use of prevalent cohort studies on disease duration, increasingly calls for appropriate methodologies to account for the biases that invariably accompany samples formed by such data. it is well-known, for example, that subjects with shorter lifetime are less likely to be present in such studies. moreover, certain covariate values could be preferentially selected into the sample, being linked to the long-term survivors. the existing methodology for estimation of the propensity score using data collected on prevalent cases requires the correct conditional survival/hazard function given the treatment and covariates. this requirement can be alleviated if the disease under study has stationary incidence, the so-called stationarity assumption. we propose a nonparametric adjustment technique based on a weighted estimating equation for estimating the propensity score which does not require modeling the conditional survival/hazard function when the stationarity assumption holds. large sample properties of the estimator is established and its small sample behavior is studied via simulation.","","2013-11-06","","['ashkan ertefaie', 'masoud asgharian', 'david stephens']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"725",1311.1438,"joint analysis of differential gene expression in multiple studies using   correlation motifs","stat.me","the standard methods for detecting differential gene expression are mostly designed for analyzing a single gene expression experiment. when data from multiple related gene expression studies are available, separately analyzing each study is not an ideal strategy as it may fail to detect important genes with consistent but relatively weak differential signals in multiple studies. jointly modeling all data allows one to borrow information across studies to improve the analysis. however, a simple concordance model, in which each gene is assumed to be differential in either all studies or none of the studies, is incapable of handling genes with study-specific differential expression. in contrast, a model that naively enumerates and analyzes all possible differential patterns across all studies can deal with study-specificity and allow information pooling, but the complexity of its parameter space grows exponentially as the number of studies increases. here we propose a ""correlation motif"" approach to address this dilemma. this approach automatically searches for a small number of latent probability vectors called ""correlation motifs"" to capture the major correlation patterns among multiple studies. the motifs provide the basis for sharing information among studies and genes. the approach improves detection of differential expression and overcomes the barrier of exponentially growing parameter space. it is capable of handling all possible study-specific differential patterns in a large number of studies. the advantages of this new approach over existing methods are illustrated using both simulated and real data.","","2013-11-06","","['yingying wei', 'hongkai ji']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"726",1311.1731,"stochastic blockmodel approximation of a graphon: theory and consistent   estimation","stat.me cs.lg cs.si physics.data-an stat.ml","non-parametric approaches for analyzing network data based on exchangeable graph models (exgm) have recently gained interest. the key object that defines an exgm is often referred to as a graphon. this non-parametric perspective on network modeling poses challenging questions on how to make inference on the graphon underlying observed network data. in this paper, we propose a computationally efficient procedure to estimate a graphon from a set of observed networks generated from it. this procedure is based on a stochastic blockmodel approximation (sba) of the graphon. we show that, by approximating the graphon with a stochastic block model, the graphon can be consistently estimated, that is, the estimation error vanishes as the size of the graph approaches infinity.","","2013-11-07","2013-11-07","['edoardo m airoldi', 'thiago b costa', 'stanley h chan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"727",1311.2079,"nonparametric multi-group membership model for dynamic networks","cs.si physics.soc-ph stat.ml","relational data-like graphs, networks, and matrices-is often dynamic, where the relational structure evolves over time. a fundamental problem in the analysis of time-varying network data is to extract a summary of the common structure and the dynamics of the underlying relations between the entities. here we build on the intuition that changes in the network structure are driven by the dynamics at the level of groups of nodes. we propose a nonparametric multi-group membership model for dynamic networks. our model contains three main components: we model the birth and death of individual groups with respect to the dynamics of the network structure via a distance dependent indian buffet process. we capture the evolution of individual node group memberships via a factorial hidden markov model. and, we explain the dynamics of the network structure by explicitly modeling the connectivity structure of groups. we demonstrate our model's capability of identifying the dynamics of latent groups in a number of different types of network data. experimental results show that our model provides improved predictive performance over existing dynamic network models on future network forecasting and missing link prediction.","","2013-11-08","","['myunghwan kim', 'jure leskovec']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"728",1311.2241,"learning gaussian graphical models with observed or latent fvss","cs.lg stat.ml","gaussian graphical models (ggms) or gauss markov random fields are widely used in many applications, and the trade-off between the modeling capacity and the efficiency of learning and inference has been an important research problem. in this paper, we study the family of ggms with small feedback vertex sets (fvss), where an fvs is a set of nodes whose removal breaks all the cycles. exact inference such as computing the marginal distributions and the partition function has complexity $o(k^{2}n)$ using message-passing algorithms, where k is the size of the fvs, and n is the total number of nodes. we propose efficient structure learning algorithms for two cases: 1) all nodes are observed, which is useful in modeling social or flight networks where the fvs nodes often correspond to a small number of high-degree nodes, or hubs, while the rest of the networks is modeled by a tree. regardless of the maximum degree, without knowing the full graph structure, we can exactly compute the maximum likelihood estimate in $o(kn^2+n^2\log n)$ if the fvs is known or in polynomial time if the fvs is unknown but has bounded size. 2) the fvs nodes are latent variables, where structure learning is equivalent to decomposing a inverse covariance matrix (exactly or approximately) into the sum of a tree-structured matrix and a low-rank matrix. by incorporating efficient inference into the learning steps, we can obtain a learning algorithm using alternating low-rank correction with complexity $o(kn^{2}+n^{2}\log n)$ per iteration. we also perform experiments using both synthetic data as well as real data of flight delays to demonstrate the modeling capacity with fvss of various sizes.","","2013-11-09","","['ying liu', 'alan s. willsky']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"729",1311.2503,"predictable feature analysis","cs.lg stat.ml","every organism in an environment, whether biological, robotic or virtual, must be able to predict certain aspects of its environment in order to survive or perform whatever task is intended. it needs a model that is capable of estimating the consequences of possible actions, so that planning, control, and decision-making become feasible. for scientific purposes, such models are usually created in a problem specific manner using differential equations and other techniques from control- and system-theory. in contrast to that, we aim for an unsupervised approach that builds up the desired model in a self-organized fashion. inspired by slow feature analysis (sfa), our approach is to extract sub-signals from the input, that behave as predictable as possible. these ""predictable features"" are highly relevant for modeling, because predictability is a desired property of the needed consequence-estimating model by definition. in our approach, we measure predictability with respect to a certain prediction model. we focus here on the solution of the arising optimization problem and present a tractable algorithm based on algebraic methods which we call predictable feature analysis (pfa). we prove that the algorithm finds the globally optimal signal, if this signal can be predicted with low error. to deal with cases where the optimal signal has a significant prediction error, we provide a robust, heuristically motivated variant of the algorithm and verify it empirically. additionally, we give formal criteria a prediction-model must meet to be suitable for measuring predictability in the pfa setting and also provide a suitable default-model along with a formal proof that it meets these criteria.","","2013-11-11","","['stefan richthofer', 'laurenz wiskott']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"730",1311.2663,"dintucker: scaling up gaussian process models on multidimensional arrays   with billions of elements","cs.lg cs.dc stat.ml","infinite tucker decomposition (inftucker) and random function prior models, as nonparametric bayesian models on infinite exchangeable arrays, are more powerful models than widely-used multilinear factorization methods including tucker and parafac decomposition, (partly) due to their capability of modeling nonlinear relationships between array elements. despite their great predictive performance and sound theoretical foundations, they cannot handle massive data due to a prohibitively high training time. to overcome this limitation, we present distributed infinite tucker (dintucker), a large-scale nonlinear tensor decomposition algorithm on mapreduce. while maintaining the predictive accuracy of inftucker, it is scalable on massive data. dintucker is based on a new hierarchical bayesian model that enables local training of inftucker on subarrays and information integration from all local training results. we use distributed stochastic gradient descent, coupled with variational inference, to train this model. we apply dintucker to multidimensional arrays with billions of elements from applications in the ""read the web"" project (carlson et al., 2010) and in information security and compare it with the state-of-the-art large-scale tensor decomposition method, gigatensor. on both datasets, dintucker achieves significantly higher prediction accuracy with less computational time.","","2013-11-11","2014-02-01","['shandian zhe', 'yuan qi', 'youngja park', 'ian molloy', 'suresh chari']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"731",1311.2791,"when does more regularization imply fewer degrees of freedom? sufficient   conditions and counter examples from lasso and ridge regression","math.st stat.ml stat.th","regularization aims to improve prediction performance of a given statistical modeling approach by moving to a second approach which achieves worse training error but is expected to have fewer degrees of freedom, i.e., better agreement between training and prediction error. we show here, however, that this expected behavior does not hold in general. in fact, counter examples are given that show regularization can increase the degrees of freedom in simple situations, including lasso and ridge regression, which are the most common regularization approaches in use. in such situations, the regularization increases both training error and degrees of freedom, and is thus inherently without merit. on the other hand, two important regularization scenarios are described where the expected reduction in degrees of freedom is indeed guaranteed: (a) all symmetric linear smoothers, and (b) linear regression versus convex constrained linear regression (as in the constrained variant of ridge regression and lasso).","","2013-11-12","","['shachar kaufman', 'saharon rosset']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"732",1311.2971,"approximate inference in continuous determinantal point processes","stat.ml cs.lg stat.me","determinantal point processes (dpps) are random point processes well-suited for modeling repulsion. in machine learning, the focus of dpp-based models has been on diverse subset selection from a discrete and finite base set. this discrete setting admits an efficient sampling algorithm based on the eigendecomposition of the defining kernel matrix. recently, there has been growing interest in using dpps defined on continuous spaces. while the discrete-dpp sampler extends formally to the continuous case, computationally, the steps required are not tractable in general. in this paper, we present two efficient dpp sampling schemes that apply to a wide range of kernel functions: one based on low rank approximations via nystrom and random fourier feature techniques and another based on gibbs sampling. we demonstrate the utility of continuous dpps in repulsive mixture modeling and synthesizing human poses spanning activity spaces.","","2013-11-12","","['raja hafiz affandi', 'emily b. fox', 'ben taskar']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"733",1311.3859,"mapping cognitive ontologies to and from the brain","stat.ml cs.lg q-bio.nc","imaging neuroscience links brain activation maps to behavior and cognition via correlational studies. due to the nature of the individual experiments, based on eliciting neural response from a small number of stimuli, this link is incomplete, and unidirectional from the causal point of view. to come to conclusions on the function implied by the activation of brain regions, it is necessary to combine a wide exploration of the various brain functions and some inversion of the statistical inference. here we introduce a methodology for accumulating knowledge towards a bidirectional link between observed brain activity and the corresponding function. we rely on a large corpus of imaging studies and a predictive engine. technically, the challenges are to find commonality between the studies without denaturing the richness of the corpus. the key elements that we contribute are labeling the tasks performed with a cognitive ontology, and modeling the long tail of rare paradigms in the corpus. to our knowledge, our approach is the first demonstration of predicting the cognitive content of completely new brain images. to that end, we propose a method that predicts the experimental paradigms across different studies.","","2013-11-15","2013-11-20","['yannick schwartz', 'bertrand thirion', 'gaël varoquaux']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"734",1311.415,"towards big topic modeling","cs.lg cs.dc cs.ir stat.ml","to solve the big topic modeling problem, we need to reduce both time and space complexities of batch latent dirichlet allocation (lda) algorithms. although parallel lda algorithms on the multi-processor architecture have low time and space complexities, their communication costs among processors often scale linearly with the vocabulary size and the number of topics, leading to a serious scalability problem. to reduce the communication complexity among processors for a better scalability, we propose a novel communication-efficient parallel topic modeling architecture based on power law, which consumes orders of magnitude less communication time when the number of topics is large. we combine the proposed communication-efficient parallel architecture with the online belief propagation (obp) algorithm referred to as pobp for big topic modeling tasks. extensive empirical results confirm that pobp has the following advantages to solve the big topic modeling problem: 1) high accuracy, 2) communication-efficient, 3) fast speed, and 4) constant memory usage when compared with recent state-of-the-art parallel lda algorithms on the multi-processor architecture.","","2013-11-17","","['jian-feng yan', 'jia zeng', 'zhi-qiang liu', 'yang gao']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"735",1311.7071,"sparse linear dynamical system with its application in multivariate   clinical time series","cs.ai cs.lg stat.ml","linear dynamical system (lds) is an elegant mathematical framework for modeling and learning multivariate time series. however, in general, it is difficult to set the dimension of its hidden state space. a small number of hidden states may not be able to model the complexities of a time series, while a large number of hidden states can lead to overfitting. in this paper, we study methods that impose an $\ell_1$ regularization on the transition matrix of an lds model to alleviate the problem of choosing the optimal number of hidden states. we incorporate a generalized gradient descent method into the maximum a posteriori (map) framework and use expectation maximization (em) to iteratively achieve sparsity on the transition matrix of an lds model. we show that our sparse linear dynamical system (slds) improves the predictive performance when compared to ordinary lds on a multivariate clinical time series dataset.","","2013-11-27","2013-12-03","['zitao liu', 'milos hauskrecht']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"736",1311.7184,"using multiple samples to learn mixture models","stat.ml cs.lg","in the mixture models problem it is assumed that there are $k$ distributions $\theta_{1},\ldots,\theta_{k}$ and one gets to observe a sample from a mixture of these distributions with unknown coefficients. the goal is to associate instances with their generating distributions, or to identify the parameters of the hidden distributions. in this work we make the assumption that we have access to several samples drawn from the same $k$ underlying distributions, but with different mixing weights. as with topic modeling, having multiple samples is often a reasonable assumption. instead of pooling the data into one sample, we prove that it is possible to use the differences between the samples to better recover the underlying structure. we present algorithms that recover the underlying structure under milder assumptions than the current state of art when either the dimensionality or the separation is high. the methods, when applied to topic modeling, allow generalization to words not present in the training data.","","2013-11-27","","['jason d lee', 'ran gilad-bachrach', 'rich caruana']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"737",1311.748,"robust regularized singular value decomposition with application to   mortality data","stat.ap","we develop a robust regularized singular value decomposition (robrsvd) method for analyzing two-way functional data. the research is motivated by the application of modeling human mortality as a smooth two-way function of age group and year. the robrsvd is formulated as a penalized loss minimization problem where a robust loss function is used to measure the reconstruction error of a low-rank matrix approximation of the data, and an appropriately defined two-way roughness penalty function is used to ensure smoothness along each of the two functional domains. by viewing the minimization problem as two conditional regularized robust regressions, we develop a fast iterative reweighted least squares algorithm to implement the method. our implementation naturally incorporates missing values. furthermore, our formulation allows rigorous derivation of leave-one-row/column-out cross-validation and generalized cross-validation criteria, which enable computationally efficient data-driven penalty parameter selection. the advantages of the new robust method over nonrobust ones are shown via extensive simulation studies and the mortality rate application.","10.1214/13-aoas649","2013-11-29","","['lingsong zhang', 'haipeng shen', 'jianhua z. huang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"738",1312.0646,"generalized blockmodeling of valued networks","stat.me","the paper presents several approaches to generalized blockmodeling of valued networks, where values of the ties are assumed to be measured on at least interval scale. the first approach is a straightforward generalization of the generalized blockmodeling of binary networks (doreian et al., 2005) to valued blockmodeling. the second approach is homogeneity blockmodeling. the basic idea of homogeneity blockmodeling is that the inconsistency of an empirical block with its ideal block can be measured by within block variability of appropriate values. new ideal blocks appropriate for blockmodeling of valued networks are presented together with definitions of their block inconsistencies.","10.1016/j.socnet.2006.04.002","2013-12-02","2013-12-04","['aleš žiberna']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"739",1312.0652,"wavelet-based scalar-on-function finite mixture regression models","stat.me","classical finite mixture regression is useful for modeling the relationship between scalar predictors and scalar responses arising from subpopulations defined by the differing associations between those predictors and responses. here we extend the classical finite mixture regression model to incorporate functional predictors by taking a wavelet-based approach in which we represent both the functional predictors and the component-specific coefficient functions in terms of an appropriate wavelet basis. in the wavelet representation of the model, the coefficients corresponding to the functional covariates become the predictors. in this setting, we typically have many more predictors than observations. hence we use a lasso-type penalization to perform variable selection and estimation. we also consider an adaptive version of our wavelet-based model. we discuss the specification of the model, provide a fitting algorithm, and apply and evaluate our method using both simulations and a real data set from a study of the relationship between cognitive ability and diffusion tensor imaging measures in subjects with multiple sclerosis.","","2013-12-02","","['adam ciarleglio', 'r. todd ogden']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"740",1312.0906,"hamiltonian monte carlo for hierarchical models","stat.me","hierarchical modeling provides a framework for modeling the complex interactions typical of problems in applied statistics. by capturing these relationships, however, hierarchical models also introduce distinctive pathologies that quickly limit the efficiency of most common methods of in- ference. in this paper we explore the use of hamiltonian monte carlo for hierarchical models and demonstrate how the algorithm can overcome those pathologies in practical applications.","","2013-12-03","","['m. j. betancourt', 'mark girolami']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"741",1312.1816,"extreme value analysis for evaluating ozone control strategies","stat.ap","tropospheric ozone is one of six criteria pollutants regulated by the us epa, and has been linked to respiratory and cardiovascular endpoints and adverse effects on vegetation and ecosystems. regional photochemical models have been developed to study the impacts of emission reductions on ozone levels. the standard approach is to run the deterministic model under new emission levels and attribute the change in ozone concentration to the emission control strategy. however, running the deterministic model requires substantial computing time, and this approach does not provide a measure of uncertainty for the change in ozone levels. recently, a reduced form model (rfm) has been proposed to approximate the complex model as a simple function of a few relevant inputs. in this paper, we develop a new statistical approach to make full use of the rfm to study the effects of various control strategies on the probability and magnitude of extreme ozone events. we fuse the model output with monitoring data to calibrate the rfm by modeling the conditional distribution of monitoring data given the rfm using a combination of flexible semiparametric quantile regression for the center of the distribution where data are abundant and a parametric extreme value distribution for the tail where data are sparse. selected parameters in the conditional distribution are allowed to vary by the rfm value and the spatial location. also, due to the simplicity of the rfm, we are able to embed the rfm in our bayesian hierarchical framework to obtain a full posterior for the model input parameters, and propagate this uncertainty to the estimation of the effects of the control strategies. we use the new framework to evaluate three potential control strategies, and find that reducing mobile-source emissions has a larger impact than reducing point-source emissions or a combination of several emission sources.","10.1214/13-aoas628","2013-12-06","","['brian reich', 'daniel cooley', 'kristen foley', 'sergey napelenok', 'benjamin shaby']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"742",1312.1895,"efficient metropolis-hastings proposal mechanisms for bayesian   regression tree models","stat.co","bayesian regression trees are flexible non-parametric models that are well suited to many modern statistical regression problems. many such tree models have been proposed, from the simple single- tree model to more complex tree ensembles. their non-parametric formulation allows for effective and efficient modeling of datasets exhibiting complex non-linear relationships between the model pre- dictors and observations. however, the mixing behavior of the markov chain monte carlo (mcmc) sampler is sometimes poor. this is because the proposals in the sampler are typically local alterations of the tree structure, such as the birth/death of leaf nodes, which does not allow for efficient traversal of the model space. this poor mixing can lead to inferential problems, such as under-representing uncertainty. in this paper, we develop novel proposal mechanisms for efficient sampling. the first is a rule perturbation proposal while the second we call tree rotation. the perturbation proposal can be seen as an efficient variation of the change proposal found in existing literature. the novel tree rotation proposal is simple to implement as it only requires local changes to the regression tree structure, yet it efficiently traverses disparate regions of the model space along contours of equal probability. when combined with the classical birth/death proposal, the resulting mcmc sampler exhibits good acceptance rates and properly represents model uncertainty in the posterior samples. we implement this sampling algorithm in the bayesian additive regression tree (bart) model and demonstrate its effectiveness on a prediction problem from computer experiments and a test function where structural tree variability is needed to fully explore the posterior.","","2013-12-06","","['m. t. pratola']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"743",1312.2041,"probabilistic models of genetic variation in structured populations   applied to global human studies","q-bio.pe q-bio.gn q-bio.qm stat.ap stat.me","modern population genetics studies typically involve genome-wide genotyping of individuals from a diverse network of ancestries. an important, unsolved problem is how to formulate and estimate probabilistic models of observed genotypes that allow for complex population structure. we formulate two general probabilistic models, and we propose computationally efficient algorithms to estimate them. first, we show how principal component analysis (pca) can be utilized to estimate a general model that includes the well-known pritchard-stephens-donnelly mixed-membership model as a special case. noting some drawbacks of this approach, we introduce a new ""logistic factor analysis"" (lfa) framework that seeks to directly model the logit transformation of probabilities underlying observed genotypes in terms of latent variables that capture population structure. we demonstrate these advances on data from the human genome diversity panel and 1000 genomes project, where we are able to identify snps that are highly differentiated with respect to structure while making minimal modeling assumptions.","10.1093/bioinformatics/btv641","2013-12-06","2015-03-03","['wei hao', 'minsun song', 'john d. storey']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"744",1312.2252,"a functional analysis of speed profiles: smoothing using derivative   information, curve registration, and functional boxplot","stat.ap","in this paper, we propose a functional analysis of a set of individual space-speed profiles corresponding to speed as function of the distance traveled by the vehicle from an initial point. this functional analysis begins with a functional modeling of space-speed profiles and the study of mathematical properties of these functions. then, in a first step, a smoothing procedure based on spline smoothing is developed in order to convert the raw data into functional objets and to filter out the measurement noise as efficiently as possible. it is shown that this smoothing step leads to a complex nonparametric regression problem that needs to take into account two constraints: the use of the derivative information, and a monotonicity constraint. the performance of the proposed two-step estimator (smooth, and then monotonize) is illustrated on simulation studies and a real data example. in a second step, we use a curve registration method based on landmarks alignment in order to construct an average speed profile representative of a set of individual speed profiles. finally, the variability of such a set is explored by the use of functional boxplots.","","2013-12-08","2014-01-20","['cindie andrieu', 'guillaume saint pierre', 'xavier bressaud']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"745",1312.2638,"vertex nomination schemes for membership prediction","stat.ml math.oc stat.ap","suppose that a graph is realized from a stochastic block model where one of the blocks is of interest, but many or all of the vertices' block labels are unobserved. the task is to order the vertices with unobserved block labels into a ``nomination list'' such that, with high probability, vertices from the interesting block are concentrated near the list's beginning. we propose several vertex nomination schemes. our basic - but principled - setting and development yields a best nomination scheme (which is a bayes-optimal analogue), and also a likelihood maximization nomination scheme that is practical to implement when there are a thousand vertices, and which is empirically near-optimal when the number of vertices is small enough to allow comparison to the best nomination scheme. we then illustrate the robustness of the likelihood maximization nomination scheme to the modeling challenges inherent in real data, using examples which include a social network involving human trafficking, the enron graph, a worm brain connectome and a political blog network.","10.1214/15-aoas834","2013-12-09","2015-11-17","['d. e. fishkind', 'v. lyzinski', 'h. pao', 'l. chen', 'c. e. priebe']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"746",1312.3958,"evidence synthesis for count distributions based on heterogeneous and   incomplete aggregated data","stat.me","the analysis of count data is commonly done using poisson models. negative binomial models are a straightforward and readily motivated generalization for the case of overdispersed data, i.e., when the observed variance is greater than expected under a poissonian model. rate and overdispersion parameters then need to be considered jointly, which in general is not trivial. here we are concerned with evidence synthesis in the case where the reporting of data is rather heterogeneous, i.e., events are reported either in terms of mean event counts, the proportion of event-free patients, or rate estimates and standard errors. either figure carries some information about the relevant parameters, and it is the joint modeling that allows for coherent inference on the parameters of interest. the methods are motivated and illustrated by a systematic review in chronic obstructive pulmonary disease.","10.1002/bimj.201300288","2013-12-13","2014-11-19","['christian röver', 'stefan andreas', 'tim friede']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"747",1312.4527,"probable convexity and its application to correlated topic models","cs.lg stat.ml","non-convex optimization problems often arise from probabilistic modeling, such as estimation of posterior distributions. non-convexity makes the problems intractable, and poses various obstacles for us to design efficient algorithms. in this work, we attack non-convexity by first introducing the concept of \emph{probable convexity} for analyzing convexity of real functions in practice. we then use the new concept to analyze an inference problem in the \emph{correlated topic model} (ctm) and related nonconjugate models. contrary to the existing belief of intractability, we show that this inference problem is concave under certain conditions. one consequence of our analyses is a novel algorithm for learning ctm which is significantly more scalable and qualitative than existing methods. finally, we highlight that stochastic gradient algorithms might be a practical choice to resolve efficiently non-convex problems. this finding might find beneficial in many contexts which are beyond probabilistic modeling.","","2013-12-16","","['khoat than', 'tu bao ho']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"748",1312.4719,"the bernstein function: a unifying framework of nonconvex penalization   in sparse estimation","stat.ml","in this paper we study nonconvex penalization using bernstein functions. since the bernstein function is concave and nonsmooth at the origin, it can induce a class of nonconvex functions for high-dimensional sparse estimation problems. we derive a threshold function based on the bernstein penalty and give its mathematical properties in sparsity modeling. we show that a coordinate descent algorithm is especially appropriate for penalized regression problems with the bernstein penalty. additionally, we prove that the bernstein function can be defined as the concave conjugate of a $\varphi$-divergence and develop a conjugate maximization algorithm for finding the sparse solution. finally, we particularly exemplify a family of bernstein nonconvex penalties based on a generalized gamma measure and conduct empirical analysis for this family.","","2013-12-17","","['zhihua zhang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"749",1312.5054,"bayesian geoadditive expectile regression","stat.me","regression classes modeling more than the mean of the response have found a lot of attention in the last years. expectile regression is a special and computationally convenient case of this family of models. expectiles offer a quantile-like characterisation of a complete distribution and include the mean as a special case. in the frequentist framework the impact of a lot of covariates with very different structures have been made possible. we propose bayesian expectile regression based on the asymmetric normal distribution. this renders possible incorporating for example linear, nonlinear, spatial and random effects in one model. furthermore a detailed inference on the estimated parameters can be conducted. proposal densities based on iterativly weighted least squares updates for the resulting markov chain monte carlo (mcmc) simulation algorithm are proposed and the potential of the approach for extending the flexibility of expectile regression towards complex semiparametric regression specifications is discussed.","","2013-12-18","","['elisabeth waldmann', 'fabian sobotka', 'thomas kneib']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"750",1312.5179,"the total variation on hypergraphs - learning on hypergraphs revisited","stat.ml cs.lg math.oc","hypergraphs allow one to encode higher-order relationships in data and are thus a very flexible modeling tool. current learning methods are either based on approximations of the hypergraphs via graphs or on tensor methods which are only applicable under special conditions. in this paper, we present a new learning framework on hypergraphs which fully uses the hypergraph structure. the key element is a family of regularization functionals based on the total variation on hypergraphs.","","2013-12-18","","['matthias hein', 'simon setzer', 'leonardo jost', 'syama sundar rangapuram']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"751",1312.5271,"systematic and multifactor risk models revisited","q-fin.rm cs.ce math.lo q-fin.cp stat.ml","systematic and multifactor risk models are revisited via methods which were already successfully developed in signal processing and in automatic control. the results, which bypass the usual criticisms on those risk modeling, are illustrated by several successful computer experiments.","","2013-12-18","","['michel fliess', 'cédric join']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"752",1312.5391,"on spatial transition probabilities as continuity measures in   categorical fields","stat.ap","models of spatial transition probabilities, or equivalently, transiogram models have been recently proposed as spatial continuity measures in categorical fields. in this paper, properties of transiogram models are examined analytically, and three important findings are reported. firstly, connections between the behaviors of auto-transiogram models near the origin and the spatial distribution of the corresponding category are carefully investigated. secondly, it is demonstrated that for the indicators of excursion sets of gaussian random fields, most of the commonly used basic mathematical forms of covariogram models are not eligible for transiograms in most cases; an exception is the exponential distance-decay function and models that are constructed from it. finally, a kernel regression method is proposed for efficient, non-parametric joint modeling of auto- and cross-transiograms, which is particularly useful for situations where the number of categories is large.","","2013-12-18","2016-06-07","['guofeng cao', 'phaedon kyriakidis', 'michael goodchild']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"753",1312.5465,"learning rates of $l^q$ coefficient regularization learning with   gaussian kernel","cs.lg stat.ml","regularization is a well recognized powerful strategy to improve the performance of a learning machine and $l^q$ regularization schemes with $0<q<\infty$ are central in use. it is known that different $q$ leads to different properties of the deduced estimators, say, $l^2$ regularization leads to smooth estimators while $l^1$ regularization leads to sparse estimators. then, how does the generalization capabilities of $l^q$ regularization learning vary with $q$? in this paper, we study this problem in the framework of statistical learning theory and show that implementing $l^q$ coefficient regularization schemes in the sample dependent hypothesis space associated with gaussian kernel can attain the same almost optimal learning rates for all $0<q<\infty$. that is, the upper and lower bounds of learning rates for $l^q$ regularization learning are asymptotically identical for all $0<q<\infty$. our finding tentatively reveals that, in some modeling contexts, the choice of $q$ might not have a strong impact with respect to the generalization capability. from this perspective, $q$ can be arbitrarily specified, or specified merely by other no generalization criteria like smoothness, computational complexity, sparsity, etc..","","2013-12-19","2014-09-24","['shaobo lin', 'jinshan zeng', 'jian fang', 'zongben xu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"754",1312.5578,"multimodal transitions for generative stochastic networks","cs.lg stat.ml","generative stochastic networks (gsns) have been recently introduced as an alternative to traditional probabilistic modeling: instead of parametrizing the data distribution directly, one parametrizes a transition operator for a markov chain whose stationary distribution is an estimator of the data generating distribution. the result of training is therefore a machine that generates samples through this markov chain. however, the previously introduced gsn consistency theorems suggest that in order to capture a wide class of distributions, the transition operator in general should be multimodal, something that has not been done before this paper. we introduce for the first time multimodal transition distributions for gsns, in particular using models in the nade family (neural autoregressive density estimator) as output distributions of the transition operator. a nade model is related to an rbm (and can thus model multimodal distributions) but its likelihood (and likelihood gradient) can be computed easily. the parameters of the nade are obtained as a learned function of the previous state of the learned markov chain. experiments clearly illustrate the advantage of such multimodal transition distributions over unimodal gsns.","","2013-12-19","2014-01-24","['sherjil ozair', 'li yao', 'yoshua bengio']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"755",1312.5889,"non-parametric bayesian modeling of complex networks","stat.ml","modeling structure in complex networks using bayesian non-parametrics makes it possible to specify flexible model structures and infer the adequate model complexity from the observed data. this paper provides a gentle introduction to non-parametric bayesian modeling of complex networks: using an infinite mixture model as running example we go through the steps of deriving the model as an infinite limit of a finite parametric model, inferring the model parameters by markov chain monte carlo, and checking the model's fit and predictive performance. we explain how advanced non-parametric models for complex networks can be derived and point out relevant literature.","10.1109/msp.2012.2235191","2013-12-20","","['mikkel n. schmidt', 'morten mørup']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"756",1312.5904,"modern statistical methods in oceanography: a hierarchical perspective","stat.me","processes in ocean physics, air-sea interaction and ocean biogeochemistry span enormous ranges in spatial and temporal scales, that is, from molecular to planetary and from seconds to millennia. identifying and implementing sustainable human practices depend critically on our understandings of key aspects of ocean physics and ecology within these scale ranges. the set of all ocean data is distorted such that three- and four-dimensional (i.e., time-dependent) in situ data are very sparse, while observations of surface and upper ocean properties from space-borne platforms have become abundant in the past few decades. precisions in observations of all types vary as well. in the face of these challenges, the interface between statistics and oceanography has proven to be a fruitful area for research and the development of useful models. with the recognition of the key importance of identifying, quantifying and managing uncertainty in data and models of ocean processes, a hierarchical perspective has become increasingly productive. as examples, we review a heterogeneous mix of studies from our own work demonstrating bayesian hierarchical model applications in ocean physics, air-sea interaction, ocean forecasting and ocean ecosystem models. this review is by no means exhaustive and we have endeavored to identify hierarchical modeling work reported by others across the broad range of ocean-related topics reported in the statistical literature. we conclude by noting relevant ocean-statistics problems on the immediate research horizon, and some technical challenges they pose, for example, in terms of nonlinearity, dimensionality and computing.","10.1214/13-sts436","2013-12-20","","['christopher k. wikle', 'ralph f. milliff', 'radu herbei', 'william b. leeds']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"757",1312.5921,"group-sparse embeddings in collective matrix factorization","stat.ml cs.lg","cmf is a technique for simultaneously learning low-rank representations based on a collection of matrices with shared entities. a typical example is the joint modeling of user-item, item-property, and user-feature matrices in a recommender system. the key idea in cmf is that the embeddings are shared across the matrices, which enables transferring information between them. the existing solutions, however, break down when the individual matrices have low-rank structure not shared with others. in this work we present a novel cmf solution that allows each of the matrices to have a separate low-rank structure that is independent of the other matrices, as well as structures that are shared only by a subset of them. we compare map and variational bayesian solutions based on alternating optimization algorithms and show that the model automatically infers the nature of each factor using group-wise sparsity. our approach supports in a principled way continuous, binary and count observations and is efficient for sparse matrices involving missing data. we illustrate the solution on a number of examples, focusing in particular on an interesting use-case of augmented multi-view learning.","","2013-12-20","2014-02-18","['arto klami', 'guillaume bouchard', 'abhishek tripathi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"758",1312.6026,"how to construct deep recurrent neural networks","cs.ne cs.lg stat.ml","in this paper, we explore different ways to extend a recurrent neural network (rnn) to a \textit{deep} rnn. we start by arguing that the concept of depth in an rnn is not as clear as it is in feedforward neural networks. by carefully analyzing and understanding the architecture of an rnn, however, we find three points of an rnn which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. based on this observation, we propose two novel architectures of a deep rnn which are orthogonal to an earlier attempt of stacking multiple recurrent layers to build a deep rnn (schmidhuber, 1992; el hihi and bengio, 1996). we provide an alternative interpretation of these deep rnns using a novel framework based on neural operators. the proposed deep rnns are empirically evaluated on the tasks of polyphonic music prediction and language modeling. the experimental result supports our claim that the proposed deep rnns benefit from the depth and outperform the conventional, shallow rnns.","","2013-12-20","2014-04-24","['razvan pascanu', 'caglar gulcehre', 'kyunghyun cho', 'yoshua bengio']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"759",1312.6481,"wildfire prediction to inform fire management: statistical science   challenges","stat.ap physics.ao-ph","wildfire is an important system process of the earth that occurs across a wide range of spatial and temporal scales. a variety of methods have been used to predict wildfire phenomena during the past century to better our understanding of fire processes and to inform fire and land management decision-making. statistical methods have an important role in wildfire prediction due to the inherent stochastic nature of fire phenomena at all scales. predictive models have exploited several sources of data describing fire phenomena. experimental data are scarce; observational data are dominated by statistics compiled by government fire management agencies, primarily for administrative purposes and increasingly from remote sensing observations. fires are rare events at many scales. the data describing fire phenomena can be zero-heavy and nonstationary over both space and time. users of fire modeling methodologies are mainly fire management agencies often working under great time constraints, thus, complex models have to be efficiently estimated. we focus on providing an understanding of some of the information needed for fire management decision-making and of the challenges involved in predicting fire occurrence, growth and frequency at regional, national and global scales.","10.1214/13-sts451","2013-12-23","","['s. w. taylor', 'douglas g. woolford', 'c. b. dean', 'david l. martell']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"760",1312.6896,"inference on dynamic models for non-gaussian random fields using inla: a   homicide rate analysis of brazilian cities","stat.ap","robust time series analysis is an important subject in statistical modeling. models based on gaussian distribution are sensitive to outliers, which may imply in a significant degradation in estimation performance as well as in prediction accuracy. state-space models, also referred as dynamic models, is a very useful way to describe the evolution of a time series variable through a structured latent evolution system. integrated nested laplace approximation (inla) is a recent approach proposed to perform fast bayesian inference in latent gaussian models which naturally comprises dynamic models. we present how to perform fast and accurate non-gaussian dynamic modeling with inla and show how these models can provide a more robust time series analysis when compared with standard dynamic models based on gaussian distributions. we formalize the framework used to fit complex non-gaussian space-state models using the r package inla and illustrate our approach in both a simulation study and on the brazilian homicide rate dataset.","","2013-12-24","2015-02-13","['renan xavier cortes', 'thiago guerrera martins', 'marcos oliveira prates', 'bráulio figueiredo alves da silva']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"761",1312.6966,"model-based functional mixture discriminant analysis with hidden process   regression for curve classification","stat.me cs.lg math.st stat.ml stat.th","in this paper, we study the modeling and the classification of functional data presenting regime changes over time. we propose a new model-based functional mixture discriminant analysis approach based on a specific hidden process regression model that governs the regime changes over time. our approach is particularly adapted to handle the problem of complex-shaped classes of curves, where each class is potentially composed of several sub-classes, and to deal with the regime changes within each homogeneous sub-class. the proposed model explicitly integrates the heterogeneity of each class of curves via a mixture model formulation, and the regime changes within each sub-class through a hidden logistic process. each class of complex-shaped curves is modeled by a finite number of homogeneous clusters, each of them being decomposed into several regimes. the model parameters of each class are learned by maximizing the observed-data log-likelihood by using a dedicated expectation-maximization (em) algorithm. comparisons are performed with alternative curve classification approaches, including functional linear discriminant analysis and functional mixture discriminant analysis with polynomial regression mixtures and spline regression mixtures. results obtained on simulated data and real data show that the proposed approach outperforms the alternative approaches in terms of discrimination, and significantly improves the curves approximation.","10.1016/j.neucom.2012.10.030","2013-12-25","","['faicel chamroukhi', 'hervé glotin', 'allou samé']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE
"762",1312.6967,"model-based clustering and segmentation of time series with changes in   regime","stat.me cs.lg math.st stat.ml stat.th","mixture model-based clustering, usually applied to multidimensional data, has become a popular approach in many data analysis problems, both for its good statistical properties and for the simplicity of implementation of the expectation-maximization (em) algorithm. within the context of a railway application, this paper introduces a novel mixture model for dealing with time series that are subject to changes in regime. the proposed approach consists in modeling each cluster by a regression model in which the polynomial coefficients vary according to a discrete hidden process. in particular, this approach makes use of logistic functions to model the (smooth or abrupt) transitions between regimes. the model parameters are estimated by the maximum likelihood method solved by an expectation-maximization algorithm. the proposed approach can also be regarded as a clustering approach which operates by finding groups of time series having common changes in regime. in addition to providing a time series partition, it therefore provides a time series segmentation. the problem of selecting the optimal numbers of clusters and segments is solved by means of the bayesian information criterion (bic). the proposed approach is shown to be efficient using a variety of simulated time series and real-world time series of electrical power consumption from rail switching operations.","10.1007/s11634-011-0096-5","2013-12-25","","['allou samé', 'faicel chamroukhi', 'gérard govaert', 'patrice aknin']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE
"763",1312.6968,"a hidden process regression model for functional data description.   application to curve discrimination","stat.me cs.lg stat.ml","a new approach for functional data description is proposed in this paper. it consists of a regression model with a discrete hidden logistic process which is adapted for modeling curves with abrupt or smooth regime changes. the model parameters are estimated in a maximum likelihood framework through a dedicated expectation maximization (em) algorithm. from the proposed generative model, a curve discrimination rule is derived using the maximum a posteriori rule. the proposed model is evaluated using simulated curves and real world curves acquired during railway switch operations, by performing comparisons with the piecewise regression approach in terms of curve modeling and classification.","10.1016/j.neucom.2009.12.023","2013-12-25","","['faicel chamroukhi', 'allou samé', 'gérard govaert', 'patrice aknin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"764",1312.6969,"time series modeling by a regression approach based on a latent process","stat.me cs.lg math.st stat.ml stat.th","time series are used in many domains including finance, engineering, economics and bioinformatics generally to represent the change of a measurement over time. modeling techniques may then be used to give a synthetic representation of such data. a new approach for time series modeling is proposed in this paper. it consists of a regression model incorporating a discrete hidden logistic process allowing for activating smoothly or abruptly different polynomial regression models. the model parameters are estimated by the maximum likelihood method performed by a dedicated expectation maximization (em) algorithm. the m step of the em algorithm uses a multi-class iterative reweighted least-squares (irls) algorithm to estimate the hidden process parameters. to evaluate the proposed approach, an experimental study on simulated data and real world data was performed using two alternative approaches: a heteroskedastic piecewise regression model using a global optimization algorithm based on dynamic programming, and a hidden markov regression model whose parameters are estimated by the baum-welch algorithm. finally, in the context of the remote monitoring of components of the french railway infrastructure, and more particularly the switch mechanism, the proposed approach has been applied to modeling and classifying time series representing the condition measurements acquired during switch operations.","10.1016/j.neunet.2009.06.040","2013-12-25","","['faicel chamroukhi', 'allou samé', 'gérard govaert', 'patrice aknin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE
"765",1312.7018,"mixture model-based functional discriminant analysis for curve   classification","stat.me cs.lg stat.ml","statistical approaches for functional data analysis concern the paradigm for which the individuals are functions or curves rather than finite dimensional vectors. in this paper, we particularly focus on the modeling and the classification of functional data which are temporal curves presenting regime changes over time. more specifically, we propose a new mixture model-based discriminant analysis approach for functional data using a specific hidden process regression model. our approach is particularly adapted to both handle the problem of complex-shaped classes of curves, where each class is composed of several sub-classes, and to deal with the regime changes within each homogeneous sub-class. the model explicitly integrates the heterogeneity of each class of curves via a mixture model formulation, and the regime changes within each sub-class through a hidden logistic process. the approach allows therefore for fitting flexible curve-models to each class of complex-shaped curves presenting regime changes through an unsupervised learning scheme, to automatically summarize it into a finite number of homogeneous clusters, each of them is decomposed into several regimes. the model parameters are learned by maximizing the observed-data log-likelihood for each class by using a dedicated expectation-maximization (em) algorithm. comparisons on simulated data and real data with alternative approaches, including functional linear discriminant analysis and functional mixture discriminant analysis with polynomial regression mixtures and spline regression mixtures, show that the proposed approach provides better results regarding the discrimination results and significantly improves the curves approximation.","10.1109/ijcnn.2012.6252818","2013-12-25","","['faicel chamroukhi', 'hervé glotin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"766",1312.7077,"language modeling with power low rank ensembles","cs.cl cs.lg stat.ml","we present power low rank ensembles (plre), a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context. our method can be understood as a generalization of n-gram modeling to non-integer n, and includes standard techniques such as absolute discounting and kneser-ney smoothing as special cases. plre training is efficient and our approach outperforms state-of-the-art modified kneser ney baselines in terms of perplexity on large corpora as well as on bleu score in a downstream machine translation task.","","2013-12-26","2014-10-03","['ankur p. parikh', 'avneesh saluja', 'chris dyer', 'eric p. xing']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"767",1312.7712,"a prospect of earthquake prediction research","stat.me physics.geo-ph","earthquakes occur because of abrupt slips on faults due to accumulated stress in the earth's crust. because most of these faults and their mechanisms are not readily apparent, deterministic earthquake prediction is difficult. for effective prediction, complex conditions and uncertain elements must be considered, which necessitates stochastic prediction. in particular, a large amount of uncertainty lies in identifying whether abnormal phenomena are precursors to large earthquakes, as well as in assigning urgency to the earthquake. any discovery of potentially useful information for earthquake prediction is incomplete unless quantitative modeling of risk is considered. therefore, this manuscript describes the prospect of earthquake predictability research to realize practical operational forecasting in the near future.","10.1214/13-sts439","2013-12-30","","['yosihiko ogata']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"768",1312.7857,"bayesian models of graphs, arrays and other exchangeable random   structures","math.st stat.ml stat.th","the natural habitat of most bayesian methods is data represented by exchangeable sequences of observations, for which de finetti's theorem provides the theoretical foundation. dirichlet process clustering, gaussian process regression, and many other parametric and nonparametric bayesian models fall within the remit of this framework; many problems arising in modern data analysis do not. this article provides an introduction to bayesian models of graphs, matrices, and other data that can be modeled by random structures. we describe results in probability theory that generalize de finetti's theorem to such data and discuss their relevance to nonparametric bayesian modeling. with the basic ideas in place, we survey example models available in the literature; applications of such models include collaborative filtering, link prediction, and graph and network analysis. we also highlight connections to recent developments in graph theory and probability, and sketch the more general mathematical foundation of bayesian methods for other types of data beyond sequences and arrays.","","2013-12-30","2015-02-13","['peter orbanz', 'daniel m. roy']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"769",1312.7869,"consistent bounded-asynchronous parameter servers for distributed ml","stat.ml cs.dc cs.lg","in distributed ml applications, shared parameters are usually replicated among computing nodes to minimize network overhead. therefore, proper consistency model must be carefully chosen to ensure algorithm's correctness and provide high throughput. existing consistency models used in general-purpose databases and modern distributed ml systems are either too loose to guarantee correctness of the ml algorithms or too strict and thus fail to fully exploit the computing power of the underlying distributed system.   many ml algorithms fall into the category of \emph{iterative convergent algorithms} which start from a randomly chosen initial point and converge to optima by repeating iteratively a set of procedures. we've found that many such algorithms are to a bounded amount of inconsistency and still converge correctly. this property allows distributed ml to relax strict consistency models to improve system performance while theoretically guarantees algorithmic correctness. in this paper, we present several relaxed consistency models for asynchronous parallel computation and theoretically prove their algorithmic correctness. the proposed consistency models are implemented in a distributed parameter server and evaluated in the context of a popular ml application: topic modeling.","","2013-12-30","2013-12-31","['jinliang wei', 'wei dai', 'abhimanu kumar', 'xun zheng', 'qirong ho', 'eric p. xing']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"770",1401.0087,"contributors of carbon dioxide in the atmosphere in europe: the surface   response analysis","stat.ap math.st stat.co stat.th","this paper is a continuation of the statistical modeling of the nonlinear relationship between atmospheric co2 and attributable variables that can account for emissions, based on data from eu countries, in order to compare the relevant findings to those obtained in the case of us data, in [1, 2]. the current study was initiated in [3], leading to the optimal second-order model, based on three linear terms and five second-order terms. we conclude this study in the present work, by finding the canonical decomposition of the nonlinear model, and by computing the specific two-dimensional confidence regions that it leads to. we then use the model in order to quantify the net effect of various risk factors, and compare to the results obtained in the us case.","","2013-12-30","","['iuliana teodorescu', 'chris tsokos']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE
"771",1401.01,"improving forecasting performance using covariate-dependent copula   models","stat.me stat.co","copulas provide an attractive approach for constructing multivariate distributions with flexible marginal distributions and different forms of dependences. of particular importance in many areas is the possibility of explicitly forecasting the tail-dependences. most of the available approaches are only able to estimate tail-dependences and correlations via nuisance parameters, but can neither be used for interpretation, nor for forecasting. aiming to improve copula forecasting performance, we propose a general bayesian approach for modeling and forecasting tail-dependences and correlations as explicit functions of covariates. the proposed covariate-dependent copula model also allows for bayesian variable selection among covariates from the marginal models as well as the copula density. the copulas we study include joe-clayton copula, clayton copula, gumbel copula and student's \emph{t}-copula. posterior inference is carried out using an efficient mcmc simulation method. our approach is applied to both simulated data and the s\&p 100 and s\&p 600 stock indices. the forecasting performance of the proposed approach is compared with other modeling strategies based on log predictive scores. value-at-risk evaluation is also preformed for model comparisons.","10.1016/j.ijforecast.2018.01.007","2013-12-31","2018-05-21","['feng li', 'yanfei kang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"772",1401.0104,"pso-mismo modeling strategy for multi-step-ahead time series prediction","cs.ai cs.lg cs.ne stat.ml","multi-step-ahead time series prediction is one of the most challenging research topics in the field of time series modeling and prediction, and is continually under research. recently, the multiple-input several multiple-outputs (mismo) modeling strategy has been proposed as a promising alternative for multi-step-ahead time series prediction, exhibiting advantages compared with the two currently dominating strategies, the iterated and the direct strategies. built on the established mismo strategy, this study proposes a particle swarm optimization (pso)-based mismo modeling strategy, which is capable of determining the number of sub-models in a self-adaptive mode, with varying prediction horizons. rather than deriving crisp divides with equal-size s prediction horizons from the established mismo, the proposed pso-mismo strategy, implemented with neural networks, employs a heuristic to create flexible divides with varying sizes of prediction horizons and to generate corresponding sub-models, providing considerable flexibility in model construction, which has been validated with simulated and real datasets.","10.1109/tcyb.2013.2265084","2013-12-31","","['yukun bao', 'tao xiong', 'zhongyi hu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"773",1401.0942,"factorized point process intensities: a spatial analysis of professional   basketball","stat.ml stat.ap","we develop a machine learning approach to represent and analyze the underlying spatial structure that governs shot selection among professional basketball players in the nba. typically, nba players are discussed and compared in an heuristic, imprecise manner that relies on unmeasured intuitions about player behavior. this makes it difficult to draw comparisons between players and make accurate player specific predictions. modeling shot attempt data as a point process, we create a low dimensional representation of offensive player types in the nba. using non-negative matrix factorization (nmf), an unsupervised dimensionality reduction technique, we show that a low-rank spatial decomposition summarizes the shooting habits of nba players. the spatial representations discovered by the algorithm correspond to intuitive descriptions of nba player types, and can be used to model other spatial effects, such as shooting accuracy.","","2014-01-05","2014-01-07","['andrew miller', 'luke bornn', 'ryan adams', 'kirk goldsberry']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"774",1401.1137,"sparse graphs using exchangeable random measures","stat.me cs.si math.st stat.ml stat.th","statistical network modeling has focused on representing the graph as a discrete structure, namely the adjacency matrix, and considering the exchangeability of this array. in such cases, the aldous-hoover representation theorem (aldous, 1981;hoover, 1979} applies and informs us that the graph is necessarily either dense or empty. in this paper, we instead consider representing the graph as a measure on $\mathbb{r}_+^2$. for the associated definition of exchangeability in this continuous space, we rely on the kallenberg representation theorem (kallenberg, 2005). we show that for certain choices of such exchangeable random measures underlying our graph construction, our network process is sparse with power-law degree distribution. in particular, we build on the framework of completely random measures (crms) and use the theory associated with such processes to derive important network properties, such as an urn representation for our analysis and network simulation. our theoretical results are explored empirically and compared to common network models. we then present a hamiltonian monte carlo algorithm for efficient exploration of the posterior distribution and demonstrate that we are able to recover graphs ranging from dense to sparse--and perform associated tests--based on our flexible crm-based formulation. we explore network properties in a range of real datasets, including facebook social circles, a political blogosphere, protein networks, citation networks, and world wide web networks, including networks with hundreds of thousands of nodes and millions of edges.","","2014-01-06","2015-03-27","['françois caron', 'emily b. fox']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE
"775",1401.1301,"covariance pattern mixture models for the analysis of multivariate   heterogeneous longitudinal data","stat.me stat.ap","we propose a novel approach for modeling multivariate longitudinal data in the presence of unobserved heterogeneity for the analysis of the health and retirement study (hrs) data. our proposal can be cast within the framework of linear mixed models with discrete individual random intercepts; however, differently from the standard formulation, the proposed covariance pattern mixture model (cpmm) does not require the usual local independence assumption. the model is thus able to simultaneously model the heterogeneity, the association among the responses and the temporal dependence structure. we focus on the investigation of temporal patterns related to the cognitive functioning in retired american respondents. in particular, we aim to understand whether it can be affected by some individual socio-economical characteristics and whether it is possible to identify some homogenous groups of respondents that share a similar cognitive profile. an accurate description of the detected groups allows government policy interventions to be opportunely addressed. results identify three homogenous clusters of individuals with specific cognitive functioning, consistent with the class conditional distribution of the covariates. the flexibility of cpmm allows for a different contribution of each regressor on the responses according to group membership. in so doing, the identified groups receive a global and accurate phenomenological characterization.","10.1214/15-aoas816","2014-01-07","2015-09-16","['laura anderlucci', 'cinzia viroli']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"776",1401.164,"quantifying intrinsic and extrinsic noise in gene transcription using   the linear noise approximation: an application to single cell data","stat.ap q-bio.qm","a central challenge in computational modeling of dynamic biological systems is parameter inference from experimental time course measurements. however, one would not only like to infer kinetic parameters but also study their variability from cell to cell. here we focus on the case where single-cell fluorescent protein imaging time series data are available for a population of cells. based on van kampen's linear noise approximation, we derive a dynamic state space model for molecular populations which is then extended to a hierarchical model. this model has potential to address the sources of variability relevant to single-cell data, namely, intrinsic noise due to the stochastic nature of the birth and death processes involved in reactions and extrinsic noise arising from the cell-to-cell variation of kinetic parameters. in order to infer such a model from experimental data, one must also quantify the measurement process where one has to allow for nonmeasurable molecular species as well as measurement noise of unknown level and variance. the availability of multiple single-cell time series data here provides a unique testbed to fit such a model and quantify these different sources of variation from experimental data.","10.1214/13-aoas669","2014-01-08","","['bärbel finkenstädt', 'dan j. woodcock', 'michal komorowski', 'claire v. harper', 'julian r. e. davis', 'mike r. h. white', 'david a. rand']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"777",1401.1915,"a new class of flexible link functions with application to species   co-occurrence in cape floristic region","stat.ap","understanding the mechanisms that allow biological species to co-occur is of great interest to ecologists. here we investigate the factors that influence co-occurrence of members of the genus protea in the cape floristic region of southwestern africa, a global hot spot of biodiversity. due to the binomial nature of our response, a critical issue is to choose appropriate link functions for the regression model. in this paper we propose a new family of flexible link functions for modeling binomial response data. by introducing a power parameter into the cumulative distribution function (c.d.f.) corresponding to a symmetric link function and its mirror reflection, greater flexibility in skewness can be achieved in both positive and negative directions. through simulated data sets and analysis of the protea co-occurrence data, we show that the proposed link function is quite flexible and performs better against link misspecification than standard link functions.","10.1214/13-aoas663","2014-01-09","","['xun jiang', 'dipak k. dey', 'rachel prunier', 'adam m. wilson', 'kent e. holsinger']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"778",1401.2163,"estimation of partially linear regression model under partial   consistency property","stat.me stat.co","in this paper, utilizing recent theoretical results in high dimensional statistical modeling, we propose a model-free yet computationally simple approach to estimate the partially linear model $y=x\beta+g(z)+\varepsilon$. motivated by the partial consistency phenomena, we propose to model $g(z)$ via incidental parameters. based on partitioning the support of $z$, a simple local average is used to estimate the response surface. the proposed method seeks to strike a balance between computation burden and efficiency of the estimators while minimizing model bias. computationally this approach only involves least squares. we show that given the inconsistent estimator of $g(z)$, a root $n$ consistent estimator of parametric component $\beta$ of the partially linear model can be obtained with little cost in efficiency. moreover, conditional on the $\beta$ estimates, an optimal estimator of $g(z)$ can then be obtained using classic nonparametric methods. the statistical inference problem regarding $\beta$ and a two-population nonparametric testing problem regarding $g(z)$ are considered. our results show that the behavior of test statistics are satisfactory. to assess the performance of our method in comparison with other methods, three simulation studies are conducted and a real dataset about risk factors of birth weights is analyzed.","","2014-01-09","","['xia cui', 'ying lu', 'heng peng']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"779",1401.2344,"exploiting multiple outcomes in bayesian principal stratification   analysis with application to the evaluation of a job training program","stat.ap","the causal effect of a randomized job training program, the jobs ii study, on trainees' depression is evaluated. principal stratification is used to deal with noncompliance to the assigned treatment. due to the latent nature of the principal strata, strong structural assumptions are often invoked to identify principal causal effects. alternatively, distributional assumptions may be invoked using a model-based approach. these often lead to weakly identified models with substantial regions of flatness in the posterior distribution of the causal effects. information on multiple outcomes is routinely collected in practice, but is rarely used to improve inference. this article develops a bayesian approach to exploit multivariate outcomes to sharpen inferences in weakly identified principal stratification models. we show that inference for the causal effect on depression is significantly improved by using the re-employment status as a secondary outcome in the jobs ii study. simulation studies are also performed to illustrate the potential gains in the estimation of principal causal effects from jointly modeling more than one outcome. this approach can also be used to assess plausibility of structural assumptions and sensitivity to deviations from these structural assumptions. two model checking procedures via posterior predictive checks are also discussed.","10.1214/13-aoas674","2014-01-10","","['alessandra mattei', 'fan li', 'fabrizia mealli']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"780",1401.2503,"does restraining end effect matter in emd-based modeling framework for   time series prediction? some experimental evidences","cs.ai stat.ap","following the ""decomposition-and-ensemble"" principle, the empirical mode decomposition (emd)-based modeling framework has been widely used as a promising alternative for nonlinear and nonstationary time series modeling and prediction. the end effect, which occurs during the sifting process of emd and is apt to distort the decomposed sub-series and hurt the modeling process followed, however, has been ignored in previous studies. addressing the end effect issue, this study proposes to incorporate end condition methods into emd-based decomposition and ensemble modeling framework for one- and multi-step ahead time series prediction. four well-established end condition methods, mirror method, coughlin's method, slope-based method, and rato's method, are selected, and support vector regression (svr) is employed as the modeling technique. for the purpose of justification and comparison, well-known nn3 competition data sets are used and four well-established prediction models are selected as benchmarks. the experimental results demonstrated that significant improvement can be achieved by the proposed emd-based svr models with end condition methods. the emd-sbm-svr model and emd-rato-svr model, in particular, achieved the best prediction performances in terms of goodness of forecast measures and equality of accuracy of competing forecasts test.","10.1016/j.neucom.2013.07.004","2014-01-11","","['tao xiong', 'yukun bao', 'zhongyi hu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"781",1401.2504,"multi-step-ahead time series prediction using multiple-output support   vector regression","cs.lg stat.ml","accurate time series prediction over long future horizons is challenging and of great interest to both practitioners and academics. as a well-known intelligent algorithm, the standard formulation of support vector regression (svr) could be taken for multi-step-ahead time series prediction, only relying either on iterated strategy or direct strategy. this study proposes a novel multiple-step-ahead time series prediction approach which employs multiple-output support vector regression (m-svr) with multiple-input multiple-output (mimo) prediction strategy. in addition, the rank of three leading prediction strategies with svr is comparatively examined, providing practical implications on the selection of the prediction strategy for multi-step-ahead forecasting while taking svr as modeling technique. the proposed approach is validated with the simulated and real datasets. the quantitative and comprehensive assessments are performed on the basis of the prediction accuracy and computational cost. the results indicate that: 1) the m-svr using mimo strategy achieves the best accurate forecasts with accredited computational load, 2) the standard svr using direct strategy achieves the second best accurate forecasts, but with the most expensive computational cost, and 3) the standard svr using iterated strategy is the worst in terms of prediction accuracy, but with the least computational cost.","10.1016/j.neucom.2013.09.010","2014-01-11","","['yukun bao', 'tao xiong', 'zhongyi hu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"782",1401.2728,"a semiparametric approach to mixed outcome latent variable models:   estimating the association between cognition and regional brain volumes","stat.ap","multivariate data that combine binary, categorical, count and continuous outcomes are common in the social and health sciences. we propose a semiparametric bayesian latent variable model for multivariate data of arbitrary type that does not require specification of conditional distributions. drawing on the extended rank likelihood method by hoff [ann. appl. stat. 1 (2007) 265-283], we develop a semiparametric approach for latent variable modeling with mixed outcomes and propose associated markov chain monte carlo estimation methods. motivated by cognitive testing data, we focus on bifactor models, a special case of factor analysis. we employ our semiparametric bayesian latent variable model to investigate the association between cognitive outcomes and mri-measured regional brain volumes.","10.1214/13-aoas675","2014-01-13","","['jonathan gruhl', 'elena a. erosheva', 'paul k. crane']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"783",1401.2957,"bayesian analysis for a class of beta mixed models","stat.ap stat.co","generalized linear mixed models (glmm) encompass large class of statistical models, with a vast range of applications areas. glmm extends the linear mixed models allowing for different types of response variable. three most common data types are continuous, counts and binary and standard distributions for these types of response variables are gaussian, poisson and binomial, respectively. despite that flexibility, there are situations where the response variable is continuous, but bounded, such as rates, percentages, indexes and proportions. in such situations the usual glmm's are not adequate because bounds are ignored and the beta distribution can be used. likelihood and bayesian inference for beta mixed models are not straightforward demanding a computational overhead. recently, a new algorithm for bayesian inference called inla (integrated nested laplace approximation) was proposed.inla allows computation of many bayesian glmms in a reasonable amount time allowing extensive comparison among models. we explore bayesian inference for beta mixed models by inla. we discuss the choice of prior distributions, sensitivity analysis and model selection measures through a real data set. the results obtained from inla are compared with those obtained by an mcmc algorithm and likelihood analysis. we analyze data from an study on a life quality index of industry workers collected according to a hierarchical sampling scheme. results show that the inla approach is suitable and faster to fit the proposed beta mixed models producing results similar to alternative algorithms and with easier handling of modeling alternatives. sensitivity analysis, measures of goodness of fit and model choice are discussed.","","2014-01-13","2014-02-10","['wagner hugo bonat', 'paulo justiniano ribeiro', 'silvia emiko shimakura']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE
"784",1401.3211,"modeling light curves for improved classification","stat.ap astro-ph.im","many synoptic surveys are observing large parts of the sky multiple times. the resulting lightcurves provide a wonderful window to the dynamic nature of the universe. however, there are many significant challenges in analyzing these light curves. these include heterogeneity of the data, irregularly sampled data, missing data, censored data, known but variable measurement errors, and most importantly, the need to classify in astronomical objects in real time using these imperfect light curves. we describe a modeling-based approach using gaussian process regression for generating critical measures representing features for the classification of such lightcurves. we demonstrate that our approach performs better by comparing it with past methods. finally, we provide future directions for use in sky-surveys that are getting even bigger by the day.","10.1002/sam.11305","2014-01-14","2014-07-01","['julian faraway', 'ashish mahabal', 'jiayang sun', 'xiaofeng wang', 'n/a yi', 'n/a wang', 'lingsong zhang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"785",1401.3409,"low-rank modeling and its applications in image analysis","cs.cv cs.lg stat.ml","low-rank modeling generally refers to a class of methods that solve problems by representing variables of interest as low-rank matrices. it has achieved great success in various fields including computer vision, data mining, signal processing and bioinformatics. recently, much progress has been made in theories, algorithms and applications of low-rank modeling, such as exact low-rank matrix recovery via convex programming and matrix completion applied to collaborative filtering. these advances have brought more and more attentions to this topic. in this paper, we review the recent advance of low-rank modeling, the state-of-the-art algorithms, and related applications in image analysis. we first give an overview to the concept of low-rank modeling and challenging problems in this area. then, we summarize the models and algorithms for low-rank matrix recovery and illustrate their advantages and limitations with numerical experiments. next, we introduce a few applications of low-rank modeling in the context of image analysis. finally, we conclude this paper with some discussions.","","2014-01-14","2014-10-22","['xiaowei zhou', 'can yang', 'hongyu zhao', 'weichuan yu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"786",1401.4425,"monte carlo simulation for lasso-type problems by estimator augmentation","stat.me stat.ml","regularized linear regression under the $\ell_1$ penalty, such as the lasso, has been shown to be effective in variable selection and sparse modeling. the sampling distribution of an $\ell_1$-penalized estimator $\hat{\beta}$ is hard to determine as the estimator is defined by an optimization problem that in general can only be solved numerically and many of its components may be exactly zero. let $s$ be the subgradient of the $\ell_1$ norm of the coefficient vector $\beta$ evaluated at $\hat{\beta}$. we find that the joint sampling distribution of $\hat{\beta}$ and $s$, together called an augmented estimator, is much more tractable and has a closed-form density under a normal error distribution in both low-dimensional ($p\leq n$) and high-dimensional ($p>n$) settings. given $\beta$ and the error variance $\sigma^2$, one may employ standard monte carlo methods, such as markov chain monte carlo and importance sampling, to draw samples from the distribution of the augmented estimator and calculate expectations with respect to the sampling distribution of $\hat{\beta}$. we develop a few concrete monte carlo algorithms and demonstrate with numerical examples that our approach may offer huge advantages and great flexibility in studying sampling distributions in $\ell_1$-penalized linear regression. we also establish nonasymptotic bounds on the difference between the true sampling distribution of $\hat{\beta}$ and its estimator obtained by plugging in estimated parameters, which justifies the validity of monte carlo simulation from an estimated sampling distribution even when $p\gg n\to \infty$.","10.1080/01621459.2014.946035","2014-01-17","2014-07-13","['qing zhou']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"787",1401.4718,"bayesian inference from non-ignorable network sampling designs","stat.me","consider a population of individuals and a network that encodes social connections among them. we are interested in making inference on finite population and super-population estimands that are a function of both individuals' responses and of the network, from a sample. neither the sampling frame nor the network are available. however, the sampling mechanism implicitly leverages the network to recruit individuals, thus partially revealing social interactions among the individuals in the sample, as well as their responses. this is a common setting that arises, for instance, in epidemiology and healthcare, where samples from hard-to-reach populations are collected using link-tracing mechanisms, including respondent-driven sampling. in this paper, we study statistical properties of popular network sampling mechanisms. we formulate the estimation problem in terms of rubin's inferential framework to explicitly account for social network structure. we then identify key modeling elements that lead to inferences with good frequentist properties when dealing with data collected through non-ignorable network sampling mechanisms. we demonstrate these methods on a study of the incidence of hiv in brazil","","2014-01-19","2016-12-09","['simon lunagomez', 'edoardo airoldi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"788",1401.5343,"clustering south african households based on their asset status using   latent variable models","stat.ap stat.me","the agincourt health and demographic surveillance system has since 2001 conducted a biannual household asset survey in order to quantify household socio-economic status (ses) in a rural population living in northeast south africa. the survey contains binary, ordinal and nominal items. in the absence of income or expenditure data, the ses landscape in the study population is explored and described by clustering the households into homogeneous groups based on their asset status. a model-based approach to clustering the agincourt households, based on latent variable models, is proposed. in the case of modeling binary or ordinal items, item response theory models are employed. for nominal survey items, a factor analysis model, similar in nature to a multinomial probit model, is used. both model types have an underlying latent variable structure - this similarity is exploited and the models are combined to produce a hybrid model capable of handling mixed data types. further, a mixture of the hybrid models is considered to provide clustering capabilities within the context of mixed binary, ordinal and nominal response data. the proposed model is termed a mixture of factor analyzers for mixed data (mfa-md). the mfa-md model is applied to the survey data to cluster the agincourt households into homogeneous groups. the model is estimated within the bayesian paradigm, using a markov chain monte carlo algorithm. intuitive groupings result, providing insight to the different socio-economic strata within the agincourt region.","10.1214/14-aoas726","2014-01-21","2014-07-31","['damien mcparland', 'isobel claire gormley', 'tyler h. mccormick', 'samuel j. clark', 'chodziwadziwa whiteson kabudula', 'mark a. collinson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"789",1401.5375,"iterative regularization for ensemble data assimilation in reservoir   models","math.na stat.ap","we propose the application of iterative regularization for the development of ensemble methods for solving bayesian inverse problems. in concrete, we construct (i) a variational iterative regularizing ensemble levenberg-marquardt method (ir-enlm) and (ii) a derivative-free iterative ensemble kalman smoother (ir-es). the aim of these methods is to provide a robust ensemble approximation of the bayesian posterior. the proposed methods are based on fundamental ideas from iterative regularization methods that have been widely used for the solution of deterministic inverse problems [21]. in this work we are interested in the application of the proposed ensemble methods for the solution of bayesian inverse problems that arise in reservoir modeling applications. the proposed ensemble methods use key aspects of the regularizing levenberg-marquardt scheme developed by hanke [16] and that we recently applied for history matching in [18].   in the case where the forward operator is linear and the prior is gaussian, we show that the proposed ir-enlm and ir-es coincide with standard randomized maximum likelihood (rml) and the ensemble smoother (es) respectively. for the general nonlinear case, we develop a numerical framework to assess the performance of the proposed ensemble methods at capturing the posterior. this framework consists of using a state-of-the art mcmc method for resolving the bayesian posterior from synthetic experiments. the resolved posterior via mcmc then provides a gold standard against to which compare the proposed ir-enlm and ir-es. we show that for the careful selection of regularization parameters, robust approximations of the posterior can be accomplished in terms of mean and variance. our numerical experiments showcase the advantage of using iterative regularization for obtaining more robust and stable approximation of the posterior than standard unregularized methods.","","2014-01-21","2014-06-24","['marco a. iglesias']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"790",1401.5833,"multiscale dictionary learning: non-asymptotic bounds and robustness","math.st stat.th","high-dimensional datasets are well-approximated by low-dimensional structures. over the past decade, this empirical observation motivated the investigation of detection, measurement, and modeling techniques to exploit these low-dimensional intrinsic structures, yielding numerous implications for high-dimensional statistics, machine learning, and signal processing. manifold learning (where the low-dimensional structure is a manifold) and dictionary learning (where the low-dimensional structure is the set of sparse linear combinations of vectors from a finite dictionary) are two prominent theoretical and computational frameworks in this area. despite their ostensible distinction, the recently-introduced geometric multi-resolution analysis (gmra) provides a robust, computationally efficient, multiscale procedure for simultaneously learning manifolds and dictionaries.   in this work, we prove non-asymptotic probabilistic bounds on the approximation error of gmra for a rich class of data-generating statistical models that includes ""noisy"" manifolds, thereby establishing the theoretical robustness of the procedure and confirming empirical observations. in particular, if a dataset aggregates near a low-dimensional manifold, our results show that the approximation error of the gmra is completely independent of the ambient dimension. our work therefore establishes gmra as a provably fast algorithm for dictionary learning with approximation and sparsity guarantees. we include several numerical experiments confirming these theoretical results, and our theoretical framework provides new tools for assessing the behavior of manifold learning and dictionary learning procedures on a large class of interesting models.","","2014-01-22","2015-12-13","['mauro maggioni', 'stanislav minsker', 'nate strawn']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"791",1401.59,"gaussian-binary restricted boltzmann machines on modeling natural image   statistics","cs.ne cs.lg stat.ml","we present a theoretical analysis of gaussian-binary restricted boltzmann machines (grbms) from the perspective of density models. the key aspect of this analysis is to show that grbms can be formulated as a constrained mixture of gaussians, which gives a much better insight into the model's capabilities and limitations. we show that grbms are capable of learning meaningful features both in a two-dimensional blind source separation task and in modeling natural images. further, we show that reported difficulties in training grbms are due to the failure of the training algorithm rather than the model itself. based on our analysis we are able to propose several training recipes, which allowed successful and fast training in our experiments. finally, we discuss the relationship of grbms to several modifications that have been proposed to improve the model.","10.1371/journal.pone.0171015","2014-01-23","","['nan wang', 'jan melchior', 'laurenz wiskott']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"792",1401.6145,"on stochastic geometry modeling of cellular uplink transmission with   truncated channel inversion power control","cs.it cs.ni math.it math.st stat.th","using stochastic geometry, we develop a tractable uplink modeling paradigm for outage probability and spectral efficiency in both single and multi-tier cellular wireless networks. the analysis accounts for per user equipment (ue) power control as well as the maximum power limitations for ues. more specifically, for interference mitigation and robust uplink communication, each ue is required to control its transmit power such that the average received signal power at its serving base station (bs) is equal to a certain threshold $\rho_o$. due to the limited transmit power, the ues employ a truncated channel inversion power control policy with a cutoff threshold of $\rho_o$. we show that there exists a transfer point in the uplink system performance that depends on the tuple: bs intensity ($\lambda$), maximum transmit power of ues ($p_u$), and $\rho_o$. that is, when $p_u$ is a tight operational constraint with respect to [w.r.t.] $\lambda$ and $\rho_o$, the uplink outage probability and spectral efficiency highly depend on the values of $\lambda$ and $\rho_o$. in this case, there exists an optimal cutoff threshold $\rho^*_o$, which depends on the system parameters, that minimizes the outage probability. on the other hand, when $p_u$ is not a binding operational constraint w.r.t. $\lambda$ and $\rho_o$, the uplink outage probability and spectral efficiency become independent of $\lambda$ and $\rho_o$. we obtain approximate yet accurate simple expressions for outage probability and spectral efficiency which reduce to closed-forms in some special cases.","","2014-01-23","","['hesham elsawy', 'ekram hossain']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"793",1401.6169,"parsimonious topic models with salient word discovery","cs.lg cs.cl cs.ir stat.ml","we propose a parsimonious topic model for text corpora. in related models such as latent dirichlet allocation (lda), all words are modeled topic-specifically, even though many words occur with similar frequencies across different topics. our modeling determines salient words for each topic, which have topic-specific probabilities, with the rest explained by a universal shared model. further, in lda all topics are in principle present in every document. by contrast our model gives sparse topic representation, determining the (small) subset of relevant topics for each document. we derive a bayesian information criterion (bic), balancing model complexity and goodness of fit. here, interestingly, we identify an effective sample size and corresponding penalty specific to each parameter type in our model. we minimize bic to jointly determine our entire model -- the topic-specific words, document-specific topics, all model parameter values, {\it and} the total number of topics -- in a wholly unsupervised fashion. results on three text corpora and an image dataset show that our model achieves higher test set likelihood and better agreement with ground-truth class labels, compared to lda and to a model designed to incorporate sparsity.","10.1109/tkde.2014.2345378","2014-01-22","2014-09-11","['hossein soleimani', 'david j. miller']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"794",1401.6595,"regularized brain reading with shrinkage and smoothing","stat.ap","functional neuroimaging measures how the brain responds to complex stimuli. however, sample sizes are modest, noise is substantial, and stimuli are high dimensional. hence, direct estimates are inherently imprecise and call for regularization. we compare a suite of approaches which regularize via shrinkage: ridge regression, the elastic net (a generalization of ridge regression and the lasso), and a hierarchical bayesian model based on small area estimation (sae). we contrast regularization with spatial smoothing and combinations of smoothing and shrinkage. all methods are tested on functional magnetic resonance imaging (fmri) data from multiple subjects participating in two different experiments related to reading, for both predicting neural response to stimuli and decoding stimuli from responses. interestingly, when the regularization parameters are chosen by cross-validation independently for every voxel, low/high regularization is chosen in voxels where the classification accuracy is high/low, indicating that the regularization intensity is a good tool for identification of relevant voxels for the cognitive task. surprisingly, all the regularization methods work about equally well, suggesting that beating basic smoothing and shrinkage will take not only clever methods, but also careful modeling.","10.1214/15-aoas837","2014-01-25","2016-02-04","['leila wehbe', 'aaditya ramdas', 'rebecca c. steorts', 'cosma rohilla shalizi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"795",1401.7116,"bayesian properties of normalized maximum likelihood and its fast   computation","cs.it cs.lg math.it stat.ml","the normalized maximized likelihood (nml) provides the minimax regret solution in universal data compression, gambling, and prediction, and it plays an essential role in the minimum description length (mdl) method of statistical modeling and estimation. here we show that the normalized maximum likelihood has a bayes-like representation as a mixture of the component models, even in finite samples, though the weights of linear combination may be both positive and negative. this representation addresses in part the relationship between mdl and bayes modeling. this representation has the advantage of speeding the calculation of marginals and conditionals required for coding and prediction applications.","","2014-01-28","","['andrew barron', 'teemu roos', 'kazuho watanabe']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"796",1401.7195,"estimation for the linear model with uncertain covariance matrices","math.st stat.th","we derive a maximum a posteriori estimator for the linear observation model, where the signal and noise covariance matrices are both uncertain. the uncertainties are treated probabilistically by modeling the covariance matrices with prior inverse-wishart distributions. the nonconvex problem of jointly estimating the signal of interest and the covariance matrices is tackled by a computationally efficient fixed-point iteration as well as an approximate variational bayes solution. the statistical performance of estimators is compared numerically to state-of-the-art estimators from the literature and shown to perform favorably.","10.1109/tsp.2014.2301973","2014-01-28","","['dave zachariah', 'nafiseh shariati', 'mats bengtsson', 'magnus jansson', 'saikat chatterjee']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"797",1401.7278,"minimax-optimal nonparametric regression in high dimensions","math.st stat.th","minimax $l_2$ risks for high-dimensional nonparametric regression are derived under two sparsity assumptions: (1) the true regression surface is a sparse function that depends only on $d=o(\log n)$ important predictors among a list of $p$ predictors, with $\log p=o(n)$; (2) the true regression surface depends on $o(n)$ predictors but is an additive function where each additive component is sparse but may contain two or more interacting predictors and may have a smoothness level different from other components. for either modeling assumption, a practicable extension of the widely used bayesian gaussian process regression method is shown to adaptively attain the optimal minimax rate (up to $\log n$ terms) asymptotically as both $n,p\to\infty$ with $\log p=o(n)$.","10.1214/14-aos1289","2014-01-28","2015-04-01","['yun yang', 'surya t. tokdar']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"798",1401.762,"bayesian nonparametric comorbidity analysis of psychiatric disorders","stat.ml cs.lg","the analysis of comorbidity is an open and complex research field in the branch of psychiatry, where clinical experience and several studies suggest that the relation among the psychiatric disorders may have etiological and treatment implications. in this paper, we are interested in applying latent feature modeling to find the latent structure behind the psychiatric disorders that can help to examine and explain the relationships among them. to this end, we use the large amount of information collected in the national epidemiologic survey on alcohol and related conditions (nesarc) database and propose to model these data using a nonparametric latent model based on the indian buffet process (ibp). due to the discrete nature of the data, we first need to adapt the observation model for discrete random variables. we propose a generative model in which the observations are drawn from a multinomial-logit distribution given the ibp matrix. the implementation of an efficient gibbs sampler is accomplished using the laplace approximation, which allows integrating out the weighting factors of the multinomial-logit likelihood model. we also provide a variational inference algorithm for this model, which provides a complementary (and less expensive in terms of computational complexity) alternative to the gibbs sampler allowing us to deal with a larger number of data. finally, we use the model to analyze comorbidity among the psychiatric disorders diagnosed by experts from the nesarc database.","","2014-01-29","","['francisco j. r. ruiz', 'isabel valera', 'carlos blanco', 'fernando perez-cruz']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"799",1401.7686,"a millennium bug still bites public health - an illustration using   cancer mortality","stat.me","accurate estimation of cancer mortality rates and the comparison across cancer sites, populations or time periods is crucial to public health, as identification of vulnerable groups who suffer the most from these diseases may lead to efficient cancer care and control with timely treatment. because cancer mortality rate varies with age, comparisons require age-standardization using a reference population. the current method of using the year 2000 population standard is standard practice, but serious concerns have been raised about its lack of justification. we have found that using the us year 2000 population standard as reference overestimates prostate cancer mortality rates by 12-91% during the period 1970-2009 across all six sampled u.s. states, and also underestimates case fatality rates by 9-78% across six cancer sites, including female breast, cervix, prostate, lung, leukemia and colon-rectum. we develop a mean reference population method to minimize the bias using mathematical optimization theory and statistical modeling. the method corrects the bias to the largest extent in terms of squared loss and can be applied broadly to studies of many diseases.","","2014-01-29","","['martina fu', 'david todem', 'wenjiang j. fu', 'shuangge ma']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"800",1401.8189,"generalized gaussian process regression model for non-gaussian   functional data","stat.me","in this paper we propose a generalized gaussian process concurrent regression model for functional data where the functional response variable has a binomial, poisson or other non-gaussian distribution from an exponential family while the covariates are mixed functional and scalar variables. the proposed model offers a nonparametric generalized concurrent regression method for functional data with multi-dimensional covariates, and provides a natural framework on modeling common mean structure and covariance structure simultaneously for repeatedly observed functional data. the mean structure provides an overall information about the observations, while the covariance structure can be used to catch up the characteristic of each individual batch. the prior specification of covariance kernel enables us to accommodate a wide class of nonlinear models. the definition of the model, the inference and the implementation as well as its asymptotic properties are discussed. several numerical examples with different non-gaussian response variables are presented. some technical details and more numerical examples as well as an extension of the model are provided as supplementary materials.","","2014-01-31","","['bo wang', 'jian qing shi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"801",1401.8236,"scalable rejection sampling for bayesian hierarchical models","stat.me stat.co","bayesian hierarchical modeling is a popular approach to capturing unobserved heterogeneity across individual units. however, standard estimation methods such as markov chain monte carlo (mcmc) can be impracticable for modeling outcomes from a large number of units. we develop a new method to sample from posterior distributions of bayesian models, without using mcmc. samples are independent, so they can be collected in parallel, and we do not need to be concerned with issues like chain convergence and autocorrelation. the algorithm is scalable under the weak assumption that individual units are conditionally independent, making it applicable for large datasets. it can also be used to compute marginal likelihoods.","","2014-01-31","2014-11-01","['michael braun', 'paul damien']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"802",1402.1128,"long short-term memory based recurrent neural network architectures for   large vocabulary speech recognition","cs.ne cs.cl cs.lg stat.ml","long short-term memory (lstm) is a recurrent neural network (rnn) architecture that has been designed to address the vanishing and exploding gradient problems of conventional rnns. unlike feedforward neural networks, rnns have cyclic connections making them powerful for modeling sequences. they have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. however, in contrast to the deep neural networks, the use of rnns in speech recognition has been limited to phone recognition in small scale tasks. in this paper, we present novel lstm based rnn architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. we train and compare lstm, rnn and dnn models at various numbers of parameters and configurations. we show that lstm models converge quickly and give state of the art speech recognition performance for relatively small sized models.","","2014-02-05","","['haşim sak', 'andrew senior', 'françoise beaufays']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"803",1402.1138,"skew-gaussian random fields","stat.me","skewness is often present in a wide range of spatial prediction problems, and modeling it in the spatial context remains a challenging problem. in this study a skew-gaussian random field is considered. the skew-gaussian random field is constructed by using the multivariate closed skew-normal distribution, which is a generalization of the traditional normal distribution. we present a metropolis-hastings algorithm for simulating realizations efficiently from the random field, and an algorithm for estimating parameters by maximum likelihood with a monte carlo approximation of the likelihood. we demonstrate and evaluate the algorithms on synthetic cases. the skewness in the skew-gaussian random field is found to be strongly influenced by the spatial correlation in the field, and the parameter estimators appear as consistent with increasing size of the random field. moreover, we use the closed skew-normal distribution in a multivariate random field predictive setting on real seismic data from the sleipner field in the north sea.","","2014-02-05","","['kjartan rimstad', 'henning omre']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"804",1402.1253,"an ensemble kushner-stratonovich (enks) nonlinear filter: additive   particle updates in non-iterative and iterative forms","stat.me","despite the cheap availability of computing resources enabling faster monte carlo simulations, the potential benefits of particle filtering in revealing accurate statistical information on the imprecisely known model parameters or modeling errors of dynamical systems, based on limited time series data, have not been quite realized. a major numerical bottleneck precipitating this under-performance, especially for higher dimensional systems, is the progressive particle impoverishment owing to weight collapse and the aim of the current work is to address this problem by replacing weight-based updates through additive ones. thus, in the context of nonlinear filtering problems, a novel additive particle update scheme, in its non-iterative and iterative forms, is proposed based on manipulations of the innovation integral in the governing kushner-stratonovich equation. numerical evidence for the identification of nonlinear and large dimensional dynamical systems indicates a substantively superior performance of the non- iterative version of the enks vis-\`a-vis most existing filters. the costlier iterative version, though conceptually elegant, mostly appears to effect a marginal improvement in the reconstruction accuracy over its non-iterative counterpart. prominent in the reported numerical comparisons are variants of the ensemble kalman filter (enkf) that also use additive updates, albeit with many inherent limitations of a kalman filter.","","2014-02-06","","['saikat sarkar', 'debasish roy']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"805",1402.1835,"a model-free estimation for the covariate-adjusted youden index and its   associated cut-point","stat.ap","in medical research, continuous markers are widely employed in diagnostic tests to distinguish diseased and non-diseased subjects. the accuracy of such diagnostic tests is commonly assessed using the receiver operating characteristic (roc) curve. to summarize an roc curve and determine its optimal cut-point, the youden index is popularly used. in literature, estimation of the youden index has been widely studied via various statistical modeling strategies on the conditional density. this paper proposes a new model-free estimation method, which directly estimates the covariate-adjusted cut-point without estimating the conditional density. consequently, covariate-adjusted youden index can be estimated based on the estimated cutpoint. the proposed method formulates the estimation problem in a large margin classification framework, which allows flexible modeling of the covariate-adjusted youden index through kernel machines. the advantage of the proposed method is demonstrated in a variety of simulated experiments as well as a real application to pima indians diabetes study.","","2014-02-08","","['tu xu', 'junhui wang', 'yixin fang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"806",1402.1909,"a bayesian nonparametric hypothesis testing approach for regression   discontinuity designs","stat.me","the regression discontinuity (rd) design is a popular approach to causal inference in non-randomized studies. this is because it can be used to identify and estimate causal effects under mild conditions. specifically, for each subject, the rd design assigns a treatment or non-treatment, depending on whether or not an observed value of an assignment variable exceeds a fixed and known cutoff value.   in this paper, we propose a bayesian nonparametric regression modeling approach to rd designs, which exploits a local randomization feature. in this approach, the assignment variable is treated as a covariate, and a scalar-valued confounding variable is treated as a dependent variable (which may be a multivariate confounder score). then, over the model's posterior distribution of locally-randomized subjects that cluster around the cutoff of the assignment variable, inference for causal effects are made within this random cluster, via two-group statistical comparisons of treatment outcomes and non-treatment outcomes.   we illustrate the bayesian nonparametric approach through the analysis of a real educational data set, to investigate the causal link between basic skills and teaching ability.","","2014-02-08","","['george karabatsos', 'stephen g. walker']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"807",1402.192,"degrees of freedom and model search","math.st stat.th","degrees of freedom is a fundamental concept in statistical modeling, as it provides a quantitative description of the amount of fitting performed by a given procedure. but, despite this fundamental role in statistics, its behavior not completely well-understood, even in some fairly basic settings. for example, it may seem intuitively obvious that the best subset selection fit with subset size k has degrees of freedom larger than k, but this has not been formally verified, nor has is been precisely studied. in large part, the current paper is motivated by this particular problem, and we derive an exact expression for the degrees of freedom of best subset selection in a restricted setting (orthogonal predictor variables). along the way, we develop a concept that we name ""search degrees of freedom""; intuitively, for adaptive regression procedures that perform variable selection, this is a part of the (total) degrees of freedom that we attribute entirely to the model selection mechanism. finally, we establish a modest extension of stein's formula to cover discontinuous functions, and discuss its potential role in degrees of freedom and search degrees of freedom calculations.","","2014-02-09","2014-11-28","['ryan j. tibshirani']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"808",1402.2734,"graph-based multivariate conditional autoregressive models","stat.me","the conditional autoregressive model is a routinely used statistical model for areal data that arise from, for instances, epidemiological, socio-economic or ecological studies. various multivariate conditional autoregressive models have also been extensively studied in the literature and it has been shown that extending from the univariate case to the multivariate case is not trivial. the difficulties lie in many aspects, including validity, interpretability, flexibility and computational feasibility of the model. in this paper, we approach the multivariate modeling from an element-based perspective instead of the traditional vector-based perspective. we focus on the joint adjacency structure of elements and discuss graphical structures for both the spatial and non-spatial domains. we assume that the graph for the spatial domain is generally known and fixed while the graph for the non-spatial domain can be unknown and random. we propose a very general specification for the multivariate conditional modeling and then focus on three special cases, which are linked to well known models in the literature. bayesian inference for parameter learning and graph learning is provided for the focused cases, and finally, an example with public health data is illustrated.","","2014-02-11","2019-07-20","['ye liang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"809",1402.2755,"imprecise dirichlet process with application to the hypothesis test on   the probability that x< y","math.st stat.me stat.th","the dirichlet process (dp) is one of the most popular bayesian nonparametric models. an open problem with the dp is how to choose its infinite dimensional parameter (base measure) in case of lack of prior information. in this work we present the imprecise dp (idp) -- a prior near-ignorance dp-based model that does not require any choice of this probability measure. it consists of a class of dps obtained by letting the normalized base measure of the dp vary in the set of all probability measures. we discuss the tight connections of this approach with bayesian robustness and in particular prior near-ignorance modeling via sets of probabilities. we use this model to perform a bayesian hypothesis test on the probability p(x<y). we study the theoretical properties of the idp test (e.g., asymptotic consistency), and compare it with the frequentist mann-whitney-wilcoxon rank test that is commonly employed as a test on p(x< y). in particular we will show that our method is more robust, in the sense that it is able to isolate instances in which the aforementioned test is virtually guessing at random.","","2014-02-12","2014-02-20","['alessio benavoli', 'francesca mangili', 'fabrizio ruggeri', 'marco zaffalon']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"810",1402.2905,"multiple quantitative trait analysis using bayesian networks","stat.me q-bio.mn","models for genome-wide prediction and association studies usually target a single phenotypic trait. however, in animal and plant genetics it is common to record information on multiple phenotypes for each individual that will be genotyped. modeling traits individually disregards the fact that they are most likely associated due to pleiotropy and shared biological basis, thus providing only a partial, confounded view of genetic effects and phenotypic interactions. in this paper we use data from a multiparent advanced generation inter-cross (magic) winter wheat population to explore bayesian networks as a convenient and interpretable framework for the simultaneous modeling of multiple quantitative traits. we show that they are equivalent to multivariate genetic best linear unbiased prediction (gblup), and that they are competitive with single-trait elastic net and single-trait gblup in predictive performance. finally, we discuss their relationship with other additive-effects models and their advantages in inference and interpretation. magic populations provide an ideal setting for this kind of investigation because the very low population structure and large sample size result in predictive models with good power and limited confounding due to relatedness.","","2014-02-12","2014-12-07","['marco scutari', 'phil howell', 'david j. balding', 'ian mackay']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"811",1402.3085,"gaussian process volatility model","stat.me stat.ml","the accurate prediction of time-changing variances is an important task in the modeling of financial data. standard econometric models are often limited as they assume rigid functional relationships for the variances. moreover, function parameters are usually learned using maximum likelihood, which can lead to overfitting. to address these problems we introduce a novel model for time-changing variances using gaussian processes. a gaussian process (gp) defines a distribution over functions, which allows us to capture highly flexible functional relationships for the variances. in addition, we develop an online algorithm to perform inference. the algorithm has two main advantages. first, it takes a bayesian approach, thereby avoiding overfitting. second, it is much quicker than current offline inference procedures. finally, our new model was evaluated on financial data and showed significant improvement in predictive performance over current standard models.","","2014-02-13","","['yue wu', 'jose miguel hernandez lobato', 'zoubin ghahramani']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"812",1402.3093,"bayesian nonparametric dependent model for partially replicated data:   the influence of fuel spills on species diversity","math.st stat.me stat.th","we introduce a dependent bayesian nonparametric model for the probabilistic modeling of membership of subgroups in a community based on partially replicated data. the focus here is on species-by-site data, i.e. community data where observations at different sites are classified in distinct species. our aim is to study the impact of additional covariates, for instance environmental variables, on the data structure, and in particular on the community diversity. to that purpose, we introduce dependence a priori across the covariates, and show that it improves posterior inference. we use a dependent version of the griffiths-engen-mccloskey distribution defined via the stick-breaking construction. this distribution is obtained by transforming a gaussian process whose covariance function controls the desired dependence. the resulting posterior distribution is sampled by markov chain monte carlo. we illustrate the application of our model to a soil microbial dataset acquired across a hydrocarbon contamination gradient at the site of a fuel spill in antarctica. this method allows for inference on a number of quantities of interest in ecotoxicology, such as diversity or effective concentrations, and is broadly applicable to the general problem of communities response to environmental variables.","10.1214/16-aoas944","2014-02-13","2015-07-27","['julyan arbel', 'kerrie mengersen', 'judith rousseau']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"813",1402.3722,"word2vec explained: deriving mikolov et al.'s negative-sampling   word-embedding method","cs.cl cs.lg stat.ml","the word2vec software of tomas mikolov and colleagues (https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and provides state-of-the-art word embeddings. the learning models behind the software are described in two research papers. we found the description of the models in these papers to be somewhat cryptic and hard to follow. while the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations.   this note is an attempt to explain equation (4) (negative sampling) in ""distributed representations of words and phrases and their compositionality"" by tomas mikolov, ilya sutskever, kai chen, greg corrado and jeffrey dean.","","2014-02-15","","['yoav goldberg', 'omer levy']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"814",1402.389,"power laws in citation distributions: evidence from scopus","cs.dl physics.soc-ph stat.ap","modeling distributions of citations to scientific papers is crucial for understanding how science develops. however, there is a considerable empirical controversy on which statistical model fits the citation distributions best. this paper is concerned with rigorous empirical detection of power-law behaviour in the distribution of citations received by the most highly cited scientific papers. we have used a large, novel data set on citations to scientific papers published between 1998 and 2002 drawn from scopus. the power-law model is compared with a number of alternative models using a likelihood ratio test. we have found that the power-law hypothesis is rejected for around half of the scopus fields of science. for these fields of science, the yule, power-law with exponential cut-off and log-normal distributions seem to fit the data better than the pure power-law model. on the other hand, when the power-law hypothesis is not rejected, it is usually empirically indistinguishable from most of the alternative models. the pure power-law model seems to be the best model only for the most highly cited papers in ""physics and astronomy"". overall, our results seem to support theories implying that the most highly cited scientific papers follow the yule, power-law with exponential cut-off or log-normal distribution. our findings suggest also that power laws in citation distributions, when present, account only for a very small fraction of the published papers (less than 1% for most of science fields) and that the power-law scaling parameter (exponent) is substantially higher (from around 3.2 to around 4.7) than found in the older literature.","","2014-02-17","","['michal brzezinski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"815",1402.4279,"a bayesian model of node interaction in networks","cs.lg stat.me stat.ml","we are concerned with modeling the strength of links in networks by taking into account how often those links are used. link usage is a strong indicator of how closely two nodes are related, but existing network models in bayesian statistics and machine learning are able to predict only wether a link exists at all. as priors for latent attributes of network nodes we explore the chinese restaurant process (crp) and a multivariate gaussian with fixed dimensionality. the model is applied to a social network dataset and a word coocurrence dataset.","","2014-02-18","2015-03-06","['ingmar schuster']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"816",1402.4296,"modeling heterogeneity in random graphs through latent space models: a   selective review","math.st stat.me stat.th","we present a selective review on probabilistic modeling of heterogeneity in random graphs. we focus on latent space models and more particularly on stochastic block models and their extensions that have undergone major developments in the last five years.","","2014-02-18","2014-09-25","['catherine matias', 'stéphane robin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"817",1402.4507,"high dimensional semiparametric scale-invariant principal component   analysis","stat.ml","we propose a new high dimensional semiparametric principal component analysis (pca) method, named copula component analysis (coca). the semiparametric model assumes that, after unspecified marginally monotone transformations, the distributions are multivariate gaussian. coca improves upon pca and sparse pca in three aspects: (i) it is robust to modeling assumptions; (ii) it is robust to outliers and data contamination; (iii) it is scale-invariant and yields more interpretable results. we prove that the coca estimators obtain fast estimation rates and are feature selection consistent when the dimension is nearly exponentially large relative to the sample size. careful experiments confirm that coca outperforms sparse pca on both synthetic and real-world datasets.","","2014-02-18","","['fang han', 'han liu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"818",1402.4732,"efficient inference of gaussian process modulated renewal processes with   application to medical event data","stat.ml cs.lg stat.ap","the episodic, irregular and asynchronous nature of medical data render them difficult substrates for standard machine learning algorithms. we would like to abstract away this difficulty for the class of time-stamped categorical variables (or events) by modeling them as a renewal process and inferring a probability density over continuous, longitudinal, nonparametric intensity functions modulating that process. several methods exist for inferring such a density over intensity functions, but either their constraints and assumptions prevent their use with our potentially bursty event streams, or their time complexity renders their use intractable on our long-duration observations of high-resolution events, or both. in this paper we present a new and efficient method for inferring a distribution over intensity functions that uses direct numeric integration and smooth interpolation over gaussian processes. we demonstrate that our direct method is up to twice as accurate and two orders of magnitude more efficient than the best existing method (thinning). importantly, the direct method can infer intensity functions over the full range of bursty to memoryless to regular events, which thinning and many other methods cannot. finally, we apply the method to clinical event data and demonstrate the face-validity of the abstraction, which is now amenable to standard learning algorithms.","","2014-02-19","","['thomas a. lasko']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"819",1402.4862,"learning the parameters of determinantal point process kernels","stat.ml cs.lg","determinantal point processes (dpps) are well-suited for modeling repulsion and have proven useful in many applications where diversity is desired. while dpps have many appealing properties, such as efficient sampling, learning the parameters of a dpp is still considered a difficult problem due to the non-convex nature of the likelihood function. in this paper, we propose using bayesian methods to learn the dpp kernel parameters. these methods are applicable in large-scale and continuous dpp settings even when the exact form of the eigendecomposition is unknown. we demonstrate the utility of our dpp learning methods in studying the progression of diabetic neuropathy based on spatial distribution of nerve fibers, and in studying human perception of diversity in images.","","2014-02-19","","['raja hafiz affandi', 'emily b. fox', 'ryan p. adams', 'ben taskar']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"820",1402.5321,"genome scans for detecting footprints of local adaptation using a   bayesian factor model","q-bio.pe stat.ap","a central part of population genomics consists of finding genomic regions implicated in local adaptation. population genomic analyses are based on genotyping numerous molecular markers and looking for outlier loci in terms of patterns of genetic differentiation. one of the most common approach for selection scan is based on statistics that measure population differentiation such as $f_{st}$. however they are important caveats with approaches related to $f_{st}$ because they require grouping individuals into populations and they additionally assume a particular model of population structure. here we implement a more flexible individual-based approach based on bayesian factor models. factor models capture population structure with latent variables called factors, which can describe clustering of individuals into populations or isolation-by-distance patterns. using hierarchical bayesian modeling, we both infer population structure and identify outlier loci that are candidates for local adaptation. as outlier loci, the hierarchical factor model searches for loci that are atypically related to population structure as measured by the latent factors. in a model of population divergence, we show that the factor model can achieve a 2-fold or more reduction of false discovery rate compared to the software bayescan or compared to a $f_{st}$ approach. we analyze the data of the human genome diversity panel to provide an example of how factor models can be used to detect local adaptation with a large number of snps. the bayesian factor model is implemented in the open-source pcadapt software.","10.1093/molbev/msu182","2014-02-21","2014-07-29","['n. duforet-frebourg', 'e. bazin', 'm. g. b. blum']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"821",1402.536,"important molecular descriptors selection using self tuned reweighted   sampling method for prediction of antituberculosis activity","cs.lg stat.ap stat.ml","in this paper, a new descriptor selection method for selecting an optimal combination of important descriptors of sulfonamide derivatives data, named self tuned reweighted sampling (strs), is developed. descriptors are defined as the descriptors with large absolute coefficients in a multivariate linear regression model such as partial least squares(pls). in this study, the absolute values of regression coefficients of pls model are used as an index for evaluating the importance of each descriptor then, based on the importance level of each descriptor, strs sequentially selects n subsets of descriptors from n monte carlo (mc) sampling runs in an iterative and competitive manner. in each sampling run, a fixed ratio (e.g. 80%) of samples is first randomly selected to establish a regresson model. next, based on the regression coefficients, a two-step procedure including rapidly decreasing function (rdf) based enforced descriptor selection and self tuned sampling (sts) based competitive descriptor selection is adopted to select the important descriptorss. after running the loops, a number of subsets of descriptors are obtained and root mean squared error of cross validation (rmsecv) of pls models established with subsets of descriptors is computed. the subset of descriptors with the lowest rmsecv is considered as the optimal descriptor subset. the performance of the proposed algorithm is evaluated by sulfanomide derivative dataset. the results reveal an good characteristic of strs that it can usually locate an optimal combination of some important descriptors which are interpretable to the biologically of interest. additionally, our study shows that better prediction is obtained by strs when compared to full descriptor set pls modeling, monte carlo uninformative variable elimination (mc-uve).","","2014-02-21","","['n/a doreswamy', 'chanabasayya m. vastrad']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"822",1402.5384,"phi-divergence statistics for the likelihood ratio order: an approach   based on log-linear models","stat.me","when some treatments are ordered according to the categories of an ordinal categorical variable (e.g., extent of side effects) in a monotone order, one might be interested in knowing wether the treatments are equally effective or not. one way to do that is to test if the likelihood ratio order is strictly verified. a method based on log-linear models is derived to make statistical inference and phi-divergence test-statistics are proposed for the test of interest. focussed on loglinear modeling, the theory associated with the asymptotic distribution of the phi-divergence test-statistics is developed. an illustrative example motivates the procedure and a simulation study for small and moderate sample sizes shows that it is possible to find phi-divergence test-statistic with an exact size closer to nominal size and higher power in comparison with the classical likelihood ratio.","","2014-02-21","","['nirian martín', 'raquel mata', 'leandro pardo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"823",1402.5397,"bayesian additive regression trees with parametric models of   heteroskedasticity","stat.me","we incorporate heteroskedasticity into bayesian additive regression trees (bart) by modeling the log of the error variance parameter as a linear function of prespecified covariates. under this scheme, the gibbs sampling procedure for the original sum-of- trees model is easily modified, and the parameters for the variance model are updated via a metropolis-hastings step. we demonstrate the promise of our approach by providing more appropriate posterior predictive intervals than homoskedastic bart in heteroskedastic settings and demonstrating the model's resistance to overfitting. our implementation will be offered in an upcoming release of the r package bartmachine.","","2014-02-21","","['justin bleich', 'adam kapelner']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"824",1402.5715,"variational particle approximations","stat.ml cs.lg","approximate inference in high-dimensional, discrete probabilistic models is a central problem in computational statistics and machine learning. this paper describes discrete particle variational inference (dpvi), a new approach that combines key strengths of monte carlo, variational and search-based techniques. dpvi is based on a novel family of particle-based variational approximations that can be fit using simple, fast, deterministic search techniques. like monte carlo, dpvi can handle multiple modes, and yields exact results in a well-defined limit. like unstructured mean-field, dpvi is based on optimizing a lower bound on the partition function; when this quantity is not of intrinsic interest, it facilitates convergence assessment and debugging. like both monte carlo and combinatorial search, dpvi can take advantage of factorization, sequential structure, and custom search operators. this paper defines dpvi particle-based approximation family and partition function lower bounds, along with the sequential dpvi and local dpvi algorithm templates for optimizing them. dpvi is illustrated and evaluated via experiments on lattice markov random fields, nonparametric bayesian mixtures and block-models, and parametric as well as non-parametric hidden markov models. results include applications to real-world spike-sorting and relational modeling problems, and show that dpvi can offer appealing time/accuracy trade-offs as compared to multiple alternatives.","","2014-02-23","2015-12-05","['ardavan saeedi', 'tejas d kulkarni', 'vikash mansinghka', 'samuel gershman']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"825",1402.5724,"model selection criteria for nonlinear mixed effects modeling","stat.me","we consider constructing model selection criteria for evaluating nonlinear mixed effects models via basis expansions. mean functions and random functions in the mixed effects model are expressed by basis expansions, then they are estimated by the maximum likelihood method. in order to select numbers of basis we derive a bayesian model selection criterion for evaluating nonlinear mixed effects models estimated by the maximum likelihood method. simulation results shows the effectiveness of the mixed effects modeling.","","2014-02-24","","['hidetoshi matsui']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"826",1402.6305,"about adaptive coding on countable alphabets: max-stable envelope   classes","cs.it math.it math.st stat.th","in this paper, we study the problem of lossless universal source coding for stationary memoryless sources on countably infinite alphabets. this task is generally not achievable without restricting the class of sources over which universality is desired. building on our prior work, we propose natural families of sources characterized by a common dominating envelope. we particularly emphasize the notion of adaptivity, which is the ability to perform as well as an oracle knowing the envelope, without actually knowing it. this is closely related to the notion of hierarchical universal source coding, but with the important difference that families of envelope classes are not discretely indexed and not necessarily nested.   our contribution is to extend the classes of envelopes over which adaptive universal source coding is possible, namely by including max-stable (heavy-tailed) envelopes which are excellent models in many applications, such as natural language modeling. we derive a minimax lower bound on the redundancy of any code on such envelope classes, including an oracle that knows the envelope. we then propose a constructive code that does not use knowledge of the envelope. the code is computationally efficient and is structured to use an {e}xpanding {t}hreshold for {a}uto-{c}ensoring, and we therefore dub it the \textsc{etac}-code. we prove that the \textsc{etac}-code achieves the lower bound on the minimax redundancy within a factor logarithmic in the sequence length, and can be therefore qualified as a near-adaptive code over families of heavy-tailed envelopes. for finite and light-tailed envelopes the penalty is even less, and the same code follows closely previous results that explicitly made the light-tailed assumption. our technical results are founded on methods from regular variation theory and concentration of measure.","","2014-02-25","2015-04-17","['boucheron stephane', 'elisabeth gassiat', 'mesrob i. ohannessian']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"827",1403.0065,"likelihood based inference for high-dimensional extreme value   distributions","stat.ap","multivariate extreme value statistical analysis is concerned with observations on several variables which are thought to possess some degree of tail-dependence. in areas such as the modeling of financial and insurance risks, or as the modeling of spatial variables, extreme value models in high dimensions (up to fifty or more) with their statistical inference procedures are needed. in this paper, we consider max-stable models for which the spectral random vectors have absolutely continuous distributions. for random samples with max-stable distributions we provide quasi-explicit analytical expressions of the full likelihoods. when the full likelihood becomes numerically intractable because of a too large dimension, it is however necessary to split the components into subgroups and to consider a composite likelihood approach. for random samples in the max-domain of attraction of a max-stable distribution, two approaches that use simpler likelihoods are possible: (i) a threshold approach that is combined with a censoring scheme, (ii) a block maxima approach that exploits the information on the occurrence times of the componentwise maxima. the asymptotic properties of the estimators are given and the utility of the methods is examined via simulation. the estimators are also compared with those derived from the pairwise composite likelihood method which has been previously proposed in the spatial extreme value literature.","","2014-03-01","2014-12-30","['alexis bienvenüe', 'christian y. robert']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"828",1403.0904,"ridge estimation of inverse covariance matrices from high-dimensional   data","stat.me","we study ridge estimation of the precision matrix in the high-dimensional setting where the number of variables is large relative to the sample size. we first review two archetypal ridge estimators and note that their utilized penalties do not coincide with common ridge penalties. subsequently, starting from a common ridge penalty, analytic expressions are derived for two alternative ridge estimators of the precision matrix. the alternative estimators are compared to the archetypes with regard to eigenvalue shrinkage and risk. the alternatives are also compared to the graphical lasso within the context of graphical modeling. the comparisons may give reason to prefer the proposed alternative estimators.","10.1016/j.csda.2016.05.012","2014-03-04","2015-09-24","['wessel n. van wieringen', 'carel f. w. peeters']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"829",1403.0921,"dynamic stochastic blockmodels for time-evolving social networks","cs.si cs.lg physics.soc-ph stat.me","significant efforts have gone into the development of statistical models for analyzing data in the form of networks, such as social networks. most existing work has focused on modeling static networks, which represent either a single time snapshot or an aggregate view over time. there has been recent interest in statistical modeling of dynamic networks, which are observed at multiple points in time and offer a richer representation of many complex phenomena. in this paper, we present a state-space model for dynamic networks that extends the well-known stochastic blockmodel for static networks to the dynamic setting. we fit the model in a near-optimal manner using an extended kalman filter (ekf) augmented with a local search. we demonstrate that the ekf-based algorithm performs competitively with a state-of-the-art algorithm based on markov chain monte carlo sampling but is significantly less computationally demanding.","10.1109/jstsp.2014.2310294","2014-03-04","","['kevin s. xu', 'alfred o. hero']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"830",1403.1124,"estimating complex causal effects from incomplete observational data","stat.me cs.lg math.st stat.ml stat.th","despite the major advances taken in causal modeling, causality is still an unfamiliar topic for many statisticians. in this paper, it is demonstrated from the beginning to the end how causal effects can be estimated from observational data assuming that the causal structure is known. to make the problem more challenging, the causal effects are highly nonlinear and the data are missing at random. the tools used in the estimation include causal models with design, causal calculus, multiple imputation and generalized additive models. the main message is that a trained statistician can estimate causal effects by judiciously combining existing tools.","","2014-03-05","2014-07-02","['juha karvanen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE
"831",1403.2397,"scalable bayesian model averaging through local information propagation","stat.me stat.co","we show that a probabilistic version of the classical forward-stepwise variable inclusion procedure can serve as a general data-augmentation scheme for model space distributions in (generalized) linear models. this latent variable representation takes the form of a markov process, thereby allowing information propagation algorithms to be applied for sampling from model space posteriors. in particular, we propose a sequential monte carlo method for achieving effective unbiased bayesian model averaging in high-dimensional problems, utilizing proposal distributions constructed using local information propagation. we illustrate our method---called lips for local information propagation based sampling---through real and simulated examples with dimensionality ranging from 15 to 1,000, and compare its performance in estimating posterior inclusion probabilities and in out-of-sample prediction to those of several other methods---namely, mcmc, bas, ibma, and lasso. in addition, we show that the latent variable representation can also serve as a modeling tool for specifying model space priors that account for knowledge regarding model complexity and conditional inclusion relationships.","","2014-03-10","2014-10-21","['li ma']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"832",1403.2695,"adaptive bayesian density regression for high-dimensional data","math.st stat.me stat.th","density regression provides a flexible strategy for modeling the distribution of a response variable $y$ given predictors $\mathbf{x}=(x_1,\ldots,x_p)$ by letting that the conditional density of $y$ given $\mathbf{x}$ as a completely unknown function and allowing its shape to change with the value of $\mathbf{x}$. the number of predictors $p$ may be very large, possibly much larger than the number of observations $n$, but the conditional density is assumed to depend only on a much smaller number of predictors, which are unknown. in addition to estimation, the goal is also to select the important predictors which actually affect the true conditional density. we consider a nonparametric bayesian approach to density regression by constructing a random series prior based on tensor products of spline functions. the proposed prior also incorporates the issue of variable selection. we show that the posterior distribution of the conditional density contracts adaptively at the truth nearly at the optimal oracle rate, determined by the unknown sparsity and smoothness levels, even in the ultra high-dimensional settings where $p$ increases exponentially with $n$. the result is also extended to the anisotropic case where the degree of smoothness can vary in different directions, and both random and deterministic predictors are considered. we also propose a technique to calculate posterior moments of the conditional density function without requiring markov chain monte carlo methods.","10.3150/14-bej663","2014-03-11","2016-01-06","['weining shen', 'subhashis ghosal']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"833",1403.35,"r-vine models for spatial time series with an application to daily mean   temperature","stat.me","we introduce an extension of r-vine copula models for the purpose of spatial dependency modeling and model based prediction at unobserved locations. the newly derived spatial r-vine model combines the flexibility of vine copulas with the classical geostatistical idea of modeling spatial dependencies by means of the distances between the variable locations. in particular the model is able to capture non-gaussian spatial dependencies. for the purpose of model development and as an illustration we consider daily mean temperature data observed at 54 monitoring stations in germany. we identify a relationship between the vine copula parameters and the station distances and exploit it in order to reduce the huge number of parameters needed to parametrize a 54-dimensional r-vine model needed to fit the data. the new distance based model parametrization results in a distinct reduction in the number of parameters and makes parameter estimation and prediction at unobserved locations feasible. the prediction capabilities are validated using adequate scoring techniques, showing a better performance of the spatial r-vine copula model compared to a gaussian spatial model.","","2014-03-14","","['tobias michael erhardt', 'claudia czado', 'ulf schepsmeier']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"834",1403.489,"modeling an augmented lagrangian for blackbox constrained optimization","stat.me stat.co","constrained blackbox optimization is a difficult problem, with most approaches coming from the mathematical programming literature. the statistical literature is sparse, especially in addressing problems with nontrivial constraints. this situation is unfortunate because statistical methods have many attractive properties: global scope, handling noisy objectives, sensitivity analysis, and so forth. to narrow that gap, we propose a combination of response surface modeling, expected improvement, and the augmented lagrangian numerical optimization framework. this hybrid approach allows the statistical model to think globally and the augmented lagrangian to act locally. we focus on problems where the constraints are the primary bottleneck, requiring expensive simulation to evaluate and substantial modeling effort to map out. in that context, our hybridization presents a simple yet effective solution that allows existing objective-oriented statistical approaches, like those based on gaussian process surrogates and expected improvement heuristics, to be applied to the constrained setting with minor modification. this work is motivated by a challenging, real-data benchmark problem from hydrology where, even with a simple linear objective function, learning a nontrivial valid region complicates the search for a global minimum.","","2014-03-19","2015-03-03","['robert b. gramacy', 'genetha a. gray', 'sebastien le digabel', 'herbert k. h. lee', 'pritam ranjan', 'garth wells', 'stefan m. wild']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"835",1403.5065,"data augmentation in rician noise model and bayesian diffusion tensor   imaging","stat.co stat.ap stat.me","mapping white matter tracts is an essential step towards understanding brain function. diffusion magnetic resonance imaging (dmri) is the only noninvasive technique which can detect in vivo anisotropies in the 3-dimensional diffusion of water molecules, which correspond to nervous fibers in the living brain. in this process, spectral data from the displacement distribution of water molecules is collected by a magnetic resonance scanner. from the statistical point of view, inverting the fourier transform from such sparse and noisy spectral measurements leads to a non-linear regression problem. diffusion tensor imaging (dti) is the simplest modeling approach postulating a gaussian displacement distribution at each volume element (voxel). typically the inference is based on a linearized log-normal regression model that can fit the spectral data at low frequencies. however such approximation fails to fit the high frequency measurements which contain information about the details of the displacement distribution but have a low signal to noise ratio. in this paper, we directly work with the rice noise model and cover the full range of $b$-values. using data augmentation to represent the likelihood, we reduce the non-linear regression problem to the framework of generalized linear models. then we construct a bayesian hierarchical model in order to perform simultaneously estimation and regularization of the tensor field. finally the bayesian paradigm is implemented by using markov chain monte carlo.","","2014-03-20","","['dario gasbarra', 'jia liu', 'juha railavo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"836",1403.515,"bayes sensitivity with fisher-rao metric","stat.me","we propose a geometric framework to assess sensitivity of bayesian procedures to modeling assumptions based on the nonparametric fisher-rao metric. while the framework is general in spirit, the focus of this article is restricted to metric-based diagnosis under two settings: assessing local and global robustness in bayesian procedures to perturbations of the likelihood and prior, and identification of influential observations. the approach is based on the square-root representation of densities which enables one to compute geodesics and geodesic distances in analytical form, facilitating the definition of naturally calibrated local and global discrepancy measures. an important feature of our approach is the definition of a geometric $\epsilon$-contamination class of sampling distributions and priors via intrinsic analysis on the space of probability density functions. we showcase the applicability of our framework on several simulated toy datasets as well as in real data settings for generalized mixed effects models, directional data and shape data.","","2014-03-20","2014-04-25","['sebastian kurtek', 'karthik bharath']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"837",1403.6015,"fast direct methods for gaussian processes","math.na astro-ph.im math.st stat.th","a number of problems in probability and statistics can be addressed using the multivariate normal (gaussian) distribution. in the one-dimensional case, computing the probability for a given mean and variance simply requires the evaluation of the corresponding gaussian density. in the $n$-dimensional setting, however, it requires the inversion of an $n \times n$ covariance matrix, $c$, as well as the evaluation of its determinant, $\det(c)$. in many cases, such as regression using gaussian processes, the covariance matrix is of the form $c = \sigma^2 i + k$, where $k$ is computed using a specified covariance kernel which depends on the data and additional parameters (hyperparameters). the matrix $c$ is typically dense, causing standard direct methods for inversion and determinant evaluation to require $\mathcal o(n^3)$ work. this cost is prohibitive for large-scale modeling. here, we show that for the most commonly used covariance functions, the matrix $c$ can be hierarchically factored into a product of block low-rank updates of the identity matrix, yielding an $\mathcal o (n\log^2 n) $ algorithm for inversion. more importantly, we show that this factorization enables the evaluation of the determinant $\det(c)$, permitting the direct calculation of probabilities in high dimensions under fairly broad assumptions on the kernel defining $k$. our fast algorithm brings many problems in marginalization and the adaptation of hyperparameters within practical reach using a single cpu core. the combination of nearly optimal scaling in terms of problem size with high-performance computing resources will permit the modeling of previously intractable problems. we illustrate the performance of the scheme on standard covariance kernels.","","2014-03-24","2015-04-04","['sivaram ambikasaran', 'daniel foreman-mackey', 'leslie greengard', 'david w. hogg', ""michael o'neil""]",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"838",1403.6706,"beyond l2-loss functions for learning sparse models","stat.ml cs.cv cs.lg math.oc","incorporating sparsity priors in learning tasks can give rise to simple, and interpretable models for complex high dimensional data. sparse models have found widespread use in structure discovery, recovering data from corruptions, and a variety of large scale unsupervised and supervised learning problems. assuming the availability of sufficient data, these methods infer dictionaries for sparse representations by optimizing for high-fidelity reconstruction. in most scenarios, the reconstruction quality is measured using the squared euclidean distance, and efficient algorithms have been developed for both batch and online learning cases. however, new application domains motivate looking beyond conventional loss functions. for example, robust loss functions such as $\ell_1$ and huber are useful in learning outlier-resilient models, and the quantile loss is beneficial in discovering structures that are the representative of a particular quantile. these new applications motivate our work in generalizing sparse learning to a broad class of convex loss functions. in particular, we consider the class of piecewise linear quadratic (plq) cost functions that includes huber, as well as $\ell_1$, quantile, vapnik, hinge loss, and smoothed variants of these penalties. we propose an algorithm to learn dictionaries and obtain sparse codes when the data reconstruction fidelity is measured using any smooth plq cost function. we provide convergence guarantees for the proposed algorithm, and demonstrate the convergence behavior using empirical experiments. furthermore, we present three case studies that require the use of plq cost functions: (i) robust image modeling, (ii) tag refinement for image annotation and retrieval and (iii) computing empirical confidence limits for subspace clustering.","","2014-03-26","","['karthikeyan natesan ramamurthy', 'aleksandr y. aravkin', 'jayaraman j. thiagarajan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"839",1403.7118,"a unified framework of constrained regression","stat.me stat.ap","generalized additive models (gams) play an important role in modeling and understanding complex relationships in modern applied statistics. they allow for flexible, data-driven estimation of covariate effects. yet researchers often have a priori knowledge of certain effects, which might be monotonic or periodic (cyclic) or should fulfill boundary conditions. we propose a unified framework to incorporate these constraints for both univariate and bivariate effect estimates and for varying coefficients. as the framework is based on component-wise boosting methods, variables can be selected intrinsically, and effects can be estimated for a wide range of different distributional assumptions. bootstrap confidence intervals for the effect estimates are derived to assess the models. we present three case studies from environmental sciences to illustrate the proposed seamless modeling framework. all discussed constrained effect estimates are implemented in the comprehensive r package mboost for model-based boosting.","10.1007/s11222-014-9520-y","2014-03-27","2014-11-07","['benjamin hofner', 'thomas kneib', 'torsten hothorn']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"840",1403.7642,"the sensitivity of college football rankings to several modeling choices","stat.ap","this paper proposes a multiple-membership generalized linear mixed model for ranking college football teams using only their win/loss records. the model results in an intractable, high-dimensional integral due to the random effects structure and nonlinear link function. we use recent data sets to explore the effect of the choice of integral approximation and other modeling assumptions on the rankings. varying the modeling assumptions sometimes leads to changes in the team rankings that could affect bowl assignments.","10.1515/1559-0410.1471","2014-03-29","","['andrew t. karl']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"841",1403.7672,"bayesian sparse graphical models for classification with application to   protein expression data","stat.me","reverse-phase protein array (rppa) analysis is a powerful, relatively new platform that allows for high-throughput, quantitative analysis of protein networks. one of the challenges that currently limit the potential of this technology is the lack of methods that allow for accurate data modeling and identification of related networks and samples. such models may improve the accuracy of biological sample classification based on patterns of protein network activation and provide insight into the distinct biological relationships underlying different types of cancer. motivated by rppa data, we propose a bayesian sparse graphical modeling approach that uses selection priors on the conditional relationships in the presence of class information. the novelty of our bayesian model lies in the ability to draw information from the network data as well as from the associated categorical outcome in a unified hierarchical model for classification. in addition, our method allows for intuitive integration of a priori network information directly in the model and allows for posterior inference on the network topologies both within and between classes. applying our methodology to an rppa data set generated from panels of human breast cancer and ovarian cancer cell lines, we demonstrate that the model is able to distinguish the different cancer cell types more accurately than several existing models and to identify differential regulation of components of a critical signaling network (the pi3k-akt pathway) between these two types of cancer. this approach represents a powerful new tool that can be used to improve our understanding of protein networks in cancer.","10.1214/14-aoas722","2014-03-29","2014-11-21","['veerabhadran baladandayuthapani', 'rajesh talluri', 'yuan ji', 'kevin r. coombes', 'yiling lu', 'bryan t. hennessy', 'michael a. davies', 'bani k. mallick']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"842",1404.0329,"toward computational cumulative biology by combining models of   biological datasets","q-bio.qm q-bio.gn stat.ml","a main challenge of data-driven sciences is how to make maximal use of the progressively expanding databases of experimental datasets in order to keep research cumulative. we introduce the idea of a modeling-based dataset retrieval engine designed for relating a researcher's experimental dataset to earlier work in the field. the search is (i) data-driven to enable new findings, going beyond the state of the art of keyword searches in annotations, (ii) modeling-driven, to both include biological knowledge and insights learned from data, and (iii) scalable, as it is accomplished without building one unified grand model of all data. assuming each dataset has been modeled beforehand, by the researchers or by database managers, we apply a rapidly computable and optimizable combination model to decompose a new dataset into contributions from earlier relevant models. by using the data-driven decomposition we identify a network of interrelated datasets from a large annotated human gene expression atlas. while tissue type and disease were major driving forces for determining relevant datasets, the found relationships were richer and the model-based search was more accurate than keyword search; it moreover recovered biologically meaningful relationships that are not straightforwardly visible from annotations, for instance, between cells in different developmental stages such as thymocytes and t-cells. data-driven links and citations matched to a large extent; the data-driven links even uncovered corrections to the publication data, as two of the most linked datasets were not highly cited and turned out to have wrong publication entries in the database.","10.1371/journal.pone.0113053","2014-04-01","","['ali faisal', 'jaakko peltonen', 'elisabeth georgii', 'johan rung', 'samuel kaski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"843",1404.0752,"an efficient search strategy for aggregation and discretization of   attributes of bayesian networks using minimum description length","stat.ml","bayesian networks are convenient graphical expressions for high dimensional probability distributions representing complex relationships between a large number of random variables. they have been employed extensively in areas such as bioinformatics, artificial intelligence, diagnosis, and risk management. the recovery of the structure of a network from data is of prime importance for the purposes of modeling, analysis, and prediction. most recovery algorithms in the literature assume either discrete of continuous but gaussian data. for general continuous data, discretization is usually employed but often destroys the very structure one is out to recover. friedman and goldszmidt suggest an approach based on the minimum description length principle that chooses a discretization which preserves the information in the original data set, however it is one which is difficult, if not impossible, to implement for even moderately sized networks. in this paper we provide an extremely efficient search strategy which allows one to use the friedman and goldszmidt discretization in practice.","","2014-04-02","","['jem corcoran', 'daniel tran', 'nicholas levine']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"844",1404.086,"a cautionary note on robust covariance plug-in methods","stat.me","many multivariate statistical methods rely heavily on the sample covariance matrix. it is well known though that the sample covariance matrix is highly non-robust. one popular alternative approach for ""robustifying"" the multivariate method is to simply replace the role of the covariance matrix with some robust scatter matrix. the aim of this paper is to point out that in some situations certain properties of the covariance matrix are needed for the corresponding robust ""plug-in"" method to be a valid approach, and that not all scatter matrices necessarily possess these important properties. in particular, the following three multivariate methods are discussed in this paper: independent components analysis, observational regression and graphical modeling. for each case, it is shown that using a symmetrized robust scatter matrix in place of the covariance matrix results in a proper robust multivariate method.","10.1093/biomet/asv022","2014-04-03","","['klaus nordhausen', 'david e. tyler']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"845",1404.124,"fast and accurate multivariate gaussian modeling of protein families:   predicting residue contacts and protein-interaction partners","q-bio.qm cond-mat.dis-nn stat.me","in the course of evolution, proteins show a remarkable conservation of their three-dimensional structure and their biological function, leading to strong evolutionary constraints on the sequence variability between homologous proteins. our method aims at extracting such constraints from rapidly accumulating sequence data, and thereby at inferring protein structure and function from sequence information alone. recently, global statistical inference methods (e.g. direct-coupling analysis, sparse inverse covariance estimation) have achieved a breakthrough towards this aim, and their predictions have been successfully implemented into tertiary and quaternary protein structure prediction methods. however, due to the discrete nature of the underlying variable (amino-acids), exact inference requires exponential time in the protein length, and efficient approximations are needed for practical applicability. here we propose a very efficient multivariate gaussian modeling approach as a variant of direct-coupling analysis: the discrete amino-acid variables are replaced by continuous gaussian random variables. the resulting statistical inference problem is efficiently and exactly solvable. we show that the quality of inference is comparable or superior to the one achieved by mean-field approximations to inference with discrete variables, as done by direct-coupling analysis. this is true for (i) the prediction of residue-residue contacts in proteins, and (ii) the identification of protein-protein interaction partner in bacterial signal transduction. an implementation of our multivariate gaussian approach is available at the website http://areeweb.polito.it/ricerca/cmp/code","10.1371/journal.pone.0092721","2014-04-04","","['carlo baldassi', 'marco zamparo', 'christoph feinauer', 'andrea procaccini', 'riccardo zecchina', 'martin weigt', 'andrea pagnani']",1,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"846",1404.3012,"bayesian image segmentations by potts prior and loopy belief propagation","cs.cv cond-mat.dis-nn cond-mat.stat-mech cs.lg stat.ml","this paper presents a bayesian image segmentation model based on potts prior and loopy belief propagation. the proposed bayesian model involves several terms, including the pairwise interactions of potts models, and the average vectors and covariant matrices of gauss distributions in color image modeling. these terms are often referred to as hyperparameters in statistical machine learning theory. in order to determine these hyperparameters, we propose a new scheme for hyperparameter estimation based on conditional maximization of entropy in the potts prior. the algorithm is given based on loopy belief propagation. in addition, we compare our conditional maximum entropy framework with the conventional maximum likelihood framework, and also clarify how the first order phase transitions in lbp's for potts models influence our hyperparameter estimation procedures.","10.7566/jpsj.83.124002","2014-04-11","2014-08-18","['kazuyuki tanaka', 'shun kataoka', 'muneki yasuda', 'yuji waizumi', 'chiou-ting hsu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"847",1404.3219,"estimating nonlinear regression errors without doing regression","stat.ml nlin.cd q-fin.st","a method for estimating nonlinear regression errors and their distributions without performing regression is presented. assuming continuity of the modeling function the variance is given in terms of conditional probabilities extracted from the data. for n data points the computational demand is n2. comparing the predicted residual errors with those derived from a linear model assumption provides a signal for nonlinearity. the method is successfully illustrated with data generated by the ikeda and lorenz maps augmented with noise. as a by-product the embedding dimensions of these maps are also extracted.","","2014-04-11","","['hong pi', 'carsten peterson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"848",1404.3287,"a modified version of the inference function for margins and interval   estimation for the bivariate clayton copula sur tobit model: an simulation   approach","stat.me","this paper extends the analysis of bivariate seemingly unrelated regression (sur) tobit model by modeling its nonlinear dependence structure through the clayton copula. the ability in capturing/modeling the lower tail dependence of the sur tobit model where some data are censored (generally, at zero point) is an additionally useful feature of the clayton copula. we propose a modified version of the inference function for margins (ifm) method (joe and xu, 1996), which we refer to as mifm method, to obtain the estimates of the marginal parameters and a better (satisfactory) estimate of the copula association parameter. more specifically, we employ the data augmentation technique in the second stage of the ifm method to generate the censored observations (i.e. to obtain continuous marginal distributions, which ensures the uniqueness of the copula) and then estimate the dependence parameter. resampling procedures (bootstrap methods) are also proposed for obtaining confidence intervals for the model parameters. a simulation study is performed in order to verify the behavior of the mifm estimates (we focus on the copula parameter estimation) and the coverage probability of different confidence intervals in datasets with different percentages of censoring and degrees of dependence. the satisfactory results from the simulation (under certain conditions) and empirical study indicate the good performance of our proposed model and methods where they are applied to model the u.s. ready-to-eat breakfast cereals and fluid milk consumption data.","","2014-04-12","","['paulo h. ferreira', 'francisco louzada']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"849",1404.356,"a hierarchical bayesian model for inference of copy number variants and   their association to gene expression","stat.ap","a number of statistical models have been successfully developed for the analysis of high-throughput data from a single source, but few methods are available for integrating data from different sources. here we focus on integrating gene expression levels with comparative genomic hybridization (cgh) array measurements collected on the same subjects. we specify a measurement error model that relates the gene expression levels to latent copy number states which, in turn, are related to the observed surrogate cgh measurements via a hidden markov model. we employ selection priors that exploit the dependencies across adjacent copy number states and investigate mcmc stochastic search techniques for posterior inference. our approach results in a unified modeling framework for simultaneously inferring copy number variants (cnv) and identifying their significant associations with mrna transcripts abundance. we show performance on simulated data and illustrate an application to data from a genomic study on human cancer cell lines.","10.1214/13-aoas705","2014-04-14","","['alberto cassese', 'michele guindani', 'mahlet g. tadesse', 'francesco falciani', 'marina vannucci']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"850",1404.3816,"a kalman filter powered by $\mathcal{h}^2$-matrices for quasi-continuous   data assimilation problems","math.na cs.na stat.co","continuously tracking the movement of a fluid or a plume in the subsurface is a challenge that is often encountered in applications, such as tracking a plume of injected co$_2$ or of a hazardous substance. advances in monitoring techniques have made it possible to collect measurements at a high frequency while the plume moves, which has the potential advantage of providing continuous high-resolution images of fluid flow with the aid of data processing. however, the applicability of this approach is limited by the high computational cost associated with having to analyze large data sets within the time constraints imposed by real-time monitoring. existing data assimilation methods have computational requirements that increase super-linearly with the size of the unknowns $m$. in this paper, we present the hikf, a new kalman filter (kf) variant powered by the hierarchical matrix approach that dramatically reduces the computational and storage cost of the standard kf from $\mathcal{o}(m^2)$ to $\mathcal{o}(m)$, while producing practically the same results. the version of hikf that is presented here takes advantage of the so-called random walk dynamical model, which is tailored to a class of data assimilation problems in which measurements are collected quasi-continuously. the proposed method has been applied to a realistic co$_2$ injection model and compared with the ensemble kalman filter (enkf). numerical results show that hikf can provide estimates that are more accurate than enkf, and also demonstrate the usefulness of modeling the system dynamics as a random walk in this context.","10.1002/2013wr014607","2014-04-15","","['judith y. li', 'sivaram ambikasaran', 'eric f. darve', 'peter k. kitanidis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"851",1404.4009,"generalizing the network scale-up method: a new estimator for the size   of hidden populations","stat.ap","the network scale-up method enables researchers to estimate the size of hidden populations, such as drug injectors and sex workers, using sampled social network data. the basic scale-up estimator offers advantages over other size estimation techniques, but it depends on problematic modeling assumptions. we propose a new generalized scale-up estimator that can be used in settings with non-random social mixing and imperfect awareness about membership in the hidden population. further, the new estimator can be used when data are collected via complex sample designs and from incomplete sampling frames. however, the generalized scale-up estimator also requires data from two samples: one from the frame population and one from the hidden population. in some situations these data from the hidden population can be collected by adding a small number of questions to already planned studies. for other situations, we develop interpretable adjustment factors that can be applied to the basic scale-up estimator. we conclude with practical recommendations for the design and analysis of future studies.","10.1177/0081175016665425","2014-04-15","2016-11-11","['dennis m. feehan', 'matthew j. salganik']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"852",1404.5097,"a fully nonparametric modelling approach to binary regression","stat.me","we propose a general nonparametric bayesian framework for binary regression, which is built from modeling for the joint response-covariate distribution. the observed binary responses are assumed to arise from underlying continuous random variables through discretization, and we model the joint distribution of these latent responses and the covariates using a dirichlet process mixture of multivariate normals. we show that the kernel of the induced mixture model for the observed data is identifiable upon a restriction on the latent variables. to allow for appropriate dependence structure while facilitating identifiability, we use a square-root-free cholesky decomposition of the covariance matrix in the normal mixture kernel. in addition to allowing for the necessary restriction, this modeling strategy provides substantial simplifications in implementation of markov chain monte carlo posterior simulation. we present two data examples taken from areas for which the methodology is especially well suited. in particular, the first example involves estimation of relationships between environmental variables, and the second develops inference for natural selection surfaces in evolutionary biology. finally, we discuss extensions to regression settings with multivariate ordinal responses.","10.1214/15-ba963si","2014-04-20","2014-08-04","['maria deyoreo', 'athanasios kottas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"853",1404.6201,"time-varying clustering of multivariate longitudinal observations","stat.me","we propose a statistical method for clustering of multivariate longitudinal data into homogeneous groups. this method relies on a time-varying extension on the classical k-means algorithm, where a multivariate vector autoregressive model is additionally assumed for modeling the evolution of clusters' centroids over time. we base the inference on a least squares specification of the model and coordinate descent algorithm. to illustrate our work, we consider a longitudinal dataset on human development. three variables are modeled, namely life expectancy, education and gross domestic product.","","2014-04-24","","['antonello maruotti', 'maurizio vichi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"854",1404.6386,"handling non-ignorable dropouts in longitudinal data: a conditional   model based on a latent markov heterogeneity structure","stat.me","we illustrate a class of conditional models for the analysis of longitudinal data suffering attrition in random effects models framework, where the subject-specific random effects are assumed to be discrete and to follow a time-dependent latent process. the latent process accounts for unobserved heterogeneity and correlation between individuals in a dynamic fashion, and for dependence between the observed process and the missing data mechanism. of particular interest is the case where the missing mechanism is non-ignorable. to deal with the topic we introduce a conditional to dropout model. a shape change in the random effects distribution is considered by directly modeling the effect of the missing data process on the evolution of the latent structure. to estimate the resulting model, we rely on the conditional maximum likelihood approach and for this aim we outline an em algorithm. the proposal is illustrated via simulations and then applied on a dataset concerning skin cancers. comparisons with other well-established methods are provided as well.","","2014-04-25","","['antonello maruotti']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"855",1404.6423,"joint analysis of snp and gene expression data in genetic association   studies of complex diseases","stat.ap","genetic association studies have been a popular approach for assessing the association between common single nucleotide polymorphisms (snps) and complex diseases. however, other genomic data involved in the mechanism from snps to disease, for example, gene expressions, are usually neglected in these association studies. in this paper, we propose to exploit gene expression information to more powerfully test the association between snps and diseases by jointly modeling the relations among snps, gene expressions and diseases. we propose a variance component test for the total effect of snps and a gene expression on disease risk. we cast the test within the causal mediation analysis framework with the gene expression as a potential mediator. for eqtl snps, the use of gene expression information can enhance power to test for the total effect of a snp-set, which is the combined direct and indirect effects of the snps mediated through the gene expression, on disease risk. we show that the test statistic under the null hypothesis follows a mixture of $\chi^2$ distributions, which can be evaluated analytically or empirically using the resampling-based perturbation method. we construct tests for each of three disease models that are determined by snps only, snps and gene expression, or include also their interactions. as the true disease model is unknown in practice, we further propose an omnibus test to accommodate different underlying disease models. we evaluate the finite sample performance of the proposed methods using simulation studies, and show that our proposed test performs well and the omnibus test can almost reach the optimal power where the disease model is known and correctly specified. we apply our method to reanalyze the overall effect of the snp-set and expression of the ormdl3 gene on the risk of asthma.","10.1214/13-aoas690","2014-04-25","","['yen-tsung huang', 'tyler j. vanderweele', 'xihong lin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"856",1404.6462,"bayesian semiparametric multivariate density deconvolution","stat.me","we consider the problem of multivariate density deconvolution when the interest lies in estimating the distribution of a vector-valued random variable but precise measurements of the variable of interest are not available, observations being contaminated with additive measurement errors. the existing sparse literature on the problem assumes the density of the measurement errors to be completely known. we propose robust bayesian semiparametric multivariate deconvolution approaches when the measurement error density is not known but replicated proxies are available for each unobserved value of the random vector. additionally, we allow the variability of the measurement errors to depend on the associated unobserved value of the vector of interest through unknown relationships which also automatically includes the case of multivariate multiplicative measurement errors. basic properties of finite mixture models, multivariate normal kernels and exchangeable priors are exploited in many novel ways to meet the modeling and computational challenges. theoretical results that show the flexibility of the proposed methods are provided. we illustrate the efficiency of the proposed methods in recovering the true density of interest through simulation experiments. the methodology is applied to estimate the joint consumption pattern of different dietary components from contaminated 24 hour recalls.","","2014-04-25","2016-12-05","['abhra sarkar', 'debdeep pati', 'bani k. mallick', 'raymond j. carroll']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"857",1404.6702,"a constrained matrix-variate gaussian process for transposable data","stat.ml cs.lg","transposable data represents interactions among two sets of entities, and are typically represented as a matrix containing the known interaction values. additional side information may consist of feature vectors specific to entities corresponding to the rows and/or columns of such a matrix. further information may also be available in the form of interactions or hierarchies among entities along the same mode (axis). we propose a novel approach for modeling transposable data with missing interactions given additional side information. the interactions are modeled as noisy observations from a latent noise free matrix generated from a matrix-variate gaussian process. the construction of row and column covariances using side information provides a flexible mechanism for specifying a-priori knowledge of the row and column correlations in the data. further, the use of such a prior combined with the side information enables predictions for new rows and columns not observed in the training data. in this work, we combine the matrix-variate gaussian process model with low rank constraints. the constrained gaussian process approach is applied to the prediction of hidden associations between genes and diseases using a small set of observed associations as well as prior covariances induced by gene-gene interaction networks and disease ontologies. the proposed approach is also applied to recommender systems data which involves predicting the item ratings of users using known associations as well as prior covariances induced by social networks. we present experimental results that highlight the performance of constrained matrix-variate gaussian process as compared to state of the art approaches in each domain.","","2014-04-26","","['oluwasanmi koyejo', 'cheng lee', 'joydeep ghosh']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"858",1404.6841,"learning subspaces of different dimension","math.st stat.me stat.th","we introduce a bayesian model for inferring mixtures of subspaces of different dimensions. the key challenge in such a mixture model is specification of prior distributions over subspaces of different dimensions. we address this challenge by embedding subspaces or grassmann manifolds into a sphere of relatively low dimension and specifying priors on the sphere. we provide an efficient sampling algorithm for the posterior distribution of the model parameters. we illustrate that a simple extension of our mixture of subspaces model can be applied to topic modeling. we also prove posterior consistency for the mixture of subspaces model. the utility of our approach is demonstrated with applications to real and simulated data.","","2014-04-27","2015-09-22","['brian st. thomas', 'lizhen lin', 'lek-heng lim', 'sayan mukherjee']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"859",1404.7197,"bayesian model comparison in genetic association analysis: linear mixed   modeling and snp set testing","stat.me q-bio.qm stat.ap","we consider the problems of hypothesis testing and model comparison under a flexible bayesian linear regression model whose formulation is closely connected with the linear mixed effect model and the parametric models for snp set analysis in genetic association studies. we derive a class of analytic approximate bayes factors and illustrate their connections with a variety of frequentist test statistics, including the wald statistic and the variance component score statistic. taking advantage of bayesian model averaging and hierarchical modeling, we demonstrate some distinct advantages and flexibilities in the approaches utilizing the derived bayes factors in the context of genetic association studies. we demonstrate our proposed methods using real or simulated numerical examples in applications of single snp association testing, multi-locus fine-mapping and snp set association testing.","","2014-04-28","2015-02-23","['xiaoquan wen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"860",1404.7236,"high dimensional semiparametric latent graphical model for mixed data","stat.ml","graphical models are commonly used tools for modeling multivariate random variables. while there exist many convenient multivariate distributions such as gaussian distribution for continuous data, mixed data with the presence of discrete variables or a combination of both continuous and discrete variables poses new challenges in statistical modeling. in this paper, we propose a semiparametric model named latent gaussian copula model for binary and mixed data. the observed binary data are assumed to be obtained by dichotomizing a latent variable satisfying the gaussian copula distribution or the nonparanormal distribution. the latent gaussian model with the assumption that the latent variables are multivariate gaussian is a special case of the proposed model. a novel rank-based approach is proposed for both latent graph estimation and latent principal component analysis. theoretically, the proposed methods achieve the same rates of convergence for both precision matrix estimation and eigenvector estimation, as if the latent variables were observed. under similar conditions, the consistency of graph structure recovery and feature selection for leading eigenvectors is established. the performance of the proposed methods is numerically assessed through simulation studies, and the usage of our methods is illustrated by a genetic dataset.","","2014-04-29","","['jianqing fan', 'han liu', 'yang ning', 'hui zou']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"861",1404.7339,"a time-varying shared frailty model with application to infectious   diseases","stat.ap","we propose a new parametric time-varying shared frailty model to represent changes over time in population heterogeneity, for use with bivariate current status data. the model uses a power transformation of a time-invariant frailty $u$, and is particularly convenient when $u$ is a member of the generalized gamma family. this model avoids some shortcomings of a previously suggested time-varying frailty model, notably time-dependent support. we describe some key properties of the model, including its relative frailty variance function in different settings and how the model can be fitted to data. we describe several applications to shared frailty modeling of bivariate current status data on infectious diseases, in which the frailty represents age-dependent heterogeneity in contact rates or susceptibility to infection.","10.1214/13-aoas693","2014-04-29","","['doyo g. enki', 'angela noufaily', 'c. paddy farrington']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"862",1404.7547,"on the impact of dimension reduction on graphical structures","stat.me","statisticians and quantitative neuroscientists have actively promoted the use of independence relationships for investigating brain networks, genomic networks, and other measurement technologies. estimation of these graphs depends on two steps. first is a feature extraction by summarizing measurements within a parcellation, regional or set definition to create nodes. secondly, these summaries are then used to create a graph representing relationships of interest. in this manuscript we study the impact of dimension reduction on graphs that describe different notions of relations among a set of random variables. we are particularly interested in undirected graphs that capture the random variables' independence and conditional independence relations. a dimension reduction procedure can be any mapping from high dimensional spaces to low dimensional spaces. we exploit a general framework for modeling the raw data and advocate that in estimating the undirected graphs, any acceptable dimension reduction procedure should be a graph-homotopic mapping, i.e., the graphical structure of the data after dimension reduction should inherit the main characteristics of the graphical structure of the raw data. we show that, in terms of inferring undirected graphs that characterize the conditional independence relations among random variables, many dimension reduction procedures, such as the mean, median, or principal components, cannot be theoretically guaranteed to be a graph-homotopic mapping. the implications of this work are broad. in the most charitable setting for researchers, where the correct node definition is known, graphical relationships can be contaminated merely via the dimension reduction. the manuscript ends with a concrete example, characterizing a subset of graphical structures such that the dimension reduction procedure using the principal components can be a graph-homotopic mapping.","","2014-04-29","2014-10-13","['fang han', 'huitong qiu', 'han liu', 'brian caffo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"863",1404.7555,"validating predictions of unobserved quantities","physics.data-an stat.me","the ultimate purpose of most computational models is to make predictions, commonly in support of some decision-making process (e.g., for design or operation of some system). the quantities that need to be predicted (the quantities of interest or qois) are generally not experimentally observable before the prediction, since otherwise no prediction would be needed. assessing the validity of such extrapolative predictions, which is critical to informed decision-making, is challenging. in classical approaches to validation, model outputs for observed quantities are compared to observations to determine if they are consistent. by itself, this consistency only ensures that the model can predict the observed quantities under the conditions of the observations. this limitation dramatically reduces the utility of the validation effort for decision making because it implies nothing about predictions of unobserved qois or for scenarios outside of the range of observations. however, there is no agreement in the scientific community today regarding best practices for validation of extrapolative predictions made using computational models. the purpose of this paper is to propose and explore a validation and predictive assessment process that supports extrapolative predictions for models with known sources of error. the process includes stochastic modeling, calibration, validation, and predictive assessment phases where representations of known sources of uncertainty and error are built, informed, and tested. the proposed methodology is applied to an illustrative extrapolation problem involving a misspecified nonlinear oscillator.","10.1016/j.cma.2014.08.023","2014-04-29","","['todd a. oliver', 'gabriel terejanu', 'christopher s. simmons', 'robert d. moser']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"864",1404.7595,"a quantile regression model for failure-time data with time-dependent   covariates","stat.me","since survival data occur over time, often important covariates that we wish to consider also change over time. such covariates are referred as time-dependent covariates. quantile regression offers flexible modeling of survival data by allowing the covariates to vary with quantiles. this paper provides a novel quantile regression model accommodating time-dependent covariates, for analyzing survival data subject to right censoring. our simple estimation technique assumes the existence of instrumental variables. in addition, we present a doubly-robust estimator in the sense of robins and rotnitzky (1992). the asymptotic properties of the estimators are rigorously studied. finite-sample properties are demonstrated by a simulation study. the utility of the proposed methodology is demonstrated using the stanford heart transplant dataset.","","2014-04-30","","['malka gorfine', 'yair goldberg', 'yaacov ritov']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"865",1404.7625,"the r package jmbayes for fitting joint models for longitudinal and   time-to-event data using mcmc","stat.co stat.ap","joint models for longitudinal and time-to-event data constitute an attractive modeling framework that has received a lot of interest in the recent years. this paper presents the capabilities of the r package jmbayes for fitting these models under a bayesian approach using markon chain monte carlo algorithms. jmbayes can fit a wide range of joint models, including among others joint models for continuous and categorical longitudinal responses, and provides several options for modeling the association structure between the two outcomes. in addition, this package can be used to derive dynamic predictions for both outcomes, and offers several tools to validate these predictions in terms of discrimination and calibration. all these features are illustrated using a real data example on patients with primary biliary cirrhosis.","","2014-04-30","","['dimitris rizopoulos']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE
"866",1404.7653,"the role of the information set for forecasting - with applications to   risk management","stat.ap q-fin.rm","predictions are issued on the basis of certain information. if the forecasting mechanisms are correctly specified, a larger amount of available information should lead to better forecasts. for point forecasts, we show how the effect of increasing the information set can be quantified by using strictly consistent scoring functions, where it results in smaller average scores. further, we show that the classical diebold-mariano test, based on strictly consistent scoring functions and asymptotically ideal forecasts, is a consistent test for the effect of an increase in a sequence of information sets on $h$-step point forecasts. for the value at risk (var), we show that the average score, which corresponds to the average quantile risk, directly relates to the expected shortfall. thus, increasing the information set will result in var forecasts which lead on average to smaller expected shortfalls. we illustrate our results in simulations and applications to stock returns for unconditional versus conditional risk management as well as univariate modeling of portfolio returns versus multivariate modeling of individual risk factors. the role of the information set for evaluating probabilistic forecasts by using strictly proper scoring rules is also discussed.","10.1214/13-aoas709","2014-04-30","","['hajo holzmann', 'matthias eulert']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"867",1405.0629,"local-aggregate modeling for big-data via distributed optimization:   applications to neuroimaging","stat.me","technological advances have led to a proliferation of structured big data that have matrix-valued covariates. we are specifically motivated to build predictive models for multi-subject neuroimaging data based on each subject's brain imaging scans. this is an ultra-high-dimensional problem that consists of a matrix of covariates (brain locations by time points) for each subject; few methods currently exist to fit supervised models directly to this tensor data. we propose a novel modeling and algorithmic strategy to apply generalized linear models (glms) to this massive tensor data in which one set of variables is associated with locations. our method begins by fitting glms to each location separately, and then builds an ensemble by blending information across locations through regularization with what we term an aggregating penalty. our so called, local-aggregate model, can be fit in a completely distributed manner over the locations using an alternating direction method of multipliers (admm) strategy, and thus greatly reduces the computational burden. furthermore, we propose to select the appropriate model through a novel sequence of faster algorithmic solutions that is similar to regularization paths. we will demonstrate both the computational and predictive modeling advantages of our methods via simulations and an eeg classification problem.","","2014-05-03","2015-05-14","['yue hu', 'genevera i. allen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"868",1405.0788,"quantifying alternative splicing from paired-end rna-sequencing data","stat.ap q-bio.gn","rna-sequencing has revolutionized biomedical research and, in particular, our ability to study gene alternative splicing. the problem has important implications for human health, as alternative splicing may be involved in malfunctions at the cellular level and multiple diseases. however, the high-dimensional nature of the data and the existence of experimental biases pose serious data analysis challenges. we find that the standard data summaries used to study alternative splicing are severely limited, as they ignore a substantial amount of valuable information. current data analysis methods are based on such summaries and are hence suboptimal. further, they have limited flexibility in accounting for technical biases. we propose novel data summaries and a bayesian modeling framework that overcome these limitations and determine biases in a nonparametric, highly flexible manner. these summaries adapt naturally to the rapid improvements in sequencing technology. we provide efficient point estimates and uncertainty assessments. the approach allows to study alternative splicing patterns for individual samples and can also be the basis for downstream analyses. we found a severalfold improvement in estimation mean square error compared popular approaches in simulations, and substantially higher consistency between replicates in experimental data. our findings indicate the need for adjusting the routine summarization and analysis of alternative splicing rna-seq studies. we provide a software implementation in the r package casper.","10.1214/13-aoas687","2014-05-05","2015-12-10","['david rossell', 'camille stephan-otto attolini', 'manuel kroiss', 'almond stöcker']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"869",1405.0803,"statistical analysis of trajectories on riemannian manifolds: bird   migration, hurricane tracking and video surveillance","stat.ap","we consider the statistical analysis of trajectories on riemannian manifolds that are observed under arbitrary temporal evolutions. past methods rely on cross-sectional analysis, with the given temporal registration, and consequently may lose the mean structure and artificially inflate observed variances. we introduce a quantity that provides both a cost function for temporal registration and a proper distance for comparison of trajectories. this distance is used to define statistical summaries, such as sample means and covariances, of synchronized trajectories and ""gaussian-type"" models to capture their variability at discrete times. it is invariant to identical time-warpings (or temporal reparameterizations) of trajectories. this is based on a novel mathematical representation of trajectories, termed transported square-root vector field (tsrvf), and the $\mathbb{l}^2$ norm on the space of tsrvfs. we illustrate this framework using three representative manifolds---$\mathbb{s}^2$, $\mathrm {se}(2)$ and shape space of planar contours---involving both simulated and real data. in particular, we demonstrate: (1) improvements in mean structures and significant reductions in cross-sectional variances using real data sets, (2) statistical modeling for capturing variability in aligned trajectories, and (3) evaluating random trajectories under these models. experimental results concern bird migration, hurricane tracking and video surveillance.","10.1214/13-aoas701","2014-05-05","","['jingyong su', 'sebastian kurtek', 'eric klassen', 'anuj srivastava']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"870",1405.1444,"understanding protein dynamics with l1-regularized reversible hidden   markov models","q-bio.bm stat.ap stat.ml","we present a machine learning framework for modeling protein dynamics. our approach uses l1-regularized, reversible hidden markov models to understand large protein datasets generated via molecular dynamics simulations. our model is motivated by three design principles: (1) the requirement of massive scalability; (2) the need to adhere to relevant physical law; and (3) the necessity of providing accessible interpretations, critical for both cellular biology and rational drug design. we present an em algorithm for learning and introduce a model selection criteria based on the physical notion of convergence in relaxation timescales. we contrast our model with standard methods in biophysics and demonstrate improved robustness. we implement our algorithm on gpus and apply the method to two large protein simulation datasets generated respectively on the ncsa bluewaters supercomputer and the folding@home distributed computing network. our analysis identifies the conformational dynamics of the ubiquitin protein critical to cellular signaling, and elucidates the stepwise activation mechanism of the c-src kinase protein.","","2014-05-06","","['robert t. mcgibbon', 'bharath ramsundar', 'mohammad m. sultan', 'gert kiss', 'vijay s. pande']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"871",1405.1491,"demonstration of enhanced monte carlo computation of the fisher   information for complex problems","stat.co","the fisher information matrix summarizes the amount of information in a set of data relative to the quantities of interest. there are many applications of the information matrix in statistical modeling, system identification and parameter estimation. this short paper reviews a feedback-based method and an independent perturbation approach for computing the information matrix for complex problems, where a closed form of the information matrix is not achievable. we show through numerical examples how these methods improve the accuracy of the estimate of the information matrix compared to the basic resampling-based approach. some relevant theory is summarized.","","2014-05-06","","['xumeng cao']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"872",1405.2601,"lp approach to statistical modeling","math.st stat.me stat.th","we present an approach to statistical data modeling and exploratory data analysis called `lp statistical data science.' it aims to generalize and unify traditional and novel statistical measures, methods, and exploratory tools. this article outlines fundamental concepts along with real-data examples to illustrate how the `lp statistical algorithm' can systematically tackle different varieties of data types, data patterns, and data structures under a coherent theoretical framework. a fundamental role is played by specially designed orthonormal basis of a random variable x for linear (hilbert space theory) representation of a general function of x, such as $\mbox{e}[y \mid x]$.","","2014-05-11","","['subhadeep mukhopadhyay', 'emanuel parzen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"873",1405.2951,"a neuron as a signal processing device","q-bio.nc stat.ml","a neuron is a basic physiological and computational unit of the brain. while much is known about the physiological properties of a neuron, its computational role is poorly understood. here we propose to view a neuron as a signal processing device that represents the incoming streaming data matrix as a sparse vector of synaptic weights scaled by an outgoing sparse activity vector. formally, a neuron minimizes a cost function comprising a cumulative squared representation error and regularization terms. we derive an online algorithm that minimizes such cost function by alternating between the minimization with respect to activity and with respect to synaptic weights. the steps of this algorithm reproduce well-known physiological properties of a neuron, such as weighted summation and leaky integration of synaptic inputs, as well as an oja-like, but parameter-free, synaptic learning rule. our theoretical framework makes several predictions, some of which can be verified by the existing data, others require further experiments. such framework should allow modeling the function of neuronal circuits without necessarily measuring all the microscopic biophysical parameters, as well as facilitate the design of neuromorphic electronics.","10.1109/acssc.2013.6810296","2014-05-12","","['tao hu', 'zaid j. towfic', 'cengiz pehlevan', 'alex genkin', 'dmitri b. chklovskii']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"874",1405.3726,"topic words analysis based on lda model","cs.si cs.dc cs.ir cs.lg stat.ml","social network analysis (sna), which is a research field describing and modeling the social connection of a certain group of people, is popular among network services. our topic words analysis project is a sna method to visualize the topic words among emails from obama.com to accounts registered in columbus, ohio. based on latent dirichlet allocation (lda) model, a popular topic model of sna, our project characterizes the preference of senders for target group of receptors. gibbs sampling is used to estimate topic and word distribution. our training and testing data are emails from the carbon-free server datagreening.com. we use parallel computing tool bashreduce for word processing and generate related words under each latent topic to discovers typical information of political news sending specially to local columbus receptors. running on two instances using paralleling tool bashreduce, our project contributes almost 30% speedup processing the raw contents, comparing with processing contents on one instance locally. also, the experimental result shows that the lda model applied in our project provides precision rate 53.96% higher than tf-idf model finding target words, on the condition that appropriate size of topic words list is selected.","","2014-05-14","","['xi qiu', 'christopher stewart']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"875",1405.388,"bayesian semiparametric hierarchical empirical likelihood spatial models","stat.me","we introduce a general hierarchical bayesian framework that incorporates a flexible nonparametric data model specification through the use of empirical likelihood methodology, which we term semiparametric hierarchical empirical likelihood (shel) models. although general dependence structures can be readily accommodated, we focus on spatial modeling, a relatively underdeveloped area in the empirical likelihood literature. importantly, the models we develop naturally accommodate spatial association on irregular lattices and irregularly spaced point-referenced data. we illustrate our proposed framework by means of a simulation study and through three real data examples. first, we develop a spatial fay-herriot model in the shel framework and apply it to the problem of small area estimation in the american community survey. next, we illustrate the shel model in the context of areal data (on an irregular lattice) through the north carolina sudden infant death syndrome (sids) dataset. finally, we analyze a point-referenced dataset from the north american breeding bird survey that considers dove counts for the state of missouri. in all cases, we demonstrate superior performance of our model, in terms of mean squared prediction error, over standard parametric analyses.","","2014-05-15","","['aaron t. porter', 'scott h. holan', 'christopher k. wikle']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"876",1405.4323,"stochastic volatility filtering with intractable likelihoods","stat.co","this paper is concerned with particle filtering for $\alpha$-stable stochastic volatility models. the $\alpha$-stable distribution provides a flexible framework for modeling asymmetry and heavy tails, which is useful when modeling financial returns. an issue with this distributional assumption is the lack of a closed form for the probability density function. to estimate the volatility of financial returns in this setting, we develop a novel auxiliary particle filter. the algorithm we develop can be easily applied to any hidden markov model for which the likelihood function is intractable or computationally expensive. the approximate target distribution of our auxiliary filter is based on the idea of approximate bayesian computation (abc). abc methods allow for inference on posterior quantities in situations when the likelihood of the underlying model is not available in closed form, but simulating samples from it is possible. the abc auxiliary particle filter (abc-apf) that we propose provides not only a good alternative to state estimation in stochastic volatility models, but it also improves on the existing abc literature. it allows for more flexibility in state estimation while improving on the accuracy through better proposal distributions in cases when the optimal importance density of the filter is unavailable in closed form. we assess the performance of the abc-apf on a simulated dataset from the $\alpha$-stable stochastic volatility model and compare it to other currently existing abc filters.","","2014-05-16","","['emilian vankov', 'katherine b. ensor']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"877",1405.4574,"kronecker pca based spatio-temporal modeling of video for dismount   classification","cs.cv stat.me","we consider the application of kronpca spatio-temporal modeling techniques [greenewald et al 2013, tsiligkaridis et al 2013] to the extraction of spatiotemporal features for video dismount classification. kronpca performs a low-rank type of dimensionality reduction that is adapted to spatio-temporal data and is characterized by the t frame multiframe mean and covariance of p spatial features. for further regularization and improved inverse estimation, we also use the diagonally corrected kronpca shrinkage methods we presented in [greenewald et al 2013]. we apply this very general method to the modeling of the multivariate temporal behavior of hog features extracted from pedestrian bounding boxes in video, with gender classification in a challenging dataset chosen as a specific application. the learned covariances for each class are used to extract spatiotemporal features which are then classified, achieving competitive classification performance.","10.1117/12.2050184","2014-05-18","","['kristjan h. greenewald', 'alfred o. hero']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"878",1405.4637,"inflated discrete beta regression models for likert and discrete rating   scale outcomes","stat.me stat.ap","discrete ordinal responses such as likert scales are regularly proposed in questionnaires and used as dependent variable in modeling. the response distribution for such scales is always discrete, with bounded support and often skewed. in addition, one particular level of the scale is frequently inflated as it cumulates respondents who invariably choose that particular level (typically the middle or one extreme of the scale) without hesitation with those who chose that alternative but might have selected a neighboring one. the inflated discrete beta regression (idbr) model addresses those four critical characteristics that have never been taken into account simultaneously by existing models. the mean and the dispersion of rates are jointly regressed on covariates using an underlying beta distribution. the probability that choosers of the inflated level invariably make that choice is also regressed on covariates. simulation studies used to evaluate the statistical properties of the idbr model suggest that it produces more precise predictions than competing models. the ability to jointly model the location and dispersion of (the distribution of) an ordinal response, as well as to characterize the profile of subject selecting an ""inflated"" alternative are the most relevant features of the idbr model. it is illustrated with the analysis of the political positioning on a ""left-right"" scale of the belgian respondents in the 2012 european social survey.","","2014-05-19","","['cedric taverne', 'philippe lambert']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"879",1405.4696,"experiences in bayesian inference in baltic salmon management","stat.me stat.ap","we review a success story regarding bayesian inference in fisheries management in the baltic sea. the management of salmon fisheries is currently based on the results of a complex bayesian population dynamic model, and managers and stakeholders use the probabilities in their discussions. we also discuss the technical and human challenges in using bayesian modeling to give practical advice to the public and to government officials and suggest future areas in which it can be applied. in particular, large databases in fisheries science offer flexible ways to use hierarchical models to learn the population dynamics parameters for those by-catch species that do not have similar large stock-specific data sets like those that exist for many target species. this information is required if we are to understand the future ecosystem risks of fisheries.","10.1214/13-sts431","2014-05-19","","['sakari kuikka', 'jarno vanhatalo', 'henni pulkkinen', 'samu mäntyniemi', 'jukka corander']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"880",1405.517,"the romes method for statistical modeling of reduced-order-model error","cs.na math.na stat.ml","this work presents a technique for statistically modeling errors introduced by reduced-order models. the method employs gaussian-process regression to construct a mapping from a small number of computationally inexpensive `error indicators' to a distribution over the true error. the variance of this distribution can be interpreted as the (epistemic) uncertainty introduced by the reduced-order model. to model normed errors, the method employs existing rigorous error bounds and residual norms as indicators; numerical experiments show that the method leads to a near-optimal expected effectivity in contrast to typical error bounds. to model errors in general outputs, the method uses dual-weighted residuals---which are amenable to uncertainty control---as indicators. experiments illustrate that correcting the reduced-order-model output with this surrogate can improve prediction accuracy by an order of magnitude; this contrasts with existing `multifidelity correction' approaches, which often fail for reduced-order models and suffer from the curse of dimensionality. the proposed error surrogates also lead to a notion of `probabilistic rigor', i.e., the surrogate bounds the error with specified probability.","10.1137/140969841","2014-05-20","2014-12-10","['martin drohmann', 'kevin carlberg']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"881",1405.5978,"blockmodeling of multilevel networks","stat.me cs.si physics.soc-ph","the article presents several approaches to the blockmodeling of multilevel network data. multilevel network data consist of networks that are measured on at least two levels (e.g. between organizations and people) and information on ties between those levels (e.g. information on which people are members of which organizations). several approaches will be considered: a separate analysis of the levels; transforming all networks to one level and blockmodeling on this level using information from all levels; and a truly multilevel approach where all levels and ties among them are modeled at the same time. advantages and disadvantages of these approaches will be discussed.","10.1016/j.socnet.2014.04.002","2014-05-23","","['aleš žiberna']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"882",1405.6531,"gaussian random functional dynamic spatio-temporal modeling of discrete   time spatial time series data","stat.me math.st stat.th","discrete time spatial time series data arise routinely in meteorological and environmental studies. inference and prediction associated with them are mostly carried out using any of the several variants of the linear state space model that are collectively called linear dynamic spatio-temporal models (ldstms). however, real world environmental processes are highly complex and are seldom representable by models with such simple linear structure. hence, nonlinear dynamic spatio-temporal models (nldstms) based on the idea of nonlinear observational and evolutionary equation have been proposed as an alternative. however, in that case, the caveat lies in selecting the specific form of nonlinearity from a large class of potentially appropriate nonlinear functions. moreover, modeling by nldstms requires precise knowledge about the dynamics underlying the data. in this article, we address this problem by introducing the gaussian random functional dynamic spatio-temporal model (grfdstm). unlike the ldstms or nldstms, in grfdstm both the functions governing the observational and evolutionary equations are composed of gaussian random functions. we exhibit many interesting theoretical properties of the grfdstm and demonstrate how model fitting and prediction can be carried out coherently in a bayesian framework. we also conduct an extensive simulation study and apply our model to a real, so2 pollution data over europe. the results are highly encouraging.","","2014-05-26","2017-08-24","['suman guha', 'sourabh bhattacharya']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"883",1405.6693,"on oracle property and asymptotic validity of bayesian generalized   method of moments","math.st stat.th","statistical inference based on moment conditions and estimating equations is of substantial interest when it is difficult to specify a full probabilistic model. we propose a bayesian flavored model selection framework based on (quasi-)posterior probabilities from the bayesian generalized method of moments (bgmm), which allows us to incorporate two important advantages of a bayesian approach: the expressiveness of posterior distributions and the convenient computational method of markov chain monte carlo (mcmc). theoretically we show that bgmm can achieve the posterior consistency for selecting the unknown true model, and that it possesses a bayesian version of the oracle property, i.e. the posterior distribution for the parameter of interest is asymptotically normal and is as informative as if the true model were known. in addition, we show that the proposed quasi-posterior is valid to be interpreted as an approximate posterior distribution given a data summary. our applications include modeling of correlated data, quantile regression, and graphical models based on partial correlations. we demonstrate the implementation of the bgmm model selection through numerical examples.","10.1016/j.jmva.2015.12.009","2014-05-26","2016-10-06","['cheng li', 'wenxin jiang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"884",1405.7227,"bayesian spatial change of support for count-valued survey data","stat.ap","we introduce bayesian spatial change of support methodology for count-valued survey data with known survey variances. our proposed methodology is motivated by the american community survey (acs), an ongoing survey administered by the u.s. census bureau that provides timely information on several key demographic variables. specifically, the acs produces 1-year, 3-year, and 5-year ""period-estimates,"" and corresponding margins of errors, for published demographic and socio-economic variables recorded over predefined geographies within the united states. despite the availability of these predefined geographies it is often of interest to data users to specify customized user-defined spatial supports. in particular, it is useful to estimate demographic variables defined on ""new"" spatial supports in ""real-time."" this problem is known as spatial change of support (cos), which is typically performed under the assumption that the data follows a gaussian distribution. however, count-valued survey data is naturally non-gaussian and, hence, we consider modeling these data using a poisson distribution. additionally, survey-data are often accompanied by estimates of error, which we incorporate into our analysis. we interpret poisson count-valued data in small areas as an aggregation of events from a spatial point process. this approach provides us with the flexibility necessary to allow acs users to consider a variety of spatial supports in ""real-time."" we demonstrate the effectiveness of our approach through a simulated example as well as through an analysis using public-use acs data.","","2014-05-28","2014-10-28","['jonathan r. bradley', 'christopher k. wikle', 'scott h. holan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"885",1405.7393,"insights from the wikipedia contest (ieee contest for data mining 2011)","cs.cy physics.soc-ph stat.ml","the wikimedia foundation has recently observed that newly joining editors on wikipedia are increasingly failing to integrate into the wikipedia editors' community, i.e. the community is becoming increasingly harder to penetrate. to sustain healthy growth of the community, the wikimedia foundation aims to quantitatively understand the factors that determine the editing behavior, and explain why most new editors become inactive soon after joining. as a step towards this broader goal, the wikimedia foundation sponsored the icdm (ieee international conference for data mining) contest for the year 2011.   the objective for the participants was to develop models to predict the number of edits that an editor will make in future five months based on the editing history of the editor. here we describe the approach we followed for developing predictive models towards this goal, the results that we obtained and the modeling insights that we gained from this exercise. in addition, towards the broader goal of wikimedia foundation, we also summarize the factors that emerged during our model building exercise as powerful predictors of future editing activity.","","2014-01-07","","['kalpit v desai', 'roopesh ranjan']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"886",1405.7746,"a class of regression models for parallel and series systems with a   random number of components","stat.me","in this paper we extend the weibull power series (wps) class of distributions and named this new class as extended weibull power series (ewps) class of distributions. the ewps distributions are related to series and parallel systems with a random num- ber of components, whereas the wps distributions (morais and barreto-souza, 2011) are related to series systems only. unlike the wps distributions, for which the weibull is a limiting special case, the weibull law is a particular case of the ewps distributions. we prove that the distributions in this class are identifiable under a simple assumption. we also prove stochastic and hazard rate order results and highlight that the shapes of the ewps distributions are markedly more flexible than the shapes of the wps distributions. we define a regression model for the ewps response random variable to model a scale parameter and its quantiles. we present the maximum likelihood estimator and prove its consistency and normal asymptotic distribution. although the construction of this class was motivated by series and parallel systems, the ewps distributions are suitable for modeling a wide range of positive data sets. to illustrate potential uses of this model, we apply it to a real data set on the tensile strength of coconut fibers and present a simple device for diagnostic purposes.","","2014-05-29","2014-07-30","['alice l. morais', 'silvia l. p. ferrari']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"887",1406.0193,"inference of sparse networks with unobserved variables. application to   gene regulatory networks","stat.ml cs.lg q-bio.mn q-bio.qm stat.ap","networks are a unifying framework for modeling complex systems and network inference problems are frequently encountered in many fields. here, i develop and apply a generative approach to network inference (rcweb) for the case when the network is sparse and the latent (not observed) variables affect the observed ones. from all possible factor analysis (fa) decompositions explaining the variance in the data, rcweb selects the fa decomposition that is consistent with a sparse underlying network. the sparsity constraint is imposed by a novel method that significantly outperforms (in terms of accuracy, robustness to noise, complexity scaling, and computational efficiency) bayesian methods and mle methods using l1 norm relaxation such as k-svd and l1--based sparse principle component analysis (pca). results from simulated models demonstrate that rcweb recovers exactly the model structures for sparsity as low (as non-sparse) as 50% and with ratio of unobserved to observed variables as high as 2. rcweb is robust to noise, with gradual decrease in the parameter ranges as the noise level increases.","","2014-06-01","","['nikolai slavov']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"888",1406.0758,"pythagoras at the bat","math.ho stat.ot","the pythagorean formula is one of the most popular ways to measure the true ability of a team. it is very easy to use, estimating a team's winning percentage from the runs they score and allow. this data is readily available on standings pages; no computationally intensive simulations are needed. normally accurate to within a few games per season, it allows teams to determine how much a run is worth in different situations. this determination helps solve some of the most important economic decisions a team faces: how much is a player worth, which players should be pursued, and how much should they be offered. we discuss the formula and these applications in detail, and provide a theoretical justification, both for the formula as well as simpler linear estimators of a team's winning percentage. the calculations and modeling are discussed in detail, and when possible multiple proofs are given. we analyze the 2012 season in detail, and see that the data for that and other recent years support our modeling conjectures. we conclude with a discussion of work in progress to generalize the formula and increase its predictive power \emph{without} needing expensive simulations, though at the cost of requiring play-by-play data.","","2014-05-29","","['steven j. miller', 'taylor corcoran', 'jennifer gossels', 'victor luo', 'jaclyn porfilio']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE
"889",1406.0801,"the cepstral model for multivariate time series: the vector exponential   model","stat.me","vector autoregressive (var) models have become a staple in the analysis of multivariate time series and are formulated in the time domain as difference equations, with an implied covariance structure. in many contexts, it is desirable to work with a stable, or at least stationary, representation. to fit such models, one must impose restrictions on the coefficient matrices to ensure that certain determinants are nonzero; which, except in special cases, may prove burdensome. to circumvent these difficulties, we propose a flexible frequency domain model expressed in terms of the spectral density matrix. specifically, this paper treats the modeling of covariance stationary vector-valued (i.e., multivariate) time series via an extension of the exponential model for the spectrum of a scalar time series. we discuss the modeling advantages of the vector exponential model and its computational facets, such as how to obtain wold coefficients from given cepstral coefficients. finally, we demonstrate the utility of our approach through simulation as well as two illustrative data examples focusing on multi-step ahead forecasting and estimation of squared coherence.","","2014-06-03","","['scott h. holan', 'tucker s. mcelroy', 'guohui wu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"890",1406.123,"statistical intercell interference modeling for capacity-coverage   tradeoff analysis in downlink cellular networks","cs.it math.it math.st stat.th","interference shapes the interplay between capacity and coverage in cellular networks. however, interference is non-deterministic and depends on various system and channel parameters including user scheduling, frequency reuse, and fading variations. we present an analytical approach for modeling the distribution of intercell interference in the downlink of cellular networks as a function of generic fading channel models and various scheduling schemes. we demonstrate the usefulness of the derived expressions in calculating location-based and average-based data rates in addition to capturing practical tradeoffs between cell capacity and coverage in downlink cellular networks.","10.1109/iccspa.2015.7081269","2014-06-04","","['naeem akl', 'jihad fahs', 'zaher dawy']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"891",1406.1845,"formal hypothesis tests for additive structure in random forests","stat.ml stat.ap","while statistical learning methods have proved powerful tools for predictive modeling, the black-box nature of the models they produce can severely limit their interpretability and the ability to conduct formal inference. however, the natural structure of ensemble learners like bagged trees and random forests has been shown to admit desirable asymptotic properties when base learners are built with proper subsamples. in this work, we demonstrate that by defining an appropriate grid structure on the covariate space, we may carry out formal hypothesis tests for both variable importance and underlying additive model structure. to our knowledge, these tests represent the first statistical tools for investigating the underlying regression structure in a context such as random forests. we develop notions of total and partial additivity and further demonstrate that testing can be carried out at no additional computational cost by estimating the variance within the process of constructing the ensemble. furthermore, we propose a novel extension of these testing procedures utilizing random projections in order to allow for computationally efficient testing procedures that retain high power even when the grid size is much larger than that of the training set.","","2014-06-06","2016-08-26","['lucas mentch', 'giles hooker']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"892",1406.2725,"bayesian benchmark dose analysis","stat.me","an important objective in environmental risk assessment is estimation of minimum exposure levels, called benchmark doses (bmds) that induce a pre-specified benchmark response (bmr) in a target population. established inferential approaches for bmd analysis typically involve one-sided, frequentist confidence limits, leading in practice to what are called benchmark dose lower limits (bmdls). appeal to bayesian modeling and credible limits for building bmdls is far less developed, however. indeed, for the few existing forms of bayesian bmds, informative prior information is seldom incorporated. we develop reparameterized quantal-response models that explicitly describe the bmd as a target parameter. our goal is to obtain an improved estimation and calculation archetype for the bmd and for the bmdl, by employing quantifiable prior belief to represent parameter uncertainty in the statistical model. implementation is facilitated via a monte carlo-based adaptive metropolis (am) algorithm to approximate the posterior distribution. an example from environmental carcinogenicity testing illustrates the calculations.","","2014-06-10","","['qijun fang', 'walter w. piegorsch', 'katherine y. barnes']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"893",1406.2845,"statistical estimation of jump rates for a specific class of piecewise   deterministic markov processes","math.st stat.th","we consider the class of piecewise deterministic markov processes (pdmp), whose state space is $\r\_{+}^{*}$, that possess an increasing deterministic motion and that shrink deterministically when they jump. well known examples for this class of processes are transmission control protocol (tcp) window size process and the processes modeling the size of a ""marked"" {\it escherichia coli} cell. having observed the pdmp until its $n$th jump, we construct a nonparametric estimator of the jump rate $\lambda$. our main result is that for $d$ a compact subset of $\r\_{+}^{*}$, if $\lambda$ is in the h{\''{o}}lder space ${\mathcal h}^s({\mathcal d})$, the squared-loss error of the estimator is asymptotically close to the rate of $n^{-s/(2s+1)}$. simulations illustrate the behavior of our estimator.","","2014-06-11","2015-03-10","['nathalie krell']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"894",1406.2989,"techniques for learning binary stochastic feedforward neural networks","stat.ml cs.lg cs.ne","stochastic binary hidden units in a multi-layer perceptron (mlp) network give at least three potential benefits when compared to deterministic mlp networks. (1) they allow to learn one-to-many type of mappings. (2) they can be used in structured prediction problems, where modeling the internal structure of the output is important. (3) stochasticity has been shown to be an excellent regularizer, which makes generalization performance potentially better in general. however, training stochastic networks is considerably more difficult. we study training using m samples of hidden activations per input. we show that the case m=1 leads to a fundamentally different behavior where the network tries to avoid stochasticity. we propose two new estimators for the training gradient and propose benchmark tests for comparing training algorithms. our experiments confirm that training stochastic networks is difficult and show that the proposed two estimators perform favorably among all the five known estimators.","","2014-06-11","2015-04-09","['tapani raiko', 'mathias berglund', 'guillaume alain', 'laurent dinh']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"895",1406.3402,"relieving and readjusting pythagoras","math.ho stat.ap","bill james invented the pythagorean expectation in the late 70's to predict a baseball team's winning percentage knowing just their runs scored and allowed. his original formula estimates a winning percentage of ${\rm rs}^2/({\rm rs}^2+{\rm ra}^2)$, where ${\rm rs}$ stands for runs scored and ${\rm ra}$ for runs allowed; later versions found better agreement with data by replacing the exponent 2 with numbers near 1.83. miller and his colleagues provided a theoretical justification by modeling runs scored and allowed by independent weibull distributions. they showed that a single weibull distribution did a very good job of describing runs scored and allowed, and led to a predicted won-loss percentage of $({\rm rs_{\rm obs}}-1/2)^\gamma / (({\rm rs_{\rm obs}}-1/2)^\gamma + ({\rm ra_{\rm obs}}-1/2)^\gamma)$, where ${\rm rs_{\rm obs}}$ and ${\rm ra_{\rm obs}}$ are the observed runs scored and allowed and $\gamma$ is the shape parameter of the weibull (typically close to 1.8). we show a linear combination of weibulls more accurately determines a team's run production and increases the prediction accuracy of a team's winning percentage by an average of about 25% (thus while the currently used variants of the original predictor are accurate to about four games a season, the new combination is accurate to about three). the new formula is more involved computationally; however, it can be easily computed on a laptop in a matter of minutes from publicly available season data. it performs as well (or slightly better) than the related pythagorean formulas in use, and has the additional advantage of having a theoretical justification for its parameter values (and not just an optimization of parameters to minimize prediction error).","","2014-06-12","2014-06-16","['victor luo', 'steven j. miller']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"896",1406.3792,"interval forecasting of electricity demand: a novel bivariate emd-based   support vector regression modeling framework","cs.lg stat.ap","highly accurate interval forecasting of electricity demand is fundamental to the success of reducing the risk when making power system planning and operational decisions by providing a range rather than point estimation. in this study, a novel modeling framework integrating bivariate empirical mode decomposition (bemd) and support vector regression (svr), extended from the well-established empirical mode decomposition (emd) based time series modeling framework in the energy demand forecasting literature, is proposed for interval forecasting of electricity demand. the novelty of this study arises from the employment of bemd, a new extension of classical empirical model decomposition (emd) destined to handle bivariate time series treated as complex-valued time series, as decomposition method instead of classical emd only capable of decomposing one-dimensional single-valued time series. this proposed modeling framework is endowed with bemd to decompose simultaneously both the lower and upper bounds time series, constructed in forms of complex-valued time series, of electricity demand on a monthly per hour basis, resulting in capturing the potential interrelationship between lower and upper bounds. the proposed modeling framework is justified with monthly interval-valued electricity demand data per hour in pennsylvania-new jersey-maryland interconnection, indicating it as a promising method for interval-valued electricity demand forecasting.","10.1016/j.ijepes.2014.06.010","2014-06-14","","['tao xiong', 'yukun bao', 'zhongyi hu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"897",1406.3852,"a low variance consistent test of relative dependency","stat.ml cs.lg stat.co","we describe a novel non-parametric statistical hypothesis test of relative dependence between a source variable and two candidate target variables. such a test enables us to determine whether one source variable is significantly more dependent on a first target variable or a second. dependence is measured via the hilbert-schmidt independence criterion (hsic), resulting in a pair of empirical dependence measures (source-target 1, source-target 2). we test whether the first dependence measure is significantly larger than the second. modeling the covariance between these hsic statistics leads to a provably more powerful test than the construction of independent hsic statistics by sub-sampling. the resulting test is consistent and unbiased, and (being based on u-statistics) has favorable convergence properties. the test can be computed in quadratic time, matching the computational complexity of standard empirical hsic estimators. the effectiveness of the test is demonstrated on several real-world problems: we identify language groups from a multilingual corpus, and we prove that tumor location is more dependent on gene expression than chromosomal imbalances. source code is available for download at https://github.com/wbounliphone/reldep.","","2014-06-15","2015-05-27","['wacha bounliphone', 'arthur gretton', 'arthur tenenhaus', 'matthew blaschko']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE
"898",1406.4068,"functional regression","stat.me stat.ap stat.co","functional data analysis (fda) involves the analysis of data whose ideal units of observation are functions defined on some continuous domain, and the observed data consist of a sample of functions taken from some population, sampled on a discrete grid. ramsay and silverman's 1997 textbook sparked the development of this field, which has accelerated in the past 10 years to become one of the fastest growing areas of statistics, fueled by the growing number of applications yielding this type of data. one unique characteristic of fda is the need to combine information both across and within functions, which ramsay and silverman called replication and regularization, respectively. this article will focus on functional regression, the area of fda that has received the most attention in applications and methodological development. first will be an introduction to basis functions, key building blocks for regularization in functional regression methods, followed by an overview of functional regression methods, split into three types: [1] functional predictor regression (scalar-on-function), [2] functional response regression (function-on-scalar) and [3] function-on-function regression. for each, the role of replication and regularization will be discussed and the methodological development described in a roughly chronological manner, at times deviating from the historical timeline to group together similar methods. the primary focus is on modeling and methodology, highlighting the modeling structures that have been developed and the various regularization approaches employed. at the end is a brief discussion describing potential areas of future development in this field.","","2014-06-16","","['jeffrey s. morris']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"899",1406.4261,"failure inference and optimization for step stress model based on   bivariate wiener model","math.st stat.th","in this paper, we consider the situation under a life test, in which the failure time of the test units are not related deterministically to an observable stochastic time varying covariate. in such a case, the joint distribution of failure time and a marker value would be useful for modeling the step stress life test. the problem of accelerating such an experiment is considered as the main aim of this paper. we present a step stress accelerated model based on a bivariate wiener process with one component as the latent (unobservable) degradation process, which determines the failure times and the other as a marker process, the degradation values of which are recorded at times of failure. parametric inference based on the proposed model is discussed and the optimization procedure for obtaining the optimal time for changing the stress level is presented. the optimization criterion is to minimize the approximate variance of the maximum likelihood estimator of a percentile of the products' lifetime distribution.","","2014-06-17","","['s. shemehsavar', 'morteza amini']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"900",1406.458,"primitives for dynamic big model parallelism","stat.ml cs.dc cs.lg","when training large machine learning models with many variables or parameters, a single machine is often inadequate since the model may be too large to fit in memory, while training can take a long time even with stochastic updates. a natural recourse is to turn to distributed cluster computing, in order to harness additional memory and processors. however, naive, unstructured parallelization of ml algorithms can make inefficient use of distributed memory, while failing to obtain proportional convergence speedups - or can even result in divergence. we develop a framework of primitives for dynamic model-parallelism, strads, in order to explore partitioning and update scheduling of model variables in distributed ml algorithms - thus improving their memory efficiency while presenting new opportunities to speed up convergence without compromising inference correctness. we demonstrate the efficacy of model-parallel algorithms implemented in strads versus popular implementations for topic modeling, matrix factorization and lasso.","","2014-06-17","","['seunghak lee', 'jin kyu kim', 'xun zheng', 'qirong ho', 'garth a. gibson', 'eric p. xing']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"901",1406.6133,"modeling of end-use energy profile: an appliance-data-driven stochastic   approach","stat.ap","in this paper, the modeling of building end-use energy profile is comprehensively investigated. top-down and bottom-up approaches are discussed with a focus on the latter for better integration with occupant information. compared to the time-of-use (tou) data used in previous bottom-up models, this work utilizes high frequency sampled appliance power consumption data from wireless sensor network, and hence builds an appliance-data-driven probability based end-use energy profile model. on/off probabilities of appliances are used in this model, to build a non-homogeneous markov chain, compared to the duration statistics based model that is widely used in other works. the simulation results show the capability of the model to capture the diversity and variability of different categories of end-use appliance energy profile, which can further help on the design of a modern robust building power system.","10.1109/iecon.2014.7049322","2014-06-24","","['zhaoyi kang', 'ming jin', 'costas j. spanos']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"902",1406.6652,"data augmentation for models based on rejection sampling","stat.co","we present a data augmentation scheme to perform markov chain monte carlo inference for models where data generation involves a rejection sampling algorithm. our idea, which seems to be missing in the literature, is a simple scheme to instantiate the rejected proposals preceding each data point. the resulting joint probability over observed and rejected variables can be much simpler than the marginal distribution over the observed variables, which often involves intractable integrals. we consider three problems, the first being the modeling of flow-cytometry measurements subject to truncation. the second is a bayesian analysis of the matrix langevin distribution on the stiefel manifold, and the third, bayesian inference for a nonparametric gaussian process density model. the latter two are instances of problems where markov chain monte carlo inference is doubly-intractable. our experiments demonstrate superior performance over state-of-the-art sampling algorithms for such problems.","","2014-06-25","2015-08-03","['vinayak rao', 'lizhen lin', 'david dunson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"903",1406.7343,"hierarchical nearest-neighbor gaussian process models for large   geostatistical datasets","stat.me","spatial process models for analyzing geostatistical data entail computations that become prohibitive as the number of spatial locations become large. this manuscript develops a class of highly scalable nearest neighbor gaussian process (nngp) models to provide fully model-based inference for large geostatistical datasets. we establish that the nngp is a well-defined spatial process providing legitimate finite-dimensional gaussian densities with sparse precision matrices. we embed the nngp as a sparsity-inducing prior within a rich hierarchical modeling framework and outline how computationally efficient markov chain monte carlo (mcmc) algorithms can be executed without storing or decomposing large matrices. the floating point operations (flops) per iteration of this algorithm is linear in the number of spatial locations, thereby rendering substantial scalability. we illustrate the computational and inferential benefits of the nngp over competing methods using simulation studies and also analyze forest biomass from a massive united states forest inventory dataset at a scale that precludes alternative dimension-reducing methods.","","2014-06-27","2016-01-01","['abhirup datta', 'sudipto banerjee', 'andrew o. finley', 'alan e. gelfand']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"904",1406.7806,"building dnn acoustic models for large vocabulary speech recognition","cs.cl cs.lg cs.ne stat.ml","deep neural networks (dnns) are now a central component of nearly all state-of-the-art speech recognition systems. building neural network acoustic models requires several design decisions including network architecture, size, and training loss function. this paper offers an empirical investigation on which aspects of dnn acoustic model design are most important for speech recognition system performance. we report dnn classifier performance and final speech recognizer word error rates, and compare dnns using several metrics to quantify factors influencing differences in task performance. our first set of experiments use the standard switchboard benchmark corpus, which contains approximately 300 hours of conversational telephone speech. we compare standard dnns to convolutional networks, and present the first experiments using locally-connected, untied neural networks for acoustic modeling. we additionally build systems on a corpus of 2,100 hours of training data by combining the switchboard and fisher corpora. this larger corpus allows us to more thoroughly examine performance of large dnn models -- with up to ten times more parameters than those typically used in speech recognition systems. our results suggest that a relatively simple dnn architecture and optimization technique produces strong results. these findings, along with previous work, help establish a set of best practices for building dnn hybrid speech recognition systems with maximum likelihood training. our experiments in dnn optimization additionally serve as a case study for training dnns with discriminative loss functions for speech tasks, as well as dnn classifiers more generally.","","2014-06-30","2015-01-20","['andrew l. maas', 'peng qi', 'ziang xie', 'awni y. hannun', 'christopher t. lengerich', 'daniel jurafsky', 'andrew y. ng']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"905",1406.7851,"nonparametric bayes modeling of populations of networks","stat.me","replicated network data are increasingly available in many research fields. in connectomic applications, inter-connections among brain regions are collected for each patient under study, motivating statistical models which can flexibly characterize the probabilistic generative mechanism underlying these network-valued data. available models for a single network are not designed specifically for inference on the entire probability mass function of a network-valued random variable and therefore lack flexibility in characterizing the distribution of relevant topological structures. we propose a flexible bayesian nonparametric approach for modeling the population distribution of network-valued data. the joint distribution of the edges is defined via a mixture model which reduces dimensionality and efficiently incorporates network information within each mixture component by leveraging latent space representations. the formulation leads to an efficient gibbs sampler and provides simple and coherent strategies for inference and goodness-of-fit assessments. we provide theoretical results on the flexibility of our model and illustrate improved performance --- compared to state-of-the-art models --- in simulations and application to human brain networks.","10.1080/01621459.2016.1219260","2014-06-30","2016-06-05","['daniele durante', 'david b. dunson', 'joshua t. vogelstein']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"906",1407.0058,"spatial postprocessing of ensemble forecasts for temperature using   nonhomogeneous gaussian regression","stat.ap","statistical postprocessing techniques are commonly used to improve the skill of ensembles of numerical weather forecasts. this paper considers spatial extensions of the well-established nonhomogeneous gaussian regression (ngr) postprocessing technique for surface temperature and a recent modification thereof in which the local climatology is included in the regression model for a locally adaptive postprocessing. in a comparative study employing 21 h forecasts from the cosmo-de ensemble predictive system over germany, two approaches for modeling spatial forecast error correlations are considered: a parametric gaussian random field model and the ensemble copula coupling approach which utilizes the spatial rank correlation structure of the raw ensemble. additionally, the ngr methods are compared to both univariate and spatial versions of the ensemble bayesian model averaging (bma) postprocessing technique.","10.1175/mwr-d-14-00210.1","2014-06-30","","['kira feldmann', 'michael scheuerer', 'thordis l. thorarinsdottir']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"907",1407.0743,"the beta-gompertz distribution","math.st stat.th","in this paper, we introduce a new four-parameter generalized version of the gompertz model which is called beta-gompertz (bg) distribution. it includes some well-known lifetime distributions such as beta-exponential and generalized gompertz distributions as special sub-models. this new distribution is quite flexible and can be used effectively in modeling survival data and reliability problems. it can have a decreasing, increasing, and bathtub-shaped failure rate function depending on its parameters. some mathematical properties of the new distribution, such as closed-form expressions for the density, cumulative distribution, hazard rate function, the $k$th order moment, moment generating function, shannon entropy, and the quantile measure are provided. we discuss maximum likelihood estimation of the bg parameters from one observed sample and derive the observed fisher's information matrix. a simulation study is performed in order to investigate this proposed estimator for parameters. at the end, in order to show the bg distribution flexibility, an application using a real data set is presented.","","2014-07-02","","['ali akbar jafari', 'saeid tahmasebi', 'morad alizadeh']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"908",1407.0886,"spatial composite likelihood inference using local c-vines","stat.me","we present a vine copula based composite likelihood approach to model spatial dependencies, which allows to perform prediction at arbitrary locations. this approach combines established methods to model (spatial) dependencies. on the one hand the geostatistical concept utilizing spatial differences between the variable locations to model the extend of spatial dependencies is applied. on the other hand the flexible class of c-vine copulas is utilized to model the spatial dependency structure locally. these local c-vine copulas are parametrized jointly, exploiting an existing relationship between the copula parameters and the respective spatial distances and elevation differences, and are combined in a composite likelihood approach. the new methodology called spatial local c-vine composite likelihood (s-lcvcl) method benefits from the fact that it is able to capture non-gaussian dependency structures. the development and validation of the new methodology is illustrated using a data set of daily mean temperatures observed at 73 observation stations spread over germany. for validation continuous ranked probability scores are utilized. comparison with two other approaches of spatial dependency modeling introduced in yet unpublished work of erhardt, czado and schepsmeier (2014) shows a preference for the local c-vine composite likelihood approach.","","2014-07-03","","['tobias michael erhardt', 'claudia czado', 'ulf schepsmeier']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"909",1407.1123,"expanding the family of grassmannian kernels: an embedding perspective","cs.cv cs.lg stat.ml","modeling videos and image-sets as linear subspaces has proven beneficial for many visual recognition tasks. however, it also incurs challenges arising from the fact that linear subspaces do not obey euclidean geometry, but lie on a special type of riemannian manifolds known as grassmannian. to leverage the techniques developed for euclidean spaces (e.g, support vector machines) with subspaces, several recent studies have proposed to embed the grassmannian into a hilbert space by making use of a positive definite kernel. unfortunately, only two grassmannian kernels are known, none of which -as we will show- is universal, which limits their ability to approximate a target function arbitrarily well. here, we introduce several positive definite grassmannian kernels, including universal ones, and demonstrate their superiority over previously-known kernels in various tasks, such as classification, clustering, sparse coding and hashing.","","2014-07-04","","['mehrtash t. harandi', 'mathieu salzmann', 'sadeep jayasumana', 'richard hartley', 'hongdong li']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"910",1407.1339,"inverse graphics with probabilistic cad models","cs.cv cs.ai stat.ml","recently, multiple formulations of vision problems as probabilistic inversions of generative models based on computer graphics have been proposed. however, applications to 3d perception from natural images have focused on low-dimensional latent scenes, due to challenges in both modeling and inference. accounting for the enormous variability in 3d object shape and 2d appearance via realistic generative models seems intractable, as does inverting even simple versions of the many-to-many computations that link 3d scenes to 2d images. this paper proposes and evaluates an approach that addresses key aspects of both these challenges. we show that it is possible to solve challenging, real-world 3d vision problems by approximate inference in generative models for images based on rendering the outputs of probabilistic cad (pcad) programs. our pcad object geometry priors generate deformable 3d meshes corresponding to plausible objects and apply affine transformations to place them in a scene. image likelihoods are based on similarity in a feature space based on standard mid-level image representations from the vision literature. our inference algorithm integrates single-site and locally blocked metropolis-hastings proposals, hamiltonian monte carlo and discriminative data-driven proposals learned from training data generated from our models. we apply this approach to 3d human pose estimation and object shape reconstruction from single images, achieving quantitative and qualitative performance improvements over state-of-the-art baselines.","","2014-07-04","","['tejas d. kulkarni', 'vikash k. mansinghka', 'pushmeet kohli', 'joshua b. tenenbaum']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"911",1407.1682,"the liability threshold model for censored twin data","stat.me","family studies provide an important tool for understanding etiology of diseases, with the key aim of discovering evidence of family aggregation and to determine if such aggregation can be attributed to genetic components. heritability and concordance estimates are routinely calculated in twin studies of diseases, as a way of quantifying such genetic contribution. the endpoint in these studies are typically defined as occurrence of a disease versus death without the disease. however, a large fraction of the subjects may still be alive at the time of follow-up without having experienced the disease thus still being at risk. ignoring this right-censoring can lead to severely biased estimates. we propose to extend the classical liability threshold model with inverse probability of censoring weighting of complete observations. this leads to a flexible way of modeling twin concordance and obtaining consistent estimates of heritability. we apply the method in simulations and to data from the population based danish twin cohort where we describe the dependence in prostate cancer occurrence in twins.","10.1016/j.csda.2015.01.014","2014-07-07","2015-01-24","['klaus k. holst', 'thomas h. scheike', 'jacob b. hjelmborg']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"912",1407.2235,"bayesian structured sparsity from gaussian fields","stat.me q-bio.gn","substantial research on structured sparsity has contributed to analysis of many different applications. however, there have been few bayesian procedures among this work. here, we develop a bayesian model for structured sparsity that uses a gaussian process (gp) to share parameters of the sparsity-inducing prior in proportion to feature similarity as defined by an arbitrary positive definite kernel. for linear regression, this sparsity-inducing prior on regression coefficients is a relaxation of the canonical spike-and-slab prior that flattens the mixture model into a scale mixture of normals. this prior retains the explicit posterior probability on inclusion parameters---now with gp probit prior distributions---but enables tractable computation via elliptical slice sampling for the latent gaussian field. we motivate development of this prior using the genomic application of association mapping, or identifying genetic variants associated with a continuous trait. our bayesian structured sparsity model produced sparse results with substantially improved sensitivity and precision relative to comparable methods. through simulations, we show that three properties are key to this improvement: i) modeling structure in the covariates, ii) significance testing using the posterior probabilities of inclusion, and iii) model averaging. we present results from applying this model to a large genomic dataset to demonstrate computational tractability.","","2014-07-08","","['barbara e. engelhardt', 'ryan p. adams']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"913",1407.4211,"a marginal sampler for $\sigma$-stable poisson-kingman mixture models","stat.co stat.ml","we investigate the class of $\sigma$-stable poisson-kingman random probability measures (rpms) in the context of bayesian nonparametric mixture modeling. this is a large class of discrete rpms which encompasses most of the the popular discrete rpms used in bayesian nonparametrics, such as the dirichlet process, pitman-yor process, the normalized inverse gaussian process and the normalized generalized gamma process. we show how certain sampling properties and marginal characterizations of $\sigma$-stable poisson-kingman rpms can be usefully exploited for devising a markov chain monte carlo (mcmc) algorithm for making inference in bayesian nonparametric mixture modeling. specifically, we introduce a novel and efficient mcmc sampling scheme in an augmented space that has a fixed number of auxiliary variables per iteration. we apply our sampling scheme for a density estimation and clustering tasks with unidimensional and multidimensional datasets, and we compare it against competing sampling schemes.","10.1080/10618600.2015.1110526","2014-07-16","2015-09-24","['maría lomelí', 'stefano favaro', 'yee whye teh']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE
"914",1407.443,"sequential logistic principal component analysis (slpca): dimensional   reduction in streaming multivariate binary-state system","stat.ml cs.lg stat.ap","sequential or online dimensional reduction is of interests due to the explosion of streaming data based applications and the requirement of adaptive statistical modeling, in many emerging fields, such as the modeling of energy end-use profile. principal component analysis (pca), is the classical way of dimensional reduction. however, traditional singular value decomposition (svd) based pca fails to model data which largely deviates from gaussian distribution. the bregman divergence was recently introduced to achieve a generalized pca framework. if the random variable under dimensional reduction follows bernoulli distribution, which occurs in many emerging fields, the generalized pca is called logistic pca (lpca). in this paper, we extend the batch lpca to a sequential version (i.e. slpca), based on the sequential convex optimization theory. the convergence property of this algorithm is discussed compared to the batch version of lpca (i.e. blpca), as well as its performance in reducing the dimension for multivariate binary-state systems. its application in building energy end-use profile modeling is also investigated.","","2014-07-16","","['zhaoyi kang', 'costas j. spanos']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"915",1407.4971,"statistical inference with different missing-data mechanisms","stat.me","when data are missing due to at most one cause from some time to next time, we can make sampling distribution inferences about the parameter of the data by modeling the missing-data mechanism correctly. proverbially, in case its mechanism is missing at random (mar), it can be ignored, but in case not missing at random (nmar), it can not be. there are no methods, however, to analyze when missing of the data can occur because of several causes despite of there being many such data in practice. hence the aim of this paper is to propose how to inference on such data. concretely, we extend the missing-data indicator from usual binary random vectors to discrete random vectors, define missing-data mechanism for every causes and research ignorability of a mixture of missing-data mechanisms such as ""mar & mar"" and ""mar & nmar"". in particular, when the combination of mechanisms is ""mar & nmar"", generally the component of mar can not be ignored, but in special case, it can be.","","2014-07-18","","['kosuke morikawa', 'yutaka kano']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"916",1407.5079,"equivalence testing for functional data with an application to comparing   pulmonary function devices","stat.ap","equivalence testing for scalar data has been well addressed in the literature, however, the same cannot be said for functional data. the resultant complexity from maintaining the functional structure of the data, rather than using a scalar transformation to reduce dimensionality, renders the existing literature on equivalence testing inadequate for the desired inference. we propose a framework for equivalence testing for functional data within both the frequentist and bayesian paradigms. this framework combines extensions of scalar methodologies with new methodology for functional data. our frequentist hypothesis test extends the two one-sided testing (tost) procedure for equivalence testing to the functional regime. we conduct this tost procedure through the use of the nonparametric bootstrap. our bayesian methodology employs a functional analysis of variance model, and uses a flexible class of gaussian processes for both modeling our data and as prior distributions. through our analysis, we introduce a model for heteroscedastic variances within a gaussian process by modeling variance curves via log-gaussian process priors. we stress the importance of choosing prior distributions that are commensurate with the prior state of knowledge and evidence regarding practical equivalence. we illustrate these testing methods through data from an ongoing method comparison study between two devices for pulmonary function testing. in so doing, we provide not only concrete motivation for equivalence testing for functional data, but also a blueprint for researchers who hope to conduct similar inference.","10.1214/14-aoas763","2014-07-18","2015-03-04","['colin b. fogarty', 'dylan s. small']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"917",1407.5962,"model comparison and assessment for single particle tracking in   biological fluids","stat.ap","state-of-the-art techniques in passive particle-tracking microscopy provide high-resolution path trajectories of diverse foreign particles in biological fluids. for particles on the order of 1 micron diameter, these paths are generally inconsistent with simple brownian motion. yet, despite an abundance of data confirming these findings and their wide-ranging scientific implications, stochastic modeling of the complex particle motion has received comparatively little attention. even among posited models, there is virtually no literature on likelihood-based inference, model comparisons, and other quantitative assessments. in this article, we develop a rigorous and computationally efficient bayesian methodology to address this gap. we analyze two of the most prevalent candidate models for 30 second paths of 1 micron diameter tracer particles in human lung mucus: fractional brownian motion (fbm) and a generalized langevin equation (gle) consistent with viscoelastic theory. our model comparisons distinctly favor gle over fbm, with the former describing the data remarkably well up to the timescales for which we have reliable information.","","2014-07-22","2015-11-29","['martin lysy', 'natesh s. pillai', 'david b. hill', 'm. gregory forest', 'john mellnik', 'paula vasquez', 'scott a. mckinley']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"918",1407.6084,"stabilized sparse ordinal regression for medical risk stratification","stat.ap stat.me","the recent wide adoption of electronic medical records (emr) presents great opportunities and challenges for data mining. the emr data is largely temporal, often noisy, irregular and high dimensional. this paper constructs a novel ordinal regression framework for predicting medical risk stratification from emr. first, a conceptual view of emr as a temporal image is constructed to extract a diverse set of features. second, ordinal modeling is applied for predicting cumulative or progressive risk. the challenges are building a transparent predictive model that works with a large number of weakly predictive features, and at the same time, is stable against resampling variations. our solution employs sparsity methods that are stabilized through domain-specific feature interaction networks. we introduces two indices that measure the model stability against data resampling. feature networks are used to generate two multivariate gaussian priors with sparse precision matrices (the laplacian and random walk). we apply the framework on a large short-term suicide risk prediction problem and demonstrate that our methods outperform clinicians to a large-margin, discover suicide risk factors that conform with mental health knowledge, and produce models with enhanced stability.","10.1007/s10115-014-0740-4","2014-07-22","","['truyen tran', 'dinh phung', 'wei luo', 'svetha venkatesh']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"919",1407.681,"dissimilarity-based sparse subset selection","cs.lg stat.ml","finding an informative subset of a large collection of data points or models is at the center of many problems in computer vision, recommender systems, bio/health informatics as well as image and natural language processing. given pairwise dissimilarities between the elements of a `source set' and a `target set,' we consider the problem of finding a subset of the source set, called representatives or exemplars, that can efficiently describe the target set. we formulate the problem as a row-sparsity regularized trace minimization problem. since the proposed formulation is, in general, np-hard, we consider a convex relaxation. the solution of our optimization finds representatives and the assignment of each element of the target set to each representative, hence, obtaining a clustering. we analyze the solution of our proposed optimization as a function of the regularization parameter. we show that when the two sets jointly partition into multiple groups, our algorithm finds representatives from all groups and reveals clustering of the sets. in addition, we show that the proposed framework can effectively deal with outliers. our algorithm works with arbitrary dissimilarities, which can be asymmetric or violate the triangle inequality. to efficiently implement our algorithm, we consider an alternating direction method of multipliers (admm) framework, which results in quadratic complexity in the problem size. we show that the admm implementation allows to parallelize the algorithm, hence further reducing the computational time. finally, by experiments on real-world datasets, we show that our proposed algorithm improves the state of the art on the two problems of scene categorization using representative images and time-series modeling and segmentation using representative~models.","","2014-07-25","2016-04-08","['ehsan elhamifar', 'guillermo sapiro', 's. shankar sastry']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"920",1407.7479,"mixed effects modeling for areal data that exhibit   multivariate-spatio-temporal dependencies","stat.me","there are many data sources available that report related variables of interest that are also referenced over geographic regions and time; however, there are relatively few general statistical methods that one can readily use that incorporate these multivariate-spatio-temporal dependencies. as such, we introduce the multivariate-spatio-temporal mixed effects model (mstm) to analyze areal data with multivariate-spatio-temporal dependencies. the proposed mstm extends the notion of moran's i basis functions to the multivariate-spatio-temporal setting. this extension leads to several methodological contributions including extremely effective dimension reduction, a dynamic linear model for multivariate-spatio-temporal areal processes, and the reduction of a high-dimensional parameter space using a novel parameter model. several examples are used to demonstrate that the mstm provides an extremely viable solution to many important problems found in different and distinct corners of the spatio-temporal statistics literature including: modeling nonseparable and nonstationary covariances, combing data from multiple repeated surveys, and analyzing massive multivariate-spatio-temporal datasets.","","2014-07-28","2014-09-04","['jonathan r. bradley', 'scott h. holan', 'christopher k. wikle']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"921",1407.7556,"entropic one-class classifiers","cs.cv cs.lg stat.ml","the one-class classification problem is a well-known research endeavor in pattern recognition. the problem is also known under different names, such as outlier and novelty/anomaly detection. the core of the problem consists in modeling and recognizing patterns belonging only to a so-called target class. all other patterns are termed non-target, and therefore they should be recognized as such. in this paper, we propose a novel one-class classification system that is based on an interplay of different techniques. primarily, we follow a dissimilarity representation based approach; we embed the input data into the dissimilarity space by means of an appropriate parametric dissimilarity measure. this step allows us to process virtually any type of data. the dissimilarity vectors are then represented through a weighted euclidean graphs, which we use to (i) determine the entropy of the data distribution in the dissimilarity space, and at the same time (ii) derive effective decision regions that are modeled as clusters of vertices. since the dissimilarity measure for the input data is parametric, we optimize its parameters by means of a global optimization scheme, which considers both mesoscopic and structural characteristics of the data represented through the graphs. the proposed one-class classifier is designed to provide both hard (boolean) and soft decisions about the recognition of test patterns, allowing an accurate description of the classification process. we evaluate the performance of the system on different benchmarking datasets, containing either feature-based or structured patterns. experimental results demonstrate the effectiveness of the proposed technique.","10.1109/tnnls.2015.2418332","2014-07-28","2015-01-11","['lorenzo livi', 'alireza sadeghian', 'witold pedrycz']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"922",1407.8392,"moneybarl: exploiting pitcher decision-making using reinforcement   learning","cs.ai stat.ap","this manuscript uses machine learning techniques to exploit baseball pitchers' decision making, so-called ""baseball iq,"" by modeling the at-bat information, pitch selection and counts, as a markov decision process (mdp). each state of the mdp models the pitcher's current pitch selection in a markovian fashion, conditional on the information immediately prior to making the current pitch. this includes the count prior to the previous pitch, his ensuing pitch selection, the batter's ensuing action and the result of the pitch.","10.1214/13-aoas712","2014-07-31","","['gagan sidhu', 'brian caffo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"923",1407.8402,"voxel-level mapping of tracer kinetics in pet studies: a statistical   approach emphasizing tissue life tables","stat.ap q-bio.qm","most radiotracers used in dynamic positron emission tomography (pet) scanning act in a linear time-invariant fashion so that the measured time-course data are a convolution between the time course of the tracer in the arterial supply and the local tissue impulse response, known as the tissue residue function. in statistical terms the residue is a life table for the transit time of injected radiotracer atoms. the residue provides a description of the tracer kinetic information measurable by a dynamic pet scan. decomposition of the residue function allows separation of rapid vascular kinetics from slower blood-tissue exchanges and tissue retention. for voxel-level analysis, we propose that residues be modeled by mixtures of nonparametrically derived basis residues obtained by segmentation of the full data volume. spatial and temporal aspects of diagnostics associated with voxel-level model fitting are emphasized. illustrative examples, some involving cancer imaging studies, are presented. data from cerebral pet scanning with $^{18}$f fluoro-deoxyglucose (fdg) and $^{15}$o water (h2o) in normal subjects is used to evaluate the approach. cross-validation is used to make regional comparisons between residues estimated using adaptive mixture models with more conventional compartmental modeling techniques. simulations studies are used to theoretically examine mean square error performance and to explore the benefit of voxel-level analysis when the primary interest is a statistical summary of regional kinetics. the work highlights the contribution that multivariate analysis tools and life-table concepts can make in the recovery of local metabolic information from dynamic pet studies, particularly ones in which the assumptions of compartmental-like models, with residues that are sums of exponentials, might not be certain.","10.1214/14-aoas732","2014-07-31","","[""finbarr o'sullivan"", 'mark muzi', 'david a. mankoff', 'janet f. eary', 'alexander m. spence', 'kenneth a. krohn']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"924",1407.8406,"analysis of multiple sclerosis lesions via spatially varying   coefficients","stat.ap","magnetic resonance imaging (mri) plays a vital role in the scientific investigation and clinical management of multiple sclerosis. analyses of binary multiple sclerosis lesion maps are typically ""mass univariate"" and conducted with standard linear models that are ill suited to the binary nature of the data and ignore the spatial dependence between nearby voxels (volume elements). smoothing the lesion maps does not entirely eliminate the non-gaussian nature of the data and requires an arbitrary choice of the smoothing parameter. here we present a bayesian spatial model to accurately model binary lesion maps and to determine if there is spatial dependence between lesion location and subject specific covariates such as ms subtype, age, gender, disease duration and disease severity measures. we apply our model to binary lesion maps derived from $t_2$-weighted mri images from 250 multiple sclerosis patients classified into five clinical subtypes, and demonstrate unique modeling and predictive capabilities over existing methods.","10.1214/14-aoas718","2014-07-31","","['tian ge', 'nicole müller-lenke', 'kerstin bendfeldt', 'thomas e. nichols', 'timothy d. johnson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"925",1408.1027,"bayesian nonparametric modeling for multivariate ordinal regression","stat.me","univariate or multivariate ordinal responses are often assumed to arise from a latent continuous parametric distribution, with covariate effects which enter linearly. we introduce a bayesian nonparametric modeling approach for univariate and multivariate ordinal regression, which is based on mixture modeling for the joint distribution of latent responses and covariates. the modeling framework enables highly flexible inference for ordinal regression relationships, avoiding assumptions of linearity or additivity in the covariate effects. in standard parametric ordinal regression models, computational challenges arise from identifiability constraints and estimation of parameters requiring nonstandard inferential techniques. a key feature of the nonparametric model is that it achieves inferential flexibility, while avoiding these difficulties. in particular, we establish full support of the nonparametric mixture model under fixed cut-off points that relate through discretization the latent continuous responses with the ordinal responses. the practical utility of the modeling approach is illustrated through application to two data sets from econometrics, an example involving regression relationships for ozone concentration, and a multirater agreement problem.","","2014-08-05","2016-09-20","['maria deyoreo', 'athanasios kottas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"926",1408.1167,"boosted markov networks for activity recognition","cs.lg cs.cv stat.ml","we explore a framework called boosted markov networks to combine the learning capacity of boosting and the rich modeling semantics of markov networks and applying the framework for video-based activity recognition. importantly, we extend the framework to incorporate hidden variables. we show how the framework can be applied for both model learning and feature selection. we demonstrate that boosted markov networks with hidden variables perform comparably with the standard maximum likelihood estimation. however, our framework is able to learn sparse models, and therefore can provide computational savings when the learned models are used for classification.","","2014-08-05","","['truyen tran', 'hung bui', 'svetha venkatesh']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"927",1408.1368,"bayesian nonparametric models for spatially indexed data of mixed type","stat.me","we develop bayesian nonparametric models for spatially indexed data of mixed type. our work is motivated by challenges that occur in environmental epidemiology, where the usual presence of several confounding variables that exhibit complex interactions and high correlations makes it difficult to estimate and understand the effects of risk factors on health outcomes of interest. the modeling approach we adopt assumes that responses and confounding variables are manifestations of continuous latent variables, and uses multivariate gaussians to jointly model these. responses and confounding variables are not treated equally as relevant parameters of the distributions of the responses only are modeled in terms of explanatory variables or risk factors. spatial dependence is introduced by allowing the weights of the nonparametric process priors to be location specific, obtained as probit transformations of gaussian markov random fields. confounding variables and spatial configuration have a similar role in the model, in that they only influence, along with the responses, the allocation probabilities of the areas into the mixture components, thereby allowing for flexible adjustment of the effects of observed confounders, while allowing for the possibility of residual spatial structure, possibly occurring due to unmeasured or undiscovered spatially varying factors. aspects of the model are illustrated in simulation studies and an application to a real data set.","","2014-08-06","2014-10-15","['georgios papageorgiou', 'sylvia richardson', 'nicky best']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"928",1408.2039,"incorporating side information in probabilistic matrix factorization   with gaussian processes","cs.lg stat.ml","probabilistic matrix factorization (pmf) is a powerful method for modeling data associ- ated with pairwise relationships, finding use in collaborative filtering, computational bi- ology, and document analysis, among other areas. in many domains, there are additional covariates that can assist in prediction. for example, when modeling movie ratings, we might know when the rating occurred, where the user lives, or what actors appear in the movie. it is difficult, however, to incorporate this side information into the pmf model. we propose a framework for incorporating side information by coupling together multi- ple pmf problems via gaussian process priors. we replace scalar latent features with func- tions that vary over the covariate space. the gp priors on these functions require them to vary smoothly and share information. we apply this new method to predict the scores of professional basketball games, where side information about the venue and date of the game are relevant for the outcome.","","2014-08-09","","['ryan prescott adams', 'george e. dahl', 'iain murray']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"929",1408.2441,"maximum likelihood estimation for stochastic differential equations   using sequential kriging-based optimization","stat.me","stochastic differential equations (sdes) are used as statistical models in many disciplines. however, intractable likelihood functions for sdes make inference challenging, and we need to resort to simulation-based techniques to estimate and maximize the likelihood function. while sequential monte carlo methods have allowed for the accurate evaluation of likelihoods at fixed parameter values, there is still a question of how to find the maximum likelihood estimate. in this article we propose an efficient gaussian-process-based method for exploring the parameter space using estimates of the likelihood from a sequential monte carlo sampler. our method accounts for the inherent monte carlo variability of the estimated likelihood, and does not require knowledge of gradients. the procedure adds potential parameter values by maximizing the so-called expected improvement, leveraging the fact that the likelihood function is assumed to be smooth. our simulations demonstrate that our method has significant computational and efficiency gains over existing grid- and gradient-based techniques. our method is applied to modeling the closing stock price of three technology firms.","","2014-08-11","","['grant schneider', 'peter f. craigmile', 'radu herbei']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"930",1408.2757,"bayesian lattice filters for time-varying autoregression and   time-frequency analysis","stat.me","modeling nonstationary processes is of paramount importance to many scientific disciplines including environmental science, ecology, and finance, among others. consequently, flexible methodology that provides accurate estimation across a wide range of processes is a subject of ongoing interest. we propose a novel approach to model-based time-frequency estimation using time-varying autoregressive models. in this context, we take a fully bayesian approach and allow both the autoregressive coefficients and innovation variance to vary over time. importantly, our estimation method uses the lattice filter and is cast within the partial autocorrelation domain. the marginal posterior distributions are of standard form and, as a convenient by-product of our estimation method, our approach avoids undesirable matrix inversions. as such, estimation is extremely computationally efficient and stable. to illustrate the effectiveness of our approach, we conduct a comprehensive simulation study that compares our method with other competing methods and find that, in most cases, our approach performs superior in terms of average squared error between the estimated and true time-varying spectral density. lastly, we demonstrate our methodology through three modeling applications; namely, insect communication signals, environmental data (wind components), and macroeconomic data (us gross domestic product (gdp) and consumption).","","2014-08-12","","['wen-hsi yang', 'scott h. holan', 'christopher k. wikle']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"931",1408.3041,"bayesian nonparametric dynamic state space modeling with circular latent   states","stat.me","state space models are well-known for their versatility in modeling dynamic systems that arise in various scientific disciplines. although parametric state space models are well studied, nonparametric approaches are much less explored in comparison. in this article we propose a novel bayesian nonparametric approach to state space modeling assuming that both the observational and evolutionary functions are unknown and are varying with time; crucially, we assume that the unknown evolutionary equation describes dynamic evolution of some latent circular random variable.   based on appropriate kernel convolution of the standard wiener process we model the time-varying observational and evolutionary functions as suitable gaussian processes that take both linear and circular variables as arguments. additionally, for the time-varying evolutionary function, we wrap the gaussian process thus constructed around the unit circle to form an appropriate circular gaussian process. we show that our process thus created satisfies desirable properties.   for the purpose of inference we develop an mcmc based methodology combining gibbs sampling and metropolis-hastings algorithms. applications to a simulated data set, a real wind speed data set and a real ozone data set demonstrated quite encouraging performances of our model and methodologies.","","2014-08-13","2015-07-22","['satyaki mazumder', 'sourabh bhattacharya']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"932",1408.3685,"hierarchical sparse bayesian learning for structural health monitoring   with incomplete modal data","stat.ap","for civil structures, structural damage due to severe loading events such as earthquakes, or due to long-term environmental degradation, usually occurs in localized areas of a structure. a new sparse bayesian probabilistic framework for computing the probability of localized stiffness reductions induced by damage is presented that uses noisy incomplete modal data from before and after possible damage. this new approach employs system modal parameters of the structure as extra variables for bayesian model updating with incomplete modal data. a specific hierarchical bayesian model is constructed that promotes spatial sparseness in the inferred stiffness reductions in a way that is consistent with the bayesian ockham razor. to obtain the most plausible model of sparse stiffness reductions together with its uncertainty within a specified class of models, the method employs an optimization scheme that iterates among all uncertain parameters, including the hierarchical hyper-parameters. the approach has four important benefits: (1) it infers spatially-sparse stiffness changes based on the identified modal parameters; (2) the uncertainty in the inferred stiffness reductions is quantified; (3) no matching of model and experimental modes is needed, and (4) solving the nonlinear eigenvalue problem of a structural model is not required. the proposed method is applied to two previously-studied examples using simulated data: a ten-story shear-building and the three-dimensional braced-frame model from the phase ii simulated benchmark problem sponsored by the iasc-asce task group on structural health monitoring. the results show that the occurrence of false-positive and false-negative damage detection is clearly reduced in the presence of modeling error. furthermore, the identified most probable stiffness loss ratios are close to their actual values.","10.1615/int.j.uncertaintyquantification.2015011808","2014-08-15","","['yong huang', 'james l. beck']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"933",1408.405,"bayesian inference for a covariance matrix","stat.me","covariance matrix estimation arises in multivariate problems including multivariate normal sampling models and regression models where random effects are jointly modeled, e.g. random-intercept, random-slope models. a bayesian analysis of these problems requires a prior on the covariance matrix. here we assess, through a simulation study and a real data set, the impact this prior choice has on posterior inference of the covariance matrix.   inverse wishart distribution is the natural choice for a covariance matrix prior because its conjugacy on normal model and simplicity, is usually available in bayesian statistical software. however inverse wishart distribution presents some undesirable properties from a modeling point of view. it can be too restrictive because assume the same amount of prior information about every variance parameters and, more important, it shows a prior relationship between the variances and correlations.   some alternatives distributions has been proposed. the scaled inverse wishart distribution, which give more flexibility on the variance priors conserving the conjugacy property but does not eliminate the prior relationship between variances and correlations. secondly, it is possible to fit separate priors for individual correlations and standard deviations. this strategy eliminates any prior relationship within the covariance matrix parameters, but it is not conjugate and therefore computationally slow.","","2014-08-18","2016-07-08","['ignacio alvarez', 'jarad niemi', 'matt simpson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"934",1408.4378,"an evidence of link between default and loss of bank loans from the   modeling of competing risks","stat.ap","in this paper, we propose a method that provides a useful technique to compare relationship between risks involved that takes customer become defaulter and debt collection process that might make this defaulter recovered. through estimation of competitive risks that lead to realization of the event of interest, we showed that there is a significant relation between the intensity of default and losses from defaulted loans in collection processes. to reach this goal, we investigate a competing risks model applied to whole credit risk cycle into a bank loans portfolio. we estimated competing causes related to occurrence of default, thereafter, comparing it with estimated competing causes that lead loans to write-off condition. in context of modeling competing risks, we used a specification of poisson distribution for numbers from competing causes and weibull distribution for failures times. the likelihood maximum estimation is used to parameters estimation and the model is applied to a real data of personal loans","","2014-08-19","","['mauro r. oliveira', 'francisco louzada']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"935",1408.4383,"an estimation of cattle movement parameters in the central states of the   us","stat.ap","the characterization of cattle demographics and especially movements is an essential component in the modeling of dynamics in cattle systems, yet for cattle systems of the united states (us), this is missing. through a large-scale maximum entropy optimization formulation, we estimate cattle movement parameters to characterize the movements of cattle across $10$ central states and $1034$ counties of the united states. inputs to the estimation problem are taken from the united states department of agriculture national agricultural statistics service database and are pre-processed in a pair of tightly constrained optimization problems to recover non-disclosed elements of data. we compare stochastic subpopulation-based movements generated from the estimated parameters to operation-based movements published by the united states department of agriculture. for future census of agriculture distributions, we propose a series of questions that enable improvements for our method without compromising the privacy of cattle operations. our novel method to estimate cattle movements across large us regions characterizes county-level stratified subpopulations of cattle for data-driven livestock modeling. our estimated movement parameters suggest a significant risk level for us cattle systems.","","2014-08-19","","['phillip schumm', 'caterina scoglio', 'h morgan scott']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"936",1408.4636,"do we always need a filter?","stat.ap","since the groundbreaking work of the kalman filter in the 1960s, considerable effort has been devoted to various discrete time filters for dynamic state estimation, especially including dozens of different types of suboptimal implementations of the bayes filters. this has been accompanied by the rapid development of simulation/approximation theories and technologies. while admitting the success of filters in many cases, this study investigates the failure cases when they are in fact ineffective for state estimation. several classic models have shown that the straightforward observation-only (o2) inference that does not need system modeling can perform better (in terms of both accuracy and computing speed) for estimation than filters. special attention has been paid to quantitatively analyze when and why a filter will not outperform the o2 inference from the information fusion perspective. thanks to the rapid development of advanced sensors, the o2 inference is not only engineering friendly and computationally fast but can also be very accurate and reliable by fusing the information received from multiple sensors. the statistical attributes of the multi-sensor o2 inference are analyzed and demonstrated through simulations. in the situation with limited sensors, the o2 approach can work jointly with existing clutter filtering and data association algorithms for multi-target tracking in clutter environments. given an adequate number of sensors, the o2 approach can employ the multi-sensor data fusion to deal with clutter and can handle the very general multi-target tracking scenario with no background information.","","2014-08-20","2015-02-18","['tiancheng li', 'juan m. corchado', 'javier bajo', 'shudong sun', 'juan f. de paz']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"937",1408.466,"joint hierarchical gaussian process model with application to forecast   in medical monitoring","stat.me stat.ml","a novel extrapolation method is proposed for longitudinal forecasting. a hierarchical gaussian process model is used to combine nonlinear population change and individual memory of the past to make prediction. the prediction error is minimized through the hierarchical design. the method is further extended to joint modeling of continuous measurements and survival events. the baseline hazard, covariate and joint effects are conveniently modeled in this hierarchical structure. the estimation and inference are implemented in fully bayesian framework using the objective and shrinkage priors. in simulation studies, this model shows robustness in latent estimation, correlation detection and high accuracy in forecasting. the model is illustrated with medical monitoring data from cystic fibrosis (cf) patients. estimation and forecasts are obtained in the measurement of lung function and records of acute respiratory events.   keyword: extrapolation, joint model, longitudinal model, hierarchical gaussian process, cystic fibrosis, medical monitoring","","2014-08-20","2014-08-22","['leo l. duan', 'john p. clancy', 'rhonda d. szczesniak']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"938",1408.5801,"a general framework for fast stagewise algorithms","stat.ml stat.co","forward stagewise regression follows a very simple strategy for constructing a sequence of sparse regression estimates: it starts with all coefficients equal to zero, and iteratively updates the coefficient (by a small amount $\epsilon$) of the variable that achieves the maximal absolute inner product with the current residual. this procedure has an interesting connection to the lasso: under some conditions, it is known that the sequence of forward stagewise estimates exactly coincides with the lasso path, as the step size $\epsilon$ goes to zero. furthermore, essentially the same equivalence holds outside of least squares regression, with the minimization of a differentiable convex loss function subject to an $\ell_1$ norm constraint (the stagewise algorithm now updates the coefficient corresponding to the maximal absolute component of the gradient).   even when they do not match their $\ell_1$-constrained analogues, stagewise estimates provide a useful approximation, and are computationally appealing. their success in sparse modeling motivates the question: can a simple, effective strategy like forward stagewise be applied more broadly in other regularization settings, beyond the $\ell_1$ norm and sparsity? the current paper is an attempt to do just this. we present a general framework for stagewise estimation, which yields fast algorithms for problems such as group-structured learning, matrix completion, image denoising, and more.","","2014-08-25","2015-06-13","['ryan j. tibshirani']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE
"939",1408.6032,"inference of cancer progression models with biological noise","stat.ml cs.lg q-bio.qm","many applications in translational medicine require the understanding of how diseases progress through the accumulation of persistent events. specialized bayesian networks called monotonic progression networks offer a statistical framework for modeling this sort of phenomenon. current machine learning tools to reconstruct bayesian networks from data are powerful but not suited to progression models. we combine the technological advances in machine learning with a rigorous philosophical theory of causation to produce polaris, a scalable algorithm for learning progression networks that accounts for causal or biological noise as well as logical relations among genetic events, making the resulting models easy to interpret qualitatively. we tested polaris on synthetically generated data and showed that it outperforms a widely used machine learning algorithm and approaches the performance of the competing special-purpose, albeit clairvoyant algorithm that is given a priori information about the model parameters. we also prove that under certain rather mild conditions, polaris is guaranteed to converge for sufficiently large sample sizes. finally, we applied polaris to point mutation and copy number variation data in prostate cancer from the cancer genome atlas (tcga) and found that there are likely three distinct progressions, one major androgen driven progression, one major non-androgen driven progression, and one novel minor androgen driven progression.","","2014-08-26","","['ilya korsunsky', 'daniele ramazzotti', 'giulio caravagna', 'bud mishra']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"940",1408.6553,"an observational study: the effect of diuretics administration on   outcomes of mortality and mean duration of i.c.u. stay","stat.ap","this thesis conducts an observational study into whether diuretics should be administered to icu patients with sepsis when length of stay in the icu and 30-day post-hospital mortality are considered. the central contribution of the thesis is a stepwise, reusable software-based approach for examining the outcome of treatment vs no-treatment decisions with observational data. the thesis implements, demonstrates and draws findings via three steps: step 1. form a study group and prepare modeling variables. step 2. model the propensity of the study group with respect to the administration of diuretics with a propensity score function and create groups of patients balanced in this propensity. step 3. statistically model each outcome with study variables to decide whether the administration of diuretics has a significant impact. additionally, the thesis presents a preliminary machine learning based method using genetic programming to predict mortality and length of stay in icu outcomes for the study group. the thesis finds, for its study group, in three of five propensity balanced quintiles, a statistically significant longer length of stay when diuretics are administered. for a less sick subset of patients (saps icu admission score < 17) the administration of diuretics has a significant negative effect on mortality.","","2014-08-11","","['daniele ramazzotti']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"941",1409.0585,"on the equivalence between deep nade and generative stochastic networks","stat.ml cs.lg","neural autoregressive distribution estimators (nades) have recently been shown as successful alternatives for modeling high dimensional multimodal distributions. one issue associated with nades is that they rely on a particular order of factorization for $p(\mathbf{x})$. this issue has been recently addressed by a variant of nade called orderless nades and its deeper version, deep orderless nade. orderless nades are trained based on a criterion that stochastically maximizes $p(\mathbf{x})$ with all possible orders of factorizations. unfortunately, ancestral sampling from deep nade is very expensive, corresponding to running through a neural net separately predicting each of the visible variables given some others. this work makes a connection between this criterion and the training criterion for generative stochastic networks (gsns). it shows that training nades in this way also trains a gsn, which defines a markov chain associated with the nade model. based on this connection, we show an alternative way to sample from a trained orderless nade that allows to trade-off computing time and quality of the samples: a 3 to 10-fold speedup (taking into account the waste due to correlations between consecutive samples of the chain) can be obtained without noticeably reducing the quality of the samples. this is achieved using a novel sampling procedure for gsns called annealed gsn sampling, similar to tempering methods that combines fast mixing (obtained thanks to steps at high noise levels) with accurate samples (obtained thanks to steps at low noise levels).","","2014-09-01","","['li yao', 'sherjil ozair', 'kyunghyun cho', 'yoshua bengio']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"942",1409.1333,"model-based regression clustering for high-dimensional data. application   to functional data","math.st stat.th","finite mixture regression models are useful for modeling the relationship between response and predictors, arising from different subpopulations. in this article, we study high-dimensional predic- tors and high-dimensional response, and propose two procedures to deal with this issue. we propose to use the lasso estimator to take into account the sparsity, and a penalty on the rank, to take into account the matrix structure. then, we extend these procedures to the functional case, where predictors and responses are functions. for this purpose, we use a wavelet-based approach. finally, for each situation, we provide algorithms, and apply and evaluate our methods both on simulations and real datasets.","","2014-09-04","2016-01-06","['emilie devijver']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"943",1409.2027,"short-term forecasting of anomalous load using rule-based triple   seasonal methods","stat.ap","numerous methods have been proposed for forecasting load for normal days. modeling of anomalous load, however, has often been ignored in the research literature. occurring on special days, such as public holidays, anomalous load conditions pose considerable modeling challenges due to their infrequent occurrence and significant deviation from normal load. to overcome these limitations, we adopt a rule-based approach, which allows incorporation of prior expert knowledge of load profiles into the statistical model. we use triple seasonal holt-winters-taylor (hwt) exponential smoothing, triple seasonal autoregressive moving average (arma), artificial neural networks (anns), and triple seasonal intraweek singular value decomposition (svd) based exponential smoothing. these methods have been shown to be competitive for modeling load for normal days. the methodological contribution of this paper is to demonstrate how these methods can be adapted to model load for special days, when used in conjunction with a rule-based approach. the proposed rule-based method is able to model normal and anomalous load in a unified framework. using nine years of half-hourly load for great britain, we evaluate point forecasts, for lead times from one half-hour up to a day ahead. a combination of two rule-based methods generated the most accurate forecasts.","10.1109/tpwrs.2013.2252929","2014-09-06","","['siddharth arora', 'james w. taylor']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"944",1409.2677,"two modeling strategies for empirical bayes estimation","stat.me","empirical bayes methods use the data from parallel experiments, for instance, observations $x_k\sim\mathcal{n}(\theta_k,1)$ for $k=1,2,\ldots,n$, to estimate the conditional distributions $\theta_k|x_k$. there are two main estimation strategies: modeling on the $\theta$ space, called ""$g$-modeling"" here, and modeling on the $x$ space, called ""$f$-modeling."" the two approaches are described and compared. a series of computational formulas are developed to assess their frequentist accuracy. several examples, both contrived and genuine, show the strengths and limitations of the two strategies.","10.1214/13-sts455","2014-09-09","","['bradley efron']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"945",1409.3518,"topic modeling of hierarchical corpora","stat.ml cs.ir cs.lg","we study the problem of topic modeling in corpora whose documents are organized in a multi-level hierarchy. we explore a parametric approach to this problem, assuming that the number of topics is known or can be estimated by cross-validation. the models we consider can be viewed as special (finite-dimensional) instances of hierarchical dirichlet processes (hdps). for these models we show that there exists a simple variational approximation for probabilistic inference. the approximation relies on a previously unexploited inequality that handles the conditional dependence between dirichlet latent variables in adjacent levels of the model's hierarchy. we compare our approach to existing implementations of nonparametric hdps. on several benchmarks we find that our approach is faster than gibbs sampling and able to learn more predictive models than existing variational methods. finally, we demonstrate the large-scale viability of our approach on two newly available corpora from researchers in computer security---one with 350,000 documents and over 6,000 internal subcategories, the other with a five-level deep hierarchy.","","2014-09-11","2015-04-13","['do-kyum kim', 'geoffrey m. voelker', 'lawrence k. saul']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"946",1409.5099,"exact least squares algorithm for signal matched synthesis filter bank:   part ii","cs.it math.it math.st stat.th","in the companion paper, we proposed a concept of signal matched whitening filter bank and developed a time and order recursive, fast least squares algorithm for the same. objective of part ii of the paper is two fold: first is to define a concept of signal matched synthesis filter bank, hence combining definitions of part i and part ii we obtain a filter bank matched to a given signal. we also develop a fast time and order recursive, least squares algorithm for obtaining the same. the synthesis filters, obtained here, reconstruct the given signal only and not every signal from the finite energy signal space (i.e. belonging to l^2(r)), as is usually done. the recursions, so obtained, result in a lattice-like structure. since the filter parameters are not directly available, we also present an order recursive algorithm for the computation of signal matched synthesis filter bank coefficients from the lattice parameters. the second objective is to explore the possibility of using synthesis side for modeling of a given stochastic process. simulation results have also been presented to validate the theory.","","2014-09-16","","['binish fatimah', 's. d. joshi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"947",1409.5676,"maigespack: a computational environment for microarray data analysis","stat.co q-bio.gn q-bio.qm","microarray technology is still an important way to assess gene expression in molecular biology, mainly because it measures expression profiles for thousands of genes simultaneously, what makes this technology a good option for some studies focused on systems biology. one of its main problem is complexity of experimental procedure, presenting several sources of variability, hindering statistical modeling. so far, there is no standard protocol for generation and evaluation of microarray data. to mitigate the analysis process this paper presents an r package, named maigespack, that helps with data organization. besides that, it makes data analysis process more robust, reliable and reproducible. also, maigespack aggregates several data analysis procedures reported in literature, for instance: cluster analysis, differential expression, supervised classifiers, relevance networks and functional classification of gene groups or gene networks.","","2014-09-19","2015-11-11","['gustavo h. esteves', 'roberto hirata']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"948",1409.5914,"nonparametric bayes modeling with sample survey weights","stat.me","in population studies, it is standard to sample data via designs in which the population is divided into strata, with the different strata assigned different probabilities of inclusion. although there have been some proposals for including sample survey weights into bayesian analyses, existing methods require complex models or ignore the stratified design underlying the survey weights. we propose a simple approach based on modeling the distribution of the selected sample as a mixture, with the mixture weights appropriately adjusted, while accounting for uncertainty in the adjustment. we focus for simplicity on dirichlet process mixtures but the proposed approach can be applied more broadly. we sketch a simple markov chain monte carlo algorithm for computation, and assess the approach via simulations and an application.","","2014-09-20","2014-09-25","['t. kunihama', 'a. h. herring', 'c. t. halpern', 'd. b. dunson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"949",1409.7086,"a two-part mixed-effects modeling framework for analyzing whole-brain   network data","stat.ap q-bio.nc q-bio.qm stat.me","whole-brain network analyses remain the vanguard in neuroimaging research, coming to prominence within the last decade. network science approaches have facilitated these analyses and allowed examining the brain as an integrated system. however, statistical methods for modeling and comparing groups of networks have lagged behind. fusing multivariate statistical approaches with network science presents the best path to develop these methods. toward this end, we propose a two-part mixed-effects modeling framework that allows modeling both the probability of a connection (presence/absence of an edge) and the strength of a connection if it exists. models within this framework enable quantifying the relationship between an outcome (e.g., disease status) and connectivity patterns in the brain while reducing spurious correlations through inclusion of confounding covariates. they also enable prediction about an outcome based on connectivity structure and vice versa, simulating networks to gain a better understanding of normal ranges of topological variability, and thresholding networks leveraging group information. thus, they provide a comprehensive approach to studying system level brain properties to further our understanding of normal and abnormal brain function.","","2014-09-24","","['sean l. simpson', 'paul j. laurienti']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"950",1409.7134,"deconvolution of high-dimensional mixtures via boosting, with   application to diffusion-weighted mri of human brain","stat.ml","diffusion-weighted magnetic resonance imaging (dwi) and fiber tractography are the only methods to measure the structure of the white matter in the living human brain. the diffusion signal has been modelled as the combined contribution from many individual fascicles of nerve fibers passing through each location in the white matter. typically, this is done via basis pursuit, but estimation of the exact directions is limited due to discretization. the difficulties inherent in modeling dwi data are shared by many other problems involving fitting non-parametric mixture models. ekanadaham et al. proposed an approach, continuous basis pursuit, to overcome discretization error in the 1-dimensional case (e.g., spike-sorting). here, we propose a more general algorithm that fits mixture models of any dimensionality without discretization. our algorithm uses the principles of l2-boost, together with refitting of the weights and pruning of the parameters. the addition of these steps to l2-boost both accelerates the algorithm and assures its accuracy. we refer to the resulting algorithm as elastic basis pursuit, or ebp, since it expands and contracts the active set of kernels as needed. we show that in contrast to existing approaches to fitting mixtures, our boosting framework (1) enables the selection of the optimal bias-variance tradeoff along the solution path, and (2) scales with high-dimensional problems. in simulations of dwi, we find that ebp yields better parameter estimates than a non-negative least squares (nnls) approach, or the standard model used in dwi, the tensor model, which serves as the basis for diffusion tensor imaging (dti). we demonstrate the utility of the method in dwi data acquired in parts of the brain containing crossings of multiple fascicles of nerve fibers.","","2014-09-24","2014-09-26","['charles zheng', 'franco pestilli', 'ariel rokem']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"951",1409.7158,"bayesian inference for tumor subclones accounting for sequencing and   structural variants","stat.me q-bio.gn","tumor samples are heterogeneous. they consist of different subclones that are characterized by differences in dna nucleotide sequences and copy numbers on multiple loci. heterogeneity can be measured through the identification of the subclonal copy number and sequence at a selected set of loci. understanding that the accurate identification of variant allele fractions greatly depends on a precise determination of copy numbers, we develop a bayesian feature allocation model for jointly calling subclonal copy numbers and the corresponding allele sequences for the same loci. the proposed method utilizes three random matrices, l, z and w to represent subclonal copy numbers (l), numbers of subclonal variant alleles (z) and cellular fractions of subclones in samples (w), respectively. the unknown number of subclones implies a random number of columns for these matrices. we use next-generation sequencing data to estimate the subclonal structures through inference on these three matrices. using simulation studies and a real data analysis, we demonstrate how posterior inference on the subclonal structure is enhanced with the joint modeling of both structure and sequencing variants on subclonal genomes. software is available at http://compgenome.org/bayclone2.","","2014-09-25","","['juhee lee', 'peter mueller', 'subhajit sengupta', 'kamalakar gulukota', 'yuan ji']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"952",1409.8621,"copula relations in compound poisson processes","math.st stat.th","we investigate in multidimensional compound poisson processes (cpp) the relation between the dependence structure of the jump distribution and the dependence structure of the respective components of the cpp itself. for this purpose the asymptotic $\lambda t\to \infty$ is considered, where $\lambda$ denotes the intensity and $t$ the time point of the cpp. for modeling the dependence structures we are using the concept of copulas. we prove that the copula of a cpp converges under quite general assumptions to a specific gaussian copula, depending on the underlying jump distribution.   let $f$ be a $d$-dimensional jump distribution $(d\geq 2)$, $\lambda>0$ and let $\psi(\lambda,f)$ be the distribution of the corresponding cpp with intensity $\lambda$ at the time point $1$. further, denote the operator which maps a $d$-dimensional distribution on its copula as $\mathcal{t}$. the starting point of our investigation was the validity of the equation \begin{equation} \label{marfreeeq} \mathcal{t}(\psi(\lambda,f))=\mathcal{t}(\psi(\lambda,\mathcal{t}f)). \end{equation} our asymptotic theory implies that this equation is, in general, not true.   a simulation study that confirms our theoretical results is given in the last section.","","2014-09-30","","['christian palmes']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"953",1410.0438,"multiple imputation of missing categorical and continuous values via   bayesian mixture models with local dependence","stat.ap stat.me","we present a nonparametric bayesian joint model for multivariate continuous and categorical variables, with the intention of developing a flexible engine for multiple imputation of missing values. the model fuses dirichlet process mixtures of multinomial distributions for categorical variables with dirichlet process mixtures of multivariate normal distributions for continuous variables. we incorporate dependence between the continuous and categorical variables by (i) modeling the means of the normal distributions as component-specific functions of the categorical variables and (ii) forming distinct mixture components for the categorical and continuous data with probabilities that are linked via a hierarchical model. this structure allows the model to capture complex dependencies between the categorical and continuous data with minimal tuning by the analyst. we apply the model to impute missing values due to item nonresponse in an evaluation of the redesign of the survey of income and program participation (sipp). the goal is to compare estimates from a field test with the new design to estimates from selected individuals from a panel collected under the old design. we show that accounting for the missing data changes some conclusions about the comparability of the distributions in the two datasets. we also perform an extensive repeated sampling simulation using similar data from complete cases in an existing sipp panel, comparing our proposed model to a default application of multiple imputation by chained equations. imputations based on the proposed model tend to have better repeated sampling properties than the default application of chained equations in this realistic setting.","","2014-10-01","2015-10-12","['jared s. murray', 'jerome p. reiter']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"954",1410.0813,"gaussian tree constraints applied to acoustic linguistic functional data","stat.ap stat.me","evolutionary models of languages are usually considered to take the form of trees. with the development of so-called tree constraints the plausibility of the tree model assumptions can be addressed by checking whether the moments of observed variables lie within regions consistent with trees. in our linguistic application, the data set comprises acoustic samples (audio recordings) from speakers of five romance languages or dialects. we wish to assess these functional data for compatibility with a hereditary tree model at the language level. a novel combination of canonical function analysis (cfa) with a separable covariance structure provides a method for generating a representative basis for the data. this resulting basis is formed of components which emphasize language differences whilst maintaining the integrity of the observational language-groupings. a previously unexploited gaussian tree constraint is then applied to component-by-component projections of the data to investigate adherence to an evolutionary tree. the results indicate that while a tree model is unlikely to be suitable for modeling all aspects of the acoustic linguistic data, certain features of the spoken romance languages highlighted by the separable-cfa basis may indeed be suitably modeled as a tree.","","2014-10-03","","['nathaniel shiers', 'john a. d. aston', 'jim q. smith', 'john s. coleman']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"955",1410.1174,"learning topology and dynamics of large recurrent neural networks","stat.ml stat.co","large-scale recurrent networks have drawn increasing attention recently because of their capabilities in modeling a large variety of real-world phenomena and physical mechanisms. this paper studies how to identify all authentic connections and estimate system parameters of a recurrent network, given a sequence of node observations. this task becomes extremely challenging in modern network applications, because the available observations are usually very noisy and limited, and the associated dynamical system is strongly nonlinear. by formulating the problem as multivariate sparse sigmoidal regression, we develop simple-to-implement network learning algorithms, with rigorous convergence guarantee in theory, for a variety of sparsity-promoting penalty forms. a quantile variant of progressive recurrent network screening is proposed for efficient computation and allows for direct cardinality control of network topology in estimation. moreover, we investigate recurrent network stability conditions in lyapunov's sense, and integrate such stability constraints into sparse network learning. experiments show excellent performance of the proposed algorithms in network topology identification and forecasting.","10.1109/tsp.2014.2358956","2014-10-05","","['yiyuan she', 'yuejia he', 'dapeng wu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE
"956",1410.1221,"scalable and efficient algorithms for the propagation of uncertainty   from data through inference to prediction for large-scale problems, with   application to flow of the antarctic ice sheet","math.oc math.na stat.co stat.me","the majority of research on efficient and scalable algorithms in computational science and engineering has focused on the forward problem: given parameter inputs, solve the governing equations to determine output quantities of interest. in contrast, here we consider the broader question: given a (large-scale) model containing uncertain parameters, (possibly) noisy observational data, and a prediction quantity of interest, how do we construct efficient and scalable algorithms to (1) infer the model parameters from the data (the deterministic inverse problem), (2) quantify the uncertainty in the inferred parameters (the bayesian inference problem), and (3) propagate the resulting uncertain parameters through the model to issue predictions with quantified uncertainties (the forward uncertainty propagation problem)? we present efficient and scalable algorithms for this end-to-end, data-to-prediction process under the gaussian approximation and in the context of modeling the flow of the antarctic ice sheet and its effect on sea level. the ice is modeled as a viscous, incompressible, creeping, shear-thinning fluid. the observational data come from insar satellite measurements of surface ice flow velocity, and the uncertain parameter field to be inferred is the basal sliding parameter. the prediction quantity of interest is the present-day ice mass flux from the antarctic continent to the ocean. we show that the work required for executing this data-to-prediction process is independent of the state dimension, parameter dimension, data dimension, and number of processor cores. the key to achieving this dimension independence is to exploit the fact that the observational data typically provide only sparse information on model parameters. this property can be exploited to construct a low rank approximation of the linearized parameter-to-observable map.","10.1016/j.jcp.2015.04.047","2014-10-05","2015-09-01","['tobin isaac', 'noemi petra', 'georg stadler', 'omar ghattas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"957",1410.1494,"regression-based covariance functions for nonstationary spatial modeling","stat.me","in many environmental applications involving spatially-referenced data, limitations on the number and locations of observations motivate the need for practical and efficient models for spatial interpolation, or kriging. a key component of models for continuously-indexed spatial data is the covariance function, which is traditionally assumed to belong to a parametric class of stationary models. however, stationarity is rarely a realistic assumption. alternative methods which more appropriately model the nonstationarity present in environmental processes often involve high-dimensional parameter spaces, which lead to difficulties in model fitting and interpretability. to overcome this issue, we build on the growing literature of covariate-driven nonstationary spatial modeling. using process convolution techniques, we propose a bayesian model for continuously-indexed spatial data based on a flexible parametric covariance regression structure for a convolution-kernel covariance matrix. the resulting model is a parsimonious representation of the kernel process, and we explore properties of the implied model, including a description of the resulting nonstationary covariance function and the interpretational benefits in the kernel parameters. furthermore, we demonstrate that our model provides a practical compromise between stationary and highly parameterized nonstationary spatial covariance functions that do not perform well in practice. we illustrate our approach through an analysis of annual precipitation data.","10.1002/env.2336","2014-10-06","2015-02-04","['mark d. risser', 'catherine a. calder']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"958",1410.1932,"a regression tree approach to identifying subgroups with differential   treatment effects","stat.me","in the fight against hard-to-treat diseases such as cancer, it is often difficult to discover new treatments that benefit all subjects. for regulatory agency approval, it is more practical to identify subgroups of subjects for whom the treatment has an enhanced effect. regression trees are natural for this task because they partition the data space. we briefly review existing regression tree algorithms. then we introduce three new ones that are practically free of selection bias and are applicable to two or more treatments, censored response variables, and missing values in the predictor variables. the algorithms extend the guide approach by using three key ideas: (i) treatment as a linear predictor, (ii) chi-squared tests to detect residual patterns and lack of fit, and (iii) proportional hazards modeling via poisson regression. importance scores with thresholds for identifying influential variables are obtained as by-products. a bootstrap technique is used to construct confidence intervals for the treatment effects in each node. real and simulated data are used to compare the methods.","","2014-10-07","","['wei-yin loh', 'xu he', 'michael man']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"959",1410.3234,"markov random fields and mass spectra discrimination","stat.ml stat.ap stat.co","for mass spectra acquired from cancer patients by maldi or seldi techniques, automated discrimination between cancer types or stages has often been implemented by machine learnings. these techniques typically generate ""black-box"" classifiers, which are difficult to interpret biologically. we develop new and efficient signature discovery algorithms leading to interpretable signatures combining the discriminating power of explicitly selected small groups of biomarkers, identified by their m/z ratios. our approach is based on rigorous stochastic modeling of ""homogeneous"" datasets of mass spectra by a versatile class of parameterized markov random fields. we present detailed algorithms validated by precise theoretical results. we also outline the successful tests of our approach to generate efficient explicit signatures for six benchmark discrimination tasks, based on mass spectra acquired from colorectal cancer patients, as well as from ovarian cancer patients.","","2014-10-13","","['ao kong', 'robert azencott']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE
"960",1410.3314,"propagation kernels","stat.ml cs.lg","we introduce propagation kernels, a general graph-kernel framework for efficiently measuring the similarity of structured data. propagation kernels are based on monitoring how information spreads through a set of given graphs. they leverage early-stage distributions from propagation schemes such as random walks to capture structural information encoded in node labels, attributes, and edge information. this has two benefits. first, off-the-shelf propagation schemes can be used to naturally construct kernels for many graph types, including labeled, partially labeled, unlabeled, directed, and attributed graphs. second, by leveraging existing efficient and informative propagation schemes, propagation kernels can be considerably faster than state-of-the-art approaches without sacrificing predictive performance. we will also show that if the graphs at hand have a regular structure, for instance when modeling image or video data, one can exploit this regularity to scale the kernel computation to large databases of graphs with thousands of nodes. we support our contributions by exhaustive experiments on a number of real-world graphs from a variety of application domains.","","2014-10-13","","['marion neumann', 'roman garnett', 'christian bauckhage', 'kristian kersting']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"961",1410.4247,"estimating restricted mean treatment effects with stacked survival   models","stat.ap","the difference in restricted mean survival times between two groups is a clinically relevant summary measure. with observational data, there may be imbalances in confounding variables between the two groups. one approach to account for such imbalances is to estimate a covariate-adjusted restricted mean difference by modeling the covariate-adjusted survival distribution and then marginalizing over the covariate distribution. we demonstrate that the mean-squared error of the restricted mean difference is bounded by the mean-squared error of the covariate-adjusted survival distribution estimators. this implies that a better estimator of the covariate-adjusted survival distributions is associated with a better estimator of the restricted mean difference. thus, this paper proposes estimating restricted mean differences with stacked survival models. stacked survival models estimate a weighted average of several survival models by minimizing predicted error. by including a range of parametric and semi-parametric models, stacked survival models can effectively estimate a covariate-adjusted survival distribution and, therefore, the restricted mean treatment effect in a wide range of scenarios. we demonstrate through a simulation study that the new estimator can perform nearly as well as cox regression when the proportional hazards assumption is satisfied and significantly better when proportional hazards is violated. the proposed estimator is also illustrated with data from the united network for organ sharing to evaluate post-lung transplant survival between large and small-volume centers.","","2014-10-15","","['andrew wey', 'david vock', 'john connett', 'kyle rudser']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"962",1410.4355,"multi-level anomaly detection on time-varying graph data","cs.si cs.lg stat.ml","this work presents a novel modeling and analysis framework for graph sequences which addresses the challenge of detecting and contextualizing anomalies in labelled, streaming graph data. we introduce a generalization of the bter model of seshadhri et al. by adding flexibility to community structure, and use this model to perform multi-scale graph anomaly detection. specifically, probability models describing coarse subgraphs are built by aggregating probabilities at finer levels, and these closely related hierarchical models simultaneously detect deviations from expectation. this technique provides insight into a graph's structure and internal context that may shed light on a detected event. additionally, this multi-scale analysis facilitates intuitive visualizations by allowing users to narrow focus from an anomalous graph to particular subgraphs or nodes causing the anomaly.   for evaluation, two hierarchical anomaly detectors are tested against a baseline gaussian method on a series of sampled graphs. we demonstrate that our graph statistics-based approach outperforms both a distribution-based detector and the baseline in a labeled setting with community structure, and it accurately detects anomalies in synthetic and real-world datasets at the node, subgraph, and graph levels. to illustrate the accessibility of information made possible via this technique, the anomaly detector and an associated interactive visualization tool are tested on ncaa football data, where teams and conferences that moved within the league are identified with perfect recall, and precision greater than 0.786.","","2014-10-16","2015-04-20","['robert a. bridges', 'john collins', 'erik m. ferragut', 'jason laska', 'blair d. sullivan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"963",1410.451,"graph-sparse lda: a topic model with structured sparsity","stat.ml cs.cl cs.lg","originally designed to model text, topic modeling has become a powerful tool for uncovering latent structure in domains including medicine, finance, and vision. the goals for the model vary depending on the application: in some cases, the discovered topics may be used for prediction or some other downstream task. in other cases, the content of the topic itself may be of intrinsic scientific interest.   unfortunately, even using modern sparse techniques, the discovered topics are often difficult to interpret due to the high dimensionality of the underlying space. to improve topic interpretability, we introduce graph-sparse lda, a hierarchical topic model that leverages knowledge of relationships between words (e.g., as encoded by an ontology). in our model, topics are summarized by a few latent concept-words from the underlying graph that explain the observed words. graph-sparse lda recovers sparse, interpretable summaries on two real-world biomedical datasets while matching state-of-the-art prediction performance.","","2014-10-16","2014-11-21","['finale doshi-velez', 'byron wallace', 'ryan adams']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"964",1410.4812,"inference and mixture modeling with the elliptical gamma distribution","stat.co math.oc stat.ml","we study modeling and inference with the elliptical gamma distribution (egd). we consider maximum likelihood (ml) estimation for egd scatter matrices, a task for which we develop new fixed-point algorithms. our algorithms are efficient and converge to global optima despite nonconvexity. moreover, they turn out to be much faster than both a well-known iterative algorithm of kent & tyler (1991) and sophisticated manifold optimization algorithms. subsequently, we invoke our ml algorithms as subroutines for estimating parameters of a mixture of egds. we illustrate our methods by applying them to model natural image statistics---the proposed egd mixture model yields the most parsimonious model among several competing approaches.","10.1016/j.csda.2016.02.009","2014-10-17","2015-12-20","['reshad hosseini', 'suvrit sra', 'lucas theis', 'matthias bethge']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE
"965",1410.4821,"convex optimization in julia","math.oc cs.ms stat.ml","this paper describes convex, a convex optimization modeling framework in julia. convex translates problems from a user-friendly functional language into an abstract syntax tree describing the problem. this concise representation of the global structure of the problem allows convex to infer whether the problem complies with the rules of disciplined convex programming (dcp), and to pass the problem to a suitable solver. these operations are carried out in julia using multiple dispatch, which dramatically reduces the time required to verify dcp compliance and to parse a problem into conic form. convex then automatically chooses an appropriate backend solver to solve the conic form problem.","","2014-10-17","","['madeleine udell', 'karanveer mohan', 'david zeng', 'jenny hong', 'steven diamond', 'stephen boyd']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"966",1410.4828,"generalized conditional gradient for sparse estimation","math.oc cs.lg stat.ml","structured sparsity is an important modeling tool that expands the applicability of convex formulations for data analysis, however it also creates significant challenges for efficient algorithm design. in this paper we investigate the generalized conditional gradient (gcg) algorithm for solving structured sparse optimization problems---demonstrating that, with some enhancements, it can provide a more efficient alternative to current state of the art approaches. after providing a comprehensive overview of the convergence properties of gcg, we develop efficient methods for evaluating polar operators, a subroutine that is required in each gcg iteration. in particular, we show how the polar operator can be efficiently evaluated in two important scenarios: dictionary learning and structured sparse estimation. a further improvement is achieved by interleaving gcg with fixed-rank local subspace optimization. a series of experiments on matrix completion, multi-class classification, multi-view dictionary learning and overlapping group lasso shows that the proposed method can significantly reduce the training cost of current alternatives.","","2014-10-17","","['yaoliang yu', 'xinhua zhang', 'dale schuurmans']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"967",1410.4934,"nonparametric model checks of single-index assumptions","math.st stat.th","semiparametric single-index assumptions are convenient and widely used dimen\-sion reduction approaches that represent a compromise between the parametric and fully nonparametric models for regressions or conditional laws. in a mean regression setup, the sim assumption means that the conditional expectation of the response given the vector of covariates is the same as the conditional expectation of the response given a scalar projection of the covariate vector. in a conditional distribution modeling, under the sim assumption the conditional law of a response given the covariate vector coincides with the conditional law given a linear combination of the covariates. several estimation techniques for single-index models are available and commonly used in applications. however, the problem of testing the goodness-of-fit seems less explored and the existing proposals still have some major drawbacks. in this paper, a novel kernel-based approach for testing sim assumptions is introduced. the covariate vector needs not have a density and only the index estimated under the sim assumption is used in kernel smoothing. hence the effect of high-dimensional covariates is mitigated while asymptotic normality of the test statistic is obtained. irrespective of the fixed dimension of the covariate vector, the new test detects local alternatives approaching the null hypothesis slower than $n^{-1/2}h^{-1/4},$ where $h$ is the bandwidth used to build the test statistic and $n$ is the sample size. a wild bootstrap procedure is proposed for finite sample corrections of the asymptotic critical values. the small sample performances of our test compared to existing procedures are illustrated through simulations.","","2014-10-18","","['samuel maistre', 'valentin patilea']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"968",1410.5522,"variational reformulation of bayesian inverse problems","stat.ml","the classical approach to inverse problems is based on the optimization of a misfit function. despite its computational appeal, such an approach suffers from many shortcomings, e.g., non-uniqueness of solutions, modeling prior knowledge, etc. the bayesian formalism to inverse problems avoids most of the difficulties encountered by the optimization approach, albeit at an increased computational cost. in this work, we use information theoretic arguments to cast the bayesian inference problem in terms of an optimization problem. the resulting scheme combines the theoretical soundness of fully bayesian inference with the computational efficiency of a simple optimization.","","2014-10-20","","['panagiotis tsilifis', 'ilias bilionis', 'ioannis katsounaros', 'nicholas zabaras']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"969",1410.6264,"capturing spatial interdependence in image features: the counting grid,   an epitomic representation for bags of features","cs.cv stat.ml","in recent scene recognition research images or large image regions are often represented as disorganized ""bags"" of features which can then be analyzed using models originally developed to capture co-variation of word counts in text. however, image feature counts are likely to be constrained in different ways than word counts in text. for example, as a camera pans upwards from a building entrance over its first few floors and then further up into the sky fig. 1, some feature counts in the image drop while others rise -- only to drop again giving way to features found more often at higher elevations. the space of all possible feature count combinations is constrained both by the properties of the larger scene and the size and the location of the window into it. to capture such variation, in this paper we propose the use of the counting grid model. this generative model is based on a grid of feature counts, considerably larger than any of the modeled images, and considerably smaller than the real estate needed to tile the images next to each other tightly. each modeled image is assumed to have a representative window in the grid in which the feature counts mimic the feature distribution in the image. we provide a learning procedure that jointly maps all images in the training set to the counting grid and estimates the appropriate local counts in it. experimentally, we demonstrate that the resulting representation captures the space of feature count combinations more accurately than the traditional models, not only when the input images come from a panning camera, but even when modeling images of different scenes from the same category.","","2014-10-23","","['alessandro perina', 'nebojsa jojic']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"970",1410.6556,"forward variable selection for sparse ultra-high dimensional varying   coefficient models","stat.me","varying coefficient models have numerous applications in a wide scope of scientific areas. while enjoying nice interpretability, they also allow flexibility in modeling dynamic impacts of the covariates. but, in the new era of big data, it is challenging to select the relevant variables when there are a large number of candidates. recently several work are focused on this important problem based on sparsity assumptions; they are subject to some limitations, however. we introduce an appealing forward variable selection procedure. it selects important variables sequentially according to a sum of squares criterion, and it employs an ebic- or bic-based stopping rule. clearly it is simple to implement and fast to compute, and it possesses many other desirable properties from both theoretical and numerical viewpoints. we establish rigorous selection consistency results when either ebic or bic is used as the stopping criterion, under some mild regularity conditions. notably, unlike existing methods, an extra screening step is not required to ensure selection consistency. even if the regularity conditions fail to hold, our procedure is still useful as an effective screening procedure in a less restrictive setup. we carried out simulation and empirical studies to show the efficacy and usefulness of our procedure.","","2014-10-23","","['ming-yen cheng', 'toshio honda', 'jin-ting zhang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"971",1410.6558,"sampling in the analysis transform domain","cs.it math.it math.na stat.me","many signal and image processing applications have benefited remarkably from the fact that the underlying signals reside in a low dimensional subspace. one of the main models for such a low dimensionality is the sparsity one. within this framework there are two main options for the sparse modeling: the synthesis and the analysis ones, where the first is considered the standard paradigm for which much more research has been dedicated. in it the signals are assumed to have a sparse representation under a given dictionary. on the other hand, in the analysis approach the sparsity is measured in the coefficients of the signal after applying a certain transformation, the analysis dictionary, on it. though several algorithms with some theory have been developed for this framework, they are outnumbered by the ones proposed for the synthesis methodology.   given that the analysis dictionary is either a frame or the two dimensional finite difference operator, we propose a new sampling scheme for signals from the analysis model that allows to recover them from their samples using any existing algorithm from the synthesis model. the advantage of this new sampling strategy is that it makes the existing synthesis methods with their theory also available for signals from the analysis framework.","","2014-10-23","2015-03-23","['raja giryes']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"972",1410.6714,"stochastic blockmodeling for online advertising","stat.ml stat.ap","online advertising is an important and huge industry. having knowledge of the website attributes can contribute greatly to business strategies for ad-targeting, content display, inventory purchase or revenue prediction. classical inferences on users and sites impose challenge, because the data is voluminous, sparse, high-dimensional and noisy. in this paper, we introduce a stochastic blockmodeling for the website relations induced by the event of online user visitation. we propose two clustering algorithms to discover the instrinsic structures of websites, and compare the performance with a goodness-of-fit method and a deterministic graph partitioning method. we demonstrate the effectiveness of our algorithms on both simulation and aol website dataset.","","2014-10-24","2014-11-17","['li chen', 'matthew patton']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"973",1410.6784,"an overview of nonparametric tests of extreme-value dependence and of   some related statistical procedures","stat.me","an overview of existing nonparametric tests of extreme-value dependence is presented. given an i.i.d.\ sample of random vectors from a continuous distribution, such tests aim at assessing whether the underlying unknown copula is of the {\em extreme-value} type or not. the existing approaches available in the literature are summarized according to how departure from extreme-value dependence is assessed. related statistical procedures useful when modeling data with this type of dependence are briefly described next. two illustrations on real data sets are then carried out using some of the statistical procedures under consideration implemented in the \textsf{r} package {\tt copula}. finally, the related problem of testing the {\em maximum domain of attraction} condition is discussed.","","2014-10-24","","['axel bücher', 'ivan kojadinovic']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"974",1410.6926,"modelling and forecasting the realized range conditional quantiles","stat.ap","several studies have focused on the realized range volatility, an estimator of the quadratic variation of financial prices, taking into account the impact of microstructure noise and jumps. however, none has considered direct modeling and forecasting of the realized range conditional quantiles. this study carries out a quantile regression analysis to fill this gap. the proposed model takes into account as quantile predictors both the lagged values of the estimated volatility and some key macroeconomic and financial variables, which provide important information about the overall market trend and risk. in this way, and without distributional assumptions on the realized range innovations, it is possible to assess the entire conditional distribution of the estimated volatility. this issue is a critical one for financial decision-makers in terms of pricing, asset allocation, and risk management. the quantile regression approach allows how the links among the involved variables change across the quantiles levels to be analyzed. in addition, a rolling analysis is performed in order to determine how the relationships that characterize the proposed model evolve over time. the analysis is applied to sixteen stocks issued by companies that operate in differing economic sectors of the u.s. market, and the forecast accuracy is validated by means of suitable tests. the results show evidence of the selected variables' relevant impacts and, particularly during periods of market stress, highlights heterogeneous effects across quantiles.","","2014-10-25","","['giovanni bonaccolto', 'massimiliano caporin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"975",1410.6948,"data assimilation of satellite fire detection in coupled atmosphere-fire   simulation by wrf-sfire","physics.data-an physics.ao-ph stat.co","currently available satellite active fire detection products from the viirs and modis instruments on polar-orbiting satellites produce detection squares in arbitrary locations. there is no global fire/no fire map, no detection under cloud cover, false negatives are common, and the detection squares are much coarser than the resolution of a fire behavior model. consequently, current active fire satellite detection products should be used to improve fire modeling in a statistical sense only, rather than as a direct input. we describe a new data assimilation method for active fire detection, based on a modification of the fire arrival time to simultaneously minimize the difference from the forecast fire arrival time and maximize the likelihood of the fire detection data. this method is inspired by contour detection methods used in computer vision, and it can be cast as a bayesian inverse problem technique, or a generalized tikhonov regularization. after the new fire arrival time on the whole simulation domain is found, the model can be re-run from a time in the past using the new fire arrival time to generate the heat fluxes and to spin up the atmospheric model until the satellite overpass time, when the coupled simulation continues from the modified state.","10.14195/978-989-26-0884-6_80","2014-10-25","","['jan mandel', 'adam k. kochanski', 'martin vejmelka', 'jonathan d. beezley']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"976",1410.7091,"on some distributed disorder detection","math.oc cs.sy math.st stat.th","multivariate data sources with components of different information value seem to appear frequently in practice. models in which the components change their homogeneity at different times are of significant importance. the fact whether any changes are influential for the whole process is determined not only by the moments of the change, but also depends on which coordinates. this is particularly important in issues such as reliability analysis of complex systems and the location of an intruder in surveillance systems. in this paper we developed a mathematical model for such sources of signals with discrete time having the markov property given the times of change. the research also comprises a multivariate detection of the transition probabilities changes at certain sensitivity level in the multidimensional process. additionally, the observation of the random vector is depicted. each chosen coordinate forms the markov process with different transition probabilities before and after some unknown moment. the aim of statisticians is to estimate the moments based on the observation of the process. the bayesian approach is used with the risk function depending on measure of chance of a false alarm and some cost of overestimation. the moment of the system's disorder is determined by the detection of transition probabilities changes at some coordinates. the overall modeling of the critical coordinates is based on the simple game.","","2014-10-26","","['krzysztof szajowski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"977",1410.735,"flexible modeling of epidemics with an empirical bayes framework","q-bio.pe stat.ap","seasonal influenza epidemics cause consistent, considerable, widespread loss annually in terms of economic burden, morbidity, and mortality. with access to accurate and reliable forecasts of a current or upcoming influenza epidemic's behavior, policy makers can design and implement more effective countermeasures. we developed a framework for in-season forecasts of epidemics using a semiparametric empirical bayes framework, and applied it to predict the weekly percentage of outpatient doctors visits for influenza-like illness, as well as the season onset, duration, peak time, and peak height, with and without additional data from google flu trends, as part of the cdc's 2013--2014 ""predict the influenza season challenge"". previous work on epidemic modeling has focused on developing mechanistic models of disease behavior and applying time series tools to explain historical data. however, these models may not accurately capture the range of possible behaviors that we may see in the future. our approach instead produces possibilities for the epidemic curve of the season of interest using modified versions of data from previous seasons, allowing for reasonable variations in the timing, pace, and intensity of the seasonal epidemics, as well as noise in observations. since the framework does not make strict domain-specific assumptions, it can easily be applied to other diseases as well. another important advantage of this method is that it produces a complete posterior distribution for any desired forecasting target, rather than mere point predictions. we report prospective influenza-like-illness forecasts that were made for the 2013--2014 u.s. influenza season, and compare the framework's cross-validated prediction error on historical data to that of a variety of simpler baseline predictors.","10.1371/journal.pcbi.1004382","2014-10-27","","['logan c. brooks', 'david c. farrow', 'sangwon hyun', 'ryan j. tibshirani', 'roni rosenfeld']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"978",1410.7365,"multiple output regression with latent noise","stat.ml","in high-dimensional data, structured noise caused by observed and unobserved factors affecting multiple target variables simultaneously, imposes a serious challenge for modeling, by masking the often weak signal. therefore, (1) explaining away the structured noise in multiple-output regression is of paramount importance. additionally, (2) assumptions about the correlation structure of the regression weights are needed. we note that both can be formulated in a natural way in a latent variable model, in which both the interesting signal and the noise are mediated through the same latent factors. under this assumption, the signal model then borrows strength from the noise model by encouraging similar effects on correlated targets. we introduce a hyperparameter for the \emph{latent signal-to-noise ratio} which turns out to be important for modelling weak signals, and an ordered infinite-dimensional shrinkage prior that resolves the rotational unidentifiability in reduced-rank regression models. simulations and prediction experiments with metabolite, gene expression, fmri measurement, and macroeconomic time series data show that our model equals or exceeds the state-of-the-art performance and, in particular, outperforms the standard approach of assuming independent noise and signal models.","","2014-10-27","2016-02-03","['jussi gillberg', 'pekka marttinen', 'matti pirinen', 'antti j. kangas', 'pasi soininen', 'mehreen ali', 'aki s. havulinna', 'marjo-riitta marjo-riitta järvelin', 'mika ala-korpela', 'samuel kaski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"979",1410.7383,"a ternary non-commutative latent factor model for scalable three-way   real tensor completion","stat.ml","motivated by large-scale collaborative-filtering applications, we present a non-commuting latent factor (nclf) tensor-completion approach for modeling three-way arrays, which is diagonal like the standard parafac, but wherein different terms distinguish different kinds of three-way relations of co-clusters, as determined by permutations of latent factors. the first key component of the algebraic representation is the usage of two non-commutative real trilinear operations as the building blocks of the approximation. these operations are the standard three dimensional triple-product and a trilinear product on a two-dimensional real vector space, which is a representation of the real clifford algebra cl(1,1) (a certain majorana spinor). both operations are purely ternary in that they cannot be decomposed into two group-operations on the relevant spaces. the second key component of the method is combining these operations using permutation-symmetry preserving linear combinations. we apply the model to the movielens and fannie mae datasets, and find that it outperforms the parafac model. we propose some future directions, such as unsupervised-learning.","","2014-10-26","2015-09-18","['guy baruch']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"980",1410.755,"learning deep dynamical models from image pixels","stat.ml cs.lg cs.ne cs.sy","modeling dynamical systems is important in many disciplines, e.g., control, robotics, or neurotechnology. commonly the state of these systems is not directly observed, but only available through noisy and potentially high-dimensional observations. in these cases, system identification, i.e., finding the measurement mapping and the transition mapping (system dynamics) in latent space can be challenging. for linear system dynamics and measurement mappings efficient solutions for system identification are available. however, in practical applications, the linearity assumptions does not hold, requiring non-linear system identification techniques. if additionally the observations are high-dimensional (e.g., images), non-linear system identification is inherently hard. to address the problem of non-linear system identification from high-dimensional observations, we combine recent advances in deep learning and system identification. in particular, we jointly learn a low-dimensional embedding of the observation by means of deep auto-encoders and a predictive transition model in this low-dimensional space. we demonstrate that our model enables learning good predictive models of dynamical systems from pixel information only.","","2014-10-28","","['niklas wahlström', 'thomas b. schön', 'marc peter deisenroth']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"981",1410.7558,"state and parameter estimation of partially observed linear ordinary   differential equations with deterministic optimal control","stat.me","ordinary differential equations are a simple but powerful framework for modeling complex systems. parameter estimation from times series can be done by nonlinear least squares (or other classical approaches), but this can give unsatisfactory results because the inverse problem can be ill-posed, even when the differential equation is linear.   following recent approaches that use approximate solutions of the ode model, we propose a new method that converts parameter estimation into an optimal control problem: our objective is to determine a control and a parameter that are as close as possible to the data. we derive then a criterion that makes a balance between discrepancy with data and with the model, and we minimize it by using optimization in functions spaces: our approach is related to the so-called deterministic kalman filtering, but different from the usual statistical kalman filtering. e show the root-$n$ consistency and asymptotic normality of the estimators for the parameter and for the states. experiments in a toy model and in a real case shows that our approach is generally more accurate and more reliable than nonlinear least squares and generalized smoothing, even in misspecified cases.","","2014-10-28","","['quentin clairon', 'nicolas brunel']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"982",1410.7616,"optimal designs for comparing curves","stat.me","we consider the optimal design problem for a comparison of two regression curves, which is used to establish the similarity between the dose response relationships of two groups. an optimal pair of designs minimizes the width of the confidence band for the difference between the two regression functions. optimal design theory (equivalence theorems, efficiency bounds) is developed for this non standard design problem and for some commonly used dose response models optimal designs are found explicitly. the results are illustrated in several examples modeling dose response relationships. it is demonstrated that the optimal pair of designs for the comparison of the regression curves is not the pair of the optimal designs for the individual models. in particular it is shown that the use of the optimal designs proposed in this paper instead of commonly used ""non-optimal"" designs yields a reduction of the width of the confidence band by more than 50%.","","2014-10-28","2014-11-18","['holger dette', 'kirsten schorning']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"983",1410.8861,"causal effect estimation methods","stat.me","relationship between two popular modeling frameworks of causal inference from observational data, namely, causal graphical model and potential outcome causal model is discussed. how some popular causal effect estimators found in applications of the potential outcome causal model, such as inverse probability of treatment weighted estimator and doubly robust estimator can be obtained by using the causal graphical model is shown. we confine to the simple case of binary outcome and treatment variables with discrete confounders and it is shown how to generalize results to cases of continuous variables.","","2014-10-31","","['priyantha wijayatunga']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"984",1411.0031,"likelihood-based inference for discretely observed birth-death-shift   processes, with applications to evolution of mobile genetic elements","stat.me q-bio.pe","continuous-time birth-death-shift (bds) processes are frequently used in stochastic modeling, with many applications in ecology and epidemiology. in particular, such processes can model evolutionary dynamics of transposable elements - important genetic markers in molecular epidemiology. estimation of the effects of individual covariates on the birth, death, and shift rates of the process can be accomplished by analyzing patient data, but inferring these rates in a discretely and unevenly observed setting presents computational challenges. we propose a mutli-type branching process approximation to bds processes and develop a corresponding expectation maximization (em) algorithm, where we use spectral techniques to reduce calculation of expected sufficient statistics to low dimensional integration. these techniques yield an efficient and robust optimization routine for inferring the rates of the bds process, and apply more broadly to multi-type branching processes where rates can depend on many covariates. after rigorously testing our methodology in simulation studies, we apply our method to study intrapatient time evolution of is6110 transposable element, a frequently used element during estimation of epidemiological clusters of mycobacterium tuberculosis infections.","","2014-10-31","2014-11-29","['jason xu', 'peter guttorp', 'midori kato-maeda', 'vladimir n. minin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"985",1411.0282,"noisy matrix completion under sparse factor models","stat.ml cs.it math.it stat.ap","this paper examines a general class of noisy matrix completion tasks where the goal is to estimate a matrix from observations obtained at a subset of its entries, each of which is subject to random noise or corruption. our specific focus is on settings where the matrix to be estimated is well-approximated by a product of two (a priori unknown) matrices, one of which is sparse. such structural models - referred to here as ""sparse factor models"" - have been widely used, for example, in subspace clustering applications, as well as in contemporary sparse modeling and dictionary learning tasks. our main theoretical contributions are estimation error bounds for sparsity-regularized maximum likelihood estimators for problems of this form, which are applicable to a number of different observation noise or corruption models. several specific implications are examined, including scenarios where observations are corrupted by additive gaussian noise or additive heavier-tailed (laplace) noise, poisson-distributed observations, and highly-quantized (e.g., one-bit) observations. we also propose a simple algorithmic approach based on the alternating direction method of multipliers for these tasks, and provide experimental evidence to support our error analyses.","10.1109/tit.2016.2549040","2014-11-02","","['akshay soni', 'swayambhoo jain', 'jarvis haupt', 'stefano gonella']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"986",1411.0288,"a general framework for mixed graphical models","math.st stat.ml stat.th","""mixed data"" comprising a large number of heterogeneous variables (e.g. count, binary, continuous, skewed continuous, among other data types) are prevalent in varied areas such as genomics and proteomics, imaging genetics, national security, social networking, and internet advertising. there have been limited efforts at statistically modeling such mixed data jointly, in part because of the lack of computationally amenable multivariate distributions that can capture direct dependencies between such mixed variables of different types. in this paper, we address this by introducing a novel class of block directed markov random fields (bdmrfs). using the basic building block of node-conditional univariate exponential families from yang et al. (2012), we introduce a class of mixed conditional random field distributions, that are then chained according to a block-directed acyclic graph to form our class of block directed markov random fields (bdmrfs). the markov independence graph structure underlying a bdmrf thus has both directed and undirected edges. we introduce conditions under which these distributions exist and are normalizable, study several instances of our models, and propose scalable penalized conditional likelihood estimators with statistical guarantees for recovering the underlying network structure. simulations as well as an application to learning mixed genomic networks from next generation sequencing expression data and mutation data demonstrate the versatility of our methods.","","2014-11-02","","['eunho yang', 'pradeep ravikumar', 'genevera i. allen', 'yulia baker', 'ying-wooi wan', 'zhandong liu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"987",1411.0416,"spatio-temporal analysis of epidemic phenomena using the r package   surveillance","stat.co cs.ce physics.data-an stat.ap","the availability of geocoded health data and the inherent temporal structure of communicable diseases have led to an increased interest in statistical models and software for spatio-temporal data with epidemic features. the open source r package surveillance can handle various levels of aggregation at which infective events have been recorded: individual-level time-stamped geo-referenced data (case reports) in either continuous space or discrete space, as well as counts aggregated by period and region. for each of these data types, the surveillance package implements tools for visualization, likelihoood inference and simulation from recently developed statistical regression frameworks capturing endemic and epidemic dynamics. altogether, this paper is a guide to the spatio-temporal modeling of epidemic phenomena, exemplified by analyses of public health surveillance data on measles and invasive meningococcal disease.","10.18637/jss.v077.i11","2014-11-03","2015-11-06","['sebastian meyer', 'leonhard held', 'michael höhle']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE
"988",1411.0599,"dynamic spatial regression models for space-varying forest stand tables","stat.ap","many forest management planning decisions are based on information about the number of trees by species and diameter per unit area. this information is commonly summarized in a stand table, where a stand is defined as a group of forest trees of sufficiently uniform species composition, age, condition, or productivity to be considered a homogeneous unit for planning purposes. typically information used to construct stand tables is gleaned from observed subsets of the forest selected using a probability-based sampling design. such sampling campaigns are expensive and hence only a small number of sample units are typically observed. this data paucity means that stand tables can only be estimated for relatively large areal units. contemporary forest management planning and spatially explicit ecosystem models require stand table input at higher spatial resolution than can be affordably provided using traditional approaches. we propose a dynamic multivariate poisson spatial regression model that accommodates both spatial correlation between observed diameter distributions and also correlation between tree counts across diameter classes within each location. to improve fit and prediction at unobserved locations, diameter specific intensities can be estimated using auxiliary data such as management history or remotely sensed information. the proposed model is used to analyze a diverse forest inventory dataset collected on the united states forest service penobscot experimental forest in bradley, maine. results demonstrate that explicitly modeling the residual spatial structure via a multivariate gaussian process and incorporating information about forest structure from lidar covariates improve model fit and can provide high spatial resolution stand table maps with associated estimates of uncertainty.","","2014-11-03","","['andrew o. finley', 'sudipto banerjee', 'aaron r. weiskittel', 'chad babcock', 'bruce d. cook']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"989",1411.0764,"a bayesian multivariate functional dynamic linear model","stat.me","we present a bayesian approach for modeling multivariate, dependent functional data. to account for the three dominant structural features in the data--functional, time dependent, and multivariate components--we extend hierarchical dynamic linear models for multivariate time series to the functional data setting. we also develop bayesian spline theory in a more general constrained optimization framework. the proposed methods identify a time-invariant functional basis for the functional observations, which is smooth and interpretable, and can be made common across multivariate observations for additional information sharing. the bayesian framework permits joint estimation of the model parameters, provides exact inference (up to mcmc error) on specific parameters, and allows generalized dependence structures. sampling from the posterior distribution is accomplished with an efficient gibbs sampling algorithm. we illustrate the proposed framework with two applications: (1) multi-economy yield curve data from the recent global recession, and (2) local field potential brain signals in rats, for which we develop a multivariate functional time series approach for multivariate time-frequency analysis. supplementary materials, including r code and the multi-economy yield curve data, are available online.","10.1080/01621459.2016.1165104","2014-11-03","2015-08-05","['daniel r. kowal', 'david s. matteson', 'david ruppert']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"990",1411.098,"mittag - leffler function distribution - a new generalization of   hyper-poisson distribution","math.st stat.th","in this paper a new generalization of the hyper-poisson distribution is proposed using the mittag-leffler function. the hyper-poisson, displaced poisson, poisson and geometric distributions among others are seen as particular cases. this mittag-leffler function distribution (mlfd) belongs to the generalized hypergeometric and generalized power series families and also arises as weighted poison distributions. mlfd is a flexible distribution with varying shapes like non-increasing with unique mode at zero, unimodal with one / two non-zero modes. it can be under, equi or over dispersed. various distributional properties like recurrence relation for pmf, cumulative distribution function, generating functions, formulae for different type of moments, their recurrence relations, index of dispersion, its classification, log-concavity, reliability properties like survival, increasing failure rate, unimodality, and stochastic ordering with respect to hyper-poisson distribution have been discussed. the distribution has been found to fare well when compared with the hyper-poisson distributions in its suitability in empirical modeling of differently dispersed count data. it is therefore expected that proposed mlfd with its interesting features and flexibility, will be a useful addition as a model for count data.","","2014-11-04","","['subrata chakraborty', 's. h. ong']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"991",1411.1045,"deep gaze i: boosting saliency prediction with feature maps trained on   imagenet","cs.cv q-bio.nc stat.ap","recent results suggest that state-of-the-art saliency models perform far from optimal in predicting fixations. this lack in performance has been attributed to an inability to model the influence of high-level image features such as objects. recent seminal advances in applying deep neural networks to tasks like object recognition suggests that they are able to capture this kind of structure. however, the enormous amount of training data necessary to train these networks makes them difficult to apply directly to saliency prediction. we present a novel way of reusing existing neural networks that have been pretrained on the task of object recognition in models of fixation prediction. using the well-known network of krizhevsky et al. (2012), we come up with a new saliency model that significantly outperforms all state-of-the-art models on the mit saliency benchmark. we show that the structure of this network allows new insights in the psychophysics of fixation selection and potentially their neural implementation. to train our network, we build on recent work on the modeling of saliency as point processes.","","2014-11-04","2015-04-09","['matthias kümmerer', 'lucas theis', 'matthias bethge']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"992",1411.12,"estimating the relative rate of recombination to mutation in bacteria   from single-locus variants using composite likelihood methods","stat.ap q-bio.pe","a number of studies have suggested using comparisons between dna sequences of closely related bacterial isolates to estimate the relative rate of recombination to mutation for that bacterial species. we consider such an approach which uses single-locus variants: pairs of isolates whose dna differ at a single gene locus. one way of deriving point estimates for the relative rate of recombination to mutation from such data is to use composite likelihood methods. we extend recent work in this area so as to be able to construct confidence intervals for our estimates, without needing to resort to computationally-intensive bootstrap procedures, and to develop a test for whether the relative rate varies across loci. both our test and method for constructing confidence intervals are obtained by modeling the dependence structure in the data, and then applying asymptotic theory regarding the distribution of estimators obtained using a composite likelihood. we applied these methods to multi-locus sequence typing (mlst) data from eight bacteria, finding strong evidence for considerable rate variation in three of these: bacillus cereus, enterococcus faecium and klebsiella pneumoniae.","10.1214/14-aoas795","2014-11-05","2015-06-02","['paul fearnhead', 'shoukai yu', 'patrick biggs', 'barbara holland', 'nigel french']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"993",1411.1352,"robust kronecker product pca for spatio-temporal covariance estimation","stat.me","kronecker pca involves the use of a space vs. time kronecker product decomposition to estimate spatio-temporal covariances. in this work the addition of a sparse correction factor is considered, which corresponds to a model of the covariance as a sum of kronecker products of low (separation) rank and a sparse matrix. this sparse correction extends the diagonally corrected kronecker pca of [greenewald et al 2013, 2014] to allow for sparse unstructured ""outliers"" anywhere in the covariance matrix, e.g. arising from variables or correlations that do not fit the kronecker model well, or from sources such as sensor noise or sensor failure. we introduce a robust pca-based algorithm to estimate the covariance under this model, extending the rearranged nuclear norm penalized ls kronecker pca approaches of [greenewald et al 2014, tsiligkaridis et al 2013]. an extension to toeplitz temporal factors is also provided, producing a parameter reduction for temporally stationary measurement modeling. high dimensional mse performance bounds are given for these extensions. finally, the proposed extension of kronpca is evaluated on both simulated and real data coming from yeast cell cycle experiments. this establishes the practical utility of robust kronecker pca in biological and other applications.","10.1109/tsp.2015.2472364","2014-11-05","2015-07-29","['kristjan greenewald', 'alfred hero']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"994",1411.1537,"large-margin determinantal point processes","stat.ml cs.cv cs.lg","determinantal point processes (dpps) offer a powerful approach to modeling diversity in many applications where the goal is to select a diverse subset. we study the problem of learning the parameters (the kernel matrix) of a dpp from labeled training data. we make two contributions. first, we show how to reparameterize a dpp's kernel matrix with multiple kernel functions, thus enhancing modeling flexibility. second, we propose a novel parameter estimation technique based on the principle of large margin separation. in contrast to the state-of-the-art method of maximum likelihood estimation, our large-margin loss function explicitly models errors in selecting the target subsets, and it can be customized to trade off different types of errors (precision vs. recall). extensive empirical studies validate our contributions, including applications on challenging document and video summarization, where flexibility in modeling the kernel matrix and balancing different errors is indispensable.","","2014-11-06","2014-11-07","['boqing gong', 'wei-lun chao', 'kristen grauman', 'fei sha']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"995",1411.199,"a totally unimodular view of structured sparsity","cs.lg stat.ml","this paper describes a simple framework for structured sparse recovery based on convex optimization. we show that many structured sparsity models can be naturally represented by linear matrix inequalities on the support of the unknown parameters, where the constraint matrix has a totally unimodular (tu) structure. for such structured models, tight convex relaxations can be obtained in polynomial time via linear programming. our modeling framework unifies the prevalent structured sparsity norms in the literature, introduces new interesting ones, and renders their tightness and tractability arguments transparent.","","2014-11-07","2015-03-03","['marwa el halabi', 'volkan cevher']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"996",1411.1997,"differential gene co-expression networks via bayesian biclustering   models","stat.me q-bio.gn q-bio.mn stat.ml","identifying latent structure in large data matrices is essential for exploring biological processes. here, we consider recovering gene co-expression networks from gene expression data, where each network encodes relationships between genes that are locally co-regulated by shared biological mechanisms. to do this, we develop a bayesian statistical model for biclustering to infer subsets of co-regulated genes whose covariation may be observed in only a subset of the samples. our biclustering method, bicmix, has desirable properties, including allowing overcomplete representations of the data, computational tractability, and jointly modeling unknown confounders and biological signals. compared with related biclustering methods, bicmix recovers latent structure with higher precision across diverse simulation scenarios. further, we develop a method to recover gene co-expression networks from the estimated sparse biclustering matrices. we apply bicmix to breast cancer gene expression data and recover a gene co-expression network that is differential across er+ and er- samples.","","2014-11-07","","['chuan gao', 'shiwen zhao', 'ian c. mcdowell', 'christopher d. brown', 'barbara e. engelhardt']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"997",1411.2305,"model-parallel inference for big topic models","cs.dc cs.lg stat.ml","in real world industrial applications of topic modeling, the ability to capture gigantic conceptual space by learning an ultra-high dimensional topical representation, i.e., the so-called ""big model"", is becoming the next desideratum after enthusiasms on ""big data"", especially for fine-grained downstream tasks such as online advertising, where good performances are usually achieved by regression-based predictors built on millions if not billions of input features. the conventional data-parallel approach for training gigantic topic models turns out to be rather inefficient in utilizing the power of parallelism, due to the heavy dependency on a centralized image of ""model"". big model size also poses another challenge on the storage, where available model size is bounded by the smallest ram of nodes. to address these issues, we explore another type of parallelism, namely model-parallelism, which enables training of disjoint blocks of a big topic model in parallel. by integrating data-parallelism with model-parallelism, we show that dependencies between distributed elements can be handled seamlessly, achieving not only faster convergence but also an ability to tackle significantly bigger model size. we describe an architecture for model-parallel inference of lda, and present a variant of collapsed gibbs sampling algorithm tailored for it. experimental results demonstrate the ability of this system to handle topic modeling with unprecedented amount of 200 billion model variables only on a low-end cluster with very limited computational resources and bandwidth.","","2014-11-09","","['xun zheng', 'jin kyu kim', 'qirong ho', 'eric p. xing']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"998",1411.2578,"upscaling uncertainty with dynamic discrepancy for a multi-scale carbon   capture system","stat.me","uncertainties from model parameters and model discrepancy from small-scale models impact the accuracy and reliability of predictions of large-scale systems. inadequate representation of these uncertainties may result in inaccurate and overconfident predictions during scale-up to larger models. hence multiscale modeling efforts must quantify the effect of the propagation of uncertainties during upscaling. using a bayesian approach, we calibrate a small-scale solid sorbent model to thermogravimetric (tga) data on a functional profile using chemistry-based priors. crucial to this effort is the representation of model discrepancy, which uses a bayesian smoothing splines (bss-anova) framework. we use an intrusive uncertainty quantification (uq) approach by including the discrepancy function within the chemical rate expressions; resulting in a set of stochastic differential equations. such an approach allows for easily propagating uncertainty by propagating the joint model parameter and discrepancy posterior into the larger-scale system of rate expressions. the broad uq framework presented here may have far-reaching impact into virtually all areas of science where multiscale modeling is used.","","2014-11-10","2014-12-16","['k. sham bhat', 'david s. mebane', 'curtis b. storlie', 'priyadarshi mahapatra']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"999",1411.2674,"the bayesian echo chamber: modeling social influence via linguistic   accommodation","stat.ml cs.cl cs.lg cs.si","we present the bayesian echo chamber, a new bayesian generative model for social interaction data. by modeling the evolution of people's language usage over time, this model discovers latent influence relationships between them. unlike previous work on inferring influence, which has primarily focused on simple temporal dynamics evidenced via turn-taking behavior, our model captures more nuanced influence relationships, evidenced via linguistic accommodation patterns in interaction content. the model, which is based on a discrete analog of the multivariate hawkes process, permits a fully bayesian inference algorithm. we validate our model's ability to discover latent influence patterns using transcripts of arguments heard by the us supreme court and the movie ""12 angry men."" we showcase our model's capabilities by using it to infer latent influence patterns from federal open market committee meeting transcripts, demonstrating state-of-the-art performance at uncovering social dynamics in group discussions.","","2014-11-10","2015-01-27","['fangjian guo', 'charles blundell', 'hanna wallach', 'katherine heller']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1000",1411.3062,"structural change in sparsity","stat.me","in the high-dimensional sparse modeling literature, it has been crucially assumed that the sparsity structure of the model is homogeneous over the entire population. that is, the identities of important regressors are invariant across the population and across the individuals in the collected sample. in practice, however, the sparsity structure may not always be invariant in the population, due to heterogeneity across different sub-populations. we consider a general, possibly non-smooth m-estimation framework, allowing a possible structural change regarding the identities of important regressors in the population. our penalized m-estimator not only selects covariates but also discriminates between a model with homogeneous sparsity and a model with a structural change in sparsity. as a result, it is not necessary to know or pretest whether the structural change is present, or where it occurs. we derive asymptotic bounds on the estimation loss of the penalized m-estimators, and achieve the oracle properties. we also show that when there is a structural change, the estimator of the threshold parameter is super-consistent. if the signal is relatively strong, the rates of convergence can be further improved and asymptotic distributional properties of the estimators including the threshold estimator can be established using an adaptive penalization. the proposed methods are then applied to quantile regression and logistic regression models and are illustrated via monte carlo experiments.","","2014-11-11","2014-11-19","['sokbae lee', 'yuan liao', 'myung hwan seo', 'youngki shin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1001",1411.3174,"non-stationary dependence structures for spatial extremes","stat.me","max-stable processes are natural models for spatial extremes because they provide suitable asymptotic approximations to the distribution of maxima of random fields. in the recent past, several parametric families of stationary max-stable models have been developed, and fitted to various types of data. however, a recurrent problem is the modeling of non-stationarity. in this paper, we develop non-stationary max-stable dependence structures in which covariates can be easily incorporated. inference is performed using pairwise likelihoods, and its performance is assessed by an extensive simulation study based on a non-stationary locally isotropic extremal $t$ model. evidence that unknown parameters are well estimated is provided, and estimation of spatial return level curves is discussed. the methodology is demonstrated with temperature maxima recorded over a complex topography. models are shown to satisfactorily capture extremal dependence.","","2014-11-12","2016-02-19","['raphael huser', 'marc g. genton']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1002",1411.3513,"inference for deformation and interference in 3d printing","stat.ap","additive manufacturing, or 3d printing, is a promising manufacturing technique marred by product deformation due to material solidification in the printing process. control of printed product deformation can be achieved by a compensation plan. however, little attention has been paid to interference in compensation, which is thought to result from the inevitable discretization of a compensation plan. we investigate interference with an experiment involving the application of discretized compensation plans to cylinders. our treatment illustrates a principled framework for detecting and modeling interference, and ultimately provides a new step toward better understanding quality control for 3d printing.","10.1214/14-aoas762","2014-11-13","","['arman sabbaghi', 'tirthankar dasgupta', 'qiang huang', 'jizhe zhang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1003",1411.369,"a novel joint location-scale testing framework for improved detection of   variants with main or interaction effects","stat.me","we propose a novel and easy-to-implement joint location-scale association testing procedure that can account for complex genetic architecture without explicitly modeling interaction effects, and is suitable for large-scale whole-genome scans and meta-analyses. we focus on fisher's method and use it to combine evidence from the standard location test and the more recent scale test, and we describe its use for single-variant, gene-set and pathway association analyses.","","2014-11-13","","['david soave', 'andrew paterson', 'lisa strug', 'lei sun']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1004",1411.3776,"measuring influence in twitter ecosystems using a counting process   modeling framework","cs.si stat.me","data extracted from social media platforms, such as twitter, are both large in scale and complex in nature, since they contain both unstructured text, as well as structured data, such as time stamps and interactions between users. a key question for such platforms is to determine influential users, in the sense that they generate interactions between members of the platform. common measures used both in the academic literature and by companies that provide analytics services are variants of the popular web-search pagerank algorithm applied to networks that capture connections between users. in this work, we develop a modeling framework using multivariate interacting counting processes to capture the detailed actions that users undertake on such platforms, namely posting original content, reposting and/or mentioning other users' postings. based on the proposed model, we also derive a novel influence measure. we discuss estimation of the model parameters through maximum likelihood and establish their asymptotic properties. the proposed model and the accompanying influence measure are illustrated on a data set covering a five year period of the twitter actions of the members of the us senate, as well as mainstream news organizations and media personalities.","","2014-11-13","","['donggeng xia', 'shawn mankad', 'george michailidis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1005",1411.4072,"learning multi-relational semantics using neural-embedding models","cs.cl cs.lg stat.ml","in this paper we present a unified framework for modeling multi-relational representations, scoring, and learning, and conduct an empirical study of several recent multi-relational embedding models under the framework. we investigate the different choices of relation operators based on linear and bilinear transformations, and also the effects of entity representations by incorporating unsupervised vectors pre-trained on extra textual resources. our results show several interesting findings, enabling the design of a simple embedding model that achieves the new state-of-the-art performance on a popular knowledge base completion task evaluated on freebase.","","2014-11-14","","['bishan yang', 'wen-tau yih', 'xiaodong he', 'jianfeng gao', 'li deng']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1006",1411.4077,"a framework for studying synaptic plasticity with neural spike train   data","stat.ml q-bio.nc","learning and memory in the brain are implemented by complex, time-varying changes in neural circuitry. the computational rules according to which synaptic weights change over time are the subject of much research, and are not precisely understood. until recently, limitations in experimental methods have made it challenging to test hypotheses about synaptic plasticity on a large scale. however, as such data become available and these barriers are lifted, it becomes necessary to develop analysis techniques to validate plasticity models. here, we present a highly extensible framework for modeling arbitrary synaptic plasticity rules on spike train data in populations of interconnected neurons. we treat synaptic weights as a (potentially nonlinear) dynamical system embedded in a fully-bayesian generalized linear model (glm). in addition, we provide an algorithm for inferring synaptic weight trajectories alongside the parameters of the glm and of the learning rules. using this method, we perform model comparison of two proposed variants of the well-known spike-timing-dependent plasticity (stdp) rule, where nonlinear effects play a substantial role. on synthetic data generated from the biophysical simulator neuron, we show that we can recover the weight trajectories, the pattern of connectivity, and the underlying learning rules.","","2014-11-14","","['scott w. linderman', 'christopher h. stock', 'ryan p. adams']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1007",1411.4314,"hierarchical and matrix structures in a large organizational email   network: visualization and modeling approaches","cs.si physics.soc-ph stat.ap","this paper presents findings from a study of the email network of a large scientific research organization, focusing on methods for visualizing and modeling organizational hierarchies within large, complex network datasets. in the first part of the paper, we find that visualization and interpretation of complex organizational network data is facilitated by integration of network data with information on formal organizational divisions and levels. by aggregating and visualizing email traffic between organizational units at various levels, we derive several insights into how large subdivisions of the organization interact with each other and with outside organizations. our analysis shows that line and program management interactions in this organization systematically deviate from the idealized pattern of interaction prescribed by ""matrix management."" in the second part of the paper, we propose a power law model for predicting degree distribution of organizational email traffic based on hierarchical relationships between managers and employees. this model considers the influence of global email announcements sent from managers to all employees under their supervision, and the role support staff play in generating email traffic, acting as agents for managers. we also analyze patterns in email traffic volume over the course of a work week.","","2014-11-16","","['benjamin h. sims', 'nikolai sinitsyn', 'stephan j. eidenbenz']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1008",1411.5172,"learning nonparametric differential equations with operator-valued   kernels and gradient matching","cs.lg stat.ml","modeling dynamical systems with ordinary differential equations implies a mechanistic view of the process underlying the dynamics. however in many cases, this knowledge is not available. to overcome this issue, we introduce a general framework for nonparametric ode models using penalized regression in reproducing kernel hilbert spaces (rkhs) based on operator-valued kernels. moreover, we extend the scope of gradient matching approaches to nonparametric ode. a smooth estimate of the solution ode is built to provide an approximation of the derivative of the ode solution which is in turn used to learn the nonparametric ode model. this approach benefits from the flexibility of penalized regression in rkhs allowing for ridge or (structured) sparse regression as well. very good results are shown on 3 different ode systems.","","2014-11-19","","['markus heinonen', ""florence d'alché-buc""]",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1009",1411.5725,"local adaptive grouped regularization and its oracle properties for   varying coefficient regression","stat.me","varying coefficient regression is a flexible technique for modeling data where the coefficients are functions of some effect-modifying parameter, often time or location in a certain domain. while there are a number of methods for variable selection in a varying coefficient regression model, the existing methods are mostly for global selection, which includes or excludes each covariate over the entire domain. presented here is a new local adaptive grouped regularization (lagr) method for local variable selection in spatially varying coefficient linear and generalized linear regression. lagr selects the covariates that are associated with the response at any point in space, and simultaneously estimates the coefficients of those covariates by tailoring the adaptive group lasso toward a local regression model with locally linear coefficient estimates. oracle properties of the proposed method are established under local linear regression and local generalized linear regression. the finite sample properties of lagr are assessed in a simulation study and for illustration, the boston housing price data set is analyzed.","","2014-11-20","","['wesley brooks', 'jun zhu', 'zudi lu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1010",1411.7009,"additive gaussian process regression","stat.me stat.co","additive-interactive regression has recently been shown to offer attractive minimax error rates over traditional nonparametric multivariate regression in a wide variety of settings, including cases where the predictor count is much larger than the sample size and many of the predictors have important effects on the response, potentially through complex interactions. we present a bayesian implementation of additive-interactive regression using an additive gaussian process (agp) prior and develop an efficient markov chain sampler that extends stochastic search variable selection in this setting. careful prior and hyper-parameter specification are developed in light of performance and computational considerations, and key innovations address difficulties in exploring a joint posterior distribution over multiple subsets of high dimensional predictor inclusion vectors. the method offers state-of-the-art support and interaction recovery while improving dramatically over competitors in terms of prediction accuracy on a diverse set of simulated and real data. results from real data studies provide strong evidence that the additive-interactive framework is an attractive modeling platform for high-dimensional nonparametric regression.","","2014-11-25","","['shaan qamar', 'surya t. tokdar']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"1011",1411.742,"optimal bayesian estimation in random covariate design with a rescaled   gaussian process prior","math.st stat.th","in bayesian nonparametric models, gaussian processes provide a popular prior choice for regression function estimation. existing literature on the theoretical investigation of the resulting posterior distribution almost exclusively assume a fixed design for covariates. the only random design result we are aware of (van der vaart & van zanten, 2011) assumes the assigned gaussian process to be supported on the smoothness class specified by the true function with probability one. this is a fairly restrictive assumption as it essentially rules out the gaussian process prior with a squared exponential kernel when modeling rougher functions. in this article, we show that an appropriate rescaling of the above gaussian process leads to a rate-optimal posterior distribution even when the covariates are independently realized from a known density on a compact set. the proofs are based on deriving sharp concentration inequalities for frequentist kernel estimators; the results might be of independent interest.","","2014-11-26","2015-03-04","['debdeep pati', 'anirban bhattacharya', 'guang cheng']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1012",1411.7481,"nonparametric bayesian inference for mean residual life functions in   survival analysis","stat.me","the mean residual life function is a key functional for a survival distribution. it has a practically useful interpretation as the expected remaining lifetime given survival up to a particular time point, and it also characterizes the survival distribution. however, it has received limited attention in terms of inference methods under a probabilistic modeling framework. we seek to provide general inference methodology for mean residual life regression. survival data often include a set of predictor variables for the survival response distribution, and in many cases it is natural to include the covariates as random variables into the modeling. we thus employ dirichlet process mixture modeling for the joint stochastic mechanism of the covariates and survival responses. this approach implies a flexible model structure for the mean residual life of the conditional response distribution, allowing general shapes for mean residual life as a function of covariates given a specific time point, as well as a function of time given particular values of the covariate vector. to expand the scope of the modeling framework, we extend the mixture model to incorporate dependence across experimental groups, such as treatment and control groups. this extension is built from a dependent dirichlet process prior for the group-specific mixing distributions, with common locations and weights that vary across groups through latent bivariate beta distributed random variables. we develop properties of the regression models, and discuss methods for prior specification and posterior inference. the different components of the methodology are illustrated with simulated data examples, and the model is also applied to a data set comprising right censored survival times.","","2014-11-27","2018-10-09","['valerie poynor', 'athanasios kottas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1013",1411.7508,"forecasting the colorado river discharge using an artificial neural   network (ann) approach","stat.ml physics.soc-ph","artificial neural network (ann) based model is a computational approach commonly used for modeling the complex relationships between input and output parameters. prediction of the flow rate of a river is a requisite for any successful water resource management and river basin planning. in the current survey, the effectiveness of an artificial neural network was examined to predict the colorado river discharge. in this modeling process, an ann model was used to relate the discharge of the colorado river to such parameters as the amount of precipitation, ambient temperature and snowpack level at a specific time of the year. the model was able to precisely study the impact of climatic parameters on the flow rate of the colorado river.","","2014-11-27","","['amirhossein mehrkesh', 'maryam ahmadi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1014",1411.7571,"a bayesian semiparametric approach to learning about gene-gene   interactions in case-control studies","stat.ap","gene-gene interactions are often regarded as playing significant roles in influencing variabilities of complex traits. although much research has been devoted to this area, to date a comprehensive statistical model that addresses the various sources of uncertainties, seem to be lacking. in this paper, we propose and develop a novel bayesian semiparametric approach composed of finite mixtures based on dirichlet processes and a hierarchical matrix-normal distribution that can comprehensively account for the unknown number of sub-populations and gene-gene interactions. then, by formulating novel and suitable bayesian tests of hypotheses we attempt to single out the roles of the genes, individually, and in interaction with other genes, in case-control studies. we also attempt to identify the significant loci associated with the disease. our model facilitates a highly efficient parallel computing methodology, combining gibbs sampling and transformation based mcmc (tmcmc). application of our ideas to biologically realistic data sets revealed quite encouraging performance. we also applied our ideas to a real, myocardial infarction dataset, and obtained interesting results that partly agree with, and also complement, the existing works in this area, to reveal the importance of sophisticated and realistic modeling of gene-gene interactions.","","2014-11-27","2018-04-17","['durba bhattacharya', 'sourabh bhattacharya']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1015",1411.7864,"efficient inference of overlapping communities in complex networks","stat.ml cs.si physics.soc-ph","we discuss two views on extending existing methods for complex network modeling which we dub the communities first and the networks first view, respectively. inspired by the networks first view that we attribute to white, boorman, and breiger (1976)[1], we formulate the multiple-networks stochastic blockmodel (mnsbm), which seeks to separate the observed network into subnetworks of different types and where the problem of inferring structure in each subnetwork becomes easier. we show how this model is specified in a generative bayesian framework where parameters can be inferred efficiently using gibbs sampling. the result is an effective multiple-membership model without the drawbacks of introducing complex definitions of ""groups"" and how they interact. we demonstrate results on the recovery of planted structure in synthetic networks and show very encouraging results on link prediction performances using multiple-networks models on a number of real-world network data sets.","","2014-11-28","","['bjarne ørum fruergaard', 'tue herlau']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1016",1411.7924,"predicting clicks in online display advertising with latent features and   side-information","stat.ml cs.lg stat.ap","we review a method for click-through rate prediction based on the work of menon et al. [11], which combines collaborative filtering and matrix factorization with a side-information model and fuses the outputs to proper probabilities in [0,1]. in addition we provide details, both for the modeling as well as the experimental part, that are not found elsewhere. we rigorously test the performance on several test data sets from consecutive days in a click-through rate prediction setup, in a manner which reflects a real-world pipeline. our results confirm that performance can be increased using latent features, albeit the differences in the measures are small but significant.","","2014-11-28","","['bjarne ørum fruergaard']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"1017",1411.7973,"bus travel time predictions using additive models","cs.lg stat.ap","many factors can affect the predictability of public bus services such as traffic, weather and local events. other aspects, such as day of week or hour of day, may influence bus travel times as well, either directly or in conjunction with other variables. however, the exact nature of such relationships between travel times and predictor variables is, in most situations, not known. in this paper we develop a framework that allows for flexible modeling of bus travel times through the use of additive models. in particular, we model travel times as a sum of linear as well as nonlinear terms that are modeled as smooth functions of predictor variables. the proposed class of models provides a principled statistical framework that is highly flexible in terms of model building. the experimental results demonstrate uniformly superior performance of our best model as compared to previous prediction methods when applied to a very large gps data set obtained from buses operating in the city of rio de janeiro.","10.1109/icdm.2014.107","2014-11-28","","['matthias kormaksson', 'luciano barbosa', 'marcos r. vieira', 'bianca zadrozny']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1018",1412.0367,"bayesian nonparametric modeling for mean residual life regression","stat.ap stat.me","the mean residual life function is a key functional for a survival distribution. it has practically useful interpretation as the expected remaining lifetime given survival up to a particular time point, and it also characterizes the survival distribution. however, it has received limited attention in terms of inference methods under a probabilistic modeling framework. in this paper, we seek to provide general inference methodology for mean residual life regression. survival data often include a set of predictor variables for the survival response distribution, and in many cases it is natural to include the covariates as random variables into the modeling. we thus propose a dirichlet process mixture modeling approach for the joint stochastic mechanism of the covariates and survival responses. this approach implies a flexible model structure for the mean residual life of the conditional response distribution, allowing general shapes for mean residual life as a function of covariates given a specific time point, as well as a function of time given particular values of the covariate vector. to expand the scope of the modeling framework, we extend the mixture model to incorporate dependence across experimental groups, such as treatment and control groups. this extension is built from a dependent dirichlet process prior for the group-specific mixing distributions, with common locations and weights that vary across groups through latent bivariate beta distributed random variables. we develop properties of the proposed regression models, and discuss methods for prior specification and posterior inference. the different components of the methodology are illustrated with simulated data sets. moreover, the modeling approach is applied to a data set comprising right censored survival times of patients with small cell lung cancer.","","2014-12-01","2018-11-05","['valerie poynor', 'athanasios kottas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1019",1412.0436,"an infra-structure for performance estimation and experimental   comparison of predictive models in r","cs.ms cs.lg cs.se stat.co","this document describes an infra-structure provided by the r package performanceestimation that allows to estimate the predictive performance of different approaches (workflows) to predictive tasks. the infra-structure is generic in the sense that it can be used to estimate the values of any performance metrics, for any workflow on different predictive tasks, namely, classification, regression and time series tasks. the package also includes several standard workflows that allow users to easily set up their experiments limiting the amount of work and information they need to provide. the overall goal of the infra-structure provided by our package is to facilitate the task of estimating the predictive performance of different modeling approaches to predictive tasks in the r environment.","","2014-12-01","2015-09-07","['luis torgo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1020",1412.0968,"avoidable errors in the modeling of outbreaks of emerging pathogens,   with special reference to ebola","q-bio.pe stat.ap","as an emergent infectious disease outbreak unfolds, public health response is reliant on information on key epidemiological quantities, such as transmission potential and serial interval. increasingly, transmission models fit to incidence data are used to estimate these parameters and guide policy. some widely-used modeling practices lead to potentially large errors in parameter estimates and, consequently, errors in model-based forecasts. even more worryingly, in such situations, confidence in parameter estimates and forecasts can itself be far over-estimated, leading to the potential for large errors that mask their own presence. fortunately, straightforward and computationally inexpensive alternatives exist that avoid these problems. here, we first use a simulation study to demonstrate potential pitfalls of the standard practice of fitting deterministic models to cumulative incidence data. next, we demonstrate an alternative based on stochastic models fit to raw data from an early phase of 2014 west africa ebola virus disease outbreak. we show not only that bias is thereby reduced, but that uncertainty in estimates and forecasts is better quantified and that, critically, lack of model fit is more readily diagnosed. we conclude with a short list of principles to guide the modeling response to future infectious disease outbreaks.","10.1098/rspb.2015.0347","2014-12-02","2015-04-01","['aaron a. king', 'matthieu domenech de cellès', 'felicia m. g. magpantay', 'pejman rohani']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1021",1412.1315,"degradation-based residual life prediction under different environments","stat.ap","degradation modeling has traditionally relied on historical signals to estimate the behavior of the underlying degradation process. many models assume that these historical signals are acquired under the same environmental conditions and can be observed along the entire lifespan of a component. in this paper, we relax these assumptions and present a more general statistical framework for modeling degradation signals that may have been collected under different types of environmental conditions. in addition, we consider applications where the historical signals are not necessarily observed continuously, that is, historical signals are sparse or fragmented. we consider the case where historical degradation signals are collected under known environmental states and another case where the environmental conditions are unknown during the acquisition of these historical data. for the first case, we use a classification algorithm to identify the environmental state of the units operating in the field. in the second case, a clustering step is required for clustering the historical degradation signals. the proposed model can provide accurate predictions of the lifetime or residual life distributions of engineering components that are still operated in the field. this is demonstrated by using simulated degradation signals as well as vibration-based degradation signals acquired from a rotating machinery setup.","10.1214/14-aoas749","2014-12-03","","['rensheng zhou', 'nicoleta serban', 'nagi gebraeel']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1022",1412.1576,"lightlda: big topic models on modest compute clusters","stat.ml cs.dc cs.ir cs.lg","when building large-scale machine learning (ml) programs, such as big topic models or deep neural nets, one usually assumes such tasks can only be attempted with industrial-sized clusters with thousands of nodes, which are out of reach for most practitioners or academic researchers. we consider this challenge in the context of topic modeling on web-scale corpora, and show that with a modest cluster of as few as 8 machines, we can train a topic model with 1 million topics and a 1-million-word vocabulary (for a total of 1 trillion parameters), on a document collection with 200 billion tokens -- a scale not yet reported even with thousands of machines. our major contributions include: 1) a new, highly efficient o(1) metropolis-hastings sampling algorithm, whose running cost is (surprisingly) agnostic of model size, and empirically converges nearly an order of magnitude faster than current state-of-the-art gibbs samplers; 2) a structure-aware model-parallel scheme, which leverages dependencies within the topic model, yielding a sampling strategy that is frugal on machine memory and network communication; 3) a differential data-structure for model storage, which uses separate data structures for high- and low-frequency words to allow extremely large models to fit in memory, while maintaining high inference speed; and 4) a bounded asynchronous data-parallel scheme, which allows efficient distributed processing of massive data via a parameter server. our distribution strategy is an instance of the model-and-data-parallel programming model underlying the petuum framework for general distributed ml, and was implemented on top of the petuum open-source system. we provide experimental evidence showing how this development puts massive models within reach on a small cluster while still enjoying proportional time cost reductions with increasing cluster size, in comparison with alternative options.","","2014-12-04","","['jinhui yuan', 'fei gao', 'qirong ho', 'wei dai', 'jinliang wei', 'xun zheng', 'eric p. xing', 'tie-yan liu', 'wei-ying ma']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1023",1412.1914,"a parametric variogram model bridging between stationary and   intrinsically stationary processes","stat.me","a simple variogram model with two parameters is presented that includes the power variogram for the fractional brownian motion, a modified de wijsian model, the generalized cauchy model and the multiquadrics model. one parameter controls the smoothness of the process. the other parameter allows for a smooth parametrization between stationary and intrinsically stationary second order processes in a gaussian framework, or between mixing and non-ergodic max-stable processes when modeling spatial extremes by a brown-resnick process.","","2014-12-05","","['martin schlather']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1024",1412.2044,"testing hypotheses via a mixture estimation model","stat.me","we consider a novel paradigm for bayesian testing of hypotheses and bayesian model comparison. our alternative to the traditional construction of posterior probabilities that a given hypothesis is true or that the data originates from a specific model is to consider the models under comparison as components of a mixture model. we therefore replace the original testing problem with an estimation one that focus on the probability weight of a given model within a mixture model. we analyze the sensitivity on the resulting posterior distribution on the weights of various prior modeling on the weights. we stress that a major appeal in using this novel perspective is that generic improper priors are acceptable, while not putting convergence in jeopardy. among other features, this allows for a resolution of the lindley-jeffreys paradox. when using a reference beta b(a,a) prior on the mixture weights, we note that the sensitivity of the posterior estimations of the weights to the choice of a vanishes with the sample size increasing and avocate the default choice a=0.5, derived from rousseau and mengersen (2011). another feature of this easily implemented alternative to the classical bayesian solution is that the speeds of convergence of the posterior mean of the weight and of the corresponding posterior probability are quite similar.","","2014-12-05","2018-12-31","['kaniav kamary', 'kerrie mengersen', 'christian p. robert', 'judith rousseau']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1025",1412.2183,"reduced-rank covariance estimation in vector autoregressive modeling","stat.ap stat.co stat.me","we consider reduced-rank modeling of the white noise covariance matrix in a large dimensional vector autoregressive (var) model. we first propose the reduced-rank covariance estimator under the setting where independent observations are available. we derive the reduced-rank estimator based on a latent variable model for the vector observation and give the analytical form of its maximum likelihood estimate. simulation results show that the reduced-rank covariance estimator outperforms two competing covariance estimators for estimating large dimensional covariance matrices from independent observations. then we describe how to integrate the proposed reduced-rank estimator into the fitting of large dimensional var models, where we consider two scenarios that require different model fitting procedures. in the var modeling context, our reduced-rank covariance estimator not only provides interpretable descriptions of the dependence structure of var processes but also leads to improvement in model-fitting and forecasting over unrestricted covariance estimators. two real data examples are presented to illustrate these fitting procedures.","","2014-12-05","","['richard a. davis', 'pengfei zang', 'tian zheng']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"1026",1412.2282,"dirichlet process mixture models for modeling and generating synthetic   versions of nested categorical data","stat.me","we present a bayesian model for estimating the joint distribution of multivariate categorical data when units are nested within groups. such data arise frequently in social science settings, for example, people living in households. the model assumes that (i) each group is a member of a group-level latent class, and (ii) each unit is a member of a unit-level latent class nested within its group-level latent class. this structure allows the model to capture dependence among units in the same group. it also facilitates simultaneous modeling of variables at both group and unit levels. we develop a version of the model that assigns zero probability to groups and units with physically impossible combinations of variables. we apply the model to estimate multivariate relationships in a subset of the american community survey. using the estimated model, we generate synthetic household data that could be disseminated as redacted public use files with high analytic validity and low disclosure risks. supplementary materials for this article are available online.","","2014-12-06","2016-10-27","['jingchen hu', 'jerome p. reiter', 'quanli wang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1027",1412.2404,"dimensionality reduction with subspace structure preservation","cs.lg stat.ml","modeling data as being sampled from a union of independent subspaces has been widely applied to a number of real world applications. however, dimensionality reduction approaches that theoretically preserve this independence assumption have not been well studied. our key contribution is to show that $2k$ projection vectors are sufficient for the independence preservation of any $k$ class data sampled from a union of independent subspaces. it is this non-trivial observation that we use for designing our dimensionality reduction technique. in this paper, we propose a novel dimensionality reduction algorithm that theoretically preserves this structure for a given dataset. we support our theoretical analysis with empirical results on both synthetic and real world data achieving \textit{state-of-the-art} results compared to popular dimensionality reduction techniques.","","2014-12-07","2016-04-06","['devansh arpit', 'ifeoma nwogu', 'venu govindaraju']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1028",1412.3565,"broom: an r package for converting statistical analysis objects into   tidy data frames","stat.co stat.me","the concept of ""tidy data"" offers a powerful framework for structuring data to ease manipulation, modeling and visualization. however, most r functions, both those built-in and those found in third-party packages, produce output that is not tidy, and that is therefore difficult to reshape, recombine, and otherwise manipulate. here i introduce the broom package, which turns the output of model objects into tidy data frames that are suited to further analysis, manipulation, and visualization with input-tidy tools. broom defines the ""tidy"", ""augment"" and ""glance"" generics, which arrange a model into three levels of tidy output respectively: the component level, the observation level, and the model level. i provide examples to demonstrate how these generics work with tidy tools to allow analysis and modeling of data that is divided into subsets, to recombine results from bootstrap replicates, and to perform simulations that investigate the effect of varying input parameters.","","2014-12-11","2014-12-19","['david robinson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"1029",1412.3705,"a topic modeling approach to ranking","cs.lg stat.ml","we propose a topic modeling approach to the prediction of preferences in pairwise comparisons. we develop a new generative model for pairwise comparisons that accounts for multiple shared latent rankings that are prevalent in a population of users. this new model also captures inconsistent user behavior in a natural way. we show how the estimation of latent rankings in the new generative model can be formally reduced to the estimation of topics in a statistically equivalent topic modeling problem. we leverage recent advances in the topic modeling literature to develop an algorithm that can learn shared latent rankings with provable consistency as well as sample and computational complexity guarantees. we demonstrate that the new approach is empirically competitive with the current state-of-the-art approaches in predicting preferences on some semi-synthetic and real world datasets.","","2014-12-11","2015-01-25","['weicong ding', 'prakash ishwar', 'venkatesh saligrama']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1030",1412.4052,"the bag-of-frames approach: a not so sufficient model for urban   soundscapes","cs.sd stat.ml","the ""bag-of-frames"" approach (bof), which encodes audio signals as the long-term statistical distribution of short-term spectral features, is commonly regarded as an effective and sufficient way to represent environmental sound recordings (soundscapes) since its introduction in an influential 2007 article. the present paper describes a concep-tual replication of this seminal article using several new soundscape datasets, with results strongly questioning the adequacy of the bof approach for the task. we show that the good accuracy originally re-ported with bof likely result from a particularly thankful dataset with low within-class variability, and that for more realistic datasets, bof in fact does not perform significantly better than a mere one-point av-erage of the signal's features. soundscape modeling, therefore, may not be the closed case it was once thought to be. progress, we ar-gue, could lie in reconsidering the problem of considering individual acoustical events within each soundscape.","","2014-12-11","2016-06-12","['mathieu lagrange', 'grégoire lafay', 'boris defreville', 'jean-julien aucouturier']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1031",1412.4079,"bayesian inference of cmb gravitational lensing","astro-ph.co astro-ph.im stat.ap","the planck satellite, along with several ground based telescopes, have mapped the cosmic microwave background (cmb) at sufficient resolution and signal-to-noise so as to allow a detection of the subtle distortions due to the gravitational influence of the intervening matter distribution. a natural modeling approach is to write a bayesian hierarchical model for the lensed cmb in terms of the unlensed cmb and the lensing potential. so far there has been no feasible algorithm for inferring the posterior distribution of the lensing potential from the lensed cmb map. we propose a solution that allows efficient markov chain monte carlo sampling from the joint posterior of the lensing potential and the unlensed cmb map using the hamiltonian monte carlo technique. the main conceptual step in the solution is a re-parameterization of cmb lensing in terms of the lensed cmb and the ""inverse lensing"" potential. we demonstrate a fast implementation on simulated data including noise and a sky cut, that uses a further acceleration based on a very mild approximation of the inverse lensing potential. we find that the resulting markov chain has short correlation lengths and excellent convergence properties, making it promising for application to high resolution cmb data sets of the future.","10.1088/0004-637x/808/2/152","2014-12-12","2015-04-23","['ethan anderes', 'benjamin wandelt', 'guilhem lavaux']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1032",1412.4237,"first order algorithms in variational image processing","math.oc cs.cv stat.ml","variational methods in imaging are nowadays developing towards a quite universal and flexible tool, allowing for highly successful approaches on tasks like denoising, deblurring, inpainting, segmentation, super-resolution, disparity, and optical flow estimation. the overall structure of such approaches is of the form ${\cal d}(ku) + \alpha {\cal r} (u) \rightarrow \min_u$ ; where the functional ${\cal d}$ is a data fidelity term also depending on some input data $f$ and measuring the deviation of $ku$ from such and ${\cal r}$ is a regularization functional. moreover $k$ is a (often linear) forward operator modeling the dependence of data on an underlying image, and $\alpha$ is a positive regularization parameter. while ${\cal d}$ is often smooth and (strictly) convex, the current practice almost exclusively uses nonsmooth regularization functionals. the majority of successful techniques is using nonsmooth and convex functionals like the total variation and generalizations thereof or $\ell_1$-norms of coefficients arising from scalar products with some frame system. the efficient solution of such variational problems in imaging demands for appropriate algorithms. taking into account the specific structure as a sum of two very different terms to be minimized, splitting algorithms are a quite canonical choice. consequently this field has revived the interest in techniques like operator splittings or augmented lagrangians. here we shall provide an overview of methods currently developed and recent results as well as some computational studies providing a comparison of different methods and also illustrating their success in applications.","","2014-12-13","","['martin burger', 'alex sawatzky', 'gabriele steidl']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1033",1412.5218,"testing mcmc code","cs.se cs.lg stat.ml","markov chain monte carlo (mcmc) algorithms are a workhorse of probabilistic modeling and inference, but are difficult to debug, and are prone to silent failure if implemented naively. we outline several strategies for testing the correctness of mcmc algorithms. specifically, we advocate writing code in a modular way, where conditional probability calculations are kept separate from the logic of the sampler. we discuss strategies for both unit testing and integration testing. as a running example, we show how a python implementation of gibbs sampling for a mixture of gaussians model can be tested.","","2014-12-16","","['roger b. grosse', 'david k. duvenaud']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1034",1412.5247,"modeling and predicting power consumption of high performance computing   jobs","stat.ap","power is becoming an increasingly important concern for large supercomputing centers. due to cost concerns, data centers are becoming increasingly limited in their ability to enhance their power infrastructure to support increased compute power on the machine-room floor. at los alamos national laboratory it is projected that future-generation supercomputers will be power-limited rather than budget-limited. that is, it will be less costly to acquire a large number of nodes than it will be to upgrade an existing data-center and machine-room power infrastructure to run that large number of nodes at full power. in the power-limited systems of the future, machines will in principle be capable of drawing more power than they have available. thus, power capping at the node/job level must be used to ensure the total system power draw remains below the available level. in this paper, we present a statistically grounded framework with which to predict (with uncertainty) how much power a given job will need and use these predictions to provide an optimal node-level power capping strategy. we model the power drawn by a given job (and subsequently by the entire machine) using hierarchical bayesian modeling with hidden markov and dirichlet process models. we then demonstrate how this model can be used inside of a power-management scheme to minimize the affect of power capping on user jobs.","","2014-12-16","2015-05-11","['curtis storlie', 'joe sexton', 'scott pakin', 'michael lang', 'brian reich', 'william rust']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1035",1412.525,"high dimensional forecasting via interpretable vector autoregression","stat.me stat.co stat.ml","vector autoregression (var) is a fundamental tool for modeling multivariate time series. however, as the number of component series is increased, the var model becomes overparameterized. several authors have addressed this issue by incorporating regularized approaches, such as the lasso in var estimation. traditional approaches address overparameterization by selecting a low lag order, based on the assumption of short range dependence, assuming that a universal lag order applies to all components. such an approach constrains the relationship between the components and impedes forecast performance. the lasso-based approaches work much better in high-dimensional situations but do not incorporate the notion of lag order selection.   we propose a new class of hierarchical lag structures (hlag) that embed the notion of lag selection into a convex regularizer. the key modeling tool is a group lasso with nested groups which guarantees that the sparsity pattern of lag coefficients honors the var's ordered structure. the hlag framework offers three structures, which allow for varying levels of flexibility. a simulation study demonstrates improved performance in forecasting and lag order selection over previous approaches, and a macroeconomic application further highlights forecasting improvements as well as hlag's convenient, interpretable output.","","2014-12-16","2018-09-08","['william b. nicholson', 'ines wilms', 'jacob bien', 'david s. matteson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE
"1036",1412.5848,"analyzing volleyball data on a compositional regression model approach:   an application to the brazilian men's volleyball super league 2011/2012 data","stat.ap stat.me","volleyball has become a competitive sport with high physical and technical performance. matches results are based on the players and teams'skills as technical and tactical strategies to succeed in a championship. at this point, some studies are carried out on the performance analysis of different match elements, contributing to the development of this sport. in this paper, we proposed a new approach to analyze volleyball data. the study is based on the compositional data methodology modeling in regression model. the parameters are obtained through the maximum likelihood. we performed a simulation study to evaluate the estimation procedure in compositional regression model and we illustrated the proposed methodology considering real data set of volleyball.","","2014-12-18","","['taciana k. o. shimizu', 'francisco louzada', 'adriano k. suzuki']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1037",1412.689,"software for distributed computation on medical databases: a   demonstration project","stat.co cs.ms cs.se","bringing together the information latent in distributed medical databases promises to personalize medical care by enabling reliable, stable modeling of outcomes with rich feature sets (including patient characteristics and treatments received). however, there are barriers to aggregation of medical data, due to lack of standardization of ontologies, privacy concerns, proprietary attitudes toward data, and a reluctance to give up control over end use. aggregation of data is not always necessary for model fitting. in models based on maximizing a likelihood, the computations can be distributed, with aggregation limited to the intermediate results of calculations on local data, rather than raw data. distributed fitting is also possible for singular value decomposition. there has been work on the technical aspects of shared computation for particular applications, but little has been published on the software needed to support the ""social networking"" aspect of shared computing, to reduce the barriers to collaboration. we describe a set of software tools that allow the rapid assembly of a collaborative computational project, based on the flexible and extensible r statistical software and other open source packages, that can work across a heterogeneous collection of database environments, with full transparency to allow local officials concerned with privacy protections to validate the safety of the method. we describe the principles, architecture, and successful test results for the site-stratified cox model and rank-k singular value decomposition (svd).","","2014-12-22","2017-02-09","['balasubramanian narasimhan', 'daniel l. rubin', 'samuel m. gross', 'marina bendersky', 'philip w. lavori']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1038",1412.7056,"clustering multi-way data: a novel algebraic approach","cs.lg cs.cv cs.it math.it stat.ml","in this paper, we develop a method for unsupervised clustering of two-way (matrix) data by combining two recent innovations from different fields: the sparse subspace clustering (ssc) algorithm [10], which groups points coming from a union of subspaces into their respective subspaces, and the t-product [18], which was introduced to provide a matrix-like multiplication for third order tensors. our algorithm is analogous to ssc in that an ""affinity"" between different data points is built using a sparse self-representation of the data. unlike ssc, we employ the t-product in the self-representation. this allows us more flexibility in modeling; infact, ssc is a special case of our method. when using the t-product, three-way arrays are treated as matrices whose elements (scalars) are n-tuples or tubes. convolutions take the place of scalar multiplication. this framework allows us to embed the 2-d data into a vector-space-like structure called a free module over a commutative ring. these free modules retain many properties of complex inner-product spaces, and we leverage that to provide theoretical guarantees on our algorithm. we show that compared to vector-space counterparts, ssmc achieves higher accuracy and better able to cluster data with less preprocessing in some image clustering problems. in particular we show the performance of the proposed method on weizmann face database, the extended yale b face database and the mnist handwritten digits database.","","2014-12-22","2015-02-21","['eric kernfeld', 'shuchin aeron', 'misha kilmer']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1039",1412.7468,"model selection in high-dimensional misspecified models","math.st stat.me stat.ml stat.th","model selection is indispensable to high-dimensional sparse modeling in selecting the best set of covariates among a sequence of candidate models. most existing work assumes implicitly that the model is correctly specified or of fixed dimensions. yet model misspecification and high dimensionality are common in real applications. in this paper, we investigate two classical kullback-leibler divergence and bayesian principles of model selection in the setting of high-dimensional misspecified models. asymptotic expansions of these principles reveal that the effect of model misspecification is crucial and should be taken into account, leading to the generalized aic and generalized bic in high dimensions. with a natural choice of prior probabilities, we suggest the generalized bic with prior probability which involves a logarithmic factor of the dimensionality in penalizing model complexity. we further establish the consistency of the covariance contrast matrix estimator in a general setting. our results and new method are supported by numerical studies.","","2014-12-23","","['pallavi basu', 'yang feng', 'jinchi lv']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE
"1040",1412.7778,"false discovery variance reduction in large scale simultaneous   hypothesis tests","stat.me","statistical dependence between hypotheses poses a significant challenge to the stability of large scale multiple hypotheses testing. ignoring it often results in an unacceptably large spread in the false positive proportion even though the average value is acceptable [21, 39, 40, 49]. however, the statistical dependence structure of data is often unknown. using a generic signalprocessing model, bayesian multiple testing, and simulations, we demonstrate that the variance of the false positive proportion can be substantially reduced even under unknown short range dependence. we do this by modeling the data generating process as a stationary ergodic binary signal process embedded in noisy observations. we derive conditional probabilities needed for the bayesian multiple testing by incorporating nearby observations into a second order taylor series approximation. simulations under general conditions are carried out to assess the validity and the variance reduction of the approach. along the way, we address the problem of sampling a random markov matrix with specified stationary distribution and lower bounds on the top absolute eigenvalues, which is of interest in its own right.","","2014-12-24","2018-10-11","['sairam rayaprolu', 'zhiyi chi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1041",1412.8594,"a new extended mixture model of residual lifetime distributions","stat.me","in this paper, we first propose a new extended mixture model of residual lifetime distributions. we show that this model is suitable in modeling residual lifetime in some practical situations. several closure properties of some well-known dependence concepts, stochastic orders and aging notions under the formation of this model, are obtained. finally, preservation properties of some stochastic orders under the formation of the model are discussed and some examples of interest are presented.","","2014-12-30","","['m. kayid', 's. izadkhah']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1042",1501.00199,"accams: additive co-clustering to approximate matrices succinctly","cs.lg stat.ml","matrix completion and approximation are popular tools to capture a user's preferences for recommendation and to approximate missing data. instead of using low-rank factorization we take a drastically different approach, based on the simple insight that an additive model of co-clusterings allows one to approximate matrices efficiently. this allows us to build a concise model that, per bit of model learned, significantly beats all factorization approaches to matrix approximation. even more surprisingly, we find that summing over small co-clusterings is more effective in modeling matrices than classic co-clustering, which uses just one large partitioning of the matrix.   following occam's razor principle suggests that the simple structure induced by our model better captures the latent preferences and decision making processes present in the real world than classic co-clustering or matrix factorization. we provide an iterative minimization algorithm, a collapsed gibbs sampler, theoretical guarantees for matrix approximation, and excellent empirical evidence for the efficacy of our approach. we achieve state-of-the-art results on the netflix problem with a fraction of the model complexity.","","2014-12-31","","['alex beutel', 'amr ahmed', 'alexander j. smola']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1043",1501.0184,"gibbs posterior inference on the minimum clinically important difference","stat.me math.st stat.th","iit is known that a statistically significant treatment may not be clinically significant. a quantity that can be used to assess clinical significance is called the minimum clinically important difference (mcid), and inference on the mcid is an important and challenging problem. modeling for the purpose of inference on the mcid is non-trivial, and concerns about bias from a misspecified parametric model or inefficiency from a nonparametric model motivate an alternative approach to balance robustness and efficiency. in particular, a recently proposed representation of the mcid as the minimizer of a suitable risk function makes it possible to construct a gibbs posterior distribution for the mcid without specifying a model. we establish the posterior convergence rate and show, numerically, that an appropriately scaled version of this gibbs posterior yields interval estimates for the mcid which are both valid and efficient even for relatively small sample sizes.","10.1016/j.jspi.2017.03.001","2015-01-08","2017-06-26","['nick syring', 'ryan martin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1044",1501.02579,"combined modeling of sparse and dense noise for improvement of relevance   vector machine","stat.ml","using a bayesian approach, we consider the problem of recovering sparse signals under additive sparse and dense noise. typically, sparse noise models outliers, impulse bursts or data loss. to handle sparse noise, existing methods simultaneously estimate the sparse signal of interest and the sparse noise of no interest. for estimating the sparse signal, without the need of estimating the sparse noise, we construct a robust relevance vector machine (rvm). in the rvm, sparse noise and ever present dense noise are treated through a combined noise model. the precision of combined noise is modeled by a diagonal matrix. we show that the new rvm update equations correspond to a non-symmetric sparsity inducing cost function. further, the combined modeling is found to be computationally more efficient. we also extend the method to block-sparse signals and noise with known and unknown block structures. through simulations, we show the performance and computation efficiency of the new rvm in several applications: recovery of sparse and block sparse signals, housing price prediction and image denoising.","","2015-01-12","","['martin sundin', 'saikat chatterjee', 'magnus jansson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1045",1501.02844,"sprite: a response model for multiple choice testing","stat.ml","item response theory (irt) models for categorical response data are widely used in the analysis of educational data, computerized adaptive testing, and psychological surveys. however, most irt models rely on both the assumption that categories are strictly ordered and the assumption that this ordering is known a priori. these assumptions are impractical in many real-world scenarios, such as multiple-choice exams where the levels of incorrectness for the distractor categories are often unknown. while a number of results exist on irt models for unordered categorical data, they tend to have restrictive modeling assumptions that lead to poor data fitting performance in practice. furthermore, existing unordered categorical models have parameters that are difficult to interpret. in this work, we propose a novel methodology for unordered categorical irt that we call sprite (short for stochastic polytomous response item model) that: (i) analyzes both ordered and unordered categories, (ii) offers interpretable outputs, and (iii) provides improved data fitting compared to existing models. we compare sprite to existing item response models and demonstrate its efficacy on both synthetic and real-world educational datasets.","","2015-01-12","","['ryan ning', 'andrew e. waters', 'christoph studer', 'richard g. baraniuk']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1046",1501.03291,"bayesian optimization for likelihood-free inference of simulator-based   statistical models","stat.ml stat.co stat.me","our paper deals with inferring simulator-based statistical models given some observed data. a simulator-based model is a parametrized mechanism which specifies how data are generated. it is thus also referred to as generative model. we assume that only a finite number of parameters are of interest and allow the generative process to be very general; it may be a noisy nonlinear dynamical system with an unrestricted number of hidden variables. this weak assumption is useful for devising realistic models but it renders statistical inference very difficult. the main challenge is the intractability of the likelihood function. several likelihood-free inference methods have been proposed which share the basic idea of identifying the parameters by finding values for which the discrepancy between simulated and observed data is small. a major obstacle to using these methods is their computational cost. the cost is largely due to the need to repeatedly simulate data sets and the lack of knowledge about how the parameters affect the discrepancy. we propose a strategy which combines probabilistic modeling of the discrepancy with optimization to facilitate likelihood-free inference. the strategy is implemented using bayesian optimization and is shown to accelerate the inference through a reduction in the number of required simulations by several orders of magnitude.","","2015-01-14","2015-12-31","['michael u. gutmann', 'jukka corander']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE
"1047",1501.03626,"spatial accessibility of pediatric primary healthcare: measurement and   inference","stat.ap","although improving financial access is in the spotlight of the current u.s. health policy agenda, this alone does not address universal and comprehensive healthcare. affordability is one barrier to healthcare, but others such as availability and accessibility, together defined as spatial accessibility, are equally important. in this paper, we develop a measurement and modeling framework that can be used to infer the impact of policy changes on disparities in spatial accessibility within and across different population groups. the underlying model for measuring spatial accessibility is optimization-based and accounts for constraints in the healthcare delivery system. the measurement method is complemented by statistical modeling and inference on the impact of various potential contributing factors to disparities in spatial accessibility. the emphasis of this study is on children's accessibility to primary care pediatricians, piloted for the state of georgia. we focus on disparities in accessibility between and within two populations: children insured by medicaid and other children. we find that disparities in spatial accessibility to pediatric primary care in georgia are significant, and resistant to many policy interventions, suggesting the need for major changes to the structure of georgia's pediatric healthcare provider network.","10.1214/14-aoas728","2015-01-15","","['mallory nobles', 'nicoleta serban', 'julie swann']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1048",1501.03861,"bayesian nonparametrics in topic modeling: a brief tutorial","stat.ml","using nonparametric methods has been increasingly explored in bayesian hierarchical modeling as a way to increase model flexibility. although the field shows a lot of promise, inference in many models, including hierachical dirichlet processes (hdp), remain prohibitively slow. one promising path forward is to exploit the submodularity inherent in indian buffet process (ibp) to derive near-optimal solutions in polynomial time. in this work, i will present a brief tutorial on bayesian nonparametric methods, especially as they are applied to topic modeling. i will show a comparison between different non-parametric models and the current state-of-the-art parametric model, latent dirichlet allocation (lda).","","2015-01-15","","['alexander spangher']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1049",1501.04053,"stochastic local interaction (sli) model: interfacing machine learning   and geostatistics","cs.lg stat.ml","machine learning and geostatistics are powerful mathematical frameworks for modeling spatial data. both approaches, however, suffer from poor scaling of the required computational resources for large data applications. we present the stochastic local interaction (sli) model, which employs a local representation to improve computational efficiency. sli combines geostatistics and machine learning with ideas from statistical physics and computational geometry. it is based on a joint probability density function defined by an energy functional which involves local interactions implemented by means of kernel functions with adaptive local kernel bandwidths. sli is expressed in terms of an explicit, typically sparse, precision (inverse covariance) matrix. this representation leads to a semi-analytical expression for interpolation (prediction), which is valid in any number of dimensions and avoids the computationally costly covariance matrix inversion.","10.1016/j.cageo.2015.05.018","2015-01-16","2015-05-05","['dionissios t. hristopulos']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1050",1501.04325,"deep belief nets for topic modeling","cs.cl cs.lg stat.ml","applying traditional collaborative filtering to digital publishing is challenging because user data is very sparse due to the high volume of documents relative to the number of users. content based approaches, on the other hand, is attractive because textual content is often very informative. in this paper we describe large-scale content based collaborative filtering for digital publishing. to solve the digital publishing recommender problem we compare two approaches: latent dirichlet allocation (lda) and deep belief nets (dbn) that both find low-dimensional latent representations for documents. efficient retrieval can be carried out in the latent representation. we work both on public benchmarks and digital media content provided by issuu, an online publishing platform. this article also comes with a newly developed deep belief nets toolbox for topic modeling tailored towards performance evaluation of the dbn model and comparisons to the lda model.","","2015-01-18","","['lars maaloe', 'morten arngren', 'ole winther']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1051",1501.0442,"longitudinal high-dimensional principal components analysis with   application to diffusion tensor imaging of multiple sclerosis","stat.ap","we develop a flexible framework for modeling high-dimensional imaging data observed longitudinally. the approach decomposes the observed variability of repeatedly measured high-dimensional observations into three additive components: a subject-specific imaging random intercept that quantifies the cross-sectional variability, a subject-specific imaging slope that quantifies the dynamic irreversible deformation over multiple realizations, and a subject-visit-specific imaging deviation that quantifies exchangeable effects between visits. the proposed method is very fast, scalable to studies including ultrahigh-dimensional data, and can easily be adapted to and executed on modest computing infrastructures. the method is applied to the longitudinal analysis of diffusion tensor imaging (dti) data of the corpus callosum of multiple sclerosis (ms) subjects. the study includes $176$ subjects observed at $466$ visits. for each subject and visit the study contains a registered dti scan of the corpus callosum at roughly 30,000 voxels.","10.1214/14-aoas748","2015-01-19","","['vadim zipunnikov', 'sonja greven', 'haochang shou', 'brian s. caffo', 'daniel s. reich', 'ciprian m. crainiceanu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1052",1501.04787,"minimax adaptive estimation of nonparametric hidden markov models","math.st stat.th","we consider stationary hidden markov models with finite state space and nonparametric modeling of the emission distributions. it has remained unknown until very recently that such models are identifiable. in this paper, we propose a new penalized least-squares esti-mator for the emission distributions which is statistically optimal and practically tractable. we prove a non asymptotic oracle inequality for our nonparametric estimator of the emission distributions. a consequence is that this new estimator is rate minimax adaptive up to a logarithmic term. our methodology is based on projections of the emission distributions onto nested subspaces of increasing complexity. the popular spectral estimators are unable to achieve the optimal rate but may be used as initial points in our procedure. simulations are given that show the improvement obtained when applying the least-squares minimization consecutively to the spectral estimation.","","2015-01-20","2015-12-27","['yohann de castro', 'élisabeth gassiat', 'claire lacour']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1053",1501.0487,"scalable multi-output label prediction: from classifier chains to   classifier trellises","stat.ml cs.cv cs.ds cs.lg stat.co","multi-output inference tasks, such as multi-label classification, have become increasingly important in recent years. a popular method for multi-label classification is classifier chains, in which the predictions of individual classifiers are cascaded along a chain, thus taking into account inter-label dependencies and improving the overall performance. several varieties of classifier chain methods have been introduced, and many of them perform very competitively across a wide range of benchmark datasets. however, scalability limitations become apparent on larger datasets when modeling a fully-cascaded chain. in particular, the methods' strategies for discovering and modeling a good chain structure constitutes a mayor computational bottleneck. in this paper, we present the classifier trellis (ct) method for scalable multi-label classification. we compare ct with several recently proposed classifier chain methods to show that it occupies an important niche: it is highly competitive on standard multi-label problems, yet it can also scale up to thousands or even tens of thousands of labels.","10.1016/j.patcog.2015.01.004","2015-01-20","","['j. read', 'l. martino', 'p. olmos', 'd. luengo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE
"1054",1501.05242,"open turns: an industrial software for uncertainty quantification in   simulation","stat.co math.st stat.th","the needs to assess robust performances for complex systems and to answer tighter regulatory processes (security, safety, environmental control, and health impacts, etc.) have led to the emergence of a new industrial simulation challenge: to take uncertainties into account when dealing with complex numerical simulation frameworks. therefore, a generic methodology has emerged from the joint effort of several industrial companies and academic institutions. edf r&d, airbus group and phimeca engineering started a collaboration at the beginning of 2005, joined by imacs in 2014, for the development of an open source software platform dedicated to uncertainty propagation by probabilistic methods, named openturns for open source treatment of uncertainty, risk 'n statistics. openturns addresses the specific industrial challenges attached to uncertainties, which are transparency, genericity, modularity and multi-accessibility. this paper focuses on openturns and presents its main features: openturns is an open source software under the lgpl license, that presents itself as a c++ library and a python tui, and which works under linux and windows environment. all the methodological tools are described in the different sections of this paper: uncertainty quantification, uncertainty propagation, sensitivity analysis and metamodeling. a section also explains the generic wrappers way to link openturns to any external code. the paper illustrates as much as possible the methodological tools on an educational example that simulates the height of a river and compares it to the height of a dyke that protects industrial facilities. at last, it gives an overview of the main developments planned for the next few years.","","2015-01-21","2015-06-05","['michaël baudin', 'anne dutfoy', 'bertrand iooss', 'anne-laure popelin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE
"1055",1501.05624,"a collaborative kalman filter for time-evolving dyadic processes","stat.ml cs.lg","we present the collaborative kalman filter (ckf), a dynamic model for collaborative filtering and related factorization models. using the matrix factorization approach to collaborative filtering, the ckf accounts for time evolution by modeling each low-dimensional latent embedding as a multidimensional brownian motion. each observation is a random variable whose distribution is parameterized by the dot product of the relevant brownian motions at that moment in time. this is naturally interpreted as a kalman filter with multiple interacting state space vectors. we also present a method for learning a dynamically evolving drift parameter for each location by modeling it as a geometric brownian motion. we handle posterior intractability via a mean-field variational approximation, which also preserves tractability for downstream calculations in a manner similar to the kalman filter. we evaluate the model on several large datasets, providing quantitative evaluation on the 10 million movielens and 100 million netflix datasets and qualitative evaluation on a set of 39 million stock returns divided across roughly 6,500 companies from the years 1962-2014.","","2015-01-22","","['san gultekin', 'john paisley']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1056",1501.06444,"stochastic block models for multiplex networks: an application to   networks of researchers","stat.me","modeling relations between individuals is a classical question in social sciences and clustering individuals according to the observed patterns of interactions allows to uncover a latent structure in the data. stochastic block model (sbm) is a popular approach for grouping the individuals with respect to their social comportment. when several relationships of various types can occur jointly between the individuals, the data are represented by multiplex networks where more than one edge can exist between the nodes. in this paper, we extend the sbm to multiplex networks in order to obtain a clustering based on more than one kind of relationship. we propose to estimate the parameters --such as the marginal probabilities of assignment to groups (blocks) and the matrix of probabilities of connections between groups-- through a variational expectation-maximization procedure. consistency of the estimates as well as statistical properties of the model are obtained. the number of groups is chosen thanks to the integrated completed likelihood criteria, a penalized likelihood criterion. multiplex stochastic block model arises in many situations but our applied example is motivated by a network of french cancer researchers. the two possible links (edges) between researchers are a direct connection or a connection through their labs. our results show strong interactions between these two kinds of connections and the groups that are obtained are discussed to emphasize the common features of researchers grouped together.","","2015-01-26","","['pierre barbillon', 'sophie donnet', 'emmanuel lazega', 'avner bar-hen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1057",1501.06502,"granger causality for state space models","math.st stat.th","granger causality, a popular method for determining causal influence between stochastic processes, is most commonly estimated via linear autoregressive modeling. however, this approach has a serious drawback: if the process being modeled has a moving average component, then the autoregressive model order is theoretically infinite, and in finite sample large empirical model orders may be necessary, resulting in weak granger-causal inference. this is particularly relevant when the process has been filtered, downsampled, or observed with (additive) noise - all of which induce a moving average component and are commonplace in application domains as diverse as econometrics and the neurosciences. by contrast, the class of autoregressive moving average models - or, equivalently, linear state space models - is closed under digital filtering, downsampling (and other forms of aggregation) as well as additive observational noise. here, we show how granger causality, conditional and unconditional, in both time and frequency domains, may be calculated simply and directly from state space model parameters, via solution of a discrete algebraic riccati equation. numerical simulations demonstrate that granger causality estimators thus derived have greater statistical power and smaller bias than pure autoregressive estimators. we conclude that the state space approach should be the default for (linear) granger causality estimation.","10.1103/physreve.91.040101","2015-01-26","2015-02-05","['lionel barnett', 'anil k. seth']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1058",1501.06629,"bayesian approach to handling informative sampling","stat.ap","in the case of informative sampling the sampling scheme explicitly or implicitly depends on the response variable. as a result, the sample distribution of response variable can- not be used for making inference about the population. in this research i investigate the problem of informative sampling from the bayesian perspective. application of the bayesian approach permits solving the problems, which arise due to complexity of the models, being used for handling informative sampling. the main objective of the re- search is to combine the elements of the classical sampling theory and bayesian analysis, for identifying and estimating the population model, and the model describing the sam- pling mechanism. utilizing the fact that inclusion probabilities are generally known, the population sum of squares of the models residuals can be estimated, implementing the techniques of the sampling theory. in this research i show, how these estimates can be incorporated in the bayesian modeling and how the full bayesian significance test (fbst), which is based on the bayesian measure of evidence for precise null hypothesis, can be utilized as a model identification tool. the results obtained by implementation of the proposed approach to estimation and identification of the sample selection model seem promising. at this point i am working on methods of estimation and identification of the population model. an interesting extension of my approach is incorporation of known population characteristics into the estimation process. some other directions for continuation of my research are highlighted in the sections which describe the proposed methodology.","","2015-01-26","","['anna sikov']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1059",1501.0732,"tensor factorization via matrix factorization","cs.lg stat.ml","tensor factorization arises in many machine learning applications, such knowledge base modeling and parameter estimation in latent variable models. however, numerical methods for tensor factorization have not reached the level of maturity of matrix factorization methods. in this paper, we propose a new method for cp tensor factorization that uses random projections to reduce the problem to simultaneous matrix diagonalization. our method is conceptually simple and also applies to non-orthogonal and asymmetric tensors of arbitrary order. we prove that a small number random projections essentially preserves the spectral information in the tensor, allowing us to remove the dependence on the eigengap that plagued earlier tensor-to-matrix reductions. experimentally, our method outperforms existing tensor factorization methods on both simulated data and two real datasets.","","2015-01-28","2015-05-18","['volodymyr kuleshov', 'arun tejasvi chaganty', 'percy liang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1060",1501.07551,"bartlett corrections in beta regression models","stat.me","we consider the issue of performing accurate small-sample testing inference in beta regression models, which are useful for modeling continuous variates that assume values in $(0,1)$, such as rates and proportions. we derive the bartlett correction to the likelihood ratio test statistic and also consider a bootstrap bartlett correction. using monte carlo simulations we compare the finite sample performances of the two corrected tests to that of the standard likelihood ratio test and also to its variant that employs skovgaard's adjustment; the latter is already available in the literature. the numerical evidence favors the corrected tests we propose. we also present an empirical application.","10.1016/j.jspi.2012.08.018","2015-01-29","","['fábio m. bayer', 'francisco cribari-neto']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1061",1502.00526,"hierarchical models for semi-competing risks data with application to   quality of end-of-life care for pancreatic cancer","stat.ap","readmission following discharge from an initial hospitalization is a key marker of quality of health care in the united states. for the most part, readmission has been used to study quality of care for patients with acute health conditions, such as pneumonia and heart failure, with analyses typically based on a logistic-normal generalized linear mixed model. applying this model to the study readmission among patients with increasingly prevalent advanced health conditions such as pancreatic cancer is problematic, however, because it ignores death as a competing risk. a more appropriate analysis is to imbed such studies within the semi-competing risks framework. to our knowledge, however, no comprehensive statistical methods have been developed for cluster-correlated semi-competing risks data. in this paper we propose a novel hierarchical modeling framework for the analysis of cluster-correlated semi-competing risks data. the framework permits parametric or non-parametric specifications for a range of model components, including baseline hazard functions and distributions for key random effects, giving analysts substantial flexibility as they consider their own analyses. estimation and inference is performed within the bayesian paradigm since it facilitates the straightforward characterization of (posterior) uncertainty for all model parameters including hospital-specific random effects. the proposed framework is used to study the risk of readmission among 5,298 medicare beneficiaries diagnosed with pancreatic cancer at 112 hospitals in the six new england states between 2000-2009, specifically to investigate the role of patient-level risk factors and to characterize variation in risk across hospitals that is not explained by differences in patient case-mix.","10.1080/01621459.2016.1164052","2015-02-02","2015-08-05","['kyu ha lee', 'francesca dominici', 'deborah schrag', 'sebastien haneuse']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1062",1502.00727,"laplacian mixture modeling for network analysis and unsupervised   learning on graphs","stat.ml","laplacian mixture models identify overlapping regions of influence in unlabeled graph and network data in a scalable and computationally efficient way, yielding useful low-dimensional representations. by combining laplacian eigenspace and finite mixture modeling methods, they provide probabilistic or fuzzy dimensionality reductions or domain decompositions for a variety of input data types, including mixture distributions, feature vectors, and graphs or networks. provable optimal recovery using the algorithm is analytically shown for a nontrivial class of cluster graphs. heuristic approximations for scalable high-performance implementations are described and empirically tested. connections to pagerank and community detection in network analysis demonstrate the wide applicability of this approach. the origins of fuzzy spectral methods, beginning with generalized heat or diffusion equations in physics, are reviewed and summarized. comparisons to other dimensionality reduction and clustering methods for challenging unsupervised machine learning problems are also discussed.","10.1371/journal.pone.0204096","2015-02-02","2018-10-01","['daniel korenblum']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1063",1502.00916,"learning planar ising models","stat.ml","inference and learning of graphical models are both well-studied problems in statistics and machine learning that have found many applications in science and engineering. however, exact inference is intractable in general graphical models, which suggests the problem of seeking the best approximation to a collection of random variables within some tractable family of graphical models. in this paper, we focus on the class of planar ising models, for which exact inference is tractable using techniques of statistical physics. based on these techniques and recent methods for planarity testing and planar embedding, we propose a simple greedy algorithm for learning the best planar ising model to approximate an arbitrary collection of binary random variables (possibly from sample data). given the set of all pairwise correlations among variables, we select a planar graph and optimal planar ising model defined on this graph to best approximate that set of correlations. we demonstrate our method in simulations and for the application of modeling senate voting records.","","2015-02-03","","['jason k. johnson', 'diane oyen', 'michael chertkov', 'praneeth netrapalli']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1064",1502.01252,"signal partitioning algorithm for highly efficient gaussian mixture   modeling in mass spectrometry","stat.co","mixture - modeling of mass spectra is an approach with many potential applications including peak detection and quantification, smoothing, de-noising, feature extraction and spectral signal compression. however, existing algorithms do not allow for automatic analyses of whole spectra. therefore, despite highlighting potential advantages of mixture modeling of mass spectra of peptide/protein mixtures and some preliminary results presented in several papers, the mixture modeling approach was so far not developed to the stage enabling systematic comparisons with existing software packages for proteomic mass spectra analyses. in this paper we present an efficient algorithm for gaussian mixture modeling of proteomic mass spectra of different types (e.g., maldi-tof profiling, maldi-ims). the main idea is automatic partitioning of protein mass spectral signal into fragments. the obtained fragments are separately decomposed into gaussian mixture models. the parameters of the mixture models of fragments are then aggregated to form the mixture model of the whole spectrum. we compare the elaborated algorithm to existing algorithms for peak detection and we demonstrate improvements of peak detection efficiency obtained by using gaussian mixture modeling. we also show applications of the elaborated algorithm to real proteomic datasets of low and high resolution.","10.1371/journal.pone.0134256","2015-02-04","2015-02-10","['andrzej polanski', 'michal marczyk', 'monika pietrowska', 'piotr widlak', 'joanna polanska']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1065",1502.01477,"using sparse polynomial chaos expansions for the global sensitivity   analysis of groundwater lifetime expectancy in a multi-layered   hydrogeological model","stat.co stat.ap","the study makes use of polynomial chaos expansions to compute sobol' indices within the frame of a global sensitivity analysis of hydro-dispersive parameters in a simplified vertical cross-section of a segment of the subsurface of the paris basin. applying conservative ranges, the uncertainty in 78 input variables is propagated upon the mean lifetime expectancy of water molecules departing from a specific location within a highly confining layer situated in the middle of the model domain. lifetime expectancy is a hydrogeological performance measure pertinent to safety analysis with respect to subsurface contaminants, such as radionuclides. the sensitivity analysis indicates that the variability in the mean lifetime expectancy can be sufficiently explained by the uncertainty in the petrofacies, \ie the sets of porosity and hydraulic conductivity, of only a few layers of the model. the obtained results provide guidance regarding the uncertainty modeling in future investigations employing detailed numerical models of the subsurface of the paris basin. moreover, the study demonstrates the high efficiency of sparse polynomial chaos expansions in computing sobol' indices for high-dimensional models.","10.1016/j.ress.2015.11.005","2015-02-05","2015-11-23","['g. deman', 'k. konakli', 'b. sudret', 'j. kerrou', 'p. perrochet', 'h. benabderrahmane']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE
"1066",1502.01908,"marginalizing gaussian process hyperparameters using sequential monte   carlo","stat.ml stat.co","gaussian process regression is a popular method for non-parametric probabilistic modeling of functions. the gaussian process prior is characterized by so-called hyperparameters, which often have a large influence on the posterior model and can be difficult to tune. this work provides a method for numerical marginalization of the hyperparameters, relying on the rigorous framework of sequential monte carlo. our method is well suited for online problems, and we demonstrate its ability to handle real-world problems with several dimensions and compare it to other marginalization methods. we also conclude that our proposed method is a competitive alternative to the commonly used point estimates maximizing the likelihood, both in terms of computational load and its ability to handle multimodal posteriors.","10.1109/camsap.2015.7383840","2015-02-06","2015-10-02","['andreas svensson', 'johan dahlin', 'thomas b. schön']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE
"1067",1502.01974,"regionalization of multiscale spatial processes using a criterion for   spatial aggregation error","stat.me","the modifiable areal unit problem and the ecological fallacy are known problems that occur when modeling multiscale spatial processes. we investigate how these forms of spatial aggregation error can guide a regionalization over a spatial domain of interest. by ""regionalization"" we mean a specification of geographies that define the spatial support for areal data. this topic has been studied vigorously by geographers, but has been given less attention by spatial statisticians. thus, we propose a criterion for spatial aggregation error (cage), which we minimize to obtain an optimal regionalization. to define cage we draw a connection between spatial aggregation error and a new multiscale representation of the karhunen-loeve (k-l) expansion. this relationship between cage and the multiscale k-l expansion leads to illuminating theoretical developments including: connections between spatial aggregation error, squared prediction error, spatial variance, and a novel extension of obled-creutin eigenfunctions. the effectiveness of our approach is demonstrated through an analysis of two datasets, one using the american community survey and one related to environmental ocean winds.","","2015-02-06","2015-12-10","['jonathan r. bradley', 'christopher k. wikle', 'scott h. holan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1068",1502.02069,"likelihood-based model selection for stochastic block models","math.st stat.th","the stochastic block model (sbm) provides a popular framework for modeling community structures in networks. however, more attention has been devoted to problems concerning estimating the latent node labels and the model parameters than the issue of choosing the number of blocks. we consider an approach based on the log likelihood ratio statistic and analyze its asymptotic properties under model misspecification. we show the limiting distribution of the statistic in the case of underfitting is normal and obtain its convergence rate in the case of overfitting. these conclusions remain valid when the average degree grows at a polylog rate. the results enable us to derive the correct order of the penalty term for model complexity and arrive at a likelihood-based model selection criterion that is asymptotically consistent. our analysis can also be extended to a degree-corrected block model (dcsbm). in practice, the likelihood function can be estimated using more computationally efficient variational methods or consistent label estimation algorithms, allowing the criterion to be applied to large networks.","","2015-02-06","2016-02-29","['y. x. rachel wang', 'peter j. bickel']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1069",1502.02355,"high dimensional errors-in-variables models with dependent measurements","math.st stat.ml stat.th","suppose that we observe $y \in \mathbb{r}^f$ and $x \in \mathbb{r}^{f \times m}$ in the following errors-in-variables model: \begin{eqnarray*} y & = & x_0 \beta^* + \epsilon \\ x & = & x_0 + w \end{eqnarray*} where $x_0$ is a $f \times m$ design matrix with independent subgaussian row vectors, $\epsilon \in \mathbb{r}^f$ is a noise vector and $w$ is a mean zero $f \times m$ random noise matrix with independent subgaussian column vectors, independent of $x_0$ and $\epsilon$. this model is significantly different from those analyzed in the literature in the sense that we allow the measurement error for each covariate to be a dependent vector across its $f$ observations. such error structures appear in the science literature when modeling the trial-to-trial fluctuations in response strength shared across a set of neurons.   under sparsity and restrictive eigenvalue type of conditions, we show that one is able to recover a sparse vector $\beta^* \in \mathbb{r}^m$ from the model given a single observation matrix $x$ and the response vector $y$. we establish consistency in estimating $\beta^*$ and obtain the rates of convergence in the $\ell_q$ norm, where $q = 1, 2$ for the lasso-type estimator, and for $q \in [1, 2]$ for a dantzig-type conic programming estimator. we show error bounds which approach that of the regular lasso and the dantzig selector in case the errors in $w$ are tending to 0.","","2015-02-08","2015-12-18","['mark rudelson', 'shuheng zhou']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"1070",1502.02367,"gated feedback recurrent neural networks","cs.ne cs.lg stat.ml","in this work, we propose a novel recurrent neural network (rnn) architecture. the proposed rnn, gated-feedback rnn (gf-rnn), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. the recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. we evaluated the proposed gf-rnn with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and python program evaluation. our empirical evaluation of different rnn units, revealed that in both tasks, the gf-rnn outperforms the conventional approaches to build deep stacked rnns. we suggest that the improvement arises because the gf-rnn can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked rnn) by learning to gate these interactions.","","2015-02-09","2015-06-17","['junyoung chung', 'caglar gulcehre', 'kyunghyun cho', 'yoshua bengio']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1071",1502.02763,"cascading bandits: learning to rank in the cascade model","cs.lg stat.ml","a search engine usually outputs a list of $k$ web pages. the user examines this list, from the first web page to the last, and chooses the first attractive page. this model of user behavior is known as the cascade model. in this paper, we propose cascading bandits, a learning variant of the cascade model where the objective is to identify $k$ most attractive items. we formulate our problem as a stochastic combinatorial partial monitoring problem. we propose two algorithms for solving it, cascadeucb1 and cascadekl-ucb. we also prove gap-dependent upper bounds on the regret of these algorithms and derive a lower bound on the regret in cascading bandits. the lower bound matches the upper bound of cascadekl-ucb up to a logarithmic factor. we experiment with our algorithms on several problems. the algorithms perform surprisingly well even when our modeling assumptions are violated.","","2015-02-09","2015-05-18","['branislav kveton', 'csaba szepesvari', 'zheng wen', 'azin ashkan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1072",1502.03042,"functional gaussian process model for bayesian nonparametric analysis","stat.ml stat.co stat.me","gaussian process is a theoretically appealing model for nonparametric analysis, but its computational cumbersomeness hinders its use in large scale and the existing reduced-rank solutions are usually heuristic. in this work, we propose a novel construction of gaussian process as a projection from fixed discrete frequencies to any continuous location. this leads to a valid stochastic process that has a theoretic support with the reduced rank in the spectral density, as well as a high-speed computing algorithm. our method provides accurate estimates for the covariance parameters and concise form of predictive distribution for spatial prediction. for non-stationary data, we adopt the mixture framework with a customized spectral dependency structure. this enables clustering based on local stationarity, while maintains the joint gaussianness. our work is directly applicable in solving some of the challenges in the spatial data, such as large scale computation, anisotropic covariance, spatio-temporal modeling, etc. we illustrate the uses of the model via simulations and an application on a massive dataset.","","2015-02-10","2015-11-23","['leo l. duan', 'xia wang', 'rhonda d. szczesniak']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE
"1073",1502.03062,"air quality and acute deaths in california, 2000-2012","stat.ap","many studies have sought to determine if there is an association between air quality and acute deaths. many consider it plausible that current levels of air quality cause acute deaths. however, several factors call causation and even association into question. observational data sets are large and complex. multiple testing and multiple modeling can lead to false positive findings. publication, confirmation and other biases are also possible problems.   moreover, the fact that most data sets used in studies evaluating the relationships among air quality and public health outcomes are not publicly available makes reproducing the claims nearly impossible. here we have built and made publicly available a dataset containing daily air quality levels, pm2.5 and ozone, daily temperature levels, minimum and maximum and daily relative humidity levels for the eight most populous california air basins. we analyzed the dataset using a moving median analysis, a standard time series analysis, and a prediction analysis within the following analysis strategy. we examine the eight air basins separately to see if estimates replicate across locations. we use leave one year out cross validation analysis to evaluate predictions. both the moving medians analysis and the standard time series analysis found little evidence for association between air quality and acute deaths. the prediction analysis process was a run as a large factorial design using different models and holding out one year at a time. among the variables used to predict acute death, most of the daily death variability was explained by time of year or weather variables. in summary, the empirical evidence is that current levels of air quality, ozone and pm2.5, are not causally related to acute deaths for california. an empirical and logical case can be made air quality is not causally related to acute deaths for the rest of the united states.","","2015-02-10","2015-05-13","['kenneth k. lopiano', 'richard l. smith', 's. stanley young']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1074",1502.03466,"dependent mat\'ern processes for multivariate time series","stat.ml","for the challenging task of modeling multivariate time series, we propose a new class of models that use dependent mat\'ern processes to capture the underlying structure of data, explain their interdependencies, and predict their unknown values. although similar models have been proposed in the econometric, statistics, and machine learning literature, our approach has several advantages that distinguish it from existing methods: 1) it is flexible to provide high prediction accuracy, yet its complexity is controlled to avoid overfitting; 2) its interpretability separates it from black-box methods; 3) finally, its computational efficiency makes it scalable for high-dimensional time series. in this paper, we use several simulated and real data sets to illustrate these advantages. we will also briefly discuss some extensions of our model.","","2015-02-11","","['alexander vandenberg-rodes', 'babak shahbaba']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1075",1502.03494,"a semiparametric spatio-temporal model for solar irradiance data","stat.ap","design and operation of a utility scale photovoltaic (pv) power plant depends on accurate modeling of the power generated, which is highly correlated with aggregate solar irradiance on the plant's pv modules. at present, aggregate solar irradiance over the area of a typical pv power plant cannot be measured directly. rather, irradiance measurements are typically available from a few, relatively small sensors and thus aggregate solar irradiance must be estimated from these data. as a step towards finding more accurate methods for estimating aggregate irradiance from avaialble measurements, we evaluate semiparametric spatio-temporal models for global horizontal irradiance. using data from a 1.2 mw pv plant located in lanai, hawaii, we show that a semiparametric model can be more accurate than simple intepolation between sensor locations. we investigate spatio-temporal models with separable and nonseparable covariance structures and find no evidence to support assuming a separable covariance structure.","","2015-02-11","2015-02-13","['joshua patrick', 'jane harvill', 'clifford hansen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1076",1502.03939,"polynomial-chaos-based kriging","stat.co stat.me stat.ml","computer simulation has become the standard tool in many engineering fields for designing and optimizing systems, as well as for assessing their reliability. to cope with demanding analysis such as optimization and reliability, surrogate models (a.k.a meta-models) have been increasingly investigated in the last decade. polynomial chaos expansions (pce) and kriging are two popular non-intrusive meta-modelling techniques. pce surrogates the computational model with a series of orthonormal polynomials in the input variables where polynomials are chosen in coherency with the probability distributions of those input variables. on the other hand, kriging assumes that the computer model behaves as a realization of a gaussian random process whose parameters are estimated from the available computer runs, i.e. input vectors and response values. these two techniques have been developed more or less in parallel so far with little interaction between the researchers in the two fields. in this paper, pc-kriging is derived as a new non-intrusive meta-modeling approach combining pce and kriging. a sparse set of orthonormal polynomials (pce) approximates the global behavior of the computational model whereas kriging manages the local variability of the model output. an adaptive algorithm similar to the least angle regression algorithm determines the optimal sparse set of polynomials. pc-kriging is validated on various benchmark analytical functions which are easy to sample for reference results. from the numerical investigations it is concluded that pc-kriging performs better than or at least as good as the two distinct meta-modeling techniques. a larger gain in accuracy is obtained when the experimental design has a limited size, which is an asset when dealing with demanding computational models.","","2015-02-13","","['r. schoebi', 'b. sudret', 'j. wiart']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE
"1077",1502.04071,"the generalized lasso with non-linear observations","cs.it math.it math.st stat.th","we study the problem of signal estimation from non-linear observations when the signal belongs to a low-dimensional set buried in a high-dimensional space. a rough heuristic often used in practice postulates that non-linear observations may be treated as noisy linear observations, and thus the signal may be estimated using the generalized lasso. this is appealing because of the abundance of efficient, specialized solvers for this program. just as noise may be diminished by projecting onto the lower dimensional space, the error from modeling non-linear observations with linear observations will be greatly reduced when using the signal structure in the reconstruction. we allow general signal structure, only assuming that the signal belongs to some set k in r^n. we consider the single-index model of non-linearity. our theory allows the non-linearity to be discontinuous, not one-to-one and even unknown. we assume a random gaussian model for the measurement matrix, but allow the rows to have an unknown covariance matrix. as special cases of our results, we recover near-optimal theory for noisy linear observations, and also give the first theoretical accuracy guarantee for 1-bit compressed sensing with unknown covariance matrix of the measurement vectors.","","2015-02-13","2015-11-16","['yaniv plan', 'roman vershynin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1078",1502.04168,"nonparametric regression using needlet kernels for spherical data","cs.lg stat.ml","needlets have been recognized as state-of-the-art tools to tackle spherical data, due to their excellent localization properties in both spacial and frequency domains.   this paper considers developing kernel methods associated with the needlet kernel for nonparametric regression problems whose predictor variables are defined on a sphere. due to the localization property in the frequency domain, we prove that the regularization parameter of the kernel ridge regression associated with the needlet kernel can decrease arbitrarily fast. a natural consequence is that the regularization term for the kernel ridge regression is not necessary in the sense of rate optimality. based on the excellent localization property in the spacial domain further, we also prove that all the $l^{q}$ $(01\leq q < \infty)$ kernel regularization estimates associated with the needlet kernel, including the kernel lasso estimate and the kernel bridge estimate, possess almost the same generalization capability for a large range of regularization parameters in the sense of rate optimality.   this finding tentatively reveals that, if the needlet kernel is utilized, then the choice of $q$ might not have a strong impact in terms of the generalization capability in some modeling contexts. from this perspective, $q$ can be arbitrarily specified, or specified merely by other no generalization criteria like smoothness, computational complexity, sparsity, etc..","","2015-02-14","2015-09-10","['shaobo lin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1079",1502.04868,"proper complex gaussian processes for regression","cs.lg stat.ml","complex-valued signals are used in the modeling of many systems in engineering and science, hence being of fundamental interest. often, random complex-valued signals are considered to be proper. a proper complex random variable or process is uncorrelated with its complex conjugate. this assumption is a good model of the underlying physics in many problems, and simplifies the computations. while linear processing and neural networks have been widely studied for these signals, the development of complex-valued nonlinear kernel approaches remains an open problem. in this paper we propose gaussian processes for regression as a framework to develop 1) a solution for proper complex-valued kernel regression and 2) the design of the reproducing kernel for complex-valued inputs, using the convolutional approach for cross-covariances. in this design we pay attention to preserve, in the complex domain, the measure of similarity between near inputs. the hyperparameters of the kernel are learned maximizing the marginal likelihood using wirtinger derivatives. besides, the approach is connected to the multiple output learning scenario. in the experiments included, we first solve a proper complex gaussian process where the cross-covariance does not cancel, a challenging scenario when dealing with proper complex signals. then we successfully use these novel results to solve some problems previously proposed in the literature as benchmarks, reporting a remarkable improvement in the estimation error.","","2015-02-17","2015-02-18","['rafael boloix-tortosa', 'f. javier payán-somet', 'eva arias-de-reyna', 'juan josé murillo-fuentes']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1080",1502.06774,"information geometry formalism for the spatially homogeneous boltzmann   equation","math.st stat.th","information geometry generalizes to infinite dimension by modeling the tangent space of the relevant manifold of probability densities with exponential orlicz spaces. we review here several properties of the exponential manifold on a suitable set $\mathcal e$ of mutually absolutely continuous densities. we study in particular the fine properties of the kullback-liebler divergence in this context. we also show that this setting is well-suited for the study of the spatially homogeneous boltzmann equation if $\mathcal e$ is a set of positive densities with finite relative entropy with respect to the maxwell density. more precisely, we analyse the boltzmann operator in the geometric setting from the point of its maxwell's weak form as a composition of elementary operations in the exponential manifold, namely tensor product, conditioning, marginalization and we prove in a geometric way the basic facts i.e., the h-theorem. we also illustrate the robustness of our method by discussing, besides the kullback-leibler divergence, also the property of hyv\""arinen divergence. this requires to generalise our approach to orlicz-sobolev spaces to include derivatives.%","","2015-02-24","2015-06-12","['bertrand lods', 'giovanni pistone']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1081",1502.06777,"statistical efficiency of structured cpd estimation applied to   wiener-hammerstein modeling","stat.co cs.na","the computation of a structured canonical polyadic decomposition (cpd) is useful to address several important modeling problems in real-world applications. in this paper, we consider the identification of a nonlinear system by means of a wiener-hammerstein model, assuming a high-order volterra kernel of that system has been previously estimated. such a kernel, viewed as a tensor, admits a cpd with banded circulant factors which comprise the model parameters. to estimate them, we formulate specialized estimators based on recently proposed algorithms for the computation of structured cpds. then, considering the presence of additive white gaussian noise, we derive a closed-form expression for the cramer-rao bound (crb) associated with this estimation problem. finally, we assess the statistical performance of the proposed estimators via monte carlo simulations, by comparing their mean-square error with the crb.","","2015-02-24","2015-06-24","['josé henrique de morais goulart', 'maxime boizard', 'rémy boyer', 'gérard favier', 'pierre comon']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1082",1502.0693,"tensor decomposition with generalized lasso penalties","stat.me stat.co stat.ml","we present an approach for penalized tensor decomposition (ptd) that estimates smoothly varying latent factors in multi-way data. this generalizes existing work on sparse tensor decomposition and penalized matrix decompositions, in a manner parallel to the generalized lasso for regression and smoothing problems. our approach presents many nontrivial challenges at the intersection of modeling and computation, which are studied in detail. an efficient coordinate-wise optimization algorithm for (ptd) is presented, and its convergence properties are characterized. the method is applied both to simulated data and real data on flu hospitalizations in texas. these results show that our penalized tensor decomposition can offer major improvements on existing methods for analyzing multi-way data that exhibit smooth spatial or temporal features.","","2015-02-24","2016-05-12","['oscar hernan madrid padilla', 'james g. scott']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE
"1083",1502.0719,"topic-adjusted visibility metric for scientific articles","stat.ml cs.lg","measuring the impact of scientific articles is important for evaluating the research output of individual scientists, academic institutions and journals. while citations are raw data for constructing impact measures, there exist biases and potential issues if factors affecting citation patterns are not properly accounted for. in this work, we address the problem of field variation and introduce an article level metric useful for evaluating individual articles' visibility. this measure derives from joint probabilistic modeling of the content in the articles and the citations amongst them using latent dirichlet allocation (lda) and the mixed membership stochastic blockmodel (mmsb). our proposed model provides a visibility metric for individual articles adjusted for field variation in citation rates, a structural understanding of citation behavior in different fields, and article recommendations which take into account article visibility and citation patterns. we develop an efficient algorithm for model fitting using variational methods. to scale up to large networks, we develop an online variant using stochastic gradient methods and case-control likelihood approximation. we apply our methods to the benchmark kdd cup 2003 dataset with approximately 30,000 high energy physics papers.","10.1214/15-aoas887","2015-02-25","2015-10-16","['linda s. l. tan', 'aik hui chan', 'tian zheng']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1084",1502.08029,"describing videos by exploiting temporal structure","stat.ml cs.ai cs.cl cs.cv cs.lg","recent progress in using recurrent neural networks (rnns) for image description has motivated the exploration of their application for video description. however, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description. in this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. first, our approach incorporates a spatial temporal 3-d convolutional neural network (3-d cnn) representation of the short temporal dynamics. the 3-d cnn representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating rnn. our approach exceeds the current state-of-art for both bleu and meteor metrics on the youtube2text dataset. we also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.","","2015-02-27","2015-09-30","['li yao', 'atousa torabi', 'kyunghyun cho', 'nicolas ballas', 'christopher pal', 'hugo larochelle', 'aaron courville']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1085",1503.00759,"a review of relational machine learning for knowledge graphs","stat.ml cs.lg","relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. in this paper, we provide a review of how such statistical models can be ""trained"" on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). in particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. the first is based on latent feature models such as tensor factorization and multiway neural networks. the second is based on mining observable patterns in the graph. we also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the web. to this end, we also discuss google's knowledge vault project as an example of such combination.","10.1109/jproc.2015.2483592","2015-03-02","2015-09-28","['maximilian nickel', 'kevin murphy', 'volker tresp', 'evgeniy gabrilovich']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1086",1503.01308,"acknowledgment of priority: usage of the lambert w function in   statistics","stat.ap","in my 2011 annals of applied statistics article [goerg (2011)] i wrote that ""whereas the lambert $w$ function plays an important role in mathematics, physics, chemistry, biology and other fields, it has not yet been used in statistics."" this was incorrect. at the time of publication i was unaware of stehl\'{\i}k (2003), who used the lambert $w$ function to derive the exact distribution of the likelihood ratio test statistic. he has also used it in more recent work such as stehl\'{\i}k (2006), stehl\'{\i}k et al. (2010), or stehl\'{\i}k (2014) amongst others. while stehl\'{i}k's use of the lambert $w$ function was focused on the distribution of the likelihood ratio test statistic, my work dealt with the modeling of skewed random variables and symmetrizing data using the lambert $w$ function as a variable transformation.","10.1214/14-aoas790","2015-03-04","","['georg m. goerg']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1087",1503.01589,"structural nested models and g-estimation: the partially realized   promise","stat.me","structural nested models (snms) and the associated method of g-estimation were first proposed by james robins over two decades ago as approaches to modeling and estimating the joint effects of a sequence of treatments or exposures. the models and estimation methods have since been extended to dealing with a broader series of problems, and have considerable advantages over the other methods developed for estimating such joint effects. despite these advantages, the application of these methods in applied research has been relatively infrequent; we view this as unfortunate. to remedy this, we provide an overview of the models and estimation methods as developed, primarily by robins, over the years. we provide insight into their advantages over other methods, and consider some possible reasons for failure of the methods to be more broadly adopted, as well as possible remedies. finally, we consider several extensions of the standard models and estimation methods.","10.1214/14-sts493","2015-03-05","","['stijn vansteelandt', 'marshall joffe']",1,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1088",1503.02098,"variations of q-q plots -- the power of our eyes!","stat.me","in statistical modeling we strive to specify models that resemble data collected in studies or observed from processes. consequently, distributional specification and parameter estimation are central to parametric models. graphical procedures, such as the quantile-quantile (q-q) plot, are arguably the most widely used method of distributional assessment, though critics find their interpretation to be overly subjective. formal goodness-of-fit tests are available and are quite powerful, but only indicate whether there is a lack of fit, not why there is lack of fit. in this paper we explore the use of the lineup protocol to inject rigor to graphical distributional assessment and compare its power to that of formal distributional tests. we find that lineups of standard q-q plots are more powerful than lineups of de-trended q-q plots and that lineup tests are more powerful than traditional tests of normality. while, we focus on diagnosing non-normality, our approach is general and can be directly extended to the assessment of other distributions.","","2015-03-06","","['adam loy', 'lendie follett', 'heike hofmann']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1089",1503.03585,"deep unsupervised learning using nonequilibrium thermodynamics","cs.lg cond-mat.dis-nn q-bio.nc stat.ml","a central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. here, we develop an approach that simultaneously achieves both flexibility and tractability. the essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. we then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. this approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. we additionally release an open source reference implementation of the algorithm.","","2015-03-12","2015-11-18","['jascha sohl-dickstein', 'eric a. weiss', 'niru maheswaranathan', 'surya ganguli']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1090",1503.04567,"learning mixed membership community models in social tagging networks   through tensor methods","cs.lg cs.si stat.ml","community detection in graphs has been extensively studied both in theory and in applications. however, detecting communities in hypergraphs is more challenging. in this paper, we propose a tensor decomposition approach for guaranteed learning of communities in a special class of hypergraphs modeling social tagging systems or folksonomies. a folksonomy is a tripartite 3-uniform hypergraph consisting of (user, tag, resource) hyperedges. we posit a probabilistic mixed membership community model, and prove that the tensor method consistently learns the communities under efficient sample complexity and separation requirements.","","2015-03-16","2015-04-22","['anima anandkumar', 'hanie sedghi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1091",1503.05237,"should optimal designers worry about consideration?","stat.ap","consideration set formation using non-compensatory screening rules is a vital component of real purchasing decisions with decades of experimental validation. marketers have recently developed statistical methods that can estimate quantitative choice models that include consideration set formation via non-compensatory screening rules. but is capturing consideration within models of choice important for design? this paper reports on a simulation study of a vehicle portfolio design when households screen over vehicle body style built to explore the importance of capturing consideration rules for optimal designers. we generate synthetic market share data, fit a variety of discrete choice models to the data, and then optimize design decisions using the estimated models. model predictive power, design ""error"", and profitability relative to ideal profits are compared as the amount of market data available increases. we find that even when estimated compensatory models provide relatively good predictive accuracy, they can lead to sub-optimal design decisions when the population uses consideration behavior; convergence of compensatory models to non-compensatory behavior is likely to require unrealistic amounts of data; and modeling heterogeneity in non-compensatory screening is more valuable than heterogeneity in compensatory trade-offs. this supports the claim that designers should carefully identify consideration behaviors before optimizing product portfolios. we also find that higher model predictive power does not necessarily imply better design decisions; that is, different model forms can provide ""descriptive"" rather than ""predictive"" information that is useful for design.","","2015-03-17","","['minhua long', 'w. ross morrow']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1092",1503.05289,"time-varying nonlinear regression models: nonparametric estimation and   model selection","math.st stat.th","this paper considers a general class of nonparametric time series regression models where the regression function can be time-dependent. we establish an asymptotic theory for estimates of the time-varying regression functions. for this general class of models, an important issue in practice is to address the necessity of modeling the regression function as nonlinear and time-varying. to tackle this, we propose an information criterion and prove its selection consistency property. the results are applied to the u.s. treasury interest rate data.","10.1214/14-aos1299","2015-03-18","","['ting zhang', 'wei biao wu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1093",1503.05852,"combining survival trials using aggregate data based on misspecified   models","math.st stat.me stat.th","the treatment effects of the same therapy observed from multiple clinical trials can often be very different. yet the patient characteristics accounting for these differences may not be identifiable in real world practice. there needs to be an unbiased way to combine the results from multiple trials and report the overall treatment effect for the general population during the development and validation of a new therapy. the non-linear structure of the maximum partial likelihood estimates for the (log) hazard ratio defined with a cox proportional hazard model leads to challenges in the statistical analyses for combining such clinical trials. in this paper, we formulated the expected overall treatment effects using various modeling assumptions. thus we are proposing efficient estimates and a version of wald test for the combined hazard ratio using only aggregate data. interpretation of the methods are provided in the framework of robust data analyses involving misspecified models.","","2015-03-19","","['tinghui yu', 'yabing mai', 'sherry liu', 'xiaofei hu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1094",1503.0625,"fast imbalanced classification of healthcare data with missing values","stat.ml cs.lg","in medical domain, data features often contain missing values. this can create serious bias in the predictive modeling. typical standard data mining methods often produce poor performance measures. in this paper, we propose a new method to simultaneously classify large datasets and reduce the effects of missing values. the proposed method is based on a multilevel framework of the cost-sensitive svm and the expected maximization imputation method for missing values, which relies on iterated regression analyses. we compare classification results of multilevel svm-based algorithms on public benchmark datasets with imbalanced classes and missing values as well as real data in health applications, and show that our multilevel svm-based method produces fast, and more accurate and robust classification results.","","2015-03-20","","['talayeh razzaghi', 'oleg roderick', 'ilya safro', 'nick marko']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1095",1503.06266,"moments of the log non-central chi-square distribution","stat.ap math.pr","the cumulants and moments of the log of the non-central chi-square distribution are derived. for example, the expected log of a chi-square random variable with v degrees of freedom is log(2) + psi(v/2). applications to modeling probability distributions are discussed.","","2015-03-21","","['steven e. pav']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1096",1503.06302,"robust estimation for mixtures of gaussian factor analyzers, based on   trimming and constraints","stat.me","mixtures of gaussian factors are powerful tools for modeling an unobserved heterogeneous population, offering - at the same time - dimension reduction and model-based clustering. unfortunately, the high prevalence of spurious solutions and the disturbing effects of outlying observations, along maximum likelihood estimation, open serious issues. in this paper we consider restrictions for the component covariances, to avoid spurious solutions, and trimming, to provide robustness against violations of normality assumptions of the underlying latent factors. a detailed aecm algorithm for this new approach is presented. simulation results and an application to the ais dataset show the aim and effectiveness of the proposed methodology.","","2015-03-21","","['l. a. garcía-escudero', 'a. gordaliza', 'f. greselin', 's. ingrassia', 'a. mayo-iscar']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1097",1503.06567,"on some provably correct cases of variational inference for topic models","cs.lg cs.ds stat.ml","variational inference is a very efficient and popular heuristic used in various forms in the context of latent variable models. it's closely related to expectation maximization (em), and is applied when exact em is computationally infeasible. despite being immensely popular, current theoretical understanding of the effectiveness of variaitonal inference based algorithms is very limited. in this work we provide the first analysis of instances where variational inference algorithms converge to the global optimum, in the setting of topic models.   more specifically, we show that variational inference provably learns the optimal parameters of a topic model under natural assumptions on the topic-word matrix and the topic priors. the properties that the topic word matrix must satisfy in our setting are related to the topic expansion assumption introduced in (anandkumar et al., 2013), as well as the anchor words assumption in (arora et al., 2012c). the assumptions on the topic priors are related to the well known dirichlet prior, introduced to the area of topic modeling by (blei et al., 2003).   it is well known that initialization plays a crucial role in how well variational based algorithms perform in practice. the initializations that we use are fairly natural. one of them is similar to what is currently used in lda-c, the most popular implementation of variational inference for topic models. the other one is an overlapping clustering algorithm, inspired by a work by (arora et al., 2014) on dictionary learning, which is very simple and efficient.   while our primary goal is to provide insights into when variational inference might work in practice, the multiplicative, rather than the additive nature of the variational inference updates forces us to use fairly non-standard proof arguments, which we believe will be of general interest.","","2015-03-23","2015-08-22","['pranjal awasthi', 'andrej risteski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1098",1503.07301,"exploring patterns in european singles charts","physics.soc-ph cs.si nlin.ps stat.ap","european singles charts are important part of the music industry responsible for creating popularity of songs. after modeling and exploring dynamics of global album sales in previous papers, we investigate patterns of hit singles popularity according to all data (1966-2015) from weekly charts (polls) in 12 western european countries. the dynamics of building popularity in various national charts is more than the economy because it depends on spread of information. in our research we have shown how countries may be affected by their neighbourhood and influenced by technological era. we have also computed correlations with geographical and cultural distances between countries in analog, digital and internet era. we have shown that time delay between the single premiere and the peak of popularity has become shorter under the influence of technology and the popularity of songs depends on geographical distances in analog (1966-1987) and internet (2004-2015) era. on the other hand, cultural distances between nations have influenced the peaks of popularity, but in the compact disc era only (1988-2003). we have also indicated the european countries in line with global trends e.g. the netherlands, the united kingdom and outsiders like italy and spain.","","2015-03-25","","['andrzej buda', 'andrzej jarynowski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1099",1503.0734,"a bayesian approach to sparse plus low rank network identification","math.oc stat.ml","we consider the problem of modeling multivariate time series with parsimonious dynamical models which can be represented as sparse dynamic bayesian networks with few latent nodes. this structure translates into a sparse plus low rank model. in this paper, we propose a gaussian regression approach to identify such a model.","","2015-03-25","2015-09-26","['mattia zorzi', 'alessandro chiuso']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1100",1503.08357,"spatial process gradients and their use in sensitivity analysis for   environmental processes","math.st stat.me stat.th","this paper develops methodology for local sensitivity analysis based on directional derivatives associated with spatial processes. formal gradient analysis for spatial processes was elaborated in previous papers, focusing on distribution theory for directional derivatives associated with a response variable assumed to follow a gaussian process model. in the current work, these ideas are extended to additionally accommodate a continuous covariate whose directional derivatives are also of interest and to relate the behavior of the directional derivatives of the response surface to those of the covariate surface. it is of interest to assess whether, in some sense, the gradients of the response follow those of the explanatory variable. the joint gaussian structure of all variables, including the directional derivatives, allows for explicit distribution theory and, hence, kriging across the spatial region using multivariate normal theory. working within a bayesian hierarchical modeling framework, posterior samples enable all gradient analysis to occur post model fitting. as a proof of concept, we show how our methodology can be applied to a standard geostatistical modeling setting using a simulation example. for a real data illustration, we work with point pattern data, deferring our gradient analysis to the intensity surface, adopting a log-gaussian cox process model. in particular, we relate elevation data to point patterns associated with several tree species in duke forest.","","2015-03-28","","['maria a. terres', 'alan e. gelfand']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1101",1503.08542,"nonparametric relational topic models through dependent gamma processes","stat.ml cs.cl cs.ir cs.lg","traditional relational topic models provide a way to discover the hidden topics from a document network. many theoretical and practical tasks, such as dimensional reduction, document clustering, link prediction, benefit from this revealed knowledge. however, existing relational topic models are based on an assumption that the number of hidden topics is known in advance, and this is impractical in many real-world applications. therefore, in order to relax this assumption, we propose a nonparametric relational topic model in this paper. instead of using fixed-dimensional probability distributions in its generative model, we use stochastic processes. specifically, a gamma process is assigned to each document, which represents the topic interest of this document. although this method provides an elegant solution, it brings additional challenges when mathematically modeling the inherent network structure of typical document network, i.e., two spatially closer documents tend to have more similar topics. furthermore, we require that the topics are shared by all the documents. in order to resolve these challenges, we use a subsampling strategy to assign each document a different gamma process from the global gamma process, and the subsampling probabilities of documents are assigned with a markov random field constraint that inherits the document network structure. through the designed posterior inference algorithm, we can discover the hidden topics and its number simultaneously. experimental results on both synthetic and real-world network datasets demonstrate the capabilities of learning the hidden topics and, more importantly, the number of topics.","","2015-03-30","","['junyu xuan', 'jie lu', 'guangquan zhang', 'richard yi da xu', 'xiangfeng luo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1102",1503.08621,"variational bayes with intractable likelihood","stat.me","variational bayes (vb) is rapidly becoming a popular tool for bayesian inference in statistical modeling. however, the existing vb algorithms are restricted to cases where the likelihood is tractable, which precludes the use of vb in many interesting situations such as in state space models and in approximate bayesian computation (abc), where application of vb methods was previously impossible. this paper extends the scope of application of vb to cases where the likelihood is intractable, but can be estimated unbiasedly. the proposed vb method therefore makes it possible to carry out bayesian inference in many statistical applications, including state space models and abc. the method is generic in the sense that it can be applied to almost all statistical models without requiring too much model-based derivation, which is a drawback of many existing vb algorithms. we also show how the proposed method can be used to obtain highly accurate vb approximations of marginal posterior distributions.","","2015-03-30","2016-08-04","['minh-ngoc tran', 'david j. nott', 'robert kohn']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1103",1503.08658,"dynamic models for estimating the effect of haart on cd4 in   observational studies: application to the aquitaine cohort study and the   swiss hiv cohort study","stat.me","highly active antiretroviral therapy (haart) has proved efficient in increasing cd4 counts in many randomized clinical trials. because randomized trials have some limitations (e.g., short duration, highly selected subjects), it is interesting to assess it using observational studies. this is challenging because treatment is started preferentially in subjects with severe conditions, in particular in subjects with low cd4 counts. this general problem had been treated using marginal structural models (msm) relying on the counterfactual formulation. another approach to causality is based on dynamical models. first, we present three discrete-time dynamic models based on linear increments (lim): the simplest model is described by one difference equation for cd4 counts; the second has an equilibrium point; the third model is based on a system of two difference equations which allows jointly modeling cd4 counts and viral load. then we consider continuous time models based on ordinary differential equations with random effects (ode-nlme). these mechanistic models allow incorporating biological knowledge when available, which leads to increased power for detecting treatment effect. inference in ode-nlme models, however, is challenging from a numerical point of view, and requires specific methods and softwares. lims are a valuable intermediary option in terms of consistency, precision and complexity. the different approaches are compared in simulation and applied to hiv cohorts (the anrs co3 aquitaine cohort and the swiss hiv cohort study).","","2015-03-30","2015-11-20","['m. prague', 'd. commenges', 'j. m. gran', 'b. ledergerber', 'j. young', 'h. furrer', 'r. thiébaut']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1104",1504.00461,"testing for pure-jump processes for high-frequency data","math.st stat.th","pure-jump processes have been increasingly popular in modeling high-frequency financial data, partially due to their versatility and flexibility. in the meantime, several statistical tests have been proposed in the literature to check the validity of using pure-jump models. however, these tests suffer from several drawbacks, such as requiring rather stringent conditions and having slow rates of convergence. in this paper, we propose a different test to check whether the underlying process of high-frequency data can be modeled by a pure-jump process. the new test is based on the realized characteristic function, and enjoys a much faster convergence rate of order $o(n^{1/2})$ (where $n$ is the sample size) versus the usual $o(n^{1/4})$ available for existing tests; it is applicable much more generally than previous tests; for example, it is robust to jumps of infinite variation and flexible modeling of the diffusion component. simulation studies justify our findings and the test is also applied to some real high-frequency financial data.","10.1214/14-aos1298","2015-04-02","","['xin-bing kong', 'zhi liu', 'bing-yi jing']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1105",1504.00975,"bias and response heterogeneity in an air quality data set","stat.ap","it is well-known that claims coming from observational studies often fail to replicate when rigorously re-tested. the technical problems include multiple testing, multiple modeling and bias. any or all of these problems can give rise to claims that will fail to replicate. there is a need for statistical methods that are easily applied, are easy to understand, and are likely to give reliable results. in particular, simple ways for reducing the influence of bias are essential. in this paper, the local control method developed by robert obenchain is explicated using a small air quality/longevity data set first analyzed in the new england journal of medicine. the benefits of our paper are twofold. first, we describe a reliable strategy for analysis of observational data. second and importantly, the global claim that longevity increases with improvements in air quality made in the nejm paper needs to be modified. there is subgroup heterogeneity in the effect of air quality on longevity (one size does not fit all), and this heterogeneity is largely explained by factors other than air quality.","","2015-04-03","2015-04-07","['s. stanley young', 'robert l. obenchain', 'christophe lambert']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1106",1504.02992,"generic identifiability of linear structural equation models by ancestor   decomposition","stat.co","linear structural equation models, which relate random variables via linear interdependencies and gaussian noise, are a popular tool for modeling multivariate joint distributions. these models correspond to mixed graphs that include both directed and bidirected edges representing the linear relationships and correlations between noise terms, respectively. a question of interest for these models is that of parameter identifiability, whether or not it is possible to recover edge coefficients from the joint covariance matrix of the random variables. for the problem of determining generic parameter identifiability, we present an algorithm that extends an algorithm from prior work by foygel, draisma, and drton (2012). the main idea underlying our new algorithm is the use of ancestral subsets of vertices in the graph in application of a decomposition idea of tian (2005).","","2015-04-12","","['mathias drton', 'luca weihs']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1107",1504.0343,"a tutorial for analyzing structural equation modelling","stat.me","this paper provides a tutorial discussion on analyzing structural equation modelling (sem). sem can be regarded as regression models with observed and unobserved indicators, have been extensively applied to practical and fundamental studies. we deliver an introduction to sem method and a detailed description of how to deal with analyzing the data with this kind of modelling. the intended audience is statisticians/mathematicians/methodologists who either know about sem or simple basic statistics especially in regression and linear/nonlinear modeling, and ph.d. students in statistics, mathematics, management, psychology, and even computer science.","","2015-04-14","","['hashem salarzadeh jenatabadi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1108",1504.03441,"an overview of path analysis: mediation analysis concept in structural   equation modeling","stat.me","this paper provides a tutorial discussion on path analysis structure with concept of structural equation modelling (sem). the paper delivers an introduction to path analysis technique and explain to how to deal with analyzing the data with this kind of statistical methodology especially with a mediator in the research model. the intended audience is statisticians, mathematicians, or methodologists who either know about sem or simple basic statistics especially in regression and linear/nonlinear modeling, and ph.d. students in statistics, mathematics, management, psychology, and even computer science.","","2015-04-14","","['hashem salarzadeh jenatabadi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1109",1504.03454,"forecasting high-dimensional realized volatility matrices using a factor   model","stat.ap stat.me","modeling and forecasting covariance matrices of asset returns play a crucial role in finance. the availability of high frequency intraday data enables the modeling of the realized covariance matrix directly. however, most models in the literature suffer from the curse of dimensionality. to solve the problem, we propose a factor model with a diagonal caw model for the factor realized covariance matrices. asymptotic theory is derived for the estimated parameters. in an extensive empirical analysis, we find that the number of parameters can be reduced significantly. furthermore, the proposed model maintains a comparable performance with a benchmark vector autoregressive model.","","2015-04-14","","['keren shen', 'jianfeng yao', 'wai keung li']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1110",1504.04489,"spatial product partition models","stat.me","when modeling geostatistical or areal data, spatial structure is commonly accommodated via a covariance function for the former and a neighborhood structure for the latter. in both cases the resulting spatial structure is a consequence of implicit spatial grouping in that observations near in space are assumed to behave similarly. it would be desirable to develop spatial methods that explicitly model the partitioning of spatial locations providing more control over resulting spatial structures and being able to better balance global vs local spatial dependence. to this end, we extend product partition models to a spatial setting so that the partitioning of locations into spatially dependent clusters is explicitly modeled. we explore the spatial structures that result from employing a spatial product partition model and demonstrate its flexibility in accommodating many types of spatial dependencies. we illustrate the method's utility through simulation studies and an education application.","","2015-04-17","","['garritt l. page', 'fernando a. quintana']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1111",1504.04636,"consistent learning by composite proximal thresholding","math.st stat.th","we investigate the modeling and the numerical solution of machine learning problems with prediction functions which are linear combinations of elements of a possibly infinite-dimensional dictionary. we propose a novel flexible composite regularization model, which makes it possible to incorporate various priors on the coefficients of the prediction function, including sparsity and hard constraints. we show that the estimators obtained by minimizing the regularized empirical risk are consistent in a statistical sense, and we design an error-tolerant composite proximal thresholding algorithm for computing such estimators. new results on the asymptotic behavior of the proximal forward-backward splitting method are derived and exploited to establish the convergence properties of the proposed algorithm. in particular, our method features a $o(1/m)$ convergence rate in objective values.","","2015-04-17","2015-12-01","['patrick l. combettes', 'saverio salzo', 'silvia villa']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1112",1504.05651,"distinguishing cause from effect based on exogeneity","cs.ai stat.me","recent developments in structural equation modeling have produced several methods that can usually distinguish cause from effect in the two-variable case. for that purpose, however, one has to impose substantial structural constraints or smoothness assumptions on the functional causal models. in this paper, we consider the problem of determining the causal direction from a related but different point of view, and propose a new framework for causal direction determination. we show that it is possible to perform causal inference based on the condition that the cause is ""exogenous"" for the parameters involved in the generating process from the cause to the effect. in this way, we avoid the structural constraints required by the sem-based approaches. in particular, we exploit nonparametric methods to estimate marginal and conditional distributions, and propose a bootstrap-based approach to test for the exogeneity condition; the testing results indicate the causal direction between two variables. the proposed method is validated on both synthetic and real data.","","2015-04-22","","['kun zhang', 'jiji zhang', 'bernhard schölkopf']",1,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1113",1504.05659,"multi-resolution spatial random-effects models for irregularly spaced   data","stat.me stat.co","the spatial random-effects model is flexible in modeling spatial covariance functions, and is computationally efficient for spatial prediction via fixed rank kriging. however, the success of this model depends on an appropriate set of basis functions. in this research, we propose a class of basis functions extracted from thin-plate splines. these functions are ordered in terms of their degrees of smoothness with a higher-order function corresponding to larger-scale features and a lower-order one corresponding to smaller-scale details, leading to a parsimonious representation for a nonstationary spatial covariance function. consequently, only a small to moderate number of functions are needed in a spatial random-effects model. the proposed class of basis functions has several advantages over commonly used ones. first, we do not need to concern about the allocation of the basis functions, but simply select the total number of functions corresponding to a resolution. second, only a small number of basis functions is usually required, which facilitates computation. third, estimation variability of model parameters can be considerably reduced, and hence more precise covariance function estimates can be obtained. fourth, the proposed basis functions depend only on the data locations but not the measurements taken at those locations, and are applicable regardless of whether the data locations are sparse or irregularly spaced. in addition, we derive a simple close-form expression for the maximum likelihood estimates of model parameters in the spatial random-effects model. some numerical examples are provided to demonstrate the effectiveness of the proposed method.","","2015-04-22","","['shengli tzeng', 'hsin-cheng huang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"1114",1504.07107,"fast sampling for bayesian max-margin models","stat.ml cs.ai cs.lg","bayesian max-margin models have shown superiority in various practical applications, such as text categorization, collaborative prediction, social network link prediction and crowdsourcing, and they conjoin the flexibility of bayesian modeling and predictive strengths of max-margin learning. however, monte carlo sampling for these models still remains challenging, especially for applications that involve large-scale datasets. in this paper, we present the stochastic subgradient hamiltonian monte carlo (hmc) methods, which are easy to implement and computationally efficient. we show the approximate detailed balance property of subgradient hmc which reveals a natural and validated generalization of the ordinary hmc. furthermore, we investigate the variants that use stochastic subsampling and thermostats for better scalability and mixing. using stochastic subgradient markov chain monte carlo (mcmc), we efficiently solve the posterior inference task of various bayesian max-margin models and extensive experimental results demonstrate the effectiveness of our approach.","","2015-04-27","2016-10-18","['wenbo hu', 'jun zhu', 'bo zhang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1115",1504.07245,"approximate bayesian computation for forward modeling in cosmology","astro-ph.co astro-ph.im stat.co","bayesian inference is often used in cosmology and astrophysics to derive constraints on model parameters from observations. this approach relies on the ability to compute the likelihood of the data given a choice of model parameters. in many practical situations, the likelihood function may however be unavailable or intractable due to non-gaussian errors, non-linear measurements processes, or complex data formats such as catalogs and maps. in these cases, the simulation of mock data sets can often be made through forward modeling. we discuss how approximate bayesian computation (abc) can be used in these cases to derive an approximation to the posterior constraints using simulated data sets. this technique relies on the sampling of the parameter set, a distance metric to quantify the difference between the observation and the simulations and summary statistics to compress the information in the data. we first review the principles of abc and discuss its implementation using a population monte-carlo (pmc) algorithm and the mahalanobis distance metric. we test the performance of the implementation using a gaussian toy model. we then apply the abc technique to the practical case of the calibration of image simulations for wide field cosmological surveys. we find that the abc analysis is able to provide reliable parameter constraints for this problem and is therefore a promising technique for other applications in cosmology and astrophysics. our implementation of the abc pmc method is made available via a public code release.","10.1088/1475-7516/2015/08/043","2015-04-27","2015-09-04","['joel akeret', 'alexandre refregier', 'adam amara', 'sebastian seehars', 'caspar hasner']",1,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1116",1504.07389,"building classifiers to predict the start of glucose-lowering   pharmacotherapy using belgian health expenditure data","stat.ml cs.ir","early diagnosis is important for type 2 diabetes (t2d) to improve patient prognosis, prevent complications and reduce long-term treatment costs. we present a novel risk profiling approach based exclusively on health expenditure data that is available to belgian mutual health insurers. we used expenditure data related to drug purchases and medical provisions to construct models that predict whether a patient will start glucose-lowering pharmacotherapy in the coming years, based on that patient's recent medical expenditure history. the design and implementation of the modeling strategy are discussed in detail and several learning methods are benchmarked for our application. our best performing model obtains between 74.9% and 76.8% area under the roc curve, which is comparable to state-of-the-art risk prediction approaches for t2d based on questionnaires. in contrast to other methods, our approach can be implemented on a population-wide scale at virtually no extra operational cost. possibly, our approach can be further improved by additional information about some risk factors of t2d that is unavailable in health expenditure data.","","2015-04-28","","['marc claesen', 'frank de smet', 'pieter gillard', 'chantal mathieu', 'bart de moor']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1117",1504.07882,"inferring network structure from interventional time-course experiments","stat.me","graphical models are widely used to study biological networks. interventions on network nodes are an important feature of many experimental designs for the study of biological networks. in this paper we put forward a causal variant of dynamic bayesian networks (dbns) for the purpose of modeling time-course data with interventions. the models inherit the simplicity and computational efficiency of dbns but allow interventional data to be integrated into network inference. we show empirical results, on both simulated and experimental data, that demonstrate the need to appropriately handle interventions when interventions form part of the design.","10.1214/15-aoas806","2015-04-29","2015-06-16","['simon e. f. spencer', 'steven m. hill', 'sach mukherjee']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1118",1505.01394,"coherence for random fields","math.st stat.me stat.th","multivariate spatial field data are increasingly common and whose modeling typically relies on building cross-covariance functions to describe cross-process relationships. an alternative viewpoint is to model the matrix of spectral measures. we develop the notions of coherence, phase and gain for multidimensional stationary processes. coherence, as a function of frequency, can be seen to be a measure of linear relationship between two spatial processes at that frequency band. we use the coherence function to illustrate fundamental limitations on a number of previously proposed constructions for multivariate processes, suggesting these options are not viable for real data. we also give natural interpretations to cross-covariance parameters of the matern class, where the smoothness indexes dependence at low frequencies while the range parameter can imply dependence at low or high frequencies. estimation follows from smoothed multivariate periodogram matrices. we illustrate the estimation and interpretation of these functions on two datasets, forecast and reanalysis sea level pressure and geopotential heights over the equatorial region. examining these functions lends insight that would otherwise be difficult to detect and model using standard cross-covariance formulations.","","2015-05-06","","['william kleiber']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1119",1505.02343,"bayesian sparse tucker models for dimension reduction and tensor   completion","cs.lg cs.na stat.ml","tucker decomposition is the cornerstone of modern machine learning on tensorial data analysis, which have attracted considerable attention for multiway feature extraction, compressive sensing, and tensor completion. the most challenging problem is related to determination of model complexity (i.e., multilinear rank), especially when noise and missing data are present. in addition, existing methods cannot take into account uncertainty information of latent factors, resulting in low generalization performance. to address these issues, we present a class of probabilistic generative tucker models for tensor decomposition and completion with structural sparsity over multilinear latent space. to exploit structural sparse modeling, we introduce two group sparsity inducing priors by hierarchial representation of laplace and student-t distributions, which facilitates fully posterior inference. for model learning, we derived variational bayesian inferences over all model (hyper)parameters, and developed efficient and scalable algorithms based on multilinear operations. our methods can automatically adapt model complexity and infer an optimal multilinear rank by the principle of maximum lower bound of model evidence. experimental results and comparisons on synthetic, chemometrics and neuroimaging data demonstrate remarkable performance of our models for recovering ground-truth of multilinear rank and missing entries.","","2015-05-10","","['qibin zhao', 'liqing zhang', 'andrzej cichocki']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1120",1505.04303,"editorial comment on the special issue of ""information in dynamical   systems and complex systems""","nlin.cd math-ph math.ds math.mp stat.ap","this special issue collects contributions from the participants of the ""information in dynamical systems and complex systems"" workshop, which cover a wide range of important problems and new approaches that lie in the intersection of information theory and dynamical systems. the contributions include theoretical characterization and understanding of the different types of information flow and causality in general stochastic processes, inference and identification of coupling structure and parameters of system dynamics, rigorous coarse-grain modeling of network dynamical systems, and exact statistical testing of fundamental information-theoretic quantities such as the mutual information. the collective efforts reported herein reflect a modern perspective of the intimate connection between dynamical systems and information flow, leading to the promise of better understanding and modeling of natural complex systems and better/optimal design of engineering systems.","10.3390/e16094992","2015-05-16","","['erik m. bollt', 'jie sun']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1121",1505.04406,"hinge-loss markov random fields and probabilistic soft logic","cs.lg cs.ai stat.ml","a fundamental challenge in developing high-impact machine learning technologies is balancing the need to model rich, structured domains with the ability to scale to big data. many important problem areas are both richly structured and large scale, from social and biological networks, to knowledge graphs and the web, to images, video, and natural language. in this paper, we introduce two new formalisms for modeling structured data, and show that they can both capture rich structure and scale to big data. the first, hinge-loss markov random fields (hl-mrfs), is a new kind of probabilistic graphical model that generalizes different approaches to convex inference. we unite three approaches from the randomized algorithms, probabilistic graphical models, and fuzzy logic communities, showing that all three lead to the same inference objective. we then define hl-mrfs by generalizing this unified objective. the second new formalism, probabilistic soft logic (psl), is a probabilistic programming language that makes hl-mrfs easy to define using a syntax based on first-order logic. we introduce an algorithm for inferring most-probable variable assignments (map inference) that is much more scalable than general-purpose convex optimization methods, because it uses message passing to take advantage of sparse dependency structures. we then show how to learn the parameters of hl-mrfs. the learned hl-mrfs are as accurate as analogous discrete models, but much more scalable. together, these algorithms enable hl-mrfs and psl to model rich, structured data at scales not previously possible.","","2015-05-17","2017-11-16","['stephen h. bach', 'matthias broecheler', 'bert huang', 'lise getoor']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1122",1505.04697,"rebar: reinforcing a matching estimator with predictions from   high-dimensional covariates","stat.me stat.ap","in causal matching designs, some control subjects are often left unmatched, and some covariates are often left unmodeled. this article introduces ""rebar,"" a method using high-dimensional modeling to incorporate these commonly discarded data without sacrificing the integrity of the matching design. after constructing a match, a researcher uses the unmatched control subjects--the remnant--to fit a machine learning model predicting control potential outcomes as a function of the full covariate matrix. the resulting predictions in the matched set are used to adjust the causal estimate to reduce confounding bias. we present theoretical results to justify the method's bias-reducing properties as well as a simulation study that demonstrates them. additionally, we illustrate the method in an evaluation of a school-level comprehensive educational reform program in arizona.","10.3102/1076998617731518","2015-05-18","2017-12-06","['adam c sales', 'ben b hansen', 'brian rowan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1123",1505.05229,"variable subset selection via ga and information complexity in mixtures   of poisson and negative binomial regression models","stat.ml stat.me","count data, for example the number of observed cases of a disease in a city, often arise in the fields of healthcare analytics and epidemiology. in this paper, we consider performing regression on multivariate data in which our outcome is a count. specifically, we derive log-likelihood functions for finite mixtures of regression models involving counts that come from a poisson distribution, as well as a negative binomial distribution when the counts are significantly overdispersed. within our proposed modeling framework, we carry out optimal component selection using the information criteria scores aic, bic, caic, and icomp. we demonstrate applications of our approach on simulated data, as well as on a real data set of hiv cases in tennessee counties from the year 2010. finally, using a genetic algorithm within our framework, we perform variable subset selection to determine the covariates that are most responsible for categorizing tennessee counties. this leads to some interesting insights into the traits of counties that have high hiv counts.","","2015-05-19","","['t. j. massaro', 'h. bozdogan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1124",1505.05257,"sparse and robust linear regression: an optimization algorithm and its   statistical properties","math.st stat.me stat.th","this paper studies sparse linear regression analysis with outliers in the responses. a parameter vector for modeling outliers is added to the standard linear regression model and then the sparse estimation problem for both coefficients and outliers is considered. the $\ell_{1}$ penalty is imposed for the coefficients, while various penalties including redescending type penalties are for the outliers. to solve the sparse estimation problem, we introduce an optimization algorithm. under some conditions, we show the algorithmic and statistical convergence property for the coefficients obtained by the algorithm. moreover, it is shown that the algorithm can recover the true support of the coefficients with probability going to one.","","2015-05-20","","['shota katayama', 'hironori fujisawa']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1125",1505.05482,"tprm: tensor partition regression models with applications in imaging   biomarker detection","stat.ap","medical imaging studies have collected high dimensional imaging data to identify imaging biomarkers for diagnosis, screening, and prognosis, among many others. these imaging data are often represented in the form of a multi-dimensional array, called a tensor. the aim of this paper is to develop a tensor partition regression modeling (tprm) framework to establish a relationship between low-dimensional clinical outcomes (e.g., diagnosis) and high dimensional tensor covariates. our tprm is a hierarchical model and efficiently integrates four components: (i) a partition model, (ii) a canonical polyadic decomposition model, (iii) a principal components model, and (iv) a generalized linear model with a sparse inducing normal mixture prior. this framework not only reduces ultra-high dimensionality to a manageable level, resulting in efficient estimation, but also optimizes prediction accuracy in the search for informative sub-tensors. posterior computation proceeds via an efficient markov chain monte carlo algorithm. simulation shows that tprm outperforms several other competing methods. we apply tprm to predict disease status (alzheimer versus control) by using structural magnetic resonance imaging data obtained from the alzheimer's disease neuroimaging initiative (adni) study.","","2015-05-20","2017-11-08","['michelle f. miranda', 'hongtu zhu', 'joseph g. ibrahim']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1126",1505.05644,"modelling the combined effects of land use and climatic changes:   coupling bioclimatic modelling with markov-chain cellular automata in a case   study in cyprus","q-bio.pe cs.cy cs.dm stat.ap","two endemic plant species in the mediterranean island of cyprus, crocus cyprius and ophrys kotschyi, were used as a case study. we have coupled climate change scenarios, and land use change models with species distribution models. future land use scenarios were modelled by initially calculating the rate of current land use changes between two time snapshots (2000 and 2006) on the island, and based on these transition probabilities markov-chain cellular automata were used to generate future land use changes for 2050. climate change scenarios a1b, a2, b1 and b2a were derived from the ipcc reports. species climatic preferences were derived from their current distributions using classification trees while habitats preferences were derived from the red data book of the flora of cyprus. a bioclimatic model for crocus cyprius was built using mean temperature of wettest quarter, max temperature of warmest month and precipitation seasonality, while for ophrys kotchyi the bioclimatic model was built using precipitation of wettest month, mean temperature of warmest quarter, isothermality, precipitation of coldest quarter, and annual precipitation. sequentially, simulation scenarios were performed regarding future species distributions by accounting climate alone and both climate and land use changes. the distribution of the two species resulting from the bioclimatic models was then filtered by future land use changes, providing the species projected potential distribution. the species projected potential distribution varies depending on the type and scenario used, but many of both species current sites/locations are projected to be outside their future potential distribution. our results demonstrate the importance of including both land use and climatic changes in predictive species modeling.","","2015-05-21","","['marianna louca', 'ioannis n. vogiatzakis', 'aristides moustakas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1127",1505.05668,"locally adaptive dynamic networks","stat.ap stat.ml","our focus is on realistically modeling and forecasting dynamic networks of face-to-face contacts among individuals. important aspects of such data that lead to problems with current methods include the tendency of the contacts to move between periods of slow and rapid changes, and the dynamic heterogeneity in the actors' connectivity behaviors. motivated by this application, we develop a novel method for locally adaptive dynamic (lady) network inference. the proposed model relies on a dynamic latent space representation in which each actor's position evolves in time via stochastic differential equations. using a state space representation for these stochastic processes and p\'olya-gamma data augmentation, we develop an efficient mcmc algorithm for posterior inference along with tractable procedures for online updating and forecasting of future networks. we evaluate performance in simulation studies, and consider an application to face-to-face contacts among individuals in a primary school.","10.1214/16-aoas971","2015-05-21","2016-08-18","['daniele durante', 'david b. dunson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"1128",1505.06472,"partial information framework: model-based aggregation of estimates from   diverse information sources","stat.me","prediction polling is an increasingly popular form of crowdsourcing in which multiple participants estimate the probability or magnitude of some future event. these estimates are then aggregated into a single forecast. historically, randomness in scientific estimation has been generally assumed to arise from unmeasured factors which are viewed as measurement noise. however, when combining subjective estimates, heterogeneity stemming from differences in the participants' information is often more important than measurement noise. this paper formalizes information diversity as an alternative source of such heterogeneity and introduces a novel modeling framework that is particularly well-suited for prediction polls. a practical specification of this framework is proposed and applied to the task of aggregating probability and point estimates from two real-world prediction polls. in both cases our model outperforms standard measurement-error-based aggregators, hence providing evidence in favor of information diversity being the more important source of heterogeneity.","","2015-05-24","2016-04-21","['ville a. satopää', 'shane t. jensen', 'robin pemantle', 'lyle h. ungar']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1129",1505.07517,"within group variable selection through the exclusive lasso","stat.me","many data sets consist of variables with an inherent group structure. the problem of group selection has been well studied, but in this paper, we seek to do the opposite: our goal is to select at least one variable from each group in the context of predictive regression modeling. this problem is np-hard, but we study the tightest convex relaxation: a composite penalty that is a combination of the $\ell_1$ and $\ell_2$ norms. our so-called exclusive lasso method performs structured variable selection by ensuring that at least one variable is selected from each group. we study our method's statistical properties and develop computationally scalable algorithms for fitting the exclusive lasso. we study the effectiveness of our method via simulations as well as using nmr spectroscopy data. here, we use the exclusive lasso to select the appropriate chemical shift from a dictionary of possible chemical shifts for each molecule in the biological sample.","","2015-05-27","","['frederick campbell', 'genevera i. allen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1130",1505.07684,"the extreme risk of personal data breaches & the erosion of privacy","stat.ap","personal data breaches from organisations, enabling mass identity fraud, constitute an \emph{extreme risk}. this risk worsens daily as an ever-growing amount of personal data are stored by organisations and on-line, and the attack surface surrounding this data becomes larger and harder to secure. further, breached information is distributed and accumulates in the hands of cyber criminals, thus driving a cumulative erosion of privacy. statistical modeling of breach data from 2000 through 2015 provides insights into this risk: a current maximum breach size of about 200 million is detected, and is expected to grow by fifty percent over the next five years. the breach sizes are found to be well modeled by an \emph{extremely heavy tailed} truncated pareto distribution, with tail exponent parameter decreasing linearly from 0.57 in 2007 to 0.37 in 2015. with this current model, given a breach contains above fifty thousand items, there is a ten percent probability of exceeding ten million. a size effect is unearthed where both the frequency and severity of breaches scale with organisation size like $s^{0.6}$. projections indicate that the total amount of breached information is expected to double from two to four billion items within the next five years, eclipsing the population of users of the internet. this massive and uncontrolled dissemination of personal identities raises fundamental concerns about privacy.","10.1140/epjb/e2015-60754-4","2015-05-28","2016-02-25","['spencer wheatley', 'thomas maillart', 'didier sornette']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1131",1505.07917,"oracally efficient estimation of functional-coefficient autoregressive   models","stat.me","nonlinear autoregressive models are very useful for modeling many natural processes, however, the size of the class of these models is large. functional-coefficient autoregressive models (fcar) are useful structures for reducing the size of the class of these models. although this structure reduces the class of nonlinear models, it is broad enough to include some common time series models as specific cases. a recent development in estimating nonlinear time series data is the spline backfitted kernel (sbk) method. this method combines the computational speed of splines with the asymptotic properties of kernel smoothing. to estimate a component function in the model, all other component functions are pre-estimated with splines and then the difference is taken of the observed time series and the pre-estimates. this difference is then used as pseudo-responses for which kernel smoothing is used to estimate the function of interest. by constructing the estimates in this way, the method does not suffer from the curse of dimensionality. in this paper, we adapt the sbk method to fcar models.","","2015-05-29","","['qiwei li']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1132",1505.08052,"batch bayesian optimization via local penalization","stat.ml","the popularity of bayesian optimization methods for efficient exploration of parameter spaces has lead to a series of papers applying gaussian processes as surrogates in the optimization of functions. however, most proposed approaches only allow the exploration of the parameter space to occur sequentially. often, it is desirable to simultaneously propose batches of parameter values to explore. this is particularly the case when large parallel processing facilities are available. these facilities could be computational or physical facets of the process being optimized. e.g. in biological experiments many experimental set ups allow several samples to be simultaneously processed. batch methods, however, require modeling of the interaction between the evaluations in the batch, which can be expensive in complex scenarios. we investigate a simple heuristic based on an estimate of the lipschitz constant that captures the most important aspect of this interaction (i.e. local repulsion) at negligible computational overhead. the resulting algorithm compares well, in running time, with much more elaborate alternatives. the approach assumes that the function of interest, $f$, is a lipschitz continuous function. a wrap-loop around the acquisition function is used to collect batches of points of certain size minimizing the non-parallelizable computational effort. the speed-up of our method with respect to previous approaches is significant in a set of computationally expensive experiments.","","2015-05-29","2015-10-14","['javier gonzález', 'zhenwen dai', 'philipp hennig', 'neil d. lawrence']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1133",1506.00323,"robust pca: optimization of the robust reconstruction error over the   stiefel manifold","stat.ml cs.lg","it is well known that principal component analysis (pca) is strongly affected by outliers and a lot of effort has been put into robustification of pca. in this paper we present a new algorithm for robust pca minimizing the trimmed reconstruction error. by directly minimizing over the stiefel manifold, we avoid deflation as often used by projection pursuit methods. in distinction to other methods for robust pca, our method has no free parameter and is computationally very efficient. we illustrate the performance on various datasets including an application to background modeling and subtraction. our method performs better or similar to current state-of-the-art methods while being faster.","","2015-05-31","","['anastasia podosinnikova', 'simon setzer', 'matthias hein']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1134",1506.00403,"a bayesian regression tree approach to identify the effect of   nanoparticles' properties on toxicity profiles","stat.ap","we introduce a bayesian multiple regression tree model to characterize relationships between physico-chemical properties of nanoparticles and their in-vitro toxicity over multiple doses and times of exposure. unlike conventional models that rely on data summaries, our model solves the low sample size issue and avoids arbitrary loss of information by combining all measurements from a general exposure experiment across doses, times of exposure, and replicates. the proposed technique integrates bayesian trees for modeling threshold effects and interactions, and penalized b-splines for dose- and time-response surface smoothing. the resulting posterior distribution is sampled by markov chain monte carlo. this method allows for inference on a number of quantities of potential interest to substantive nanotoxicology, such as the importance of physico-chemical properties and their marginal effect on toxicity. we illustrate the application of our method to the analysis of a library of 24 nano metal oxides.","10.1214/14-aoas797","2015-06-01","","['cecile low-kam', 'donatello telesca', 'zhaoxia ji', 'haiyuan zhang', 'tian xia', 'jeffrey i. zink', 'andre e. nel']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1135",1506.00728,"network assisted analysis to reveal the genetic basis of autism","stat.me stat.ap","while studies show that autism is highly heritable, the nature of the genetic basis of this disorder remains illusive. based on the idea that highly correlated genes are functionally interrelated and more likely to affect risk, we develop a novel statistical tool to find more potentially autism risk genes by combining the genetic association scores with gene co-expression in specific brain regions and periods of development. the gene dependence network is estimated using a novel partial neighborhood selection (pns) algorithm, where node specific properties are incorporated into network estimation for improved statistical and computational efficiency. then we adopt a hidden markov random field (hmrf) model to combine the estimated network and the genetic association scores in a systematic manner. the proposed modeling framework can be naturally extended to incorporate additional structural information concerning the dependence between genes. using currently available genetic association data from whole exome sequencing studies and brain gene expression levels, the proposed algorithm successfully identified 333 genes that plausibly affect autism risk.","10.1214/15-aoas844","2015-06-01","2015-11-17","['li liu', 'jing lei', 'kathryn roeder']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1136",1506.00878,"efficient maximum approximated likelihood inference for tukey's g-and-h   distribution","stat.me","tukey's $g$-and-$h$ distribution has been a powerful tool for data exploration and modeling since its introduction. however, two long standing challenges associated with this distribution family have remained unsolved until this day: how to find an optimal estimation procedure and how to make valid statistical inference on unknown parameters. to overcome these two challenges, a computationally efficient estimation procedure based on maximizing an approximated likelihood function of the tukey's $g$-and-$h$ distribution is proposed and is shown to have the same estimation efficiency as the maximum likelihood estimator under mild conditions. the asymptotic distribution of the proposed estimator is derived and a series of approximated likelihood ratio test statistics are developed to conduct hypothesis tests involving two shape parameters of tukey's $g$-and-$h$ distribution. simulation examples and an analysis of air pollution data are used to demonstrate the effectiveness of the proposed estimation and testing procedures.","","2015-06-02","","['ganggang xu', 'marc g. genton']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1137",1506.01583,"a causal inference approach to network meta-analysis","stat.me","while standard meta-analysis pools the results from randomized trials that compare two treatments, network meta-analysis aggregates the results of randomized trials comparing a wider variety of treatment options. however, it is unclear whether the aggregation of effect estimates across heterogeneous populations will be consistent for a meaningful parameter when not all treatments are evaluated on each population. drawing from counterfactual theory and the causal inference framework, we define the population of interest in a network meta-analysis and define the target parameter under a series of nonparametric structural assumptions. this allows us to determine the requirements for identifiability of this parameter, enabling a description of the conditions under which network meta-analysis is appropriate and when it might mislead decision making. we then adapt several modeling strategies from the causal inference literature to obtain consistent estimation of the intervention-specific mean outcome and model-independent contrasts between treatments. finally, we perform a reanalysis of a systematic review to compare the efficacy of antibiotics on suspected or confirmed methicillin-resistant \emph{staphylococcus aureus} in hospitalized patients.","","2015-06-04","2016-08-11","['mireille e. schnitzer', 'russell j. steele', 'michèle bally', 'ian shrier']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1138",1506.02087,"global gene expression analysis using machine learning methods","q-bio.qm cs.ce cs.lg stat.ml","microarray is a technology to quantitatively monitor the expression of large number of genes in parallel. it has become one of the main tools for global gene expression analysis in molecular biology research in recent years. the large amount of expression data generated by this technology makes the study of certain complex biological problems possible and machine learning methods are playing a crucial role in the analysis process. at present, many machine learning methods have been or have the potential to be applied to major areas of gene expression analysis. these areas include clustering, classification, dynamic modeling and reverse engineering.   in this thesis, we focus our work on using machine learning methods to solve the classification problems arising from microarray data. we first identify the major types of the classification problems; then apply several machine learning methods to solve the problems and perform systematic tests on real and artificial datasets. we propose improvement to existing methods. specifically, we develop a multivariate and a hybrid feature selection method to obtain high classification performance for high dimension classification problems. using the hybrid feature selection method, we are able to identify small sets of features that give predictive accuracy that is as good as that from other methods which require many more features.","","2015-06-05","","['min xu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1139",1506.02194,"fast mixing for discrete point processes","stat.ml math.st stat.th","we investigate the systematic mechanism for designing fast mixing markov chain monte carlo algorithms to sample from discrete point processes under the dobrushin uniqueness condition for gibbs measures. discrete point processes are defined as probability distributions $\mu(s)\propto \exp(\beta f(s))$ over all subsets $s\in 2^v$ of a finite set $v$ through a bounded set function $f:2^v\rightarrow \mathbb{r}$ and a parameter $\beta>0$. a subclass of discrete point processes characterized by submodular functions (which include log-submodular distributions, submodular point processes, and determinantal point processes) has recently gained a lot of interest in machine learning and shown to be effective for modeling diversity and coverage. we show that if the set function (not necessarily submodular) displays a natural notion of decay of correlation, then, for $\beta$ small enough, it is possible to design fast mixing markov chain monte carlo methods that yield error bounds on marginal approximations that do not depend on the size of the set $v$. the sufficient conditions that we derive involve a control on the (discrete) hessian of set functions, a quantity that has not been previously considered in the literature. we specialize our results for submodular functions, and we discuss canonical examples where the hessian can be easily controlled.","","2015-06-06","","['patrick rebeschini', 'amin karbasi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"1140",1506.02196,"classification and regression using an outer approximation   projection-gradient method","math.st stat.ml stat.th","this paper deals with sparse feature selection and grouping for classification and regression. the classification or regression problems under consideration consists in minimizing a convex empirical risk function subject to an $\ell^1$ constraint, a pairwise $\ell^\infty$ constraint, or a pairwise $\ell^1$ constraint. existing work, such as the lasso formulation, has focused mainly on lagrangian penalty approximations, which often require ad hoc or computationally expensive procedures to determine the penalization parameter. we depart from this approach and address the constrained problem directly via a splitting method. the structure of the method is that of the classical gradient-projection algorithm, which alternates a gradient step on the objective and a projection step onto the lower level set modeling the constraint. the novelty of our approach is that the projection step is implemented via an outer approximation scheme in which the constraint set is approximated by a sequence of simple convex sets consisting of the intersection of two half-spaces. convergence of the iterates generated by the algorithm is established for a general smooth convex minimization problem with inequality constraints. experiments on both synthetic and biological data show that our method outperforms penalty methods.","","2015-06-06","2017-03-23","['michel barlaud', 'wafa belhajali', 'patrick l. combettes', 'lionel fillatre']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"1141",1506.02565,"learning to select pre-trained deep representations with bayesian   evidence framework","cs.cv cs.lg stat.ml","we propose a bayesian evidence framework to facilitate transfer learning from pre-trained deep convolutional neural networks (cnns). our framework is formulated on top of a least squares svm (ls-svm) classifier, which is simple and fast in both training and testing, and achieves competitive performance in practice. the regularization parameters in ls-svm is estimated automatically without grid search and cross-validation by maximizing evidence, which is a useful measure to select the best performing cnn out of multiple candidates for transfer learning; the evidence is optimized efficiently by employing aitken's delta-squared process, which accelerates convergence of fixed point update. the proposed bayesian evidence framework also provides a good solution to identify the best ensemble of heterogeneous cnns through a greedy algorithm. our bayesian evidence framework for transfer learning is tested on 12 visual recognition datasets and illustrates the state-of-the-art performance consistently in terms of prediction accuracy and modeling efficiency.","","2015-06-08","2016-04-24","['yong-deok kim', 'taewoong jang', 'bohyung han', 'seungjin choi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1142",1506.02686,"the licors cabinet: nonparametric algorithms for spatio-temporal   prediction","stat.ml cs.lg","spatio-temporal data is intrinsically high dimensional, so unsupervised modeling is only feasible if we can exploit structure in the process. when the dynamics are local in both space and time, this structure can be exploited by splitting the global field into many lower-dimensional ""light cones"". we review light cone decompositions for predictive state reconstruction, introducing three simple light cone algorithms. these methods allow for tractable inference of spatio-temporal data, such as full-frame video. the algorithms make few assumptions on the underlying process yet have good predictive performance and can provide distributions over spatio-temporal data, enabling sophisticated probabilistic inference.","","2015-06-08","2016-09-14","['george d. montanez', 'cosma rohilla shalizi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1143",1506.02699,"community detection in multi-relational data with restricted multi-layer   stochastic blockmodel","stat.ml","in recent years there has been an increased interest in statistical analysis of data with multiple types of relations among a set of entities. such multi-relational data can be represented as multi-layer graphs where the set of vertices represents the entities and multiple types of edges represent the different relations among them. for community detection in multi-layer graphs, we consider two random graph models, the multi-layer stochastic blockmodel (mlsbm) and a model with a restricted parameter space, the restricted multi-layer stochastic blockmodel (rmlsbm). we derive consistency results for community assignments of the maximum likelihood estimators (mles) in both models where mlsbm is assumed to be the true model, and either the number of nodes or the number of types of edges or both grow. we compare mles in the two models with other baseline approaches, such as separate modeling of layers, aggregating the layers and majority voting. rmlsbm is shown to have advantage over mlsbm when either the growth rate of the number of communities is high or the growth rate of the average degree of the component graphs in the multi-graph is low. we also derive minimax rates of error and sharp thresholds for achieving consistency of community detection in both models, which are then used to compare the multi-layer models with a baseline model, the aggregate stochastic block model. the simulation studies and real data applications confirm the superior performance of the multi-layer approaches in comparison to the baseline procedures.","10.1214/16-ejs1211","2015-06-08","2016-01-21","['subhadeep paul', 'yuguo chen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1144",1506.03101,"provable bayesian inference via particle mirror descent","cs.lg stat.co stat.ml","bayesian methods are appealing in their flexibility in modeling complex data and ability in capturing uncertainty in parameters. however, when bayes' rule does not result in tractable closed-form, most approximate inference algorithms lack either scalability or rigorous guarantees. to tackle this challenge, we propose a simple yet provable algorithm, \emph{particle mirror descent} (pmd), to iteratively approximate the posterior density. pmd is inspired by stochastic functional mirror descent where one descends in the density space using a small batch of data points at each iteration, and by particle filtering where one uses samples to approximate a function. we prove result of the first kind that, with $m$ particles, pmd provides a posterior density estimator that converges in terms of $kl$-divergence to the true posterior in rate $o(1/\sqrt{m})$. we demonstrate competitive empirical performances of pmd compared to several approximate inference algorithms in mixture models, logistic regression, sparse gaussian processes and latent dirichlet allocation on large scale datasets.","","2015-06-09","2016-05-05","['bo dai', 'niao he', 'hanjun dai', 'le song']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE
"1145",1506.03478,"generative image modeling using spatial lstms","stat.ml cs.cv cs.lg","modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. we here introduce a recurrent image model based on multi-dimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. our model scales to images of arbitrary size and its likelihood is computationally tractable. we find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.","","2015-06-10","2015-09-18","['lucas theis', 'matthias bethge']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1146",1506.03784,"sparse partially collapsed mcmc for parallel inference in topic models","stat.ml stat.me","topic models, and more specifically the class of latent dirichlet allocation (lda), are widely used for probabilistic modeling of text. mcmc sampling from the posterior distribution is typically performed using a collapsed gibbs sampler. we propose a parallel sparse partially collapsed gibbs sampler and compare its speed and efficiency to state-of-the-art samplers for topic models on five well-known text corpora of differing sizes and properties. in particular, we propose and compare two different strategies for sampling the parameter block with latent topic indicators. the experiments show that the increase in statistical inefficiency from only partial collapsing is smaller than commonly assumed, and can be more than compensated by the speedup from parallelization and sparsity on larger corpora. we also prove that the partially collapsed samplers scale well with the size of the corpus. the proposed algorithm is fast, efficient, exact, and can be used in more modeling situations than the ordinary collapsed sampler.","","2015-06-11","2017-08-15","['måns magnusson', 'leif jonsson', 'mattias villani', 'david broman']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1147",1506.03824,"a constructive spatio-temporal approach to modeling spatial covariance","stat.me","i present an approach for modeling areal spatial covariance by considering the stationary distribution of a spatio-temporal markov random walk. in the areal data case, this stationary distribution corresponds to an intrinsic simultaneous autoregressive (sar) model for spatial correlation, and provides a principled approach to specifying areal spatial models when a spatio-temporal generating process can be assumed. i apply the approach to a study of spatial genetic variation of trout in a stream network in connecticut, usa, and a study of crime rates in neighborhoods of columbus, oh, usa.","","2015-06-11","2015-07-03","['ephraim m. hanks']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1148",1506.03993,"bootstrap bartlett correction in inflated beta regression","stat.me","the inflated beta regression model aims to enable the modeling of responses in the intervals $(0,1]$, $[0,1)$ or $[0,1]$. in this model, hypothesis testing is often performed based on the likelihood ratio statistic. the critical values are obtained from asymptotic approximations, which may lead to distortions of size in small samples. in this sense, this paper proposes the bootstrap bartlett correction to the statistic of likelihood ratio in the inflated beta regression model. the proposed adjustment only requires a simple monte carlo simulation. through extensive monte carlo simulations the finite sample performance (size and power) of the proposed corrected test is compared to the usual likelihood ratio test and the skovgaard adjustment already proposed in the literature. the numerical results evidence that inference based on the proposed correction is much more reliable than that based on the usual likelihood ratio statistics and the skovgaard adjustment. at the end of the work, an application to real data is also presented.","10.1080/03610918.2015.1065326","2015-06-12","","['laís h. loose', 'fábio m. bayer', 'tarciana l. pereira']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1149",1506.04125,"a risk management approach to capital allocation","q-fin.rm math.pr stat.ap","the european insurance sector will soon be faced with the application of solvency 2 regulation norms. it will create a real change in risk management practices. the orsa approach of the second pillar makes the capital allocation an important exercise for all insurers and specially for groups. considering multi-branches firms, capital allocation has to be based on a multivariate risk modeling. several allocation methods are present in the literature and insurers practices. in this paper, we present a new risk allocation method, we study its coherence using an axiomatic approach, and we try to define what the best allocation choice for an insurance group is.","","2015-06-12","","['véronique maume-deschamps', 'didier rullière', 'khalil said']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1150",1506.04324,"weak convergence of the empirical process and the rescaled empirical   distribution function in the skorokhod product space","math.pr math.st stat.th","we prove the asymptotic independence of the empirical process $\alpha_n = \sqrt{n}( f_n - f)$ and the rescaled empirical distribution function $\beta_n = n (f_n(\tau+\frac{\cdot}{n})-f_n(\tau))$, where $f$ is an arbitrary cdf, differentiable at some point $\tau$, and $f_n$ the corresponding empricial cdf. this seems rather counterintuitive, since, for every $n \in n$, there is a deterministic correspondence between $\alpha_n$ and $\beta_n$. precisely, we show that the pair $(\alpha_n,\beta_n)$ converges in law to a limit having independent components, namely a time-transformed brownian bridge and a two-sided poisson process. since these processes have jumps, in particular if $f$ itself has jumps, the skorokhod product space $d(r) \times d(r)$ is the adequate choice for modeling this convergence in. we develop a short convergence theory for $d(r) \times d(r)$ by establishing the classical principle, devised by yu. v. prokhorov, that finite-dimensional convergence and tightness imply weak convergence. several tightness criteria are given. finally, the convergence of the pair $(\alpha_n,\beta_n)$ implies convergence of each of its components, thus, in passing, we provide a thorough proof of these known convergence results in a very general setting. in fact, the condition on $f$ to be differentiable in at least one point is only required for $\beta_n$ to converge and can be further weakened.","10.1137/s0040585x97984486","2015-06-13","","['dietmar ferger', 'daniel vogel']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1151",1506.04448,"fast and guaranteed tensor decomposition via sketching","stat.ml cs.lg","tensor candecomp/parafac (cp) decomposition has wide applications in statistical learning of latent variable models and in data mining. in this paper, we propose fast and randomized tensor cp decomposition algorithms based on sketching. we build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. we develop novel methods for randomized computation of tensor contractions via ffts, without explicitly forming the tensors. such tensor contractions are encountered in decomposition methods such as tensor power iterations and alternating least squares. we also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. we then combine these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest algorithm on both sparse and dense tensors. the quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. we apply the method for topic modeling and obtain competitive results.","","2015-06-14","2015-10-20","['yining wang', 'hsiao-yu tung', 'alexander smola', 'animashree anandkumar']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1152",1506.04452,"modeling the association structure in doubly robust gee for longitudinal   ordinal missing data","stat.me","generalized estimation equations (gee) are a well-known method for the analysis of categorical longitudinal responses. gee method has computational simplicity and population parameter interpretation. in the presence of missing data it is only valid under the strong assumption of missing completely at random. a doubly robust estimator (drgee) for correlated ordinal longitudinal data is a nice approach for handling intermittently missing response and covariate under the mar mechanism. independent working correlation is the standard way in drgee. however, when covariate is not time stationary, efficiency can be gained using a structured association. the goal of this paper is to extend the drgee estimator to allow modeling the association structure by means of either the correlation coefficient or local odds ratio. simulation results revealed better performance of the local odds ratio parametrization, specially for small samples. the method is applied to a data set related to rheumatic mitral stenosis.","","2015-06-14","","['josé luiz p. da silva', 'enrico a. colosimo', 'fábio n. demarqui']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1153",1506.04967,"parsimonious mixed models","stat.me","the analysis of experimental data with mixed-effects models requires decisions about the specification of the appropriate random-effects structure. recently, barr, levy, scheepers, and tily, 2013 recommended fitting `maximal' models with all possible random effect components included. estimation of maximal models, however, may not converge. we show that failure to converge typically is not due to a suboptimal estimation algorithm, but is a consequence of attempting to fit a model that is too complex to be properly supported by the data, irrespective of whether estimation is based on maximum likelihood or on bayesian hierarchical modeling with uninformative or weakly informative priors. importantly, even under convergence, overparameterization may lead to uninterpretable models. we provide diagnostic tools for detecting overparameterization and guiding model simplification.","","2015-06-16","2018-05-26","['douglas bates', 'reinhold kliegl', 'shravan vasishth', 'harald baayen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1154",1506.04989,"statistical evidence measured on a properly calibrated scale across   nested and non-nested hypothesis comparisons","math.st stat.th","statistical modeling is often used to measure the strength of evidence for or against hypotheses on given data. we have previously proposed an information-dynamic framework in support of a properly calibrated measurement scale for statistical evidence, borrowing some mathematics from thermodynamics, and showing how an evidential analogue of the ideal gas equation of state could be used to measure evidence for a one-sided binomial hypothesis comparison (coin is fair versus coin is biased towards heads). here we take three important steps forward in generalizing the framework beyond this simple example. we (1) extend the scope of application to other forms of hypothesis comparison in the binomial setting; (2) show that doing so requires only the original ideal gas equation plus one simple extension, which has the form of the van der waals equation; (3) begin to develop the principles required to resolve a key constant, which enables us to calibrate the measurement scale across applications, and which we find to be related to the familiar statistical concept of degrees of freedom. this paper thus moves our information-dynamic theory substantially closer to the goal of producing a practical, properly calibrated measure of statistical evidence for use in general applications.","","2015-06-16","","['v. j vieland', 's-c. seok']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1155",1506.05275,"breaking the curse of dimensionality in conditional moment inequalities   for discrete choice models","stat.me math.st stat.th","this paper studies inference of preference parameters in semiparametric discrete choice models when these parameters are not point-identified and the identified set is characterized by a class of conditional moment inequalities. exploring the semiparametric modeling restrictions, we show that the identified set can be equivalently formulated by moment inequalities conditional on only two continuous indexing variables. such formulation holds regardless of the covariate dimension, thereby breaking the curse of dimensionality for nonparametric inference based on the underlying conditional moment inequalities. we further apply this dimension reducing characterization approach to the monotone single index model and to a variety of semiparametric models under which the sign of conditional expectation of a certain transformation of the outcome is the same as that of the indexing variable.","","2015-06-17","2018-11-23","['le-yu chen', 'sokbae lee']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1156",1506.056,"causality on cross-sectional data: stable specification search in   constrained structural equation modeling","stat.ml cs.lg","causal modeling has long been an attractive topic for many researchers and in recent decades there has seen a surge in theoretical development and discovery algorithms. generally discovery algorithms can be divided into two approaches: constraint-based and score-based. the constraint-based approach is able to detect common causes of the observed variables but the use of independence tests makes it less reliable. the score-based approach produces a result that is easier to interpret as it also measures the reliability of the inferred causal relationships, but it is unable to detect common confounders of the observed variables. a drawback of both score-based and constrained-based approaches is the inherent instability in structure estimation. with finite samples small changes in the data can lead to completely different optimal structures. the present work introduces a new hypothesis-free score-based causal discovery algorithm, called stable specification search, that is robust for finite samples based on recent advances in stability selection using subsampling and selection algorithms. structure search is performed over structural equation models. our approach uses exploratory search but allows incorporation of prior background knowledge. we validated our approach on one simulated data set, which we compare to the known ground truth, and two real-world data sets for chronic fatigue syndrome and attention deficit hyperactivity disorder, which we compare to earlier medical studies. the results on the simulated data set show significant improvement over alternative approaches and the results on the real-word data sets show consistency with the hypothesis driven models constructed by medical experts.","10.1016/j.asoc.2016.10.003","2015-06-18","2016-07-14","['ridho rahmadi', 'perry groot', 'marianne heins', 'hans knoop', 'tom heskes']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1157",1506.05843,"dependent multinomial models made easy: stick breaking with the   p\'olya-gamma augmentation","stat.ml","many practical modeling problems involve discrete data that are best represented as draws from multinomial or categorical distributions. for example, nucleotides in a dna sequence, children's names in a given state and year, and text documents are all commonly modeled with multinomial distributions. in all of these cases, we expect some form of dependency between the draws: the nucleotide at one position in the dna strand may depend on the preceding nucleotides, children's names are highly correlated from year to year, and topics in text may be correlated and dynamic. these dependencies are not naturally captured by the typical dirichlet-multinomial formulation. here, we leverage a logistic stick-breaking representation and recent innovations in p\'olya-gamma augmentation to reformulate the multinomial distribution in terms of latent variables with jointly gaussian likelihoods, enabling us to take advantage of a host of bayesian inference techniques for gaussian models with minimal overhead.","","2015-06-18","","['scott w. linderman', 'matthew j. johnson', 'ryan p. adams']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1158",1506.05936,"sampling constrained probability distributions using spherical   augmentation","stat.co stat.ml","statistical models with constrained probability distributions are abundant in machine learning. some examples include regression models with norm constraints (e.g., lasso), probit, many copula models, and latent dirichlet allocation (lda). bayesian inference involving probability distributions confined to constrained domains could be quite challenging for commonly used sampling algorithms. in this paper, we propose a novel augmentation technique that handles a wide range of constraints by mapping the constrained domain to a sphere in the augmented space. by moving freely on the surface of this sphere, sampling algorithms handle constraints implicitly and generate proposals that remain within boundaries when mapped back to the original space. our proposed method, called {spherical augmentation}, provides a mathematically natural and computationally efficient framework for sampling from constrained probability distributions. we show the advantages of our method over state-of-the-art sampling algorithms, such as exact hamiltonian monte carlo, using several examples including truncated gaussian distributions, bayesian lasso, bayesian bridge regression, reconstruction of quantized stationary gaussian process, and lda for topic modeling.","","2015-06-19","","['shiwei lan', 'babak shahbaba']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE
"1159",1506.06169,"a model-based approach for analog spatio-temporal dynamic forecasting","stat.me stat.ap","analog forecasting has been applied in a variety of fields for predicting future states of complex nonlinear systems that require flexible forecasting methods. past analog methods have almost exclu- sively been used in an empirical framework without the structure of a model-based approach. we propose a bayesian model framework for analog forecasting, building upon previous analog methods but accounting for parameter uncertainty. thus, unlike traditional analog forecasting methods, the use of bayesian modeling allows one to rigorously quantify uncertainty to obtain realistic posterior predictive distributions. the model is applied to the long-lead time forecasting of mid-may averaged soil moisture anomalies in iowa over a high-resolution grid of spatial locations. sea surface tem- perature (sst) is used to find past time periods with similar trajectories to the current pre-forecast period. the analog model is developed on projection coefficients from a basis expansion of the soil moisture and sst fields. separate models are constructed for locations falling in each iowa crop reporting district (crd) and the forecasting ability of the proposed model is compared against a variety of alternative methods and metrics.","","2015-06-19","2016-02-12","['patrick l. mcdermott', 'christopher k. wikle']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1160",1506.06268,"bayesian nonparametric modeling of higher order markov chains","stat.me","we consider the problem of flexible modeling of higher order markov chains when an upper bound on the order of the chain is known but the true order and nature of the serial dependence are unknown. we propose bayesian nonparametric methodology based on conditional tensor factorizations, which can characterize any transition probability with a specified maximal order. the methodology selects the important lags and captures higher order interactions among the lags, while also facilitating calculation of bayes factors for a variety of hypotheses of interest. we design efficient markov chain monte carlo algorithms for posterior computation, allowing for uncertainty in the set of important lags to be included and in the nature and order of the serial dependence. the methods are illustrated using simulation experiments and real world applications.","","2015-06-20","2015-10-20","['abhra sarkar', 'david b. dunson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1161",1506.06272,"aligning where to see and what to tell: image caption with region-based   attention and scene factorization","cs.cv cs.lg stat.ml","recent progress on automatic generation of image captions has shown that it is possible to describe the most salient information conveyed by images with accurate and meaningful sentences. in this paper, we propose an image caption system that exploits the parallel structures between images and sentences. in our model, the process of generating the next word, given the previously generated ones, is aligned with the visual perception experience where the attention shifting among the visual regions imposes a thread of visual ordering. this alignment characterizes the flow of ""abstract meaning"", encoding what is semantically shared by both the visual scene and the text description. our system also makes another novel modeling contribution by introducing scene-specific contexts that capture higher-level semantic information encoded in an image. the contexts adapt language models for word generation to specific scene types. we benchmark our system and contrast to published results on several popular datasets. we show that using either region-based attention or scene-specific contexts improves systems without those components. furthermore, combining these two modeling ingredients attains the state-of-the-art performance.","","2015-06-20","","['junqi jin', 'kun fu', 'runpeng cui', 'fei sha', 'changshui zhang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1162",1506.06707,"non-normal mixtures of experts","stat.me cs.lg stat.ml","mixture of experts (moe) is a popular framework for modeling heterogeneity in data for regression, classification and clustering. for continuous data which we consider here in the context of regression and cluster analysis, moe usually use normal experts, that is, expert components following the gaussian distribution. however, for a set of data containing a group or groups of observations with asymmetric behavior, heavy tails or atypical observations, the use of normal experts may be unsuitable and can unduly affect the fit of the moe model. in this paper, we introduce new non-normal mixture of experts (nnmoe) which can deal with these issues regarding possibly skewed, heavy-tailed data and with outliers. the proposed models are the skew-normal moe and the robust $t$ moe and skew $t$ moe, respectively named snmoe, tmoe and stmoe. we develop dedicated expectation-maximization (em) and expectation conditional maximization (ecm) algorithms to estimate the parameters of the proposed models by monotonically maximizing the observed data log-likelihood. we describe how the presented models can be used in prediction and in model-based clustering of regression data. numerical experiments carried out on simulated data show the effectiveness and the robustness of the proposed models in terms modeling non-linear regression functions as well as in model-based clustering. then, to show their usefulness for practical applications, the proposed models are applied to the real-world data of tone perception for musical data analysis, and the one of temperature anomalies for the analysis of climate change data.","","2015-06-22","2015-06-28","['faicel chamroukhi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1163",1506.07504,"objective variables for probabilistic revenue maximization in   second-price auctions with reserve","stat.ml cs.ai cs.gt cs.lg stat.ap","many online companies sell advertisement space in second-price auctions with reserve. in this paper, we develop a probabilistic method to learn a profitable strategy to set the reserve price. we use historical auction data with features to fit a predictor of the best reserve price. this problem is delicate - the structure of the auction is such that a reserve price set too high is much worse than a reserve price set too low. to address this we develop objective variables, a new framework for combining probabilistic modeling with optimal decision-making. objective variables are ""hallucinated observations"" that transform the revenue maximization task into a regularized maximum likelihood estimation problem, which we solve with an em algorithm. this framework enables a variety of prediction mechanisms to set the reserve price. as examples, we study objective variable methods with regression, kernelized regression, and neural networks on simulated and real data. our methods outperform previous approaches both in terms of scalability and profit.","","2015-06-24","","['maja r. rudolph', 'joseph g. ellis', 'david m. blei']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"1164",1506.07564,"spectral likelihood expansions for bayesian inference","stat.co","a spectral approach to bayesian inference is presented. it pursues the emulation of the posterior probability density. the starting point is a series expansion of the likelihood function in terms of orthogonal polynomials. from this spectral likelihood expansion all statistical quantities of interest can be calculated semi-analytically. the posterior is formally represented as the product of a reference density and a linear combination of polynomial basis functions. both the model evidence and the posterior moments are related to the expansion coefficients. this formulation avoids markov chain monte carlo simulation and allows one to make use of linear least squares instead. the pros and cons of spectral bayesian inference are discussed and demonstrated on the basis of simple applications from classical statistics and inverse modeling.","10.1016/j.jcp.2015.12.047","2015-06-24","2016-04-26","['joseph b. nagel', 'bruno sudret']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1165",1506.07836,"bayesian inference for the brown-resnick process, with an application to   extreme low temperatures","stat.me stat.ap","the brown-resnick max-stable process has proven to be well-suited for modeling extremes of complex environmental processes, but in many applications its likelihood function is intractable and inference must be based on a composite likelihood, thereby preventing the use of classical bayesian techniques. in this paper we exploit a case in which the full likelihood of a brown-resnick process can be calculated, using componentwise maxima and their partitions in terms of individual events, and we propose two new approaches to inference. the first estimates the partitions using declustering, while the second uses random partitions in a markov chain monte carlo algorithm. we use these approaches to construct a bayesian hierarchical model for extreme low temperatures in northern fennoscandia.","10.1214/16-aoas980","2015-06-25","2016-10-17","['emeric thibaud', 'juha aalto', 'daniel s. cooley', 'anthony c. davison', 'juha heikkinen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1166",1506.07947,"collaboratively learning preferences from ordinal data","cs.lg cs.it math.it stat.ml","in applications such as recommendation systems and revenue management, it is important to predict preferences on items that have not been seen by a user or predict outcomes of comparisons among those that have never been compared. a popular discrete choice model of multinomial logit model captures the structure of the hidden preferences with a low-rank matrix. in order to predict the preferences, we want to learn the underlying model from noisy observations of the low-rank matrix, collected as revealed preferences in various forms of ordinal data. a natural approach to learn such a model is to solve a convex relaxation of nuclear norm minimization. we present the convex relaxation approach in two contexts of interest: collaborative ranking and bundled choice modeling. in both cases, we show that the convex relaxation is minimax optimal. we prove an upper bound on the resulting error with finite samples, and provide a matching information-theoretic lower bound.","","2015-06-25","","['sewoong oh', 'kiran k. thekumparampil', 'jiaming xu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1167",1506.07959,"factorized asymptotic bayesian inference for factorial hidden markov   models","stat.ml","factorial hidden markov models (fhmms) are powerful tools of modeling sequential data. learning fhmms yields a challenging simultaneous model selection issue, i.e., selecting the number of multiple markov chains and the dimensionality of each chain. our main contribution is to address this model selection issue by extending factorized asymptotic bayesian (fab) inference to fhmms. first, we offer a better approximation of marginal log-likelihood than the previous fab inference. our key idea is to integrate out transition probabilities, yet still apply the laplace approximation to emission probabilities. second, we prove that if there are two very similar hidden states in an fhmm, i.e. one is redundant, then fab will almost surely shrink and eliminate one of them, making the model parsimonious. experimental results show that fab for fhmms significantly outperforms state-of-the-art nonparametric bayesian ifhmm and variational fhmm in model selection accuracy, with competitive held-out perplexity.","","2015-06-26","","['shaohua li', 'ryohei fujimaki', 'chunyan miao']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1168",1506.07997,"an efficient post-selection inference on high-order interaction models","stat.ml","finding statistically significant high-order interaction features in predictive modeling is important but challenging task. the difficulty lies in the fact that, for a recent applications with high-dimensional covariates, the number of possible high-order interaction features would be extremely large. identifying statistically significant features from such a huge pool of candidates would be highly challenging both in computational and statistical senses. to work with this problem, we consider a two stage algorithm where we first select a set of high-order interaction features by marginal screening, and then make statistical inferences on the regression model fitted only with the selected features. such statistical inferences are called post-selection inference (psi), and receiving an increasing attention in the literature. one of the seminal recent advancements in psi literature is the works by lee et al. where the authors presented an algorithmic framework for computing exact sampling distributions in psi. a main challenge when applying their approach to our high-order interaction models is to cope with the fact that psi in general depends not only on the selected features but also on the unselected features, making it hard to apply to our extremely high-dimensional high-order interaction models. the goal of this paper is to overcome this difficulty by introducing a novel efficient method for psi. our key idea is to exploit the underlying tree structure among high-order interaction features, and to develop a pruning method of the tree which enables us to quickly identify a group of unselected features that are guaranteed to have no influence on psi. the experimental results indicate that the proposed method allows us to reliably identify statistically significant high-order interaction features with reasonable computational cost.","","2015-06-26","","['s. suzumura', 'k. nakagawa', 'k. tsuda', 'i. takeuchi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1169",1506.08796,"longitudinal functional data analysis","stat.me","we consider analysis of dependent functional data that are correlated because of a longitudinal-based design: each subject is observed at repeated time visits and for each visit we record a functional variable. we propose a novel parsimonious modeling framework for the repeatedly observed functional variables that allows to extract low dimensional features. the proposed methodology accounts for the longitudinal design, is designed for the study of the dynamic behavior of the underlying process, and is computationally fast. theoretical properties of this framework are studied and numerical investigation confirms excellent behavior in finite samples. the proposed method is motivated by and applied to a diffusion tensor imaging study of multiple sclerosis. using shiny (chang et al., 2015) we implement interactive plots to help visualize longitudinal functional data as well as the various components and prediction obtained using the proposed method.","","2015-06-29","","['so young park', 'ana-maria staicu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1170",1506.09035,"bayesian model averaging in model-based clustering and density   estimation","stat.co","we propose bayesian model averaging (bma) as a method for postprocessing the results of model-based clustering. given a number of competing models, appropriate model summaries are averaged, using the posterior model probabilities, instead of being taken from a single ""best"" model. we demonstrate the use of bma in model-based clustering for a number of datasets. we show that bma provides a useful summary of the clustering of observations while taking model uncertainty into account. further, we show that bma in conjunction with model-based clustering gives a competitive method for density estimation in a multivariate setting. applying bma in the model-based context is fast and can give enhanced modeling performance.","","2015-06-30","","['niamh russell', 'thomas brendan murphy', 'adrian e raftery']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1171",1507.0022,"bigeometric organization of deep nets","stat.ml cs.lg","in this paper, we build an organization of high-dimensional datasets that cannot be cleanly embedded into a low-dimensional representation due to missing entries and a subset of the features being irrelevant to modeling functions of interest. our algorithm begins by defining coarse neighborhoods of the points and defining an expected empirical function value on these neighborhoods. we then generate new non-linear features with deep net representations tuned to model the approximate function, and re-organize the geometry of the points with respect to the new representation. finally, the points are locally z-scored to create an intrinsic geometric organization which is independent of the parameters of the deep net, a geometry designed to assure smoothness with respect to the empirical function. we examine this approach on data from the center for medicare and medicaid services hospital quality initiative, and generate an intrinsic low-dimensional organization of the hospitals that is smooth with respect to an expert driven function of quality.","","2015-07-01","","['alexander cloninger', 'ronald r. coifman', 'nicholas downing', 'harlan m. krumholz']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1172",1507.00672,"the elusive present: hidden past and future dependency and why we build   models","cond-mat.stat-mech cs.it math.ds math.it nlin.cd stat.ml","modeling a temporal process as if it is markovian assumes the present encodes all of the process's history. when this occurs, the present captures all of the dependency between past and future. we recently showed that if one randomly samples in the space of structured processes, this is almost never the case. so, how does the markov failure come about? that is, how do individual measurements fail to encode the past? and, how many are needed to capture dependencies between the past and future? here, we investigate how much information can be shared between the past and future, but not be reflected in the present. we quantify this elusive information, give explicit calculational methods, and draw out the consequences. the most important of which is that when the present hides past-future dependency we must move beyond sequence-based statistics and build state-based models.","10.1103/physreve.93.022143","2015-07-02","","['pooneh m. ara', 'ryan g. james', 'james p. crutchfield']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1173",1507.01242,"modeling for dynamic ordinal regression relationships: an application to   estimating maturity of rockfish in california","stat.ap","we develop a bayesian nonparametric framework for modeling ordinal regression relationships which evolve in discrete time. the motivating application involves a key problem in fisheries research on estimating dynamically evolving relationships between age, length and maturity, the latter recorded on an ordinal scale. the methodology builds from nonparametric mixture modeling for the joint stochastic mechanism of covariates and latent continuous responses. this approach yields highly flexible inference for ordinal regression functions while at the same time avoiding the computational challenges of parametric models. a novel dependent dirichlet process prior for time-dependent mixing distributions extends the model to the dynamic setting. the methodology is used for a detailed study of relationships between maturity, age, and length for chilipepper rockfish, using data collected over 15 years along the coast of california.","","2015-07-05","","['maria deyoreo', 'athanasios kottas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1174",1507.01825,"comparing biomarkers as trial level general surrogates","stat.me","an intermediate response measure that accurately predicts efficacy in a new setting can reduce trial cost and time to product licensure. in this paper, we define a trial level general surrogate as a trial level intermediate response that accurately predicts trial level clinical responses. methods for evaluating trial level general surrogates have been developed previously. many methods in the literature use trial level intermediate responses for prediction. however, all existing methods focus on surrogate evaluation and prediction in new settings, rather than comparison of candidate trial level surrogates, and few formalize the use of cross validation to quantify the expected prediction error. our proposed method uses bayesian non-parametric modeling and cross-validation to estimate the absolute prediction error for use in evaluating and comparing candidate trial level general surrogates. simulations show that our method performs well across a variety of scenarios. we use our method to evaluate and to compare candidate trial level general surrogates in several multi-national trials of a pentavalent rotavirus vaccine. we identify two immune measures that have potential value as trial level general surrogates and use the measures to predict efficacy in a trial with no clinical outcomes measured.","","2015-07-07","","['erin e. gabriel', 'michael j. daniels', 'm. elizabeth halloran']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1175",1507.02356,"intrinsic non-stationary covariance function for climate modeling","stat.ml cs.lg","designing a covariance function that represents the underlying correlation is a crucial step in modeling complex natural systems, such as climate models. geospatial datasets at a global scale usually suffer from non-stationarity and non-uniformly smooth spatial boundaries. a gaussian process regression using a non-stationary covariance function has shown promise for this task, as this covariance function adapts to the variable correlation structure of the underlying distribution. in this paper, we generalize the non-stationary covariance function to address the aforementioned global scale geospatial issues. we define this generalized covariance function as an intrinsic non-stationary covariance function, because it uses intrinsic statistics of the symmetric positive definite matrices to represent the characteristic length scale and, thereby, models the local stochastic process. experiments on a synthetic and real dataset of relative sea level changes across the world demonstrate improvements in the error metrics for the regression estimates using our newly proposed approach.","","2015-07-08","","['chintan a. dalal', 'vladimir pavlovic', 'robert e. kopp']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1176",1507.02537,"modeling asymptotically independent spatial extremes based on laplace   random fields","stat.me stat.ot","we tackle the modeling of threshold exceedances in asymptotically independent stochastic processes by constructions based on laplace random fields. these are defined as gaussian random fields scaled with a stochastic variable following an exponential distribution. this framework yields useful asymptotic properties while remaining statistically convenient. univariate distribution tails are of the half exponential type and are part of the limiting generalized pareto distributions for threshold exceedances. after normalizing marginal tail distributions in data, a standard laplace field can be used to capture spatial dependence among extremes. asymptotic properties of laplace fields are explored and compared to the classical framework of asymptotic dependence. multivariate joint tail decay rates for laplace fields are slower than for gaussian fields with the same covariance structure; hence they provide more conservative estimates of very extreme joint risks while maintaining asymptotic independence. statistical inference is illustrated on extreme wind gusts in the netherlands where a comparison to the gaussian dependence model shows a better goodness-of-fit in terms of akaike's criterion. in this specific application we fit the well-adapted weibull distribution as univariate tail model, such that the normalization of univariate tail distributions can be done through a simple power transformation of data.","10.1016/j.spasta.2016.01.001","2015-07-09","2016-03-08","['thomas opitz']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE
"1177",1507.02801,"adaptive mixtures of factor analyzers","stat.ml cs.it cs.lg math.it","a mixture of factor analyzers is a semi-parametric density estimator that generalizes the well-known mixtures of gaussians model by allowing each gaussian in the mixture to be represented in a different lower-dimensional manifold. this paper presents a robust and parsimonious model selection algorithm for training a mixture of factor analyzers, carrying out simultaneous clustering and locally linear, globally nonlinear dimensionality reduction. permitting different number of factors per mixture component, the algorithm adapts the model complexity to the data complexity. we compare the proposed algorithm with related automatic model selection algorithms on a number of benchmarks. the results indicate the effectiveness of this fast and robust approach in clustering, manifold learning and class-conditional modeling.","","2015-07-10","2015-10-22","['heysem kaya', 'albert ali salah']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1178",1507.04208,"combinatorial cascading bandits","cs.lg stat.ml","we propose combinatorial cascading bandits, a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one. the weights of the items are binary, stochastic, and drawn independently of each other. the agent observes the index of the first chosen item whose weight is zero. this observation model arises in network routing, for instance, where the learning agent may only observe the first link in the routing path which is down, and blocks the path. we propose a ucb-like algorithm for solving our problems, combcascade; and prove gap-dependent and gap-free upper bounds on its $n$-step regret. our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting, a non-linear reward function and partial observability. we evaluate combcascade on two real-world problems and show that it performs well even when our modeling assumptions are violated. we also demonstrate that our setting requires a new learning algorithm.","","2015-07-15","2015-11-17","['branislav kveton', 'zheng wen', 'azin ashkan', 'csaba szepesvari']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1179",1507.04777,"sparse probit linear mixed model","stat.ml cs.lg","linear mixed models (lmms) are important tools in statistical genetics. when used for feature selection, they allow to find a sparse set of genetic traits that best predict a continuous phenotype of interest, while simultaneously correcting for various confounding factors such as age, ethnicity and population structure. formulated as models for linear regression, lmms have been restricted to continuous phenotypes. we introduce the sparse probit linear mixed model (probit-lmm), where we generalize the lmm modeling paradigm to binary phenotypes. as a technical challenge, the model no longer possesses a closed-form likelihood function. in this paper, we present a scalable approximate inference algorithm that lets us fit the model to high-dimensional data sets. we show on three real-world examples from different domains that in the setup of binary labels, our algorithm leads to better prediction accuracies and also selects features which show less correlation with the confounding factors.","10.1007/s10994-017-5652-6","2015-07-16","2017-07-17","['stephan mandt', 'florian wenzel', 'shinichi nakajima', 'john p. cunningham', 'christoph lippert', 'marius kloft']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1180",1507.05135,"review of functional data analysis","stat.me","with the advance of modern technology, more and more data are being recorded continuously during a time interval or intermittently at several discrete time points. they are both examples of ""functional data"", which have become a prevailing type of data. functional data analysis (fda) encompasses the statistical methodology for such data. broadly interpreted, fda deals with the analysis and theory of data that are in the form of functions. this paper provides an overview of fda, starting with simple statistical notions such as mean and covariance functions, then covering some core techniques, the most popular of which is functional principal component analysis (fpca). fpca is an important dimension reduction tool and in sparse data situations can be used to impute functional data that are sparsely observed. other dimension reduction approaches are also discussed. in addition, we review another core technique, functional linear regression, as well as clustering and classification of functional data. beyond linear and single or multiple index methods we touch upon a few nonlinear approaches that are promising for certain applications. they include additive and other nonlinear functional regression models, such as time warping, manifold learning, and dynamic modeling with empirical differential equations. the paper concludes with a brief discussion of future directions.","","2015-07-17","","['jane-ling wang', 'jeng-min chiou', 'hans-georg mueller']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1181",1507.05253,"the population posterior and bayesian inference on streams","stat.ml","many modern data analysis problems involve inferences from streaming data. however, streaming data is not easily amenable to the standard probabilistic modeling approaches, which assume that we condition on finite data. we develop population variational bayes, a new approach for using bayesian modeling to analyze streams of data. it approximates a new type of distribution, the population posterior, which combines the notion of a population distribution of the data with bayesian inference in a probabilistic model. we study our method with latent dirichlet allocation and dirichlet process mixtures on several large-scale data sets.","","2015-07-19","2015-07-21","['james mcinerney', 'rajesh ranganath', 'david m. blei']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1182",1507.05899,"sparsity in multivariate extremes with applications to anomaly detection","stat.ml","capturing the dependence structure of multivariate extreme events is a major concern in many fields involving the management of risks stemming from multiple sources, e.g. portfolio monitoring, insurance, environmental risk management and anomaly detection. one convenient (non-parametric) characterization of extremal dependence in the framework of multivariate extreme value theory (evt) is the angular measure, which provides direct information about the probable 'directions' of extremes, that is, the relative contribution of each feature/coordinate of the 'largest' observations. modeling the angular measure in high dimensional problems is a major challenge for the multivariate analysis of rare events. the present paper proposes a novel methodology aiming at exhibiting a sparsity pattern within the dependence structure of extremes. this is done by estimating the amount of mass spread by the angular measure on representative sets of directions, corresponding to specific sub-cones of $r^d\_+$. this dimension reduction technique paves the way towards scaling up existing multivariate evt methods. beyond a non-asymptotic study providing a theoretical validity framework for our method, we propose as a direct application a --first-- anomaly detection algorithm based on multivariate evt. this algorithm builds a sparse 'normal profile' of extreme behaviours, to be confronted with new (possibly abnormal) extreme observations. illustrative experimental results provide strong empirical evidence of the relevance of our approach.","","2015-07-21","2016-03-14","['nicolas goix', 'anne sabourin', 'stéphan clémençon']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1183",1507.06352,"co-clustering of nonsmooth graphons","math.st cs.si stat.th","performance bounds are given for exploratory co-clustering/ blockmodeling of bipartite graph data, where we assume the rows and columns of the data matrix are samples from an arbitrary population. this is equivalent to assuming that the data is generated from a nonsmooth graphon. it is shown that co-clusters found by any method can be extended to the row and column populations, or equivalently that the estimated blockmodel approximates a blocked version of the generative graphon, with estimation error bounded by $o_p(n^{-1/2})$. analogous performance bounds are also given for degree-corrected blockmodels and random dot product graphs, with error rates depending on the dimensionality of the latent variable space.","","2015-07-22","","['david choi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1184",1507.0671,"nonparametric bayesian regression on manifolds via brownian motion","math.st stat.ap stat.th","this paper proposes a novel framework for manifold-valued regression and establishes its consistency as well as its contraction rate. it assumes a predictor with values in the interval $[0,1]$ and response with values in a compact riemannian manifold $m$. this setting is useful for applications such as modeling dynamic scenes or shape deformations, where the visual scene or the deformed objects can be modeled by a manifold. the proposed framework is nonparametric and uses the heat kernel (and its associated brownian motion) on manifolds as an averaging procedure. it directly generalizes the use of the gaussian kernel (as a natural model of additive noise) in vector-valued regression problems. in order to avoid explicit dependence on estimates of the heat kernel, we follow a bayesian setting, where brownian motion on $m$ induces a prior distribution on the space of continuous functions $c([0,1], m)$. for the case of discretized brownian motion, we establish the consistency of the posterior distribution in terms of the $l_{q}$ distances for any $1 \leq q < \infty$. most importantly, we establish contraction rate of order $o(n^{-1/4+\epsilon})$ for any fixed $\epsilon>0$, where $n$ is the number of observations. for the continuous brownian motion we establish weak consistency.","","2015-07-23","","['xu wang', 'gilad lerman']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE
"1185",1507.06947,"fast and accurate recurrent neural network acoustic models for speech   recognition","cs.cl cs.lg cs.ne stat.ml","we have recently shown that deep long short-term memory (lstm) recurrent neural networks (rnns) outperform feed forward deep neural networks (dnns) as acoustic models for speech recognition. more recently, we have shown that the performance of sequence trained context dependent (cd) hidden markov model (hmm) acoustic models using such lstm rnns can be equaled by sequence trained phone models initialized with connectionist temporal classification (ctc). in this paper, we present techniques that further improve performance of lstm rnn acoustic models for large vocabulary speech recognition. we show that frame stacking and reduced frame rate lead to more accurate models and faster decoding. cd phone modeling leads to further improvements. we also present initial results for lstm rnn models outputting words directly.","","2015-07-24","","['haşim sak', 'andrew senior', 'kanishka rao', 'françoise beaufays']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1186",1507.0707,"the extremal index and the maximum of a dependent stationary pulse load   process observed above a high threshold","stat.ap stat.co","observing a load process above high thresholds, modeling it as a pulse process with random occurrence times and magnitudes, and extrapolating life-time maximum or design loads from the data is a common task in structural reliability analyses. in this paper, we consider a stationary live load sequence that arrive according to a dependent point process and allow for a weakened mixing-type dependence in the load pulse magnitudes that asymptotically decreases to zero with increasing separation in the sequence. inclusion of dependence in the model eliminates the unnecessary conservatism introduced by the i.i.d. (independent and identically distributed) assumption often made in determining maximum live load distribution. the scale of fluctuation of the loading process is used to identify clusters of exceedances above high thresholds which in turn is used to estimate the extremal index of the process. a bayesian updating of the empirical distribution function, derived from the distribution of order statistics in a dependent stationary series, is performed. the pulse arrival instants are modeled as a cox process goverened by a stationary lognormal intensity. an illustrative example utilizes in-service peak strain data from ambient traffic collected on a high volume highway bridge, and analyzes the asymptotic behavior of the maximum load.","10.1016/j.strusafe.2006.05.001","2015-07-25","","['baidurya bhattacharya']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE
"1187",1507.07304,"the two ignored components of random variation","math.st stat.th","a random phenomenon may have two sources of random variation: an unstable identity and a set of external variation-generating factors. when only a single source is active, two mutually exclusive extreme scenarios may ensue that result in the exponential or the normal, the only truly univariate distributions. all other supposedly univariate random variation observed in nature is truly bivariate. in this article, we elaborate on this new paradigm for random variation and develop a general bivariate distribution to reflect it. it is shown that numerous current univariate distributions are special cases of an approximation to the new bivariate distribution. we first show that the exponential and the normal are special cases of a single distribution represented by a response modeling methodology model. we then develop a general bivariate distribution commensurate with the new paradigm, its properties are discussed and its moments developed. an approximating assumption results in a univariate general distribution that is shown to include as exact special cases widely used distributions like generalized gamma, log-normal, f, t, and cauchy. compound distributions and their relationship to the new paradigm are addressed. empirical observations that comply with predictions derived from the new paradigm corroborate its scientific validity.","","2015-07-27","","['haim shore']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1188",1507.07664,"time-varying network models","math.st stat.th","we introduce the exchangeable rewiring process for modeling time-varying networks. the process fulfills fundamental mathematical and statistical properties and can be easily constructed from the novel operation of random rewiring. we derive basic properties of the model, including consistency under subsampling, exchangeability, and the feller property. a reversible sub-family related to the erd\h{o}s-r\'{e}nyi model arises as a special case.","10.3150/14-bej617","2015-07-28","","['harry crane']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1189",1507.08396,"tag-weighted topic model for large-scale semi-structured documents","cs.cl cs.ir cs.lg stat.ml","to date, there have been massive semi-structured documents (ssds) during the evolution of the internet. these ssds contain both unstructured features (e.g., plain text) and metadata (e.g., tags). most previous works focused on modeling the unstructured text, and recently, some other methods have been proposed to model the unstructured text with specific tags. to build a general model for ssds remains an important problem in terms of both model fitness and efficiency. we propose a novel method to model the ssds by a so-called tag-weighted topic model (twtm). twtm is a framework that leverages both the tags and words information, not only to learn the document-topic and topic-word distributions, but also to infer the tag-topic distributions for text mining tasks. we present an efficient variational inference method with an em algorithm for estimating the model parameters. meanwhile, we propose three large-scale solutions for our model under the mapreduce distributed computing platform for modeling large-scale ssds. the experimental results show the effectiveness, efficiency and the robustness by comparing our model with the state-of-the-art methods in document modeling, tags prediction and text classification. we also show the performance of the three distributed solutions in terms of time and accuracy on document modeling.","","2015-07-30","","['shuangyin li', 'jiefei li', 'guan huang', 'ruiyang tan', 'rong pan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1190",1507.08401,"capturing multivariate spatial dependence: model, estimate and then   predict","stat.me","physical processes rarely occur in isolation, rather they influence and interact with one another. thus, there is great benefit in modeling potential dependence between both spatial locations and different processes. it is the interaction between these two dependencies that is the focus of genton and kleiber's paper under discussion. we see the problem of ensuring that any multivariate spatial covariance matrix is nonnegative definite as important, but we also see it as a means to an end. that ""end"" is solving the scientific problem of predicting a multivariate field. [arxiv:1507.08017].","10.1214/15-sts517","2015-07-30","","['noel cressie', 'sandy burden', 'walter davis', 'pavel n. krivitsky', 'payam mokhtarian', 'thomas suesse', 'andrew zammit-mangion']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1191",1507.08436,"robustness to outliers in location-scale parameter model using   log-regularly varying distributions","math.st stat.th","estimating the location and scale parameters is common in statistics, using, for instance, the well-known sample mean and standard deviation. however, inference can be contaminated by the presence of outliers if modeling is done with light-tailed distributions such as the normal distribution. in this paper, we study robustness to outliers in location-scale parameter models using both the bayesian and frequentist approaches. we find sufficient conditions (e.g., on tail behavior of the model) to obtain whole robustness to outliers, in the sense that the impact of the outliers gradually decreases to nothing as the conflict grows infinitely. to this end, we introduce the family of log-pareto-tailed symmetric distributions that belongs to the larger family of log-regularly varying distributions.","10.1214/15-aos1316","2015-07-30","","['alain desgagné']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1192",1507.08613,"local likelihood estimation for covariance functions with   spatially-varying parameters: the convospat package for r","stat.co","in spite of the interest in and appeal of convolution-based approaches for nonstationary spatial modeling, off-the-shelf software for model fitting does not as of yet exist. convolution-based models are highly flexible yet notoriously difficult to fit, even with relatively small data sets. the general lack of pre-packaged options for model fitting makes it difficult to compare new methodology in nonstationary modeling with other existing methods, and as a result most new models are simply compared to stationary models. using a convolution-based approach, we present a new nonstationary covariance function for spatial gaussian process models that allows for efficient computing in two ways: first, by representing the spatially-varying parameters via a discrete mixture or ""mixture component"" model, and second, by estimating the mixture component parameters through a local likelihood approach. in order to make computation for a convolution-based nonstationary spatial model readily available, this paper also presents and describes the convospat package for r. the nonstationary model is fit to both a synthetic data set and a real data application involving annual precipitation to demonstrate the capabilities of the package.","","2015-07-30","2017-02-03","['mark d. risser', 'catherine a. calder']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1193",1507.08727,"large scale signal detection: a unified perspective","math.st stat.me stat.th","there is an overwhelmingly large literature and algorithms already available on `large scale inference problems' based on different modeling techniques and cultures. our primary goal in this paper is \emph{not to add one more new methodology} to the existing toolbox but instead (a) to clarify the mystery how these different simultaneous inference methods are \emph{connected}, (b) to provide an alternative more intuitive derivation of the formulas that leads to \emph{simpler} expressions, and (c) to develop a \emph{unified} algorithm for practitioners. a detailed discussion on representation, estimation, inference, and model selection is given. applications to a variety of real and simulated datasets show promise. we end with several future research directions.","","2015-07-30","2017-03-31","['subhadeep mukhopadhyay']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1194",1508.00317,"time-series modeling with undecimated fully convolutional neural   networks","stat.ml cs.lg","we present a new convolutional neural network-based time-series model. typical convolutional neural network (cnn) architectures rely on the use of max-pooling operators in between layers, which leads to reduced resolution at the top layers. instead, in this work we consider a fully convolutional network (fcn) architecture that uses causal filtering operations, and allows for the rate of the output signal to be the same as that of the input signal. we furthermore propose an undecimated version of the fcn, which we refer to as the undecimated fully convolutional neural network (ufcnn), and is motivated by the undecimated wavelet transform. our experimental results verify that using the undecimated version of the fcn is necessary in order to allow for effective time-series modeling. the ufcnn has several advantages compared to other time-series models such as the recurrent neural network (rnn) and long short-term memory (lstm), since it does not suffer from either the vanishing or exploding gradients problems, and is therefore easier to train. convolution operations can also be implemented more efficiently compared to the recursion that is involved in rnn-based models. we evaluate the performance of our model in a synthetic target tracking task using bearing only measurements generated from a state-space model, a probabilistic modeling of polyphonic music sequences problem, and a high frequency trading task using a time-series of ask/bid quotes and their corresponding volumes. our experimental results using synthetic and real datasets verify the significant advantages of the ufcnn compared to the rnn and lstm baselines.","","2015-08-03","","['roni mittelman']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1195",1508.00635,"bayesian mixtures of spatial spline regressions","stat.me cs.lg stat.co stat.ml","this work relates the framework of model-based clustering for spatial functional data where the data are surfaces. we first introduce a bayesian spatial spline regression model with mixed-effects (bssr) for modeling spatial function data. the bssr model is based on nodal basis functions for spatial regression and accommodates both common mean behavior for the data through a fixed-effects part, and variability inter-individuals thanks to a random-effects part. then, in order to model populations of spatial functional data issued from heterogeneous groups, we integrate the bssr model into a mixture framework. the resulting model is a bayesian mixture of spatial spline regressions with mixed-effects (bmssr) used for density estimation and model-based surface clustering. the models, through their bayesian formulation, allow to integrate possible prior knowledge on the data structure and constitute a good alternative to recent mixture of spatial spline regressions model estimated in a maximum likelihood framework via the expectation-maximization (em) algorithm. the bayesian model inference is performed by markov chain monte carlo (mcmc) sampling. we derive two gibbs sampler to infer the bssr and the bmssr models and apply them on simulated surfaces and a real problem of handwritten digit recognition using the mnist data set. the obtained results highlight the potential benefit of the proposed bayesian approaches for modeling surfaces possibly dispersed in particular in clusters.","","2015-08-03","","['faicel chamroukhi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE
"1196",1508.01023,"a review of heterogeneous data mining for brain disorders","cs.lg cs.ce cs.db q-bio.nc stat.ap","with rapid advances in neuroimaging techniques, the research on brain disorder identification has become an emerging area in the data mining community. brain disorder data poses many unique challenges for data mining research. for example, the raw data generated by neuroimaging experiments is in tensor representations, with typical characteristics of high dimensionality, structural complexity and nonlinear separability. furthermore, brain connectivity networks can be constructed from the tensor data, embedding subtle interactions between brain regions. other clinical measures are usually available reflecting the disease status from different perspectives. it is expected that integrating complementary information in the tensor data and the brain network data, and incorporating other clinical parameters will be potentially transformative for investigating disease mechanisms and for informing therapeutic interventions. many research efforts have been devoted to this area. they have achieved great success in various applications, such as tensor-based modeling, subgraph pattern mining, multi-view feature analysis. in this paper, we review some recent data mining methods that are used for analyzing brain disorders.","","2015-08-05","","['bokai cao', 'xiangnan kong', 'philip s. yu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1197",1508.0128,"empirical bayesian analysis of simultaneous changepoints in multiple   data sequences","stat.me stat.ap stat.co","copy number variations in cancer cells and volatility fluctuations in stock prices are commonly manifested as changepoints occurring at the same positions across related data sequences. we introduce a bayesian modeling framework, basic, that employs a changepoint prior to capture the co-occurrence tendency in data of this type. we design efficient algorithms to sample from and maximize over the basic changepoint posterior and develop a monte carlo expectation-maximization procedure to select prior hyperparameters in an empirical bayes fashion. we use the resulting basic framework to analyze dna copy number variations in the nci-60 cancer cell lines and to identify important events that affected the price volatility of s&p 500 stocks from 2000 to 2009.","","2015-08-06","2017-04-13","['zhou fan', 'lester mackey']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"1198",1508.0134,"universal approximation of edge density in large graphs","cs.si cs.db stat.ml","in this paper, we present a novel way to summarize the structure of large graphs, based on non-parametric estimation of edge density in directed multigraphs. following coclustering approach, we use a clustering of the vertices, with a piecewise constant estimation of the density of the edges across the clusters, and address the problem of automatically and reliably inferring the number of clusters, which is the granularity of the coclustering. we use a model selection technique with data-dependent prior and obtain an exact evaluation criterion for the posterior probability of edge density estimation models. we demonstrate, both theoretically and empirically, that our data-dependent modeling technique is consistent, resilient to noise, valid non asymptotically and asymptotically behaves as an universal approximator of the true edge density in directed multigraphs. we evaluate our method using artificial graphs and present its practical interest on real world graphs. the method is both robust and scalable. it is able to extract insightful patterns in the unsupervised learning setting and to provide state of the art accuracy when used as a preparation step for supervised learning.","","2015-08-06","","['marc boullé']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1199",1508.01596,"sublinear partition estimation","stat.ml cs.lg","the output scores of a neural network classifier are converted to probabilities via normalizing over the scores of all competing categories. computing this partition function, $z$, is then linear in the number of categories, which is problematic as real-world problem sets continue to grow in categorical types, such as in visual object recognition or discriminative language modeling. we propose three approaches for sublinear estimation of the partition function, based on approximate nearest neighbor search and kernel feature maps and compare the performance of the proposed approaches empirically.","","2015-08-06","","['pushpendre rastogi', 'benjamin van durme']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1200",1508.01922,"the discrete dantzig selector: estimating sparse linear models via mixed   integer linear optimization","stat.me math.oc math.st stat.co stat.ml stat.th","we propose a novel high-dimensional linear regression estimator: the discrete dantzig selector, which minimizes the number of nonzero regression coefficients subject to a budget on the maximal absolute correlation between the features and residuals. motivated by the significant advances in integer optimization over the past 10-15 years, we present a mixed integer linear optimization (milo) approach to obtain certifiably optimal global solutions to this nonconvex optimization problem. the current state of algorithmics in integer optimization makes our proposal substantially more computationally attractive than the least squares subset selection framework based on integer quadratic optimization, recently proposed in [8] and the continuous nonconvex quadratic optimization framework of [33]. we propose new discrete first-order methods, which when paired with state-of-the-art milo solvers, lead to good solutions for the discrete dantzig selector problem for a given computational budget. we illustrate that our integrated approach provides globally optimal solutions in significantly shorter computation times, when compared to off-the-shelf milo solvers. we demonstrate both theoretically and empirically that in a wide range of regimes the statistical properties of the discrete dantzig selector are superior to those of popular $\ell_{1}$-based approaches. we illustrate that our approach can handle problem instances with p = 10,000 features with certifiable optimality making it a highly scalable combinatorial variable selection approach in sparse linear modeling.","","2015-08-08","2017-01-19","['rahul mazumder', 'peter radchenko']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE
"1201",1508.02201,"extrinsic local regression on manifold-valued data","math.st stat.th","we propose an extrinsic regression framework for modeling data with manifold valued responses and euclidean predictors. regression with manifold responses has wide applications in shape analysis, neuroscience, medical imaging and many other areas. our approach embeds the manifold where the responses lie onto a higher dimensional euclidean space, obtains a local regression estimate in that space, and then projects this estimate back onto the image of the manifold. outside the regression setting both intrinsic and extrinsic approaches have been proposed for modeling i.i.d manifold-valued data. however, to our knowledge our work is the first to take an extrinsic approach to the regression problem. the proposed extrinsic regression framework is general, computationally efficient and theoretically appealing. asymptotic distributions and convergence rates of the extrinsic regression estimates are derived and a large class of examples are considered indicating the wide applicability of our approach.","","2015-08-10","","['lizhen lin', 'brian st. thomas', 'hongtu zhu', 'david b. dunson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1202",1508.03103,"adaptive trait evolution in random environment","stat.ap","current phylogenetic comparative methods generally employ the ornstein-uhlenbeck(ou) process for modeling trait evolution. being able of tracking the optimum of a trait within a group of related species, the ou process provides information about the stabilizing selection where the population mean adopts a particular trait value. the optima of a trait may follow certain stochastic dynamics along the evolutionary history. in this paper, we extend the current framework by adopting a rate of evolution which behave according to pertinent stochastic dynamics. the novel model is applied to analyze about 225 datasets collected from the existing literature. results validate that the new framework provides a better fit for the majority of these datasets.","","2015-08-12","","['dwueng-chwuan jhwueng', 'vasileios maroulas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1203",1508.03662,"comparison of uncertainty of two precipitation prediction models","physics.ao-ph physics.geo-ph stat.ap","meteorological inputs are an important part of subsurface flow and transport modeling. the choice of source for meteorological data used as inputs has significant impacts on the results of subsurface flow and transport studies. one method to obtain the meteorological data required for flow and transport studies is the use of weather generating models. this paper compares the difference in performance of two weather generating models at technical area 54 of los alamos national lab. technical area 54 is contains several waste pits for low-level radioactive waste and is the site for subsurface flow and transport studies. this makes the comparison of the performance of the two weather generators at this site particularly valuable.","","2015-08-14","","['stephen shield', 'zhenxue dai']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1204",1508.03747,"nonparametric distributed learning architecture for big data: algorithm   and applications","stat.ap stat.co stat.me","dramatic increases in the size and complexity of modern datasets have made traditional ""centralized"" statistical inference prohibitive. in addition to computational challenges associated with big data learning, the presence of numerous data types (e.g. discrete, continuous, categorical, etc.) makes automation and scalability difficult. a question of immediate concern is how to design a data-intensive statistical inference architecture without changing the basic statistical modeling principles developed for ""small"" data over the last century. to address this problem, we present metalp, a flexible, distributed statistical modeling framework.","","2015-08-15","2018-02-26","['scott bruce', 'zeda li', 'hsiang-chieh yang', 'subhadeep mukhopadhyay']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"1205",1508.03821,"vertical modeling: analysis of competing risks data with a cure   proportion","stat.me","in this paper, we extend the vertical modeling approach for the analysis of survival data with competing risks to incorporate a cured fraction in the population, that is, a proportion of the population for which none of the competing events can occur. the proposed method has three components: the proportion of cure, the risk of failure, irrespective of the cause, and the relative risk of a certain cause of failure, given a failure occurred. covariates may affect each of these components. an appealing aspect of the method is that it is a natural extension to competing risks of the semi-parametric mixture cure model in ordinary survival analysis; thus, causes of failure are assigned only if a failure occurs. this contrasts with the existing mixture cure model for competing risks of larson and dinse, which conditions at the onset on the future status presumably attained. regression parameter estimates are obtained using an em-algorithm. the performance of the estimators is evaluated in a simulation study. the method is illustrated using a melanoma cancer data set.","","2015-08-16","","['m. a. nicolaie', 'j. m. g. taylor', 'c. legrand']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1206",1508.04211,"scalable bayesian non-negative tensor factorization for massive count   data","stat.ml cs.lg","we present a bayesian non-negative tensor factorization model for count-valued tensor data, and develop scalable inference algorithms (both batch and online) for dealing with massive tensors. our generative model can handle overdispersed counts as well as infer the rank of the decomposition. moreover, leveraging a reparameterization of the poisson distribution as a multinomial facilitates conjugacy in the model and enables simple and efficient gibbs sampling and variational bayes (vb) inference updates, with a computational cost that only depends on the number of nonzeros in the tensor. the model also provides a nice interpretability for the factors; in our model, each factor corresponds to a ""topic"". we develop a set of online inference algorithms that allow further scaling up the model to massive tensors, for which batch inference methods may be infeasible. we apply our framework on diverse real-world applications, such as \emph{multiway} topic modeling on a scientific publications database, analyzing a political science data set, and analyzing a massive household transactions data set.","","2015-08-18","","['changwei hu', 'piyush rai', 'changyou chen', 'matthew harding', 'lawrence carin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1207",1508.04319,"non-stationary gaussian process regression with hamiltonian monte carlo","stat.ml","we present a novel approach for fully non-stationary gaussian process regression (gpr), where all three key parameters -- noise variance, signal variance and lengthscale -- can be simultaneously input-dependent. we develop gradient-based inference methods to learn the unknown function and the non-stationary model parameters, without requiring any model approximations. we propose to infer full parameter posterior with hamiltonian monte carlo (hmc), which conveniently extends the analytical gradient-based gpr learning by guiding the sampling with model gradients. we also learn the map solution from the posterior by gradient ascent. in experiments on several synthetic datasets and in modelling of temporal gene expression, the nonstationary gpr is shown to be necessary for modeling realistic input-dependent dynamics, while it performs comparably to conventional stationary or previous non-stationary gpr models otherwise.","","2015-08-18","","['markus heinonen', 'henrik mannerström', 'juho rousu', 'samuel kaski', 'harri lähdesmäki']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1208",1508.04841,"unimodal clustering using isotonic regression: iso-split","stat.me","a limitation of many clustering algorithms is the requirement to tune adjustable parameters for each application or even for each dataset. some techniques require an \emph{a priori} estimate of the number of clusters while density-based techniques usually require a scale parameter. other parametric methods, such as mixture modeling, make assumptions about the underlying cluster distributions. here we introduce a non-parametric clustering method that does not involve tunable parameters and only assumes that clusters are unimodal, in the sense that they have a single point of maximal density when projected onto any line, and that clusters are separated from one another by a separating hyperplane of relatively lower density. the technique uses a non-parametric variant of hartigan's dip statistic using isotonic regression as the kernel operation repeated at every iteration. we compare the method against k-means++, dbscan, and gaussian mixture methods and show in simulations that it performs better than these standard methods in many situations. the algorithm is suited for low-dimensional datasets with a large number of observations, and was motivated by the problem of ""spike sorting"" in neural electrical recordings. source code is freely available.","","2015-08-19","2016-05-18","['jeremy f. magland', 'alex h. barnett']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1209",1508.05412,"joint modeling of longitudinal drug using pattern and time to first   relapse in cocaine dependence treatment data","stat.ap","an important endpoint variable in a cocaine rehabilitation study is the time to first relapse of a patient after the treatment. we propose a joint modeling approach based on functional data analysis to study the relationship between the baseline longitudinal cocaine-use pattern and the interval censored time to first relapse. for the baseline cocaine-use pattern, we consider both self-reported cocaine-use amount trajectories and dichotomized use trajectories. variations within the generalized longitudinal trajectories are modeled through a latent gaussian process, which is characterized by a few leading functional principal components. the association between the baseline longitudinal trajectories and the time to first relapse is built upon the latent principal component scores. the mean and the eigenfunctions of the latent gaussian process as well as the hazard function of time to first relapse are modeled nonparametrically using penalized splines, and the parameters in the joint model are estimated by a monte carlo em algorithm based on metropolis-hastings steps. an akaike information criterion (aic) based on effective degrees of freedom is proposed to choose the tuning parameters, and a modified empirical information is proposed to estimate the variance-covariance matrix of the estimators.","10.1214/15-aoas852","2015-08-21","2015-11-17","['jun ye', 'yehua li', 'yongtao guan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1210",1508.05571,"robust sparse gaussian graphical modeling","stat.me","gaussian graphical modeling has been widely used to explore various network structures, such as gene regulatory networks and social networks. we often use a penalized maximum likelihood approach with the $l_1$ penalty for learning a high-dimensional graphical model. however, the penalized maximum likelihood procedure is sensitive to outliers. to overcome this problem, we introduce a robust estimation procedure based on the $\gamma$-divergence. the proposed method has a redescending property, which is known as a desirable property in robust statistics. the parameter estimation procedure is constructed using the majorize-minimization algorithm, which guarantees that the objective function monotonically decreases at each iteration. extensive simulation studies showed that our procedure performed much better than the existing methods, in particular, when the contamination ratio was large. two real data analyses were carried out to illustrate the usefulness of our proposed procedure.","","2015-08-23","2017-06-12","['kei hirose', 'hironori fujisawa', 'jun sese']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1211",1508.05973,"a review of nonparametric hypothesis tests of isotropy properties in   spatial data","stat.me stat.ot","an important aspect of modeling spatially-referenced data is appropriately specifying the covariance function of the random field. a practitioner working with spatial data is presented a number of choices regarding the structure of the dependence between observations. one of these choices is determining whether or not an isotropic covariance function is appropriate. isotropy implies that spatial dependence does not depend on the direction of the spatial separation between sampling locations. misspecification of isotropy properties (directional dependence) can lead to misleading inferences, e.g., inaccurate predictions and parameter estimates. a researcher may use graphical diagnostics, such as directional sample variograms, to decide whether the assumption of isotropy is reasonable. these graphical techniques can be difficult to assess, open to subjective interpretations, and misleading. hypothesis tests of the assumption of isotropy may be more desirable. to this end, a number of tests of directional dependence have been developed using both the spatial and spectral representations of random fields. we provide an overview of nonparametric methods available to test the hypotheses of isotropy and symmetry in spatial data. we summarize test properties, discuss important considerations and recommendations in choosing and implementing a test, compare several of the methods via a simulation study, and propose a number of open research questions. several of the reviewed methods can be implemented in r using our package sptest, available on cran.","","2015-08-24","2015-11-04","['zachary d. weller', 'jennifer a. hoeting']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE
"1212",1508.06303,"restricted indian buffet processes","stat.me","latent feature models are a powerful tool for modeling data with globally-shared features. nonparametric exchangeable models such as the indian buffet process offer modeling flexibility by letting the number of latent features be unbounded. however, current models impose implicit distributions over the number of latent features per data point, and these implicit distributions may not match our knowledge about the data. in this paper, we demonstrate how the restricted indian buffet process circumvents this restriction, allowing arbitrary distributions over the number of features in an observation. we discuss several alternative constructions of the model and use the insights gained to develop markov chain monte carlo and variational methods for simulation and posterior inference.","","2015-08-25","","['finale doshi-velez', 'sinead a. williamson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1213",1508.06446,"nested hierarchical dirichlet processes for multi-level non-parametric   admixture modeling","stat.ml cs.lg","dirichlet process(dp) is a bayesian non-parametric prior for infinite mixture modeling, where the number of mixture components grows with the number of data items. the hierarchical dirichlet process (hdp), is an extension of dp for grouped data, often used for non-parametric topic modeling, where each group is a mixture over shared mixture densities. the nested dirichlet process (ndp), on the other hand, is an extension of the dp for learning group level distributions from data, simultaneously clustering the groups. it allows group level distributions to be shared across groups in a non-parametric setting, leading to a non-parametric mixture of mixtures. the ncrf extends the ndp for multilevel non-parametric mixture modeling, enabling modeling topic hierarchies. however, the ndp and ncrf do not allow sharing of distributions as required in many applications, motivating the need for multi-level non-parametric admixture modeling. we address this gap by proposing multi-level nested hdps (nhdp) where the base distribution of the hdp is itself a hdp at each level thereby leading to admixtures of admixtures at each level. because of couplings between various hdp levels, scaling up is naturally a challenge during inference. we propose a multi-level nested chinese restaurant franchise (ncrf) representation for the nested hdp, with which we outline an inference algorithm based on gibbs sampling. we evaluate our model with the two level nhdp for non-parametric entity topic modeling where an inner hdp creates a countably infinite set of topic mixtures and associates them with author entities, while an outer hdp associates documents with these author entities. in our experiments on two real world research corpora, the nhdp is able to generalize significantly better than existing models and detect missing author entities with a reasonable level of accuracy.","","2015-08-26","2015-08-27","['lavanya sita tekumalla', 'priyanka agrawal', 'indrajit bhattacharya']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1214",1508.06476,"standardized drought indices: a novel uni- and multivariate approach","stat.ap stat.me","as drought is among the natural hazards which affects people and economies worldwide and often results in huge monetary losses sophisticated methods for drought monitoring and decision making are needed. several different approaches to quantify drought have been developed during past decades. however, most of these drought indices suffer from different shortcomings and do not account for the multiple driving factors which promote drought conditions and their inter-dependencies. we provide a novel methodology for the calculation of (multivariate) drought indices, which combines the advantages of existing approaches and omits their disadvantages. moreover, our approach benefits from the flexibility of vine copulas in modeling multivariate non-gaussian inter-variable dependence structures. a three-variate data example is used in order to investigate drought conditions in europe and to illustrate and reason the different modeling steps. the data analysis shows the appropriateness of the described methodology. comparison to well-established drought indices shows the benefits of our multivariate approach. the validity of the new methodology is verified by comparing the spatial extent of historic drought events based on different drought indices. further, we show that the assumption of non-gaussian dependence structures is well-grounded in this real-world application.","","2015-08-26","","['tobias m. erhardt', 'claudia czado']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1215",1508.06615,"character-aware neural language models","cs.cl cs.ne stat.ml","we describe a simple neural language model that relies only on character-level inputs. predictions are still made at the word-level. our model employs a convolutional neural network (cnn) and a highway network over characters, whose output is given to a long short-term memory (lstm) recurrent neural network language model (rnn-lm). on the english penn treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. on languages with rich morphology (arabic, czech, french, german, spanish, russian), the model outperforms word-level/morpheme-level lstm baselines, again with fewer parameters. the results suggest that on many languages, character inputs are sufficient for language modeling. analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.","","2015-08-26","2015-12-01","['yoon kim', 'yacine jernite', 'david sontag', 'alexander m. rush']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1216",1508.07083,"detecting abrupt changes in the spectra of high-energy astrophysical   sources","stat.ap astro-ph.im","variable-intensity astronomical sources are the result of complex and often extreme physical processes. abrupt changes in source intensity are typically accompanied by equally sudden spectral shifts, i.e., sudden changes in the wavelength distribution of the emission. this article develops a method for modeling photon counts collected from observation of such sources. we embed change points into a marked poisson process, where photon wavelengths are regarded as marks and both the poisson intensity parameter and the distribution of the marks are allowed to change. to the best of our knowledge this is the first effort to embed change points into a marked poisson process. between the change points, the spectrum is modeled non-parametrically using a mixture of a smooth radial basis expansion and a number of local deviations from the smooth term representing spectral emission lines. because the model is over parameterized we employ an $\ell_1$ penalty. the tuning parameter in the penalty and the number of change points are determined via the minimum description length principle. our method is validated via a series of simulation studies and its practical utility is illustrated in the analysis of the ultra-fast rotating yellow giant star known as fk com.","","2015-08-27","2015-12-10","['raymond k. w. wong', 'vinay l. kashyap', 'thomas c. m. lee', 'david a. van dyk']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1217",1508.07103,"regularized kernel recursive least square algoirthm","cs.lg stat.ml","in most adaptive signal processing applications, system linearity is assumed and adaptive linear filters are thus used. the traditional class of supervised adaptive filters rely on error-correction learning for their adaptive capability. the kernel method is a powerful nonparametric modeling tool for pattern analysis and statistical signal processing. through a nonlinear mapping, kernel methods transform the data into a set of points in a reproducing kernel hilbert space. krls achieves high accuracy and has fast convergence rate in stationary scenario. however the good performance is obtained at a cost of high computation complexity. sparsification in kernel methods is know to related to less computational complexity and memory consumption.","","2015-08-28","","['songlin zhao']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1218",1508.07497,"varx-l: structured regularization for large vector autoregressions with   exogenous variables","stat.ap","the vector autoregression (var) has long proven to be an effective method for modeling the joint dynamics of macroeconomic time series as well as forecasting. a major shortcoming of the var that has hindered its applicability is its heavy parameterization: the parameter space grows quadratically with the number of series included, quickly exhausting the available degrees of freedom. consequently, forecasting using vars is intractable for low-frequency, high-dimensional macroeconomic data. however, empirical evidence suggests that vars that incorporate more component series tend to result in more accurate forecasts. conventional methods that allow for the estimation of large vars either tend to require ad hoc subjective specifications or are computationally infeasible. moreover, as global economies become more intricately intertwined, there has been substantial interest in incorporating the impact of stochastic, unmodeled exogenous variables. vector autoregression with exogenous variables (varx) extends the var to allow for the inclusion of unmodeled variables, but it similarly faces dimensionality challenges.   we introduce the varx-l framework, a structured family of varx models, and provide methodology that allows for both efficient estimation and accurate forecasting in high-dimensional analysis. varx-l adapts several prominent scalar regression regularization techniques to a vector time series context in order to greatly reduce the parameter space of var and varx models. we also highlight a compelling extension that allows for shrinking toward reference models, such as a vector random walk. we demonstrate the efficacy of varx-l in both low- and high-dimensional macroeconomic forecasting applications and simulated data examples. our methodology is easily reproducible in a publicly available r package.","","2015-08-29","2017-02-27","['william nicholson', 'david matteson', 'jacob bien']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1219",1508.07511,"a bayesian hierarchical model for prediction of latent health states   from multiple data sources with application to active surveillance of   prostate cancer","stat.me stat.ap","in this article, we present a bayesian hierarchical model for predicting a latent health state from longitudinal clinical measurements. model development is motivated by the need to integrate multiple sources of data to improve clinical decisions about whether to remove or irradiate a patient's prostate cancer. existing modeling approaches are extended to accommodate measurement error in cancer state determinations based on biopsied tissue, clinical measurements possibly not missing at random, and informative partial observation of the true state. the proposed model enables estimation of whether an individual's underlying prostate cancer is aggressive, requiring surgery and/or radiation, or indolent, permitting continued surveillance. these individualized predictions can then be communicated to clinicians and patients to inform decision-making. we demonstrate the model with data from a cohort of low risk prostate cancer patients at johns hopkins university and assess predictive accuracy among a subset for whom true cancer state is observed. simulation studies confirm model performance and explore the impact of adjusting for informative missingness on true state predictions. r code and simulated data available at https://github.com/rycoley/prediction-prostate-surveillance.","","2015-08-29","2016-06-01","['r. yates coley', 'aaron j. fisher', 'mufaddal mamawala', 'h. ballentine carter', 'kenneth j. pienta', 'scott l. zeger']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1220",1508.07753,"learning structures of bayesian networks for variable groups","stat.ml cs.ai","bayesian networks, and especially their structures, are powerful tools for representing conditional independencies and dependencies between random variables. in applications where related variables form a priori known groups, chosen to represent different ""views"" to or aspects of the same entities, one may be more interested in modeling dependencies between groups of variables rather than between individual variables. motivated by this, we study prospects of representing relationships between variable groups using bayesian network structures. we show that for dependency structures between groups to be expressible exactly, the data have to satisfy the so-called groupwise faithfulness assumption. we also show that one cannot learn causal relations between groups using only groupwise conditional independencies, but also variable-wise relations are needed. additionally, we present algorithms for finding the groupwise dependency structures.","10.1016/j.ijar.2017.05.006","2015-08-31","2017-06-01","['pekka parviainen', 'samuel kaski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1221",1509.00519,"importance weighted autoencoders","cs.lg stat.ml","the variational autoencoder (vae; kingma, welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. it typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. as we show empirically, the vae objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. we present the importance weighted autoencoder (iwae), a generative model with the same architecture as the vae, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. in the iwae, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the vae modeling assumptions. we show empirically that iwaes learn richer latent space representations than vaes, leading to improved test log-likelihood on density estimation benchmarks.","","2015-09-01","2016-11-07","['yuri burda', 'roger grosse', 'ruslan salakhutdinov']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1222",1509.01228,"machine learning model of the swift/bat trigger algorithm for long grb   population studies","astro-ph.he physics.data-an stat.ml","to draw inferences about gamma-ray burst (grb) source populations based on swift observations, it is essential to understand the detection efficiency of the swift burst alert telescope (bat). this study considers the problem of modeling the swift/bat triggering algorithm for long grbs, a computationally expensive procedure, and models it using machine learning algorithms. a large sample of simulated grbs from lien 2014 is used to train various models: random forests, boosted decision trees (with adaboost), support vector machines, and artificial neural networks. the best models have accuracies of $\gtrsim97\%$ ($\lesssim 3\%$ error), which is a significant improvement on a cut in grb flux which has an accuracy of $89.6\%$ ($10.4\%$ error). these models are then used to measure the detection efficiency of swift as a function of redshift $z$, which is used to perform bayesian parameter estimation on the grb rate distribution. we find a local grb rate density of $n_0 \sim 0.48^{+0.41}_{-0.23} \ {\rm gpc}^{-3} {\rm yr}^{-1}$ with power-law indices of $n_1 \sim 1.7^{+0.6}_{-0.5}$ and $n_2 \sim -5.9^{+5.7}_{-0.1}$ for grbs above and below a break point of $z_1 \sim 6.8^{+2.8}_{-3.2}$. this methodology is able to improve upon earlier studies by more accurately modeling swift detection and using this for fully bayesian model fitting. the code used in this is analysis is publicly available online (https://github.com/pbgraff/swiftgrb_peanalysis).","10.3847/0004-637x/818/1/55","2015-09-03","2016-02-08","['philip b graff', 'amy y lien', 'john g baker', 'takanori sakamoto']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1223",1509.01275,"modeling long-term outcomes and treatment effects after androgen   deprivation therapy for prostate cancer","stat.ap","analyzing outcomes in long-term cancer survivor studies can be complex. the effects of predictors on the failure process may be difficult to assess over longer periods of time, as the commonly used assumption of proportionality of hazards holding over an extended period is often questionable. in this manuscript, we compare seven different survival models that estimate the hazard rate and the effects of proportional and non-proportional covariates. in particular, we focus on an extension of the the multi-resolution hazard (mrh) estimator, combining a non-proportional hierarchical mrh approach with a data-driven pruning algorithm that allows for computational efficiency and produces robust estimates even in times of few observed failures. using data from a large-scale randomized prostate cancer clinical trial, we examine patterns of biochemical failure and estimate the time-varying effects of androgen deprivation therapy treatment and other covariates. we compare the impact of different modeling strategies and smoothness assumptions on the estimated treatment effect. our results show that the benefits of treatment diminish over time, possibly with implications for future treatment protocols.","","2015-09-03","","['yolanda hagar', 'james j. dignam', 'vanja dukic']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1224",1509.02124,"bayesian latent pattern mixture models for handling attrition in panel   studies with refreshment samples","stat.me stat.ap","many panel studies collect refreshment samples---new, randomly sampled respondents who complete the questionnaire at the same time as a subsequent wave of the panel. with appropriate modeling, these samples can be leveraged to correct inferences for biases caused by non-ignorable attrition. we present such a model when the panel includes many categorical survey variables. the model relies on a bayesian latent pattern mixture model, in which an indicator for attrition and the survey variables are modeled jointly via a latent class model. we allow the multinomial probabilities within classes to depend on the attrition indicator, which offers additional flexibility over standard applications of latent class models. we present results of simulation studies that illustrate the benefits of this flexibility. we apply the model to correct attrition bias in an analysis of data from the 2007-2008 associated press/yahoo news election panel study.","","2015-09-07","","['yajuan si', 'jerome p. reiter', 'd. sunshine hillygus']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1225",1509.03108,"a closer look at testing the ""no-treatment-effect"" hypothesis in a   comparative experiment","stat.me","standard tests of the ""no-treatment-effect"" hypothesis for a comparative experiment include permutation tests, the wilcoxon rank sum test, two-sample $t$ tests, and fisher-type randomization tests. practitioners are aware that these procedures test different no-effect hypotheses and are based on different modeling assumptions. however, this awareness is not always, or even usually, accompanied by a clear understanding or appreciation of these differences. borrowing from the rich literatures on causality and finite-population sampling theory, this paper develops a modeling framework that affords answers to several important questions, including: exactly what hypothesis is being tested, what model assumptions are being made, and are there other, perhaps better, approaches to testing a no-effect hypothesis? the framework lends itself to clear descriptions of three main inference approaches: process-based, randomization-based, and selection-based. it also promotes careful consideration of model assumptions and targets of inference, and highlights the importance of randomization. along the way, fisher-type randomization tests are compared to permutation tests and a less well-known neyman-type randomization test. a simulation study compares the operating characteristics of the neyman-type randomization test to those of the other more familiar tests.","10.1214/15-sts513","2015-09-10","","['joseph b. lang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1226",1509.03813,"functional generalized autoregressive conditional heteroskedasticity","stat.me","heteroskedasticity is a common feature of financial time series and is commonly addressed in the model building process through the use of arch and garch processes. more recently multivariate variants of these processes have been in the focus of research with attention given to methods seeking an efficient and economic estimation of a large number of model parameters. due to the need for estimation of many parameters, however, these models may not be suitable for modeling now prevalent high-frequency volatility data. one potentially useful way to bypass these issues is to take a functional approach. in this paper, theory is developed for a new functional version of the generalized autoregressive conditionally heteroskedastic process, termed fgarch. the main results are concerned with the structure of the fgarch(1,1) process, providing criteria for the existence of a strictly stationary solutions both in the space of square-integrable and continuous functions. an estimation procedure is introduced and its consistency verified. a small empirical study highlights potential applications to intraday volatility estimation.","","2015-09-13","2015-12-17","['alexander aue', 'lajos horvath', 'daniel pellatt']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1227",1509.0388,"stochastic simulators based optimization by gaussian process metamodels   - application to maintenance investments planning issues","math.st stat.th","this paper deals with the construction of a metamodel (i.e. a simplified mathematical model) for a stochastic computer code (also called stochastic numerical model or stochastic simulator), where stochastic means that the code maps the realization of a random variable. the goal is to get, for a given model input, the main information about the output probability distribution by using this metamodel and without running the computer code. in practical applications, such a metamodel enables one to have estimations of every possible random variable properties, such as the expectation, the probability of exceeding a threshold or any quantile. the present work is concentrated on the emulation of the quantile function of the stochastic simulator by interpolating well chosen basis function and metamodeling their coefficients (using the gaussian process metamodel). this quantile function metamodel is then used to treat a simple optimization strategy maintenance problem using a stochastic code, in order to optimize the quantile of an economic indicator. using the gaussian process framework, an adaptive design method (called qfei) is defined by extending in our case the well known ego algorithm. this allows to obtain an ""optimal"" solution using a small number of simulator runs.","","2015-09-13","","['thomas browne', 'bertrand iooss', 'loïc le gratiet', 'jérome lonchampt']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1228",1509.03938,"robust reduced rank regression","math.st stat.ap stat.me stat.th","in high-dimensional multivariate regression problems, enforcing low rank in the coefficient matrix offers effective dimension reduction, which greatly facilitates parameter estimation and model interpretation. however, commonly-used reduced-rank methods are sensitive to data corruption, as the low-rank dependence structure between response variables and predictors is easily distorted by outliers. we propose a robust reduced-rank regression approach for joint modeling and outlier detection. the problem is formulated as a regularized multivariate regression with a sparse mean-shift parametrization, which generalizes and unifies some popular robust multivariate methods. an efficient thresholding-based iterative procedure is developed for optimization. we show that the algorithm is guaranteed to converge, and the coordinatewise minimum point produced is statistically accurate under regularity conditions. our theoretical investigations focus on nonasymptotic robust analysis, which demonstrates that joint rank reduction and outlier detection leads to improved prediction accuracy. in particular, we show that redescending $\psi$-functions can essentially attain the minimax optimal error rate, and in some less challenging problems convex regularization guarantees the same low error rate. the performance of the proposed method is examined by simulation studies and real data examples.","","2015-09-13","2017-07-15","['yiyuan she', 'kun chen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE
"1229",1509.0394,"causal inference in repeated observational studies: a case study of ebay   product releases","stat.ap","causal inference in observational studies is notoriously difficult, due to the fact that the experimenter is not in charge of the treatment assignment mechanism. many potential con- founding factors (pcfs) exist in such a scenario, and if one seeks to estimate the causal effect of the treatment on a response, one needs to control for such factors. identifying all relevant pcfs may be difficult (or impossible) given a single observational study. instead, we argue that if one can observe a sequence of similar treatments over the course of a lengthy time period, one can identify patterns of behavior in the experimental subjects that are correlated with the response of interest and control for those patterns directly. specifically, in our case-study we find and control for an early-adopter effect: the scenario in which the magnitude of the response is highly correlated with how quickly one adopts a treatment after its release.   we provide a flexible hierarchical bayesian framework that controls for such early-adopter effects in the analysis of the effects of multiple sequential treatments. the methods are presented and evaluated in the context of a detailed case-study involving product updates (newer versions of the same product) from ebay, inc. the users in our study upgrade (or not) to a new version of the product at their own volition and timing. our response variable is a measure of user actions, and we study the behavior of a large set of users (n = 10.5 million) in a targeted subset of ebay categories over a period of one year. we find that (a) naive causal estimates are hugely misleading and (b) our method, which is relatively insensitive to modeling assumptions and exhibits good out-of-sample predictive validation, yields sensible causal estimates that offer ebay a stable basis for decision-making.","","2015-09-13","","['vadim von brzeski', 'matt taddy', 'david draper']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1230",1509.03977,"optimization of anemia treatment in hemodialysis patients via   reinforcement learning","stat.ml cs.ai cs.lg","objective: anemia is a frequent comorbidity in hemodialysis patients that can be successfully treated by administering erythropoiesis-stimulating agents (esas). esas dosing is currently based on clinical protocols that often do not account for the high inter- and intra-individual variability in the patient's response. as a result, the hemoglobin level of some patients oscillates around the target range, which is associated with multiple risks and side-effects. this work proposes a methodology based on reinforcement learning (rl) to optimize esa therapy.   methods: rl is a data-driven approach for solving sequential decision-making problems that are formulated as markov decision processes (mdps). computing optimal drug administration strategies for chronic diseases is a sequential decision-making problem in which the goal is to find the best sequence of drug doses. mdps are particularly suitable for modeling these problems due to their ability to capture the uncertainty associated with the outcome of the treatment and the stochastic nature of the underlying process. the rl algorithm employed in the proposed methodology is fitted q iteration, which stands out for its ability to make an efficient use of data.   results: the experiments reported here are based on a computational model that describes the effect of esas on the hemoglobin level. the performance of the proposed method is evaluated and compared with the well-known q-learning algorithm and with a standard protocol. simulation results show that the performance of q-learning is substantially lower than fqi and the protocol.   conclusion: although prospective validation is required, promising results demonstrate the potential of rl to become an alternative to current protocols.","10.1016/j.artmed.2014.07.004","2015-09-14","","['pablo escandell-montero', 'milena chermisi', 'josé m. martínez-martínez', 'juan gómez-sanchis', 'carlo barbieri', 'emilio soria-olivas', 'flavio mari', 'joan vila-francés', 'andrea stopper', 'emanuele gatti', 'josé d. martín-guerrero']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1231",1509.04634,"modeling and interpolation of the ambient magnetic field by gaussian   processes","cs.ro stat.ml","anomalies in the ambient magnetic field can be used as features in indoor positioning and navigation. by using maxwell's equations, we derive and present a bayesian non-parametric probabilistic modeling approach for interpolation and extrapolation of the magnetic field. we model the magnetic field components jointly by imposing a gaussian process (gp) prior on the latent scalar potential of the magnetic field. by rewriting the gp model in terms of a hilbert space representation, we circumvent the computational pitfalls associated with gp modeling and provide a computationally efficient and physically justified modeling tool for the ambient magnetic field. the model allows for sequential updating of the estimate and time-dependent changes in the magnetic field. the model is shown to work well in practice in different applications: we demonstrate mapping of the magnetic field both with an inexpensive raspberry pi powered robot and on foot using a standard smartphone.","","2015-09-15","2018-03-21","['arno solin', 'manon kok', 'niklas wahlström', 'thomas b. schön', 'simo särkkä']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1232",1509.04834,"multi-species distribution modeling using penalized mixture of   regressions","stat.ap","multi-species distribution modeling, which relates the occurrence of multiple species to environmental variables, is an important tool used by ecologists for both predicting the distribution of species in a community and identifying the important variables driving species co-occurrences. recently, dunstan, foster and darnell [ecol. model. 222 (2011) 955-963] proposed using finite mixture of regression (fmr) models for multi-species distribution modeling, where species are clustered based on their environmental response to form a small number of ""archetypal responses."" as an illustrative example, they applied their mixture model approach to a presence-absence data set of 200 marine organisms, collected along the great barrier reef in australia. little attention, however, was given to the problem of model selection - since the archetypes (mixture components) may depend on different but likely overlapping sets of covariates, a method is needed for performing variable selection on all components simultaneously. in this article, we consider using penalized likelihood functions for variable selection in fmr models. we propose two penalties which exploit the grouped structure of the covariates, that is, each covariate is represented by a group of coefficients, one for each component. this leads to an attractive form of shrinkage that allows a covariate to be removed from all components simultaneously. both penalties are shown to possess specific forms of variable selection consistency, with simulations indicating they outperform other methods which do not take into account the grouped structure. when applied to the great barrier reef data set, penalized fmr models offer more insight into the important variables driving species co-occurrence in the marine community (compared to previous results where no model selection was conducted), while offering a computationally stable method of modeling complex species-environment relationships (through regularization).","10.1214/15-aoas813","2015-09-16","","['francis k. c. hui', 'david i. warton', 'scott d. foster']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1233",1509.04984,"on modeling of variability in mixture experiments with noise variables","stat.ap","in mixture experiments with noise variables or process variables that can not be controlled, investigate and try to control the variability of the response variable is very important for quality improvement in industrial processes. thus, modeling the variability in mixture experiments with noise variables becomes necessary and has been considered in literature with approaches that require the choice of a quadratic loss function or by using the delta method. in this paper, we make use of the delta method and also propose an alternative approach, which is based on the joint modeling of mean and dispersion (jmmd). we consider a mixture experiment involving noise variables and we use the techniques of jmmd and of the delta method to get models for both mean and variance of the response variable. following the taguchi's ideas about robust parameter design we build and solve an optimization problem for minimizing the variance while holding the mean on the target. at the end we provide a discussion about the two methodologies considered.","","2015-09-16","","['edmilson rodrigues pinto', 'leandro alves pereira', 'aurélia aparecida de araújo rodrigues']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1234",1509.05305,"boosting bayesian parameter inference of nonlinear stochastic   differential equation models by hamiltonian scale separation","cs.ds stat.co","parameter inference is a fundamental problem in data-driven modeling. given observed data that is believed to be a realization of some parameterized model, the aim is to find parameter values that are able to explain the observed data. in many situations, the dominant sources of uncertainty must be included into the model, for making reliable predictions. this naturally leads to stochastic models. stochastic models render parameter inference much harder, as the aim then is to find a distribution of likely parameter values. in bayesian statistics, which is a consistent framework for data-driven learning, this so-called posterior distribution can be used to make probabilistic predictions. we propose a novel, exact and very efficient approach for generating posterior parameter distributions, for stochastic differential equation models calibrated to measured time-series. the algorithm is inspired by re-interpreting the posterior distribution as a statistical mechanics partition function of an object akin to a polymer, where the measurements are mapped on heavier beads compared to those of the simulated data. to arrive at distribution samples, we employ a hamiltonian monte carlo approach combined with a multiple time-scale integration. a separation of time scales naturally arises if either the number of measurement points or the number of simulation points becomes large. furthermore, at least for 1d problems, we can decouple the harmonic modes between measurement points and solve the fastest part of their dynamics analytically. our approach is applicable to a wide range of inference problems and is highly parallelizable.","10.1103/physreve.93.043313","2015-09-17","2016-04-19","['carlo albert', 'simone ulzega', 'ruedi stoop']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1235",1509.05453,"graph estimation for matrix-variate gaussian data","stat.me math.st stat.th","matrix-variate gaussian graphical models (ggm) have been widely used for modeling matrix-variate data. since the support of sparse precision matrix represents the conditional independence graph among matrix entries, conducting support recovery yields valuable information. a commonly used approach is the penalized log-likelihood method. however, due to the complicated structure of precision matrices in the form of kronecker product, the log-likelihood is non-convex, which presents challenges for both computation and theoretical analysis. in this paper, we propose an alternative approach by formulating the support recovery problem into a multiple testing problem. a new test statistic is developed and based on that, we use the popular benjamini and hochberg's procedure to control false discovery rate (fdr) asymptotically. our method involves only convex optimization, making it computationally attractive. theoretically, our method allows very weak conditions, i.e., even when the sample size is finite and the dimensions go to infinity, the asymptotic normality of the test statistics and fdr control can still be guaranteed. we further provide the power analysis result. the finite sample performance of the proposed method is illustrated by both simulated and real data analysis.","","2015-09-17","2017-08-30","['xi chen', 'weidong liu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1236",1509.06365,"expanding the computation of mixture models by the use of hermite   polynomials and ideals","stat.co","mixture models have found uses in many areas. to list a few: unsupervised learning, empirical bayes, latent class and trait models. the current applications of mixture models to empirical data is limited to computing a mixture model from the same parametric family, e.g. gaussians or poissons. in this paper it is shown that by using hermite polynomials and ideals, the modeling of a mixture process can be extended to include different families in terms of their cumulative distribution functions (cdfs)","","2015-09-21","","['andrew clark']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1237",1509.06428,"large-scale mode identification and data-driven sciences","stat.me math.st stat.th","bump-hunting or mode identification is a fundamental problem that arises in almost every scientific field of data-driven discovery. surprisingly, very few data modeling tools are available for automatic (not requiring manual case-by-base investigation), objective (not subjective), and nonparametric (not based on restrictive parametric model assumptions) mode discovery, which can scale to large data sets. this article introduces lpmode--an algorithm based on a new theory for detecting multimodality of a probability density. we apply lpmode to answer important research questions arising in various fields from environmental science, ecology, econometrics, analytical chemistry to astronomy and cancer genomics.","","2015-09-21","2016-11-08","['subhadeep mukhopadhyay']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1238",1509.07185,"sptest: an r package implementing nonparametric tests of isotropy","stat.co","an important step of modeling spatially-referenced data is appropriately specifying the second order properties of the random field. a scientist developing a model for spatial data has a number of options regarding the nature of the dependence between observations. one of these options is deciding whether or not the dependence between observations depends on direction, or, in other words, whether or not the spatial covariance function is isotropic. isotropy implies that spatial dependence is a function of only the distance and not the direction of the spatial separation between sampling locations. a researcher may use graphical techniques, such as directional sample semivariograms, to determine whether an assumption of isotropy holds. these graphical diagnostics can be difficult to assess, subject to personal interpretation, and potentially misleading as they typically do not include a measure of uncertainty. in order to escape these issues, a hypothesis test of the assumption of isotropy may be more desirable. to avoid specification of the covariance function, a number of nonparametric tests of isotropy have been developed using both the spatial and spectral representations of random fields. several of these nonparametric tests are implemented in the r package sptest, available on cran. we demonstrate how graphical techniques and the hypothesis tests programmed in sptest can be used in practice to assess isotropy properties.","","2015-09-23","2015-11-16","['zachary d. weller']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1239",1509.07497,"high dimensional data modeling techniques for detection of chemical   plumes and anomalies in hyperspectral images and movies","stat.ml","we briefly review recent progress in techniques for modeling and analyzing hyperspectral images and movies, in particular for detecting plumes of both known and unknown chemicals. for detecting chemicals of known spectrum, we extend the technique of using a single subspace for modeling the background to a ""mixture of subspaces"" model to tackle more complicated background. furthermore, we use partial least squares regression on a resampled training set to boost performance. for the detection of unknown chemicals we view the problem as an anomaly detection problem, and use novel estimators with low-sampled complexity for intrinsically low-dimensional data in high-dimensions that enable us to model the ""normal"" spectra and detect anomalies. we apply these algorithms to benchmark data sets made available by the automated target detection program co-funded by nsf, dtra and nga, and compare, when applicable, to current state-of-the-art algorithms, with favorable results.","","2015-09-24","2016-01-29","['n/a yi', 'n/a wang', 'guangliang chen', 'mauro maggioni']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1240",1509.07804,"from statistician to data scientist","stat.ot","according to a recent report from the european commission, the world generates every minute 1.7 million of billions of data bytes, the equivalent of 360,000 dvds, and companies that build their decision-making processes by exploiting these data increase their productivity. the treatment and valorization of massive data has consequences on the employment of graduate students in statistics. which additional skills do students trained in statistics need to acquire to become data scientists ? how to evolve training so that future graduates can adapt to rapid changes in this area, without neglecting traditional jobs and the fundamental and lasting foundation for the training? after considering the notion of big data and questioning the emergence of a ""new"" science: data science, we present the current developments in the training of engineers in mathematical and modeling at insa toulouse.","","2015-09-25","2015-12-22","['philippe besse', 'beatrice laurent']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE
"1241",1509.07982,"targeted fused ridge estimation of inverse covariance matrices from   multiple high-dimensional data classes","stat.me q-bio.mn stat.ml","we consider the problem of jointly estimating multiple inverse covariance matrices from high-dimensional data consisting of distinct classes. an $\ell_2$-penalized maximum likelihood approach is employed. the suggested approach is flexible and generic, incorporating several other $\ell_2$-penalized estimators as special cases. in addition, the approach allows specification of target matrices through which prior knowledge may be incorporated and which can stabilize the estimation procedure in high-dimensional settings. the result is a targeted fused ridge estimator that is of use when the precision matrices of the constituent classes are believed to chiefly share the same structure while potentially differing in a number of locations of interest. it has many applications in (multi)factorial study designs. we focus on the graphical interpretation of precision matrices with the proposed estimator then serving as a basis for integrative or meta-analytic gaussian graphical modeling. situations are considered in which the classes are defined by data sets and subtypes of diseases. the performance of the proposed estimator in the graphical modeling setting is assessed through extensive simulation experiments. its practical usability is illustrated by the differential network modeling of 12 large-scale gene expression data sets of diffuse large b-cell lymphoma subtypes. the estimator and its related procedures are incorporated into the r-package rags2ridges.","","2015-09-26","2020-03-26","['anders ellern bilgrau', 'carel f. w. peeters', 'poul svante eriksen', 'martin bøgsted', 'wessel n. van wieringen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1242",1509.0821,"mixture modeling based probabilistic situation awareness","stat.ap","the problem of situational awareness (saw) is investigated from the probabilistic modeling point of view. taking the situation as a hidden variable, we introduce a hidden markov model (hmm) and an extended state space model (essm) to mathematically express the dynamic evolution law of the situation and the relationships between the situation and the observable quantities. we use the gaussian mixture model (gmm) to formulate expert knowledge, which is needed in building the hmm and essm. we show that the essm model is preferable as compared with hmm, since using essm, we can also get a real time estimate of the pivot variable that connects the situation with the observable quantities. the effectiveness and efficiency of both models are tested through a simulated experiment about threat surveillance.","","2015-09-28","2015-10-06","['bin liu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1243",1509.08999,"asynchronous gibbs sampling","stat.co","gibbs sampling is a markov chain monte carlo (mcmc) method often used in bayesian learning. mcmc methods can be difficult to deploy on parallel and distributed systems due to their inherently sequential nature. we study asynchronous gibbs sampling, which achieves parallelism by simply ignoring sequential requirements. this method has been shown to produce good empirical results for some hierarchical models, and is popular in the topic modeling community, but was also shown to diverge for other targets. we introduce a theoretical framework for analyzing asynchronous gibbs sampling and other extensions of mcmc that do not possess the markov property. we prove that asynchronous gibbs can be modified so that it converges under appropriate regularity conditions -- we call this the exact asynchronous gibbs algorithm. we study asynchronous gibbs on a set of examples by comparing the exact and approximate algorithms, including two where it works well, and one where it fails dramatically. we conclude with a set of heuristics to describe settings where the algorithm can be effectively used.","","2015-09-29","2020-02-29","['alexander terenin', 'daniel simpson', 'david draper']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1244",1510.00071,"asymptotic confidence bands for copulas based on the local linear kernel   estimator","stat.me","in this paper we establish asymptotic simultaneous confidence bands for copulas based on the local linear kernel estimator proposed by chen and huang [1]. for this, we prove under smoothness conditions on the copula function, a uniform in bandwidth law of the iterated logarithm for the maximal deviation of this estimator from its expectation. we also show that the bias term converges uniformly to zero with a precise rate. the performance of these bands is illustrated in a simulation study. an application based on pseudo-panel data is also provided for modeling dependence.","","2015-09-30","","['diam ba', 'cheikh tidiane seck', 'gane samb lo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1245",1510.00775,"quantifying and mitigating the effect of preferential sampling on   phylodynamic inference","stat.me q-bio.pe","phylodynamics seeks to estimate effective population size fluctuations from molecular sequences of individuals sampled from a population of interest. one way to accomplish this task formulates an observed sequence data likelihood exploiting a coalescent model for the sampled individuals' genealogy and then integrating over all possible genealogies via monte carlo or, less efficiently, by conditioning on one genealogy estimated from the sequence data. however, when analyzing sequences sampled serially through time, current methods implicitly assume either that sampling times are fixed deterministically by the data collection protocol or that their distribution does not depend on the size of the population. through simulation, we first show that, when sampling times do probabilistically depend on effective population size, estimation methods may be systematically biased. to correct for this deficiency, we propose a new model that explicitly accounts for preferential sampling by modeling the sampling times as an inhomogeneous poisson process dependent on effective population size. we demonstrate that in the presence of preferential sampling our new model not only reduces bias, but also improves estimation precision. finally, we compare the performance of the currently used phylodynamic methods with our proposed model through clinically-relevant, seasonal human influenza examples.","10.1371/journal.pcbi.1004789","2015-10-03","","['michael d. karcher', 'julia a. palacios', 'trevor bedford', 'marc a. suchard', 'vladimir n. minin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1246",1510.01722,"structured transforms for small-footprint deep learning","stat.ml cs.cv cs.lg","we consider the task of building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices. we propose a unified framework to learn a broad family of structured parameter matrices that are characterized by the notion of low displacement rank. our structured transforms admit fast function and gradient evaluation, and span a rich range of parameter sharing configurations whose statistical modeling capacity can be explicitly tuned along a continuum from structured to unstructured. experimental results show that these transforms can significantly accelerate inference and forward/backward passes during training, and offer superior accuracy-compactness-speed tradeoffs in comparison to a number of existing techniques. in keyword spotting applications in mobile speech recognition, our methods are much more effective than standard linear low-rank bottleneck layers and nearly retain the performance of state of the art models, while providing more than 3.5-fold compression.","","2015-10-06","","['vikas sindhwani', 'tara n. sainath', 'sanjiv kumar']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1247",1510.02557,"a model-based approach to climate reconstruction using tree-ring data","stat.ap","quantifying long-term historical climate is fundamental to understanding recent climate change. most instrumentally recorded climate data are only available for the past 200 years, so proxy observations from natural archives are often considered. we describe a model-based approach to reconstructing climate defined in terms of raw tree-ring measurement data that simultaneously accounts for non-climatic and climatic variability. in this approach we specify a joint model for the tree-ring data and climate variable that we fit using bayesian inference. we consider a range of prior densities and compare the modeling approach to current methodology using an example case of scots pine from tornetrask, sweden to reconstruct growing season temperature. we describe how current approaches translate into particular model assumptions. we explore how changes to various components in the model-based approach affect the resulting reconstruction. we show that minor changes in model specification can have little effect on model fit but lead to large changes in the predictions. in particular, the periods of relatively warmer and cooler temperatures are robust between models, but the magnitude of the resulting temperatures are highly model dependent. such sensitivity may not be apparent with traditional approaches because the underlying statistical model is often hidden or poorly described.","","2015-10-08","","['matthew r. schofield', 'richard j. barker', 'andrew gelman', 'edward r. cook', 'keith r. briffa']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1248",1510.02855,"atomnet: a deep convolutional neural network for bioactivity prediction   in structure-based drug discovery","cs.lg cs.ne q-bio.bm stat.ml","deep convolutional neural networks comprise a subclass of deep neural networks (dnn) with a constrained architecture that leverages the spatial and temporal structure of the domain they model. convolutional networks achieve the best predictive performance in areas such as speech and image recognition by hierarchically composing simple local features into complex models. although dnns have been used in drug discovery for qsar and ligand-based bioactivity predictions, none of these models have benefited from this powerful convolutional architecture. this paper introduces atomnet, the first structure-based, deep convolutional neural network designed to predict the bioactivity of small molecules for drug discovery applications. we demonstrate how to apply the convolutional concepts of feature locality and hierarchical composition to the modeling of bioactivity and chemical interactions. in further contrast to existing dnn techniques, we show that atomnet's application of local convolutional filters to structural target information successfully predicts new active molecules for targets with no previously known modulators. finally, we show that atomnet outperforms previous docking approaches on a diverse set of benchmarks by a large margin, achieving an auc greater than 0.9 on 57.8% of the targets in the dude benchmark.","","2015-10-09","","['izhar wallach', 'michael dzamba', 'abraham heifets']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1249",1510.03385,"optimal etf selection for passive investing","q-fin.st stat.ap","this paper considers the problem of isolating a small number of exchange traded funds (etfs) that suffice to capture the fundamental dimensions of variation in u.s. financial markets. first, the data is fit to a vector-valued bayesian regression model, which is a matrix-variate generalization of the well known stochastic search variable selection (ssvs) of george and mcculloch (1993). etf selection is then performed using the decoupled shrinkage and selection (dss) procedure described in hahn and carvalho (2015), adapted in two ways: to the vector-response setting and to incorporate stochastic covariates. the selected set of etfs is obtained under a number of different penalty and modeling choices. optimal portfolios are constructed from selected etfs by maximizing the sharpe ratio posterior mean, and they are compared to the (unknown) optimal portfolio based on the full bayesian model. we compare our selection results to popular etf advisor wealthfront.com. additionally, we consider selecting etfs by modeling a large set of mutual funds.","","2015-10-12","2015-11-28","['david puelz', 'carlos m. carvalho', 'p. richard hahn']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1250",1510.03966,"natural exponential families: resolution of a conjecture and existence   of reduction functions","math.st stat.th","one-parameter natural exponential family (nef) plays fundamental roles in probability and statistics. this article contains two independent results: (a) a conjecture of bar-lev, bshouty and enis states that a polynomial with a simple root at $0$ and a complex root with positive imaginary part is the variance function of some nef with mean domain $\left(0,\infty\right)$ if and only if the real part of the complex root is not positive. this conjecture is resolved. the positive answer to this conjecture enlarges existing family of polynomials that are able to generate nefs, and it helps prevent practitioners from choosing incompatible functions as variance functions for statistical modeling using nefs. (b) if a random variable $\xi$ has parametric distributions that form a infinitely divisible nef whose induced measure is absolutely continuous with respect to its basis measure, then there exists a deterministic function $h$, called ""reduction function"", such that $\mathbb{e} \left(h\left(\xi\right)\right)=\mathbb{v}\left(\xi\right)$, i.e., $h\left(\xi\right)$ is an unbiased estimator of the variance of $\xi$. the reduction function has applications to estimating latent, low-dimensional structures and to dimension reduction in the first and/or second moments in high-dimensional data.","10.1016/j.spl.2016.06.016; 10.1016/j.spl.2018.02.010","2015-10-14","2016-03-19","['xiongzhi chen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1251",1510.04161,"d-vine copula based quantile regression","stat.me","quantile regression, that is the prediction of conditional quantiles, has steadily gained importance in statistical modeling and financial applications. the authors introduce a new semiparametric quantile regression method based on sequentially fitting a likelihood optimal d-vine copula to given data resulting in highly flexible models with easily extractable conditional quantiles. as a subclass of regular vine copulas, d-vines enable the modeling of multivariate copulas in terms of bivariate building blocks, a so-called pair-copula construction (pcc). the proposed algorithm works fast and accurate even in high dimensions and incorporates an automatic variable selection by maximizing the conditional log-likelihood. further, typical issues of quantile regression such as quantile crossing or transformations, interactions and collinearity of variables are automatically taken care of. in a simulation study the improved accuracy and saved computational time of the approach in comparison with established quantile regression methods is highlighted. an extensive financial application to international credit default swap (cds) data including stress testing and value-at-risk (var) prediction demonstrates the usefulness of the proposed method.","","2015-10-14","2016-11-16","['daniel kraus', 'claudia czado']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1252",1510.04482,"a two-component normal mixture alternative to the fay-herriot model","stat.me","this article considers a robust hierarchical bayesian approach to deal with random effects of small area means when some of these effects assume extreme values, resulting in outliers. in presence of outliers, the standard fay-herriot model, used for modeling area-level data, under normality assumptions of the random effects may overestimate random effects variance, thus provides less than ideal shrinkage towards the synthetic regression predictions and inhibits borrowing information. even a small number of substantive outliers of random effects result in a large estimate of the random effects variance in the fay-herriot model, thereby achieving little shrinkage to the synthetic part of the model or little reduction in posterior variance associated with the regular bayes estimator for any of the small areas. while a scale mixture of normal distributions with known mixing distribution for the random effects has been found to be effective in presence of outliers, the solution depends on the mixing distribution. as a possible alternative solution to the problem, a two-component normal mixture model has been proposed based on noninformative priors on the model variance parameters, regression coefficients and the mixing probability. data analysis and simulation studies based on real, simulated and synthetic data show advantage of the proposed method over the standard bayesian fay-herriot solution derived under normality of random effects.","","2015-10-15","2015-10-22","['adrijo chakraborty', 'gauri sankar datta', 'abhyuday mandal']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1253",1510.05078,"a general method for robust bayesian modeling","stat.ml","robust bayesian models are appealing alternatives to standard models, providing protection from data that contains outliers or other departures from the model assumptions. historically, robust models were mostly developed on a case-by-case basis; examples include robust linear regression, robust mixture models, and bursty topic models. in this paper we develop a general approach to robust bayesian modeling. we show how to turn an existing bayesian model into a robust model, and then develop a generic strategy for computing with it. we use our method to study robust variants of several models, including linear regression, poisson regression, logistic regression, and probabilistic topic models. we discuss the connections between our methods and existing approaches, especially empirical bayes and james-stein estimation.","","2015-10-17","2016-09-06","['chong wang', 'david m. blei']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1254",1510.06395,"the odd generalized exponential linear failure rate distribution","math.st stat.th","in this paper we propose a new lifetime model, called the odd generalized exponential linear failure rate distribution. some statistical properties of the proposed distribution such as the moments, the quantiles, the median, and the mode are investigated. the method of maximum likelihood is used for estimating the model parameters. an applications to real data is carried out to illustrate that the new distribution is more flexible and effective than other popular distributions in modeling lifetime data.","","2015-10-21","","['m. a. el-damcese', 'abdelfattah mustafa', 'b. s. el-desouky', 'm. e. mustafa']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1255",1510.06971,"the partial vine copula: a dependence measure and approximation based on   the simplifying assumption","stat.me","simplified vine copulas (svcs), or pair-copula constructions, have become an important tool in high-dimensional dependence modeling. so far, specification and estimation of svcs has been conducted under the simplifying assumption, i.e., all bivariate conditional copulas of the vine are assumed to be bivariate unconditional copulas. we introduce the partial vine copula (pvc) which provides a new multivariate dependence measure and which plays a major role in the approximation of multivariate distributions by svcs. the pvc is a particular svc where to any edge a j-th order partial copula is assigned and constitutes a multivariate analogue of the bivariate partial copula. we investigate to what extent the pvc describes the dependence structure of the underlying copula. we show that the pvc does not minimize the kullback-leibler divergence from the true copula and that the best approximation satisfying the simplifying assumption is given by a vine pseudo-copula. however, under regularity conditions, step-wise estimators of pair-copula constructions converge to the pvc irrespective of whether the simplifying assumption holds or not. moreover, we elucidate why the pvc is the best feasible svc approximation in practice.","","2015-10-23","2017-06-11","['fabian spanhel', 'malte s. kurz']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1256",1510.0785,"a bayesian model for microarray datasets merging","stat.me q-bio.qm","the aggregation of microarray datasets originating from different studies is still a difficult open problem. currently, best results are generally obtained by the so-called meta-analysis approach, which aggregates results from individual datasets, instead of analyzing aggre-gated datasets. in order to tackle such aggregation problems, it is necessary to correct for interstudy variability prior to aggregation. the goal of this paper is to present a new approach for microarray datasets merging, based upon explicit modeling of interstudy variability and gene variability. we develop and demonstrate a new algorithm for microarray datasets merging. the underlying model assumes normally distributed intrinsic gene expressions, distorted by a study-dependent nonlinear transformation, and study dependent (normally distributed) observation noise. the algorithm addresses both parameter estimation (the parameters being gene expression means and variances, observation noise variances and the nonlinear transformations) and data adjustment, and yields as a result adjusted datasets suitable for aggregation. the method is validated on two case studies. the first one concerns e. coli expression data, artificially distorted by given nonlinear transformations and additive observation noise. the proposed method is able to correct for the distortion, and yields adjusted datasets from which the relevant biological effects can be recovered, as shown by a standard differential analysis. the second case study concerns the aggregation of two real prostate cancer datasets. after adjustment using the proposed algorithm, a differential analysis performed on adjusted datasets yields a larger number of differentially expressed genes (between control and tumor data). the proposed method has been implemented using the statistical software r 1, and bioconductor packages 2. the source code (valid for merging two datasets), as well as the datasets used for the validation, and some complementary results, are made available on the web site","","2015-10-27","","['marie-christine roubaud', 'bruno torrésani']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1257",1510.0844,"priors on exchangeable directed graphs","math.st stat.me stat.ml stat.th","directed graphs occur throughout statistical modeling of networks, and exchangeability is a natural assumption when the ordering of vertices does not matter. there is a deep structural theory for exchangeable undirected graphs, which extends to the directed case via measurable objects known as digraphons. using digraphons, we first show how to construct models for exchangeable directed graphs, including special cases such as tournaments, linear orderings, directed acyclic graphs, and partial orderings. we then show how to construct priors on digraphons via the infinite relational digraphon model (di-irm), a new bayesian nonparametric block model for exchangeable directed graphs, and demonstrate inference on synthetic data.","10.1214/16-ejs1185","2015-10-28","2016-12-16","['diana cai', 'nathanael ackerman', 'cameron freer']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE
"1258",1510.08633,"nonconvex penalization in sparse estimation: an approach based on the   bernstein function","stat.ml","in this paper we study nonconvex penalization using bernstein functions whose first-order derivatives are completely monotone. the bernstein function can induce a class of nonconvex penalty functions for high-dimensional sparse estimation problems. we derive a thresholding function based on the bernstein penalty and discuss some important mathematical properties in sparsity modeling. we show that a coordinate descent algorithm is especially appropriate for regression problems penalized by the bernstein function. we also consider the application of the bernstein penalty in classification problems and devise a proximal alternating linearized minimization method. based on theory of the kurdyka-lojasiewicz inequality, we conduct convergence analysis of these alternating iteration procedures. we particularly exemplify a family of bernstein nonconvex penalties based on a generalized gamma measure and conduct empirical analysis for this family.","","2015-10-29","","['zhihua zhang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1259",1511.00054,"gaussian process random fields","cs.lg stat.ml","gaussian processes have been successful in both supervised and unsupervised machine learning tasks, but their computational complexity has constrained practical applications. we introduce a new approximation for large-scale gaussian processes, the gaussian process random field (gprf), in which local gps are coupled via pairwise potentials. the gprf likelihood is a simple, tractable, and parallelizeable approximation to the full gp marginal likelihood, enabling latent variable modeling and hyperparameter selection on large datasets. we demonstrate its effectiveness on synthetic spatial data as well as a real-world application to seismic event location.","","2015-10-30","","['david a. moore', 'stuart j. russell']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1260",1511.00148,"a survey on measuring indirect discrimination in machine learning","cs.cy stat.ap","nowadays, many decisions are made using predictive models built on historical data.predictive models may systematically discriminate groups of people even if the computing process is fair and well-intentioned. discrimination-aware data mining studies how to make predictive models free from discrimination, when historical data, on which they are built, may be biased, incomplete, or even contain past discriminatory decisions. discrimination refers to disadvantageous treatment of a person based on belonging to a category rather than on individual merit. in this survey we review and organize various discrimination measures that have been used for measuring discrimination in data, as well as in evaluating performance of discrimination-aware predictive models. we also discuss related measures from other disciplines, which have not been used for measuring discrimination, but potentially could be suitable for this purpose. we computationally analyze properties of selected measures. we also review and discuss measuring procedures, and present recommendations for practitioners. the primary target audience is data mining, machine learning, pattern recognition, statistical modeling researchers developing new methods for non-discriminatory predictive modeling. in addition, practitioners and policy makers would use the survey for diagnosing potential discrimination by predictive models.","","2015-10-31","","['indre zliobaite']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1261",1511.00154,"a bayesian nonparametric approach to reconstruction and prediction of   random dynamical systems","stat.ap stat.me","we propose a bayesian nonparametric mixture model for the reconstruction and prediction from observed time series data, of discretized stochastic dynamical systems, based on markov chain monte carlo methods (mcmc). our results can be used by researchers in physical modeling interested in a fast and accurate estimation of low dimensional stochastic models when the size of the observed time series is small and the noise process (perhaps) is non-gaussian. the inference procedure is demonstrated specifically in the case of polynomial maps of arbitrary degree and when a geometric stick breaking mixture process prior over the space of densities, is applied to the additive errors. our method is parsimonious compared to bayesian nonparametric techniques based on dirichlet process mixtures, flexible and general. simulations based on synthetic time series are presented.","","2015-10-31","2017-09-30","['christos merkatas', 'konstantinos kaloudis', 'spyridon j. hatjispyros']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1262",1511.00352,"spatial semantic scan: jointly detecting subtle events and their spatial   footprint","cs.lg cs.cl stat.ml","many methods have been proposed for detecting emerging events in text streams using topic modeling. however, these methods have shortcomings that make them unsuitable for rapid detection of locally emerging events on massive text streams. we describe spatially compact semantic scan (scss) that has been developed specifically to overcome the shortcomings of current methods in detecting new spatially compact events in text streams. scss employs alternating optimization between using semantic scan to estimate contrastive foreground topics in documents, and discovering spatial neighborhoods with high occurrence of documents containing the foreground topics. we evaluate our method on emergency department chief complaints dataset (ed dataset) to verify the effectiveness of our method in detecting real-world disease outbreaks from free-text ed chief complaint data.","","2015-11-01","2016-05-28","['abhinav maurya']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1263",1511.00521,"posterior predictive p-values with fisher randomization tests in   noncompliance settings: test statistics vs discrepancy variables","stat.me","in randomized experiments with noncompliance, tests may focus on compliers rather than on the overall sample. rubin (1998) put forth such a method, and argued that testing for the complier average causal effect and averaging permutation based p-values over the posterior distribution of the compliance status could increase power, as compared to general intent-to-treat tests. the general scheme is to repeatedly do a two-step process of imputing missing compliance statuses and conducting a permutation test with the completed data. in this paper, we explore this idea further, comparing the use of discrepancy measures, which depend on unknown but imputed parameters, to classical test statistics and exploring different approaches for imputing the unknown compliance statuses. we also examine consequences of model misspecification in the imputation step, and discuss to what extent this additional modeling undercuts the permutation test's model independence. we find that, especially for discrepancy measures, modeling choices can impact both power and validity. in particular, imputing missing compliance statuses assuming the null can radically reduce power, but not doing so can jeopardize validity. fortunately, covariates predictive of compliance status can mitigate these results. finally, we compare this overall approach to bayesian model-based tests, that is tests that are directly derived from posterior credible intervals, under both correct and incorrect model specification. we find that adding the permutation step in an otherwise bayesian approach improves robustness to model specification without substantial loss of power.","","2015-11-02","2016-02-20","['laura forastiere', 'fabrizia mealli', 'luke miratrix']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1264",1511.01169,"adaqn: an adaptive quasi-newton algorithm for training rnns","cs.lg math.oc stat.ml","recurrent neural networks (rnns) are powerful models that achieve exceptional performance on several pattern recognition problems. however, the training of rnns is a computationally difficult task owing to the well-known ""vanishing/exploding"" gradient problem. algorithms proposed for training rnns either exploit no (or limited) curvature information and have cheap per-iteration complexity, or attempt to gain significant curvature information at the cost of increased per-iteration cost. the former set includes diagonally-scaled first-order methods such as adagrad and adam, while the latter consists of second-order algorithms like hessian-free newton and k-fac. in this paper, we present adaqn, a stochastic quasi-newton algorithm for training rnns. our approach retains a low per-iteration cost while allowing for non-diagonal scaling through a stochastic l-bfgs updating scheme. the method uses a novel l-bfgs scaling initialization scheme and is judicious in storing and retaining l-bfgs curvature pairs. we present numerical experiments on two language modeling tasks and show that adaqn is competitive with popular rnn training algorithms.","","2015-11-03","2016-02-23","['nitish shirish keskar', 'albert s. berahas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1265",1511.01654,"a bayesian approach to the evaluation of risk-based microbiological   criteria for \uppercasecampylobacter in broiler meat","stat.ap","shifting from traditional hazard-based food safety management toward risk-based management requires statistical methods for evaluating intermediate targets in food production, such as microbiological criteria (mc), in terms of their effects on human risk of illness. a fully risk-based evaluation of mc involves several uncertainties that are related to both the underlying quantitative microbiological risk assessment (qmra) model and the production-specific sample data on the prevalence and concentrations of microbes in production batches. we used bayesian modeling for statistical inference and evidence synthesis of two sample data sets. thus, parameter uncertainty was represented by a joint posterior distribution, which we then used to predict the risk and to evaluate the criteria for acceptance of production batches. we also applied the bayesian model to compare alternative criteria, accounting for the statistical uncertainty of parameters, conditional on the data sets. comparison of the posterior mean relative risk, $e(\mathit{rr}|\mathrm{data})=e(p(\mathrm{illness}|\mathrm{criterion is met})/p(\mathrm{illness})|\mathrm{data})$, and relative posterior risk, $\mathit{rpr}=p(\mathrm{illness}|\mathrm{data, criterion is met})/p(\mathrm{illness}|\mathrm{data})$, showed very similar results, but computing is more efficient for rpr. based on the sample data, together with the qmra model, one could achieve a relative risk of 0.4 by insisting that the default criterion be fulfilled for acceptance of each batch.","10.1214/15-aoas845","2015-11-05","","['jukka ranta', 'roland lindqvist', 'ingrid hansson', 'pirkko tuominen', 'maarten nauta']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1266",1511.02796,"bayesian inference in cumulative distribution fields","stat.ml stat.me","one approach for constructing copula functions is by multiplication. given that products of cumulative distribution functions (cdfs) are also cdfs, an adjustment to this multiplication will result in a copula model, as discussed by liebscher (j mult analysis, 2008). parameterizing models via products of cdfs has some advantages, both from the copula perspective (e.g., it is well-defined for any dimensionality) and from general multivariate analysis (e.g., it provides models where small dimensional marginal distributions can be easily read-off from the parameters). independently, huang and frey (j mach learn res, 2011) showed the connection between certain sparse graphical models and products of cdfs, as well as message-passing (dynamic programming) schemes for computing the likelihood function of such models. such schemes allows models to be estimated with likelihood-based methods. we discuss and demonstrate mcmc approaches for estimating such models in a bayesian context, their application in copula modeling, and how message-passing can be strongly simplified. importantly, our view of message-passing opens up possibilities to scaling up such methods, given that even dynamic programming is not a scalable solution for calculating likelihood functions in many models.","","2015-11-09","","['ricardo silva']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1267",1511.03046,"improvement of code behaviour in a design of experiments by metamodeling","stat.co","it is now common practice in nuclear engineering to base extensive studies on numerical computer models. these studies require to run computer codes in potentially thousands of numerical configurations and without expert individual controls on the computational and physical aspects of each simulations.in this paper, we compare different statistical metamodeling techniques and show how metamodels can help to improve the global behaviour of codes in these extensive studies. we consider the metamodeling of the germinal thermalmechanical code by kriging, kernel regression and neural networks. kriging provides the most accurate predictions while neural networks yield the fastest metamodel functions. all three metamodels can conveniently detect strong computation failures. it is however significantly more challenging to detect code instabilities, that is groups of computations that are all valid, but numerically inconsistent with one another. for code instability detection, we find that kriging provides the most useful tools.","","2015-11-10","","['françois bachoc', 'jean-marc martinez', 'karim ammar']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1268",1511.0312,"the cave of shadows. addressing the human factor with generalized   additive mixed models","stat.ap","generalized additive mixed models are introduced as an extension of the generalized linear mixed model which makes it possible to deal with temporal autocorrelational structure in experimental data. this autocorrelational structure is likely to be a consequence of learning, fatigue, or the ebb and flow of attention within an experiment (the `human factor'). unlike molecules or plots of barley, subjects in psycholinguistic experiments are intelligent beings that depend for their survival on constant adaptation to their environment, including the environment of an experiment. three data sets illustrate that the human factor may interact with predictors of interest, both factorial and metric. we also show that, especially within the framework of the generalized additive model, in the nonlinear world, fitting maximally complex models that take every possible contingency into account is ill-advised as a modeling strategy. alternative modeling strategies are discussed for both confirmatory and exploratory data analysis.","","2015-11-10","2016-11-14","['harald baayen', 'shravan vasishth', 'douglas bates', 'reinhold kliegl']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1269",1511.03229,"learning communities in the presence of errors","cs.ds cs.lg math.st stat.th","we study the problem of learning communities in the presence of modeling errors and give robust recovery algorithms for the stochastic block model (sbm). this model, which is also known as the planted partition model, is widely used for community detection and graph partitioning in various fields, including machine learning, statistics, and social sciences. many algorithms exist for learning communities in the stochastic block model, but they do not work well in the presence of errors.   in this paper, we initiate the study of robust algorithms for partial recovery in sbm with modeling errors or noise. we consider graphs generated according to the stochastic block model and then modified by an adversary. we allow two types of adversarial errors, feige---kilian or monotone errors, and edge outlier errors. mossel, neeman and sly (stoc 2015) posed an open question about whether an almost exact recovery is possible when the adversary is allowed to add $o(n)$ edges. our work answers this question affirmatively even in the case of $k>2$ communities.   we then show that our algorithms work not only when the instances come from sbm, but also work when the instances come from any distribution of graphs that is $\epsilon m$ close to sbm in the kullback---leibler divergence. this result also works in the presence of adversarial errors. finally, we present almost tight lower bounds for two communities.","","2015-11-10","2016-06-24","['konstantin makarychev', 'yury makarychev', 'aravindan vijayaraghavan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1270",1511.03947,"bayesian analysis of dynamic linear topic models","stat.ml cs.lg stat.me","in dynamic topic modeling, the proportional contribution of a topic to a document depends on the temporal dynamics of that topic's overall prevalence in the corpus. we extend the dynamic topic model of blei and lafferty (2006) by explicitly modeling document level topic proportions with covariates and dynamic structure that includes polynomial trends and periodicity. a markov chain monte carlo (mcmc) algorithm that utilizes polya-gamma data augmentation is developed for posterior inference. conditional independencies in the model and sampling are made explicit, and our mcmc algorithm is parallelized where possible to allow for inference in large corpora. to address computational bottlenecks associated with polya-gamma sampling, we appeal to the central limit theorem to develop a gaussian approximation to the polya-gamma random variable. this approximation is fast and reliable for parameter values relevant in the text mining domain. our model and inference algorithm are validated with multiple simulation examples, and we consider the application of modeling trends in pubmed abstracts. we demonstrate that sharing information across documents is critical for accurately estimating document-specific topic proportions. we also show that explicitly modeling polynomial and periodic behavior improves our ability to predict topic prevalence at future time points.","","2015-11-12","","['chris glynn', 'surya t. tokdar', 'david l. banks', 'brian howard']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1271",1511.04128,"a widely linear complex autoregressive process of order one","stat.me math.st stat.th","we propose a simple stochastic process for modeling improper or noncircular complex-valued signals. the process is a natural extension of a complex-valued autoregressive process, extended to include a widely linear autoregressive term. this process can then capture elliptical, as opposed to circular, stochastic oscillations in a bivariate signal. the process is order one and is more parsimonious than alternative stochastic modeling approaches in the literature. we provide conditions for stationarity, and derive the form of the covariance and relation sequence of this model. we describe how parameter estimation can be efficiently performed both in the time and frequency domain. we demonstrate the practical utility of the process in capturing elliptical oscillations that are naturally present in seismic signals.","10.1109/tsp.2016.2599503","2015-11-12","2017-03-15","['adam m. sykulski', 'sofia c. olhede', 'jonathan m. lilly']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1272",1511.04817,"probabilistic segmentation via total variation regularization","stat.ml","we present a convex approach to probabilistic segmentation and modeling of time series data. our approach builds upon recent advances in multivariate total variation regularization, and seeks to learn a separate set of parameters for the distribution over the observations at each time point, but with an additional penalty that encourages the parameters to remain constant over time. we propose efficient optimization methods for solving the resulting (large) optimization problems, and a two-stage procedure for estimating recurring clusters under such models, based upon kernel density estimation. finally, we show on a number of real-world segmentation tasks, the resulting methods often perform as well or better than existing latent variable models, while being substantially easier to train.","","2015-11-15","","['matt wytock', 'j. zico kolter']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1273",1511.05042,"an exploration of softmax alternatives belonging to the spherical loss   family","cs.ne cs.lg stat.ml","in a multi-class classification problem, it is standard to model the output of a neural network as a categorical distribution conditioned on the inputs. the output must therefore be positive and sum to one, which is traditionally enforced by a softmax. this probabilistic mapping allows to use the maximum likelihood principle, which leads to the well-known log-softmax loss. however the choice of the softmax function seems somehow arbitrary as there are many other possible normalizing functions. it is thus unclear why the log-softmax loss would perform better than other loss alternatives. in particular vincent et al. (2015) recently introduced a class of loss functions, called the spherical family, for which there exists an efficient algorithm to compute the updates of the output weights irrespective of the output size. in this paper, we explore several loss functions from this family as possible alternatives to the traditional log-softmax. in particular, we focus our investigation on spherical bounds of the log-softmax loss and on two spherical log-likelihood losses, namely the log-spherical softmax suggested by vincent et al. (2015) and the log-taylor softmax that we introduce. although these alternatives do not yield as good results as the log-softmax loss on two language modeling tasks, they surprisingly outperform it in our experiments on mnist and cifar-10, suggesting that they might be relevant in a broad range of applications.","","2015-11-16","2016-02-28","['alexandre de brébisson', 'pascal vincent']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1274",1511.05121,"deep kalman filters","stat.ml cs.lg","kalman filters are one of the most influential models of time-varying phenomena. they admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption in a variety of disciplines. motivated by recent variational methods for learning deep generative models, we introduce a unified algorithm to efficiently learn a broad spectrum of kalman filters. of particular interest is the use of temporal generative models for counterfactual inference. we investigate the efficacy of such models for counterfactual inference, and to that end we introduce the ""healing mnist"" dataset where long-term structure, noise and actions are applied to sequences of digits. we show the efficacy of our method for modeling this dataset. we further show how our model can be used for counterfactual inference for patients, based on electronic health record data of 8,000 patients over 4.5 years.","","2015-11-16","2015-11-25","['rahul g. krishnan', 'uri shalit', 'david sontag']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1275",1511.05174,"cross-scale predictive dictionaries","cs.cv stat.ml","sparse representations using data dictionaries provide an efficient model particularly for signals that do not enjoy alternate analytic sparsifying transformations. however, solving inverse problems with sparsifying dictionaries can be computationally expensive, especially when the dictionary under consideration has a large number of atoms. in this paper, we incorporate additional structure on to dictionary-based sparse representations for visual signals to enable speedups when solving sparse approximation problems. the specific structure that we endow onto sparse models is that of a multi-scale modeling where the sparse representation at each scale is constrained by the sparse representation at coarser scales. we show that this cross-scale predictive model delivers significant speedups, often in the range of 10-60$\times$, with little loss in accuracy for linear inverse problems associated with images, videos, and light fields.","","2015-11-16","2018-09-03","['vishwanath saragadam', 'xin li', 'aswin sankaranarayanan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1276",1511.05303,"an invitation to coupling and copulas: with applications to multisensory   modeling","stat.me q-bio.qm q-fin.rm","this paper presents an introduction to the stochastic concepts of \emph{coupling} and \emph{copula}. coupling means the construction of a joint distribution of two or more random variables that need not be defined on one and the same probability space, whereas a copula is a function that joins a multivariate distribution to its one-dimensional margins. their role in stochastic modeling is illustrated by examples from multisensory perception. pointers to more advanced and recent treatments are provided.","","2015-11-17","","['hans colonius']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1277",1511.0536,"inferring constructs of effective teaching from classroom observations:   an application of bayesian exploratory factor analysis without restrictions","stat.ap","ratings of teachers' instructional practices using standardized classroom observation instruments are increasingly being used for both research and teacher accountability. there are multiple instruments in use, each attempting to evaluate many dimensions of teaching and classroom activities, and little is known about what underlying teaching quality attributes are being measured. we use data from multiple instruments collected from 458 middle school mathematics and english language arts teachers to inform research and practice on teacher performance measurement by modeling latent constructs of high-quality teaching. we make inferences about these constructs using a novel approach to bayesian exploratory factor analysis (efa) that, unlike commonly used approaches for identifying factor loadings in bayesian efa, is invariant to how the data dimensions are ordered. applying this approach to ratings of lessons reveals two distinct teaching constructs in both mathematics and english language arts: (1) quality of instructional practices; and (2) quality of teacher management of classrooms. we demonstrate the relationships of these constructs to other indicators of teaching quality, including teacher content knowledge and student performance on standardized tests.","10.1214/15-aoas833","2015-11-17","","['j. r. lockwood', 'terrance d. savitsky', 'daniel f. mccaffrey']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1278",1511.05372,"bayesian analysis of ambulatory blood pressure dynamics with application   to irregularly spaced sparse data","stat.ap","ambulatory cardiovascular (cv) measurements provide valuable insights into individuals' health conditions in ""real-life,"" everyday settings. current methods of modeling ambulatory cv data do not consider the dynamic characteristics of the full data set and their relationships with covariates such as caffeine use and stress. we propose a stochastic differential equation (sde) in the form of a dual nonlinear ornstein--uhlenbeck (ou) model with person-specific covariates to capture the morning surge and nighttime dipping dynamics of ambulatory cv data. to circumvent the data analytic constraint that empirical measurements are typically collected at irregular and much larger time intervals than those evaluated in simulation studies of sdes, we adopt a bayesian approach with a regularized brownian bridge sampler (rbbs) and an efficient multiresolution (mr) algorithm to fit the proposed sde. the mr algorithm can produce more efficient mcmc samples that is crucial for valid parameter estimation and inference. using this model and algorithm to data from the duke behavioral investigation of hypertension study, results indicate that age, caffeine intake, gender and race have effects on distinct dynamic characteristics of the participants' cv trajectories.","10.1214/15-aoas846","2015-11-17","2017-01-10","['zhao-hua lu', 'sy-miin chow', 'andrew sherwood', 'hongtu zhu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1279",1511.05491,"supervised dimension reduction for ordinal predictors","math.st stat.me stat.th","in applications involving ordinal predictors, common approaches to reduce dimensionality are either extensions of unsupervised techniques such as principal component analysis, or variable selection procedures that rely on modeling the regression function. in this paper, a supervised dimension reduction method tailored to ordered categorical predictors is introduced. it uses a model-based dimension reduction approach, inspired by extending sufficient dimension reductions to the context of latent gaussian variables. the reduction is chosen without modeling the response as a function of the predictors and does not impose any distributional assumption on the response or on the response given the predictors. a likelihood-based estimator of the reduction is derived and an iterative expectation-maximization type algorithm is proposed to alleviate the computational load and thus make the method more practical. a regularized estimator, which simultaneously achieves variable selection and dimension reduction, is also presented. performance of the proposed method is evaluated through simulations and a real data example for socioeconomic index construction, comparing favorably to widespread use techniques.","","2015-11-17","2017-10-12","['liliana forzani', 'rodrigo garcía arancibia', 'pamela llop', 'diego tomassi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1280",1511.0635,"structured prediction energy networks","cs.lg stat.ml","we introduce structured prediction energy networks (spens), a flexible framework for structured prediction. a deep architecture is used to define an energy function of candidate labels, and then predictions are produced by using back-propagation to iteratively optimize the energy with respect to the labels. this deep architecture captures dependencies between labels that would lead to intractable graphical models, and performs structure learning by automatically learning discriminative features of the structured output. one natural application of our technique is multi-label classification, which traditionally has required strict prior assumptions about the interactions between labels to ensure tractable learning and prediction. we are able to apply spens to multi-label problems with substantially larger label sets than previous applications of structured prediction, while modeling high-order interactions using minimal structural assumptions. overall, deep learning provides remarkable tools for learning features of the inputs to a prediction problem, and this work extends these techniques to learning features of structured outputs. our experiments provide impressive performance on a variety of benchmark multi-label classification tasks, demonstrate that our technique can be used to provide interpretable structure learning, and illuminate fundamental trade-offs between feed-forward and iterative structured prediction.","","2015-11-19","2016-06-23","['david belanger', 'andrew mccallum']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1281",1511.06391,"order matters: sequence to sequence for sets","stat.ml cs.cl cs.lg","sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. in many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. for instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. in this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. we then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. in addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. we show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models.","","2015-11-19","2016-02-23","['oriol vinyals', 'samy bengio', 'manjunath kudlur']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1282",1511.06644,"recurrent gaussian processes","cs.lg stat.ml","we define recurrent gaussian processes (rgp) models, a general family of bayesian nonparametric models with recurrent gp priors which are able to learn dynamical patterns from sequential data. similar to recurrent neural networks (rnns), rgps can have different formulations for their internal states, distinct inference methods and be extended with deep structures. in such context, we propose a novel deep rgp model whose autoregressive states are latent, thereby performing representation and dynamical learning simultaneously. to fully exploit the bayesian nature of the rgp model we develop the recurrent variational bayes (revarb) framework, which enables efficient inference and strong regularization through coherent propagation of uncertainty across the rgp layers and states. we also introduce a rgp extension where variational parameters are greatly reduced by being reparametrized through rnn-based sequential recognition models. we apply our model to the tasks of nonlinear system identification and human motion modeling. the promising obtained results indicate that our rgp model maintains its highly flexibility while being able to avoid overfitting and being applicable even when larger datasets are not available.","","2015-11-20","2016-02-24","['césar lincoln c. mattos', 'zhenwen dai', 'andreas damianou', 'jeremy forth', 'guilherme a. barreto', 'neil d. lawrence']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1283",1511.06909,"blackout: speeding up recurrent neural network language models with very   large vocabularies","cs.lg cs.cl cs.ne stat.ml","we propose blackout, an approximation algorithm to efficiently train massive recurrent neural network language models (rnnlms) with million word vocabularies. blackout is motivated by using a discriminative loss, and we describe a new sampling strategy which significantly reduces computation while improving stability, sample efficiency, and rate of convergence. one way to understand blackout is to view it as an extension of the dropout strategy to the output layer, wherein we use a discriminative training loss and a weighted sampling scheme. we also establish close connections between blackout, importance sampling, and noise contrastive estimation (nce). our experiments, on the recently released one billion word language modeling benchmark, demonstrate scalability and accuracy of blackout; we outperform the state-of-the art, and achieve the lowest perplexity scores on this dataset. moreover, unlike other established methods which typically require gpus or cpu clusters, we show that a carefully implemented version of blackout requires only 1-10 days on a single machine to train a rnnlm with a million word vocabulary and billions of parameters on one billion words. although we describe blackout in the context of rnnlm training, it can be used to any networks with large softmax output layers.","","2015-11-21","2016-03-31","['shihao ji', 's. v. n. vishwanathan', 'nadathur satish', 'michael j. anderson', 'pradeep dubey']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1284",1511.07367,"black box variational inference for state space models","stat.ml","latent variable time-series models are among the most heavily used tools from machine learning and applied statistics. these models have the advantage of learning latent structure both from noisy observations and from the temporal ordering in the data, where it is assumed that meaningful correlation structure exists across time. a few highly-structured models, such as the linear dynamical system with linear-gaussian observations, have closed-form inference procedures (e.g. the kalman filter), but this case is an exception to the general rule that exact posterior inference in more complex generative models is intractable. consequently, much work in time-series modeling focuses on approximate inference procedures for one particular class of models. here, we extend recent developments in stochastic variational inference to develop a `black-box' approximate inference technique for latent variable models with latent dynamical structure. we propose a structured gaussian variational approximate posterior that carries the same intuition as the standard kalman filter-smoother but, importantly, permits us to use the same inference approach to approximate the posterior of much more general, nonlinear latent variable generative models. we show that our approach recovers accurate estimates in the case of basic models with closed-form posteriors, and more interestingly performs well in comparison to variational approaches that were designed in a bespoke fashion for specific non-conjugate models.","","2015-11-23","","['evan archer', 'il memming park', 'lars buesing', 'john cunningham', 'liam paninski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1285",1511.07492,"polynomial meta-models with canonical low-rank approximations: numerical   insights and comparison to sparse polynomial chaos expansions","math.na stat.co","the growing need for uncertainty analysis of complex computational models has led to an expanding use of meta-models across engineering and sciences. the efficiency of meta-modeling techniques relies on their ability to provide statistically-equivalent analytical representations based on relatively few evaluations of the original model. polynomial chaos expansions (pce) have proven a powerful tool for developing meta-models in a wide range of applications; the key idea thereof is to expand the model response onto a basis made of multivariate polynomials obtained as tensor products of appropriate univariate polynomials. the classical pce approach nevertheless faces the ""curse of dimensionality"", namely the exponential increase of the basis size with increasing input dimension. to address this limitation, the sparse pce technique has been proposed, in which the expansion is carried out on only a few relevant basis terms that are automatically selected by a suitable algorithm. an alternative for developing meta-models with polynomial functions in high-dimensional problems is offered by the newly emerged low-rank approximations (lra) approach. by exploiting the tensor-product structure of the multivariate basis, lra can provide polynomial representations in highly compressed formats. through extensive numerical investigations, we herein first shed light on issues relating to the construction of canonical lra with a particular greedy algorithm involving a sequential updating of the polynomial coefficients along separate dimensions. canonical lra exhibit smaller errors than sparse pce in cases when the number of available model evaluations is small with respect to the input dimension. by introducing the conditional generalization error, we further demonstrate that canonical lra tend to outperform sparse pce in the prediction of extreme model responses.","10.1016/j.jcp.2016.06.005","2015-11-23","2016-04-15","['katerina konakli', 'bruno sudret']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1286",1511.07903,"flexible design for $\alpha$-duplex communications in multi-tier   cellular networks","cs.it math.it math.st stat.th","backward compatibility is an essential ingredient for the success of new technologies. in the context of in-band full-duplex (fd) communication, fd base stations (bss) should support half-duplex (hd) users' equipment (ues) without sacrificing the foreseen fd gains. this paper presents flexible and tractable modeling framework for multi-tier cellular networks with fd bss and fd/hd ues. the presented model is based on stochastic geometry and accounts for the intrinsic vulnerability of uplink transmissions. the results show that fd ues are not necessarily required to harvest rate gains from fd bss. in particular, the results show that adding fd ues to fd bss offers a maximum of $5\%$ rate gain over fd bss and hd ues case if multi-user diversity is exploited, which is a marginal gain compared to the burden required to implement fd transceivers at the ues' side. to this end, we shed light on practical scenarios where hd ues operation with fd bss outperforms the operation when both the bss and ues are fd and we find a closed form expression for the critical value of the self-interference attenuation power required for the fd ues to outperform hd ues.","","2015-11-24","2016-04-20","['ahmad alammouri', 'hesham elsawy', 'mohamed-slim alouini']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1287",1511.08343,"the automatic statistician: a relational perspective","cs.lg stat.ml","gaussian processes (gps) provide a general and analytically tractable way of modeling complex time-varying, nonparametric functions. the automatic bayesian covariance discovery (abcd) system constructs natural-language description of time-series data by treating unknown time-series data nonparametrically using gp with a composite covariance kernel function. unfortunately, learning a composite covariance kernel with a single time-series data set often results in less informative kernel that may not give qualitative, distinctive descriptions of data. we address this challenge by proposing two relational kernel learning methods which can model multiple time-series data sets by finding common, shared causes of changes. we show that the relational kernel learning methods find more accurate models for regression problems on several real-world data sets; us stock data, us house price index data and currency exchange rate data.","","2015-11-26","2016-02-11","['yunseong hwang', 'anh tong', 'jaesik choi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1288",1511.084,"regularizing rnns by stabilizing activations","cs.ne cs.cl cs.lg stat.ml","we stabilize the activations of recurrent neural networks (rnns) by penalizing the squared distance between successive hidden states' norms.   this penalty term is an effective regularizer for rnns including lstms and irnns, improving performance on character-level language modeling and phoneme recognition, and outperforming weight noise and dropout.   we achieve competitive performance (18.6\% per) on the timit phoneme recognition task for rnns evaluated without beam search or an rnn transducer.   with this penalty term, irnn can achieve similar performance to lstm on language modeling, although adding the penalty term to the lstm results in superior performance.   our penalty term also prevents the exponential growth of irnn's activations outside of their training horizon, allowing them to generalize to much longer sequences.","","2015-11-26","2016-04-26","['david krueger', 'roland memisevic']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1289",1511.08551,"regularized em algorithms: a unified framework and statistical   guarantees","cs.lg stat.ml","latent variable models are a fundamental modeling tool in machine learning applications, but they present significant computational and analytical challenges. the popular em algorithm and its variants, is a much used algorithmic tool; yet our rigorous understanding of its performance is highly incomplete. recently, work in balakrishnan et al. (2014) has demonstrated that for an important class of problems, em exhibits linear local convergence. in the high-dimensional setting, however, the m-step may not be well defined. we address precisely this setting through a unified treatment using regularization. while regularization for high-dimensional problems is by now well understood, the iterative em algorithm requires a careful balancing of making progress towards the solution while identifying the right structure (e.g., sparsity or low-rank). in particular, regularizing the m-step using the state-of-the-art high-dimensional prescriptions (e.g., wainwright (2014)) is not guaranteed to provide this balance. our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors. we specialize our general framework to sparse gaussian mixture models, high-dimensional mixed regression, and regression with missing variables, obtaining statistical guarantees for each of these examples.","","2015-11-26","2015-12-05","['xinyang yi', 'constantine caramanis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1290",1511.08627,"on asymptotic properties of the separating hill estimator","math.st stat.th","modeling and understanding multivariate extreme events is challenging, but of great importance in various applications - e.g. in biostatistics, climatology, and finance. the separating hill estimator can be used in estimating the extreme value index of a heavy tailed multivariate elliptical distribution. we consider the asymptotic behavior of the separating hill estimator under estimated location and scatter. the asymptotic properties of the separating hill estimator are known under elliptical distribution with known location and scatter. however, the effect of estimation of the location and scatter has previously been examined only in a simulation study. we show, analytically, that the separating hill estimator is consistent and asymptotically normal under estimated location and scatter, when certain mild conditions are met.","","2015-11-27","2016-01-12","['matias heikkilä', 'yves dominicy', 'pauliina ilmonen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1291",1511.08775,"adjusted priors for bayes factors involving reparameterized order   constraints","stat.me stat.ap","many psychological theories that are instantiated as statistical models imply order constraints on the model parameters. to fit and test such restrictions, order constraints of the form $\theta_i \leq \theta_j$ can be reparameterized with auxiliary parameters $\eta\in [0,1]$ to replace the original parameters by $\theta_i = \eta\cdot\theta_j$. this approach is especially common in multinomial processing tree (mpt) modeling because the reparameterized, less complex model also belongs to the mpt class. here, we discuss the importance of adjusting the prior distributions for the auxiliary parameters of a reparameterized model. this adjustment is important for computing the bayes factor, a model selection criterion that measures the evidence in favor of an order constraint by trading off model fit and complexity. we show that uniform priors for the auxiliary parameters result in a bayes factor that differs from the one that is obtained using a multivariate uniform prior on the order-constrained original parameters. as a remedy, we derive the adjusted priors for the auxiliary parameters of the reparameterized model. the practical relevance of the problem is underscored with a concrete example using the multi-trial pair-clustering model.","10.1016/j.jmp.2016.05.004","2015-11-27","2016-05-07","['daniel w. heck', 'eric-jan wagenmakers']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1292",1512.0015,"optimal estimation and completion of matrices with biclustering   structures","math.st stat.ml stat.th","biclustering structures in data matrices were first formalized in a seminal paper by john hartigan (1972) where one seeks to cluster cases and variables simultaneously. such structures are also prevalent in block modeling of networks. in this paper, we develop a unified theory for the estimation and completion of matrices with biclustering structures, where the data is a partially observed and noise contaminated data matrix with a certain biclustering structure. in particular, we show that a constrained least squares estimator achieves minimax rate-optimal performance in several of the most important scenarios. to this end, we derive unified high probability upper bounds for all sub-gaussian data and also provide matching minimax lower bounds in both gaussian and binary cases. due to the close connection of graphon to stochastic block models, an immediate consequence of our general results is a minimax rate-optimal estimator for sparse graphons.","","2015-12-01","2018-10-22","['chao gao', 'yu lu', 'zongming ma', 'harrison h. zhou']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"1293",1512.00487,"the joint projected and skew normal","stat.ap","we introduce a new multivariate circular linear distribution suitable for modeling direction and speed in (multiple) animal movement data. to properly account for specific data features, such as heterogeneity and time dependence, a hidden markov model is used. parameters are estimated under a bayesian framework and we provide computational details to implement the markov chain monte carlo algorithm.   the proposed model is applied to a dataset of six free-ranging maremma sheepdogs. its predictive performance, as well as the interpretability of the results, are compared to those given by hidden markov models built on all the combinations of von mises (circular), wrapped cauchy (circular), gamma (linear) and weibull (linear) distributions","","2015-12-01","2016-06-26","['gianluca mastrantonio']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1294",1512.00984,"fast low-rank matrix learning with nonconvex regularization","cs.na cs.lg stat.ml","low-rank modeling has a lot of important applications in machine learning, computer vision and social network analysis. while the matrix rank is often approximated by the convex nuclear norm, the use of nonconvex low-rank regularizers has demonstrated better recovery performance. however, the resultant optimization problem is much more challenging. a very recent state-of-the-art is based on the proximal gradient algorithm. however, it requires an expensive full svd in each proximal step. in this paper, we show that for many commonly-used nonconvex low-rank regularizers, a cutoff can be derived to automatically threshold the singular values obtained from the proximal operator. this allows the use of power method to approximate the svd efficiently. besides, the proximal operator can be reduced to that of a much smaller matrix projected onto this leading subspace. convergence, with a rate of o(1/t) where t is the number of iterations, can be guaranteed. extensive experiments are performed on matrix completion and robust principal component analysis. the proposed method achieves significant speedup over the state-of-the-art. moreover, the matrix solution obtained is more accurate and has a lower rank than that of the traditional nuclear norm regularizer.","","2015-12-03","","['quanming yao', 'james t. kwok', 'wenliang zhong']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1295",1512.01272,"crosscat: a fully bayesian nonparametric method for analyzing   heterogeneous, high dimensional data","cs.ai stat.co stat.ml","there is a widespread need for statistical methods that can analyze high-dimensional datasets with- out imposing restrictive or opaque modeling assumptions. this paper describes a domain-general data analysis method called crosscat. crosscat infers multiple non-overlapping views of the data, each consisting of a subset of the variables, and uses a separate nonparametric mixture to model each view. crosscat is based on approximately bayesian inference in a hierarchical, nonparamet- ric model for data tables. this model consists of a dirichlet process mixture over the columns of a data table in which each mixture component is itself an independent dirichlet process mixture over the rows; the inner mixture components are simple parametric models whose form depends on the types of data in the table. crosscat combines strengths of mixture modeling and bayesian net- work structure learning. like mixture modeling, crosscat can model a broad class of distributions by positing latent variables, and produces representations that can be efficiently conditioned and sampled from for prediction. like bayesian networks, crosscat represents the dependencies and independencies between variables, and thus remains accurate when there are multiple statistical signals. inference is done via a scalable gibbs sampling scheme; this paper shows that it works well in practice. this paper also includes empirical results on heterogeneous tabular data of up to 10 million cells, such as hospital cost and quality measures, voting records, unemployment rates, gene expression measurements, and images of handwritten digits. crosscat infers structure that is consistent with accepted findings and common-sense knowledge in multiple domains and yields predictive accuracy competitive with generative, discriminative, and model-free alternatives.","","2015-12-03","","['vikash mansinghka', 'patrick shafto', 'eric jonas', 'cap petschulat', 'max gasner', 'joshua b. tenenbaum']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE
"1296",1512.01408,"neuron's eye view: inferring features of complex stimuli from neural   responses","stat.ml q-bio.nc","experiments that study neural encoding of stimuli at the level of individual neurons typically choose a small set of features present in the world --- contrast and luminance for vision, pitch and intensity for sound --- and assemble a stimulus set that systematically varies along these dimensions. subsequent analysis of neural responses to these stimuli typically focuses on regression models, with experimenter-controlled features as predictors and spike counts or firing rates as responses. unfortunately, this approach requires knowledge in advance about the relevant features coded by a given population of neurons. for domains as complex as social interaction or natural movement, however, the relevant feature space is poorly understood, and an arbitrary \emph{a priori} choice of features may give rise to confirmation bias. here, we present a bayesian model for exploratory data analysis that is capable of automatically identifying the features present in unstructured stimuli based solely on neuronal responses. our approach is unique within the class of latent state space models of neural activity in that it assumes that firing rates of neurons are sensitive to multiple discrete time-varying features tied to the \emph{stimulus}, each of which has markov (or semi-markov) dynamics. that is, we are modeling neural activity as driven by multiple simultaneous stimulus features rather than intrinsic neural dynamics. we derive a fast variational bayesian inference algorithm and show that it correctly recovers hidden features in synthetic data, as well as ground-truth stimulus features in a prototypical neural dataset. to demonstrate the utility of the algorithm, we also apply it to cluster neural responses and demonstrate successful recovery of features corresponding to monkeys and faces in the image set.","","2015-12-04","2016-11-21","['n/a xin', 'n/a chen', 'jeffrey m beck', 'john m pearson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1297",1512.01631,"hierarchical sparse modeling: a choice of two group lasso formulations","stat.me math.st stat.co stat.ml stat.th","demanding sparsity in estimated models has become a routine practice in statistics. in many situations, we wish to require that the sparsity patterns attained honor certain problem-specific constraints. hierarchical sparse modeling (hsm) refers to situations in which these constraints specify that one set of parameters be set to zero whenever another is set to zero. in recent years, numerous papers have developed convex regularizers for this form of sparsity structure, which arises in many areas of statistics including interaction modeling, time series analysis, and covariance estimation. in this paper, we observe that these methods fall into two frameworks, the group lasso (gl) and latent overlapping group lasso (log), which have not been systematically compared in the context of hsm. the purpose of this paper is to provide a side-by-side comparison of these two frameworks for hsm in terms of their statistical properties and computational efficiency. we call special attention to gl's more aggressive shrinkage of parameters deep in the hierarchy, a property not shared by log. in terms of computation, we introduce a finite-step algorithm that exactly solves the proximal operator of log for a certain simple hsm structure; we later exploit this to develop a novel path-based block coordinate descent scheme for general hsm structures. both algorithms greatly improve the computational performance of log. finally, we compare the two methods in the context of covariance estimation, where we introduce a new sparsely-banded estimator using log, which we show achieves the statistical advantages of an existing gl-based method but is simpler to express and more efficient to compute.","10.1214/17-sts622","2015-12-05","2017-11-29","['xiaohan yan', 'jacob bien']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE
"1298",1512.01639,"pjait systems for the iwslt 2015 evaluation campaign enhanced by   comparable corpora","cs.cl stat.ml","in this paper, we attempt to improve statistical machine translation (smt) systems on a very diverse set of language pairs (in both directions): czech - english, vietnamese - english, french - english and german - english. to accomplish this, we performed translation model training, created adaptations of training settings for each language pair, and obtained comparable corpora for our smt systems. innovative tools and data adaptation techniques were employed. the ted parallel text corpora for the iwslt 2015 evaluation campaign were used to train language models, and to develop, tune, and test the system. in addition, we prepared wikipedia-based comparable corpora for use with our smt system. this data was specified as permissible for the iwslt 2015 evaluation. we explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the kenlm language modeling tool. to evaluate the effects of different preparations on translation results, we conducted experiments and used the bleu, nist and ter metrics. our results indicate that our approach produced a positive impact on smt quality.","","2015-12-05","","['krzysztof wołk', 'krzysztof marasek']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1299",1512.03036,"semi-parametric models for accelerated destructive degradation test data   analysis","stat.me","accelerated destructive degradation tests (addt) are widely used in industry to evaluate materials' long term properties. even though there has been tremendous statistical research in nonparametric methods, the current industrial practice is still to use application-specific parametric models to describe addt data. the challenge of using a nonparametric approach comes from the need to retain the physical meaning of degradation mechanisms and also perform extrapolation for predictions at the use condition. motivated by this challenge, we propose a semi-parametric model to describe addt data. we use monotonic b-splines to model the degradation path, which not only provides flexible models with few assumptions, but also retains the physical meaning of degradation mechanisms (e.g., the degradation path is monotonically decreasing). parametric models, such as the arrhenius model, are used for modeling the relationship between the degradation and accelerating variable, allowing for extrapolation to the use conditions. we develop an efficient procedure to estimate model parameters. we also use simulation to validate the developed procedures and demonstrate the robustness of the semi-parametric model under model misspecification. finally, the proposed method is illustrated by multiple industrial applications.","","2015-12-09","","['yimeng xie', 'caleb b. king', 'yili hong', 'qingyu yang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1300",1512.03232,"extremal dependence concepts","stat.me","the probabilistic characterization of the relationship between two or more random variables calls for a notion of dependence. dependence modeling leads to mathematical and statistical challenges, and recent developments in extremal dependence concepts have drawn a lot of attention to probability and its applications in several disciplines. the aim of this paper is to review various concepts of extremal positive and negative dependence, including several recently established results, reconstruct their history, link them to probabilistic optimization problems, and provide a list of open questions in this area. while the concept of extremal positive dependence is agreed upon for random vectors of arbitrary dimensions, various notions of extremal negative dependence arise when more than two random variables are involved. we review existing popular concepts of extremal negative dependence given in literature and introduce a novel notion, which in a general sense includes the existing ones as particular cases. even if much of the literature on dependence is focused on positive dependence, we show that negative dependence plays an equally important role in the solution of many optimization problems. while the most popular tool used nowadays to model dependence is that of a copula function, in this paper we use the equivalent concept of a set of rearrangements. this is not only for historical reasons. rearrangement functions describe the relationship between random variables in a completely deterministic way, allow a deeper understanding of dependence itself, and have several advantages on the approximation of solutions in a broad class of optimization problems.","10.1214/15-sts525","2015-12-10","","['giovanni puccetti', 'ruodu wang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1301",1512.033,"inference in topic models: sparsity and trade-off","stat.ml","topic models are popular for modeling discrete data (e.g., texts, images, videos, links), and provide an efficient way to discover hidden structures/semantics in massive data. one of the core problems in this field is the posterior inference for individual data instances. this problem is particularly important in streaming environments, but is often intractable. in this paper, we investigate the use of the frank-wolfe algorithm (fw) for recovering sparse solutions to posterior inference. from detailed elucidation of both theoretical and practical aspects, fw exhibits many interesting properties which are beneficial to topic modeling. we then employ fw to design fast methods, including ml-fw, for learning latent dirichlet allocation (lda) at large scales. extensive experiments show that to reach the same predictiveness level, ml-fw can perform tens to thousand times faster than existing state-of-the-art methods for learning lda from massive/streaming data.","","2015-12-10","","['khoat than', 'tu bao ho']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1302",1512.03444,"cross-validated variable selection in tree-based methods improves   predictive performance","stat.ml","recursive partitioning approaches producing tree-like models are a long standing staple of predictive modeling, in the last decade mostly as ``sub-learners'' within state of the art ensemble methods like boosting and random forest. however, a fundamental flaw in the partitioning (or splitting) rule of commonly used tree building methods precludes them from treating different types of variables equally. this most clearly manifests in these methods' inability to properly utilize categorical variables with a large number of categories, which are ubiquitous in the new age of big data. such variables can often be very informative, but current tree methods essentially leave us a choice of either not using them, or exposing our models to severe overfitting. we propose a conceptual framework to splitting using leave-one-out (loo) cross validation for selecting the splitting variable, then performing a regular split (in our case, following cart's approach) for the selected variable. the most important consequence of our approach is that categorical variables with many categories can be safely used in tree building and are only chosen if they contribute to predictive power. we demonstrate in extensive simulation and real data analysis that our novel splitting approach significantly improves the performance of both single tree models and ensemble methods that utilize trees. importantly, we design an algorithm for loo splitting variable selection which under reasonable assumptions does not increase the overall computational complexity compared to cart for two-class classification. for regression tasks, our approach carries an increased computational burden, replacing a o(log(n)) factor in cart splitting rule search with an o(n) term.","","2015-12-10","","['amichai painsky', 'saharon rosset']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1303",1512.03769,"a bayesian generalized car model for correlated signal detection","stat.me","over the last decade, large-scale multiple testing has found itself at the forefront of modern data analysis. in many applications data are correlated, so that the observed test statistic used for detecting a non-null case, or signal, at each location in a dataset carries some information about the chances of a true signal at other locations. brown, lazar, datta, jang, and mcdowell (2014) proposed in the neuroimaging context a bayesian multiple testing model that accounts for the dependence of each volume element on the behavior of its neighbors through a conditional autoregressive (car) model. here, we propose a generalized car model that allows for inclusion of points with no neighbors at all, something that is not possible under conventional car models. we consider also neighborhoods based on criteria other than physical location, such as genetic pathways in microarray determined from existing biological knowledge. this generalization provides a unified framework for the simultaneous modeling of dependent and independent cases, resulting in stronger bayesian learning in the posterior and increased precision in the estimates of interesting signals. we justify the selected prior distribution and prove that the resulting posterior distribution is proper. we illustrate the effectiveness and applicability of our proposed model by using it to analyze both simulated and real microarray data in which the genes exhibit dependence that is determined by physical adjacency on a chromosome or predefined gene pathways.","10.5705/ss.202015.0382","2015-12-11","2017-02-07","['d. andrew brown', 'gauri s. datta', 'nicole a. lazar']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1304",1512.05219,"learning a hybrid architecture for sequence regression and annotation","stat.ml","when learning a hidden markov model (hmm), sequen- tial observations can often be complemented by real-valued summary response variables generated from the path of hid- den states. such settings arise in numerous domains, includ- ing many applications in biology, like motif discovery and genome annotation. in this paper, we present a flexible frame- work for jointly modeling both latent sequence features and the functional mapping that relates the summary response variables to the hidden state sequence. the algorithm is com- patible with a rich set of mapping functions. results show that the availability of additional continuous response vari- ables can simultaneously improve the annotation of the se- quential observations and yield good prediction performance in both synthetic data and real-world datasets.","","2015-12-16","","['yizhe zhang', 'ricardo henao', 'lawrence carin', 'jianling zhong', 'alexander j. hartemink']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1305",1512.05225,"means and covariance functions for geostatistical compositional data: an   axiomatic approach","stat.me","this work focuses on the characterization of the central tendency of a sample of compositional data. it provides new results about theoretical properties of means and covariance functions for compositional data, with an axiomatic perspective. original results that shed new light on the geostatistical modeling of compositional data are presented. as a first result, it is shown that the weighted arithmetic mean is the only central tendency characteristic satisfying a small set of axioms, namely continuity, reflexivity and marginal stability. moreover, this set of axioms also implies that the weights must be identical for all parts of the composition. this result has deep consequences on the spatial multivariate covariance modeling of compositional data. in a geostatistical setting, it is shown as a second result that the proportional model of covariance functions (i.e., the product of a covariance matrix and a single correlation function) is the only model that provides identical kriging weights for all components of the compositional data. as a consequence of these two results, the proportional model of covariance function is the only covariance model compatible with reflexivity and marginal stability.","","2015-12-16","2017-10-23","['denis allard', 'thierry marchant']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1306",1512.06273,"a marked cox model for ibnr claims: model and theory","stat.ap","incurred but not reported (ibnr) loss reserving is an important issue for property & casualty (p&c) insurers. the modeling of the claim arrival process, especially its temporal dependence, has not been closely examined in many of the current loss reserving models.   in this paper, we propose modeling the claim arrival process together with its reporting delays as a marked cox process. our model is versatile in modeling temporal dependence, allowing also for natural interpretations. this paper focuses mainly on the theoretical aspects of the proposed model. we show that the associated reported claim process and   ibnr claim process are both marked cox processes with easily convertible intensity functions and marking distributions. the proposed model can also account for fluctuations in the exposure. by an order statistics property, we show that the corresponding discretely observed process preserves all the information about the claim arrival epochs. finally, we derive closed-form expressions for both the autocorrelation function (acf) and the distributions of the numbers of reported claims and ibnr claims. model estimation and its applications are considered in a subsequent paper, badescu et al.(2015b)","","2015-12-19","","['andrei l. badescu', 'x. sheldon lin', 'dameng tang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1307",1512.0706,"stochastic simulators based optimization by gaussian process metamodels   -- application to maintenance investments planning issues","stat.me math.st stat.th","this paper deals with the optimization of industrial asset management strategies, whose profitability is characterized by the net present value (npv) indicator which is assessed by a monte carlo simulator. the developed method consists in building a metamodel of this stochastic simulator, allowing to get, for a given model input, the npv probability distribution without running the simulator. the present work is concentrated on the emulation of the quantile function of the stochastic simulator by interpolating well chosen basis functions and metamodeling their coefficients (using the gaussian process metamodel). this quantile function metamodel is then used to treat a problem of strategy maintenance optimization (four systems installed on different plants), in order to optimize an npv quantile. using the gaussian process framework, an adaptive design method (called qfei) is defined by extending in our case the well known ego algorithm. this allows to obtain an ""optimal"" solution using a small number of simulator runs.","","2015-12-22","2016-05-03","['thomas browne', 'bertrand iooss', 'loïc le gratiet', 'jérôme lonchampt', 'emmanuel remy']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1308",1512.07082,"designing for situation awareness of future power grids: an indicator   system based on linear eigenvalue statistics of large random matrices","stat.me","future power grids are fundamentally different from current ones, both in size and in complexity; this trend imposes challenges for situation awareness (sa) based on classical indicators, which are usually model-based and deterministic. as an alternative, this paper proposes a statistical indicator system based on linear eigenvalue statistics (less) of large random matrices: 1) from a data modeling viewpoint, we build, starting from power flows equations, the random matrix models (rmms) only using the real-time data flow in a statistical manner; 2) for a data analysis that is fully driven from rmms, we put forward the high-dimensional indicators, called less that have some unique statistical features such as gaussian properties; and 3) we develop a three-dimensional (3d) power-map to visualize the system, respectively, from a high-dimensional viewpoint and a low-dimensional one. therefore, a statistical methodology of sa is employed; it conducts sa with a model-free and data-driven procedure, requiring no knowledge of system topologies, units operation/control models, causal relationship, etc. this methodology has numerous advantages, such as sensitivity, universality, speed, and flexibility. in particular, its robustness against bad data is highlighted, with potential advantages in cyber security. the theory of big data based stability for on-line operations may prove feasible along with this line of work, although this critical development will be reported elsewhere.","10.1109/access.2016.2581838","2015-12-22","2016-07-06","['xing he', 'robert c. qiu', 'qian ai', 'lei chu', 'xinyi xu', 'zenan ling']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1309",1512.07273,"computationally efficient distribution theory for bayesian inference of   high-dimensional dependent count-valued data","stat.me","we introduce a bayesian approach for multivariate spatio-temporal prediction for high-dimensional count-valued data. our primary interest is when there are possibly millions of data points referenced over different variables, geographic regions, and times. this problem requires extensive methodological advancements, as jointly modeling correlated data of this size leads to the so-called ""big n problem."" the computational complexity of prediction in this setting is further exacerbated by acknowledging that count-valued data are naturally non-gaussian. thus, we develop a new computationally efficient distribution theory for this setting. in particular, we introduce a multivariate log-gamma distribution and provide substantial theoretical development including: results regarding conditional distributions, marginal distributions, an asymptotic relationship with the multivariate normal distribution, and full-conditional distributions for a gibbs sampler. to incorporate dependence between variables, regions, and time points, a multivariate spatio-temporal mixed effects model (mstm) is used. the results in this manuscript are extremely general, and can be used for data that exhibit fewer sources of dependency than what we consider (e.g., multivariate, spatial-only, or spatio-temporal-only data). hence, the implications of our modeling framework may have a large impact on the general problem of jointly modeling correlated count-valued data. we show the effectiveness of our approach through a simulation study. additionally, we demonstrate our proposed methodology with an important application analyzing data obtained from the longitudinal employer-household dynamics (lehd) program, which is administered by the u.s. census bureau.","","2015-12-22","","['jonathan r. bradley', 'scott h. holan', 'christopher k. wikle']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1310",1512.07336,"latent variable modeling with diversity-inducing mutual angular   regularization","cs.lg stat.ml","latent variable models (lvms) are a large family of machine learning models providing a principled and effective way to extract underlying patterns, structure and knowledge from observed data. due to the dramatic growth of volume and complexity of data, several new challenges have emerged and cannot be effectively addressed by existing lvms: (1) how to capture long-tail patterns that carry crucial information when the popularity of patterns is distributed in a power-law fashion? (2) how to reduce model complexity and computational cost without compromising the modeling power of lvms? (3) how to improve the interpretability and reduce the redundancy of discovered patterns? to addresses the three challenges discussed above, we develop a novel regularization technique for lvms, which controls the geometry of the latent space during learning to enable the learned latent components of lvms to be diverse in the sense that they are favored to be mutually different from each other, to accomplish long-tail coverage, low redundancy, and better interpretability. we propose a mutual angular regularizer (mar) to encourage the components in lvms to have larger mutual angles. the mar is non-convex and non-smooth, entailing great challenges for optimization. to cope with this issue, we derive a smooth lower bound of the mar and optimize the lower bound instead. we show that the monotonicity of the lower bound is closely aligned with the mar to qualify the lower bound as a desirable surrogate of the mar. using neural network (nn) as an instance, we analyze how the mar affects the generalization performance of nn. on two popular latent variable models --- restricted boltzmann machine and distance metric learning, we demonstrate that mar can effectively capture long-tail patterns, reduce model complexity without sacrificing expressivity and improve interpretability.","","2015-12-22","","['pengtao xie', 'yuntian deng', 'eric xing']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1311",1512.07451,"emulation of multivariate simulators using thin-plate splines with   application to atmospheric dispersion","stat.me","it is often desirable to build a statistical emulator of a complex computer simulator in order to perform analysis which would otherwise be computationally infeasible. we propose methodology to model multivariate output from a computer simulator taking into account output structure in the responses. the utility of this approach is demonstrated by applying it to a chemical and biological hazard prediction model. predicting the hazard area that results from an accidental or deliberate chemical or biological release is imperative in civil and military planning and also in emergency response. the hazard area resulting from such a release is highly structured in space and we therefore propose the use of a thin-plate spline to capture the spatial structure and fit a gaussian process emulator to the coefficients of the resultant basis functions. we compare and contrast four different techniques for emulating multivariate output: dimension-reduction using (i) a fully bayesian approach with a principal component basis, (ii) a fully bayesian approach with a thin-plate spline basis, assuming that the basis coefficients are independent, and (iii) a ""plug-in"" bayesian approach with a thin-plate spline basis and a separable covariance structure; and (iv) a functional data modeling approach using a tensor-product (separable) gaussian process. we develop methodology for the two thin-plate spline emulators and demonstrate that these emulators significantly outperform the principal component emulator. further, the separable thin-plate spline emulator, which accounts for the dependence between basis coefficients, provides substantially more realistic quantification of uncertainty, and is also computationally more tractable, allowing fast emulation. for high resolution output data, it also offers substantial predictive and computational advantages over the tensor-product gaussian process emulator.","10.1137/140970148","2015-12-23","2016-10-26","['veronica e. bowman', 'david c. woods']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1312",1512.07607,"dynamic social networks based on movement","stat.ap stat.me","network modeling techniques provide a means for quantifying social structure in populations of individuals. data used to define social connectivity are often expensive to collect and based on case-specific, ad hoc criteria. moreover, in applications involving animal social networks, collection of these data is often opportunistic and can be invasive. frequently, the social network of interest for a given population is closely related to the way individuals move. thus telemetry data, which are minimally-invasive and relatively inexpensive to collect, present an alternative source of information. we develop a framework for using telemetry data to infer social relationships among animals. to achieve this, we propose a bayesian hierarchical model with an underlying dynamic social network controlling movement of individuals via two mechanisms: an attractive effect, and an aligning effect. we demonstrate the model and its ability to accurately identify complex social behavior in simulation, and apply our model to telemetry data arising from killer whales. using auxiliary information about the study population, we investigate model validity and find the inferred dynamic social network is consistent with killer whale ecology and expert knowledge.","","2015-12-23","2016-09-20","['henry r. scharf', 'mevin b. hooten', 'bailey k. fosdick', 'devin s. johnson', 'josh m. london', 'john w. durban']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1313",1512.08269,"statistical and computational guarantees for the baum-welch algorithm","stat.ml cs.it math.it math.st stat.th","the hidden markov model (hmm) is one of the mainstays of statistical modeling of discrete time series, with applications including speech recognition, computational biology, computer vision and econometrics. estimating an hmm from its observation process is often addressed via the baum-welch algorithm, which is known to be susceptible to local optima. in this paper, we first give a general characterization of the basin of attraction associated with any global optimum of the population likelihood. by exploiting this characterization, we provide non-asymptotic finite sample guarantees on the baum-welch updates, guaranteeing geometric convergence to a small ball of radius on the order of the minimax rate around a global optimum. as a concrete example, we prove a linear rate of convergence for a hidden markov mixture of two isotropic gaussians given a suitable mean separation and an initialization within a ball of large radius around (one of) the true parameters. to our knowledge, these are the first rigorous local convergence guarantees to global optima for the baum-welch algorithm in a setting where the likelihood function is nonconvex. we complement our theoretical results with thorough numerical simulations studying the convergence of the baum-welch algorithm and illustrating the accuracy of our predictions.","","2015-12-27","","['fanny yang', 'sivaraman balakrishnan', 'martin j. wainwright']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"1314",1512.08731,"a variational em method for mixed membership models with multivariate   rank data: an analysis of public policy preferences","stat.me stat.ap","in this article, we consider modeling ranked responses from a heterogeneous population. specifically, we analyze data from the eurobarometer 34.1 survey regarding public policy preferences towards drugs, alcohol and aids. such policy preferences are likely to exhibit substantial differences within as well as across european nations reflecting a wide variety of cultures, political affiliations, ideological perspectives and common practices. we use a mixed membership model to account for multiple subgroups with differing preferences and to allow each individual to possess partial membership in more than one subgroup. previous methods for fitting mixed membership models to rank data in a univariate setting have utilized an mcmc approach and do not estimate the relative frequency of each subgroup. we propose a variational em approach for fitting mixed membership models with multivariate rank data. our method allows for fast approximate inference and explicitly estimates the subgroup sizes. analyzing the eurobarometer 34.1 data, we find interpretable subgroups which generally agree with the ""left vs right"" classification of political ideologies.","","2015-12-29","2017-02-24","['y. samuel wang', 'ross matsueda', 'elena a. erosheva']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1315",1512.09251,"solving the g-problems in less than 500 iterations: improved efficient   constrained optimization by surrogate modeling and adaptive parameter control","math.oc cs.ne stat.ml","constrained optimization of high-dimensional numerical problems plays an important role in many scientific and industrial applications. function evaluations in many industrial applications are severely limited and no analytical information about objective function and constraint functions is available. for such expensive black-box optimization tasks, the constraint optimization algorithm cobra was proposed, making use of rbf surrogate modeling for both the objective and the constraint functions. cobra has shown remarkable success in solving reliably complex benchmark problems in less than 500 function evaluations. unfortunately, cobra requires careful adjustment of parameters in order to do so.   in this work we present a new self-adjusting algorithm sacobra, which is based on cobra and capable to achieve high-quality results with very few function evaluations and no parameter tuning. it is shown with the help of performance profiles on a set of benchmark problems (g-problems, mopta08) that sacobra consistently outperforms any cobra algorithm with fixed parameter setting. we analyze the importance of the several new elements in sacobra and find that each element of sacobra plays a role to boost up the overall optimization performance. we discuss the reasons behind and get in this way a better understanding of high-quality rbf surrogate modeling.","","2015-12-31","","['samineh bagheri', 'wolfgang konen', 'michael emmerich', 'thomas bäck']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1316",1601.00389,"interpreting latent variables in factor models via convex optimization","stat.me math.oc math.st stat.th","latent or unobserved phenomena pose a significant difficulty in data analysis as they induce complicated and confounding dependencies among a collection of observed variables. factor analysis is a prominent multivariate statistical modeling approach that addresses this challenge by identifying the effects of (a small number of) latent variables on a set of observed variables. however, the latent variables in a factor model are purely mathematical objects that are derived from the observed phenomena, and they do not have any interpretation associated to them. a natural approach for attributing semantic information to the latent variables in a factor model is to obtain measurements of some additional plausibly useful covariates that may be related to the original set of observed variables, and to associate these auxiliary covariates to the latent variables. in this paper, we describe a systematic approach for identifying such associations. our method is based on solving computationally tractable convex optimization problems, and it can be viewed as a generalization of the minimum-trace factor analysis procedure for fitting factor models via convex optimization. we analyze the theoretical consistency of our approach in a high-dimensional setting as well as its utility in practice via experimental demonstrations with real data.","","2016-01-04","2016-11-02","['armeen taeb', 'venkat chandrasekaran']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1317",1601.00496,"nonparametric modeling of dynamic functional connectivity in fmri data","stat.ap q-bio.nc stat.ml","dynamic functional connectivity (fc) has in recent years become a topic of interest in the neuroimaging community. several models and methods exist for both functional magnetic resonance imaging (fmri) and electroencephalography (eeg), and the results point towards the conclusion that fc exhibits dynamic changes. the existing approaches modeling dynamic connectivity have primarily been based on time-windowing the data and k-means clustering. we propose a non-parametric generative model for dynamic fc in fmri that does not rely on specifying window lengths and number of dynamic states. rooted in bayesian statistical modeling we use the predictive likelihood to investigate if the model can discriminate between a motor task and rest both within and across subjects. we further investigate what drives dynamic states using the model on the entire data collated across subjects and task/rest. we find that the number of states extracted are driven by subject variability and preprocessing differences while the individual states are almost purely defined by either task or rest. this questions how we in general interpret dynamic fc and points to the need for more research on what drives dynamic fc.","","2016-01-04","2016-06-08","['søren f. v. nielsen', 'kristoffer h. madsen', 'rasmus røge', 'mikkel n. schmidt', 'morten mørup']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"1318",1601.00595,"robust non-linear regression: a greedy approach employing kernels with   application to image denoising","cs.lg stat.ml","we consider the task of robust non-linear regression in the presence of both inlier noise and outliers. assuming that the unknown non-linear function belongs to a reproducing kernel hilbert space (rkhs), our goal is to estimate the set of the associated unknown parameters. due to the presence of outliers, common techniques such as the kernel ridge regression (krr) or the support vector regression (svr) turn out to be inadequate. instead, we employ sparse modeling arguments to explicitly model and estimate the outliers, adopting a greedy approach. the proposed robust scheme, i.e., kernel greedy algorithm for robust denoising (kgard), is inspired by the classical orthogonal matching pursuit (omp) algorithm. specifically, the proposed method alternates between a krr task and an omp-like selection step. theoretical results concerning the identification of the outliers are provided. moreover, kgard is compared against other cutting edge methods, where its performance is evaluated via a set of experiments with various types of noise. finally, the proposed robust estimation framework is applied to the task of image denoising, and its enhanced performance in the presence of outliers is demonstrated.","10.1109/tsp.2017.2708029","2016-01-04","2016-08-03","['george papageorgiou', 'pantelis bouboulis', 'sergios theodoridis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1319",1601.01462,"bayesian inference for the extremal dependence","stat.me","a simple approach for modeling multivariate extremes is to consider the vector of component-wise maxima and their max-stable distributions. the extremal dependence can be inferred by estimating the angular measure or, alternatively, the pickands dependence function. we propose a nonparametric bayesian model that allows, in the bivariate case, the simultaneous estimation of both functional representations through the use of polynomials in the bernstein form. the constraints required to provide a valid extremal dependence are addressed in a straightforward manner, by placing a prior on the coefficients of the bernstein polynomials which gives probability one to the set of valid functions. the prior is extended to the polynomial degree, making our approach fully nonparametric. although the analytical expression of the posterior is unknown, inference is possible via a trans-dimensional mcmc scheme. we show the efficiency of the proposed methodology by means of a simulation study. the extremal behaviour of log-returns of daily exchange rates between the pound sterling vs the u.s. dollar and the pound sterling vs the japanese yen is analysed for illustrative purposes.","","2016-01-07","2017-02-02","['giulia marcon', 'simone a. padoan', 'n/a antoniano-villalobos']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1320",1601.01557,"quaternion-valued single-phase model for three-phase power systems","stat.ap","in this work, a quaternion-valued model is proposed in lieu of the clarke's \alpha, \beta transformation to convert three-phase quantities to a hypercomplex single-phase signal. the concatenated signal can be used for harmonic distortion detection in three-phase power systems. in particular, the proposed model maps all the harmonic frequencies into frequencies in the quaternion domain, while the clarke's transformation-based methods will fail to detect the zero sequence voltages. based on the quaternion-valued model, the fourier transform, the minimum variance distortionless response (mvdr) algorithm and the multiple signal classification (music) algorithm are presented as examples to detect harmonic distortion. simulations are provided to demonstrate the potentials of this new modeling method.","","2016-01-07","","['xiaoming gou', 'zhiwen liu', 'wei liu', 'yougen xu', 'jiabin wang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1321",1601.02461,"modeling multivariate mixed-response functional data","stat.me","we propose a bayesian modeling framework for jointly analyzing multiple functional responses of different types (e.g. binary and continuous data). our approach is based on a multivariate latent gaussian process and models the dependence among the functional responses through the dependence of the latent process. our framework easily accommodates additional covariates. we offer a way to estimate the multivariate latent covariance, allowing for implementation of multivariate functional principal components analysis (fpca) to specify basis expansions and simplify computation. we demonstrate our method through both simulation studies and an application to real data from a periodontal study.","","2016-01-11","","['beth a. tidemann-miller', 'brian j. reich', 'ana-maria staicu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1322",1601.02596,"consistent estimation for partition-wise regression and classification   models","stat.me","partition-wise models offer a flexible approach for modeling complex and multidimensional data that are capable of producing interpretable results. they are based on partitioning the observed data into regions, each of which is modeled with a simple submodel. the success of this approach highly depends on the quality of the partition, as too large a region could lead to a non-simple submodel, while too small a region could inflate estimation variance. this paper proposes an automatic procedure for choosing the partition (i.e., the number of regions and the boundaries between regions) as well as the submodels for the regions. it is shown that, under the assumption of the existence of a true partition, the proposed partition estimator is statistically consistent. the methodology is demonstrated for both regression and classification problems.","10.1109/tsp.2017.2698407","2016-01-11","","['rex c. y. cheung', 'alexander aue', 'thomas c. m. lee']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1323",1601.02698,"efficient markov chain monte carlo sampling for hierarchical hidden   markov models","stat.co","traditional markov chain monte carlo (mcmc) sampling of hidden markov models (hmms) involves latent states underlying an imperfect observation process, and generates posterior samples for top-level parameters concurrently with nuisance latent variables. when potentially many hmms are embedded within a hierarchical model, this can result in prohibitively long mcmc runtimes. we study combinations of existing methods, which are shown to vastly improve computational efficiency for these hierarchical models while maintaining the modeling flexibility provided by embedded hmms. the methods include discrete filtering of the hmm likelihood to remove latent states, reduced data representations, and a novel procedure for dynamic block sampling of posterior dimensions. the first two methods have been used in isolation in existing application-specific software, but are not generally available for incorporation in arbitrary model structures. using the nimble package for r, we develop and test combined computational approaches using three examples from ecological capture-recapture, although our methods are generally applicable to any embedded discrete hmms. these combinations provide several orders of magnitude improvement in mcmc sampling efficiency, defined as the rate of generating effectively independent posterior samples. in addition to being computationally significant for this class of hierarchical models, this result underscores the potential for vast improvements to mcmc sampling efficiency which can result from combinations of known algorithms.","","2016-01-11","","['daniel turek', 'perry de valpine', 'christopher j. paciorek']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1324",1601.03124,"online prediction of dyadic data with heterogeneous matrix factorization","cs.lg stat.ml","dyadic data prediction (ddp) is an important problem in many research areas. this paper develops a novel fully bayesian nonparametric framework which integrates two popular and complementary approaches, discrete mixed membership modeling and continuous latent factor modeling into a unified heterogeneous matrix factorization~(hemf) model, which can predict the unobserved dyadics accurately. the hemf can determine the number of communities automatically and exploit the latent linear structure for each bicluster efficiently. we propose a variational bayesian method to estimate the parameters and missing data. we further develop a novel online learning approach for variational inference and use it for the online learning of hemf, which can efficiently cope with the important large-scale ddp problem. we evaluate the performance of our method on the eachmoive, movielens and netflix prize collaborative filtering datasets. the experiment shows that, our model outperforms state-of-the-art methods on all benchmarks. compared with stochastic gradient method (sgd), our online learning approach achieves significant improvement on the estimation accuracy and robustness.","","2016-01-12","","['guangyong chen', 'fengyuan zhu', 'pheng ann heng']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1325",1601.03443,"fitting bayesian item response models in stata and stan","stat.co","stata users have access to two easy-to-use implementations of bayesian inference: stata's native {\tt bayesmh} function and statastan, which calls the general bayesian engine stan. we compare these on two models that are important for education research: the rasch model and the hierarchical rasch model. stan (as called from stata) fits a more general range of models than can be fit by {\tt bayesmh} and is also more scalable, in that it could easily fit models with at least ten times more parameters than could be fit using stata's native bayesian implementation. in addition, stan runs between two and ten times faster than {\tt bayesmh} as measured in effective sample size per second: that is, compared to stan, it takes stata's built-in bayesian engine twice to ten times as long to get inferences with equivalent precision. we attribute stan's advantage in flexibility to its general modeling language, and its advantages in scalability and speed to an efficient sampling algorithm: hamiltonian monte carlo using the no-u-turn sampler. in order to further investigate scalability, we also compared to the package jags, which performed better than stata's native bayesian engine but not as well as statastan.   given its advantages in speed, generality, and scalability, and that stan is open-source and can be run directly from stata using statastan, we recommend that stata users adopt stan as their bayesian inference engine of choice.","","2016-01-13","2016-12-13","['robert l. grant', 'daniel c. furr', 'bob carpenter', 'andrew gelman']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1326",1601.03474,"kernel regression on manifolds and its application to modeling   disconnected anatomic structures","stat.ap","we present a unified statistical approach to modeling disconnected 3d anatomical structures extracted from medical images. due to image acquisition and preprocessing noises, it is expected the imaging data is noisy. the surface coordinates of the structures are regressed using the weighted linear combination of laplace-beltrami (lb) eigenfunctions to smooth out noisy data and perform statistical analysis. the method is applied in characterizing the 3d growth pattern of human hyoid bone between ages 0 and 20. we detected a significant age effect on localized parts of the hyoid bone.","","2016-01-13","2017-09-27","['moo k. chung', 'nagesh adluru', 'houri k. vorperian']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1327",1601.03501,"efficient nonparametric estimation of causal mediation effects","stat.me","an essential goal of program evaluation and scientific research is the investigation of causal mechanisms. over the past several decades, causal mediation analysis has been used in medical and social sciences to decompose the treatment effect into the natural direct and indirect effects. however, all of the existing mediation analysis methods rely on parametric modeling assumptions in one way or another, typically requiring researchers to specify multiple regression models involving the treatment, mediator, outcome, and pre-treatment confounders. to overcome this limitation, we propose a novel nonparametric estimation method for causal mediation analysis that eliminates the need for applied researchers to model multiple conditional distributions. the proposed method balances a certain set of empirical moments between the treatment and control groups by weighting each observation; in particular, we establish that the proposed estimator is globally semiparametric efficient. we also show how to consistently estimate the asymptotic variance of the proposed estimator without additional efforts. finally, we extend the proposed method to other relevant settings including the causal mediation analysis with multiple mediators.","","2016-01-14","","['k. c. g. chan', 'k. imai', 's. c. p. yam', 'z. zhang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1328",1601.03822,"on the consistency of inversion-free parameter estimation for gaussian   random fields","math.st cs.lg stat.ml stat.th","gaussian random fields are a powerful tool for modeling environmental processes. for high dimensional samples, classical approaches for estimating the covariance parameters require highly challenging and massive computations, such as the evaluation of the cholesky factorization or solving linear systems. recently, anitescu, chen and stein \cite{m.anitescu} proposed a fast and scalable algorithm which does not need such burdensome computations. the main focus of this article is to study the asymptotic behavior of the algorithm of anitescu et al. (acs) for regular and irregular grids in the increasing domain setting. consistency, minimax optimality and asymptotic normality of this algorithm are proved under mild differentiability conditions on the covariance function. despite the fact that acs's method entails a non-concave maximization, our results hold for any stationary point of the objective function. a numerical study is presented to evaluate the efficiency of this algorithm for large data sets.","10.1016/j.jmva.2016.06.003","2016-01-15","2016-06-21","['hossein keshavarz', 'clayton scott', 'xuanlong nguyen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"1329",1601.04096,"goodness-of-fit statistics for approximate bayesian computation","stat.me","approximate bayesian computation is a statistical framework that uses numerical simulations to calibrate and compare models. instead of computing likelihood functions, approximate bayesian computation relies on numerical simulations, which makes it applicable to complex models in ecology and evolution. as usual for statistical modeling, evaluating goodness-of-fit is a fundamental step for approximate bayesian computation. here, we introduce a goodness-of-fit approach based on hypothesis-testing. we introduce two test statistics based on the mean distance between numerical summaries of the data and simulated ones. one test statistic relies on summaries simulated with the prior predictive distribution whereas the other one relies on simulations from the posterior predictive distribution. for different coalescent models, we find that the statistics are well calibrated, meaning that the type i error can be controlled. however, the statistical power of the two statistics is extremely variable across models ranging from 20% to 100%. the difference of power between the two statistics is negligible in models of demographic inference but substantial in an additional and purely statistical example. when analyzing resequencing data to evaluate models of human demography, the two statistics confirm that an out-of-africa bottleneck cannot be rejected for asiatic and european data. we also consider two speciation models in the context of a butterfly species complex. one goodness-of-fit statistic indicates a poor fit for both models, and the numerical summaries causing the poor fit were identified using posterior predictive checks. statistical tests for goodness-of-fit should foster evaluation of model fit in approximate bayesian computation. the test statistic based on simulations from the prior predictive distribution is implemented in the gfit function of the r abc package.","","2016-01-15","","['louisiane lemaire', 'flora jay', 'i-hung lee', 'katalin csilléry', 'michael g. b. blum']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1330",1601.04331,"a bayesian nonparametric markovian model for nonstationary time series","stat.me","stationary time series models built from parametric distributions are, in general, limited in scope due to the assumptions imposed on the residual distribution and autoregression relationship. we present a modeling approach for univariate time series data, which makes no assumptions of stationarity, and can accommodate complex dynamics and capture nonstandard distributions. the model for the transition density arises from the conditional distribution implied by a bayesian nonparametric mixture of bivariate normals. this implies a flexible autoregressive form for the conditional transition density, defining a time-homogeneous, nonstationary, markovian model for real-valued data indexed in discrete-time. to obtain a more computationally tractable algorithm for posterior inference, we utilize a square-root-free cholesky decomposition of the mixture kernel covariance matrix. results from simulated data suggest the model is able to recover challenging transition and predictive densities. we also illustrate the model on time intervals between eruptions of the old faithful geyser. extensions to accommodate higher order structure and to develop a state-space model are also discussed.","","2016-01-17","2016-05-03","['maria deyoreo', 'athanasios kottas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1331",1601.04549,"incremental semiparametric inverse dynamics learning","stat.ml cs.lg cs.ro","this paper presents a novel approach for incremental semiparametric inverse dynamics learning. in particular, we consider the mixture of two approaches: parametric modeling based on rigid body dynamics equations and nonparametric modeling based on incremental kernel methods, with no prior information on the mechanical properties of the system. this yields to an incremental semiparametric approach, leveraging the advantages of both the parametric and nonparametric models. we validate the proposed technique learning the dynamics of one arm of the icub humanoid robot.","","2016-01-18","","['raffaello camoriano', 'silvio traversaro', 'lorenzo rosasco', 'giorgio metta', 'francesco nori']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1332",1601.04736,"a consistent direct method for estimating parameters in ordinary   differential equations models","stat.ap","ordinary differential equations provide an attractive framework for modeling temporal dynamics in a variety of scientific settings. we show how consistent estimation for parameters in ode models can be obtained by modifying a direct (non-iterative) least squares method similar to the direct methods originally developed by himmelbau, jones and bischoff. our method is called the bias-corrected least squares (bcls) method since it is a modification of least squares methods known to be biased. consistency of the bcls method is established and simulations are used to compare the bcls method to other methods for parameter estimation in ode models.","","2016-01-18","","['sarah e. holte']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1333",1601.05078,"understanding past population dynamics: bayesian coalescent-based   modeling with covariates","stat.me q-bio.pe","effective population size characterizes the genetic variability in a population and is a parameter of paramount importance in population genetics. kingman's coalescent process enables inference of past population dynamics directly from molecular sequence data, and researchers have developed a number of flexible coalescent-based models for bayesian nonparametric estimation of the effective population size as a function of time. a major goal of demographic reconstruction is understanding the association between the effective population size and potential explanatory factors. building upon bayesian nonparametric coalescent-based approaches, we introduce a flexible framework that incorporates time-varying covariates through gaussian markov random fields. to approximate the posterior distribution, we adapt efficient markov chain monte carlo algorithms designed for highly structured gaussian models. incorporating covariates into the demographic inference framework enables the modeling of associations between the effective population size and covariates while accounting for uncertainty in population histories. furthermore, it can lead to more precise estimates of population dynamics. we apply our model to four examples. we reconstruct the demographic history of raccoon rabies in north america and find a significant association with the spatiotemporal spread of the outbreak. next, we examine the effective population size trajectory of the denv-4 virus in puerto rico along with viral isolate count data and find similar cyclic patterns. we compare the population history of the hiv-1 crf02_ag clade in cameroon with hiv incidence and prevalence data and find that the effective population size is more reflective of incidence rate. finally, we explore the hypothesis that the population dynamics of musk ox during the late quaternary period were related to climate change.","","2016-01-19","","['mandev s. gill', 'philippe lemey', 'shannon n. bennett', 'roman biek', 'marc a. suchard']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1334",1601.05408,"basis function models for animal movement","stat.me","advances in satellite-based data collection techniques have served as a catalyst for new statistical methodology to analyze these data. in wildlife ecological studies, satellite-based data and methodology have provided a wealth of information about animal space use and the investigation of individual-based animal-environment relationships. with the technology for data collection improving dramatically over time, we are left with massive archives of historical animal telemetry data of varying quality. while many contemporary statistical approaches for inferring movement behavior are specified in discrete time, we develop a flexible continuous-time stochastic integral equation framework that is amenable to reduced-rank second-order covariance parameterizations. we demonstrate how the associated first-order basis functions can be constructed to mimic behavioral characteristics in realistic trajectory processes using telemetry data from mule deer and mountain lion individuals in western north america. our approach is parallelizable and provides inference for heterogeneous trajectories using nonstationary spatial modeling techniques that are feasible for large telemetry data sets.","","2016-01-20","2016-10-06","['mevin b. hooten', 'devin s. johnson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1335",1601.05886,"parsimonious and powerful composite likelihood testing for group   difference and genotype-phenotype association","stat.me stat.ap","testing the association between a phenotype and many genetic variants from case-control data is essential in genome-wide association study (gwas). this is a challenging task as many such variants are correlated or non-informative. similarities exist in testing the population difference between two groups of high dimensional data with intractable full likelihood function. testing may be tackled by a maximum composite likelihood (mcl) not entailing the full likelihood, but current mcl tests are subject to power loss for involving non-informative or redundant sub-likelihoods. in this paper, we develop a forward search and test method for simultaneous powerful group difference testing and informative sub-likelihoods composition. our method constructs a sequence of wald-type test statistics by including only informative sub-likelihoods progressively so as to improve the test power under local sparsity alternatives. numerical studies show that it achieves considerable improvement over the available tests as the modeling complexity grows. our method is further validated by testing the motivating gwas data on breast cancer with interesting results obtained.","","2016-01-22","","['zhendong huang', 'davide ferrari', 'guoqi qian']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1336",1601.05936,"exploiting low-dimensional structures to enhance dnn based acoustic   modeling in speech recognition","cs.cl cs.lg stat.ml","we propose to model the acoustic space of deep neural network (dnn) class-conditional posterior probabilities as a union of low-dimensional subspaces. to that end, the training posteriors are used for dictionary learning and sparse coding. sparse representation of the test posteriors using this dictionary enables projection to the space of training data. relying on the fact that the intrinsic dimensions of the posterior subspaces are indeed very small and the matrix of all posteriors belonging to a class has a very low rank, we demonstrate how low-dimensional structures enable further enhancement of the posteriors and rectify the spurious errors due to mismatch conditions. the enhanced acoustic modeling method leads to improvements in continuous speech recognition task using hybrid dnn-hmm (hidden markov model) framework in both clean and noisy conditions, where upto 15.4% relative reduction in word error rate (wer) is achieved.","10.1109/icassp.2016.7472767","2016-01-22","","['pranay dighe', 'gil luyet', 'afsaneh asaei', 'herve bourlard']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1337",1601.08002,"adaptive scaling for soft-thresholding estimator","stat.me","soft-thresholding is a sparse modeling method that is typically applied to wavelet denoising in statistical signal processing and analysis. it has a single parameter that controls a threshold level on wavelet coefficients and, simultaneously, amount of shrinkage for coefficients of un-removed components. this parametrization is possible to cause excess shrinkage, thus, estimation bias at a sparse representation; i.e. there is a dilemma between sparsity and prediction accuracy. to relax this problem, we considered to introduce positive scaling on soft-thresholding estimator, by which threshold level and amount of shrinkage are independently controlled. especially, in this paper, we proposed component-wise and data-dependent scaling in a setting of non-parametric orthogonal regression problem including discrete wavelet transform. we call our scaling method adaptive scaling. we here employed soft-thresholding method based on lars(least angle regression), by which the model selection problem reduces to the determination of the number of un-removed components. we derived a risk under lars-based soft-thresholding with the proposed adaptive scaling and established a model selection criterion as an unbiased estimate of the risk. we also analyzed some properties of the risk curve and found that the model selection criterion is possible to select a model with low risk and high sparsity compared to a naive soft-thresholding method. this theoretical speculation was verified by a simple numerical experiment and an application to wavelet denoising.","","2016-01-29","","['katsuyuki hagiwara']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1338",1601.08068,"system identification through online sparse gaussian process regression   with input noise","stat.ml cs.lg cs.sy","there has been a growing interest in using non-parametric regression methods like gaussian process (gp) regression for system identification. gp regression does traditionally have three important downsides: (1) it is computationally intensive, (2) it cannot efficiently implement newly obtained measurements online, and (3) it cannot deal with stochastic (noisy) input points. in this paper we present an algorithm tackling all these three issues simultaneously. the resulting sparse online noisy input gp (sonig) regression algorithm can incorporate new noisy measurements in constant runtime. a comparison has shown that it is more accurate than similar existing regression algorithms. when applied to non-linear black-box system modeling, its performance is competitive with existing non-linear arx models.","","2016-01-29","2017-08-15","['hildo bijl', 'thomas b. schön', 'jan-willem van wingerden', 'michel verhaegen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1339",1601.08099,"chaos in fractionally integrated generalized autoregressive conditional   heteroskedastic processes","q-fin.mf math.ds q-fin.st stat.ot","fractionally integrated generalized autoregressive conditional heteroskedasticity (figarch) arises in modeling of financial time series. figarch is essentially governed by a system of nonlinear stochastic difference equations ${u_t}$ = ${z_t}$ $(1-\sum\limits_{j=1}^q \beta_j l^j)\sigma_{t}^2 = \omega+(1-\sum\limits_{j=1}^q \beta_j l^j - (\sum\limits_{k=1}^p \varphi_k l^k) (1-l)^d) u_t^2$, where $\omega\in$ r, and $\beta_j\in$ r are constant parameters, $\{u_t\}_{{t\in}^+}$ and $\{\sigma_t\}_{{t\in}^+}$ are the discrete time real valued stochastic processes which represent figarch (p,d,q) and stochastic volatility, respectively. moreover, l is the backward shift operator, i.e. $l^d u_t \equiv u_{t-d}$ (d is the fractional differencing parameter 0$<$d$<$1).   in this work, we have studied the chaoticity properties of figarch (p,d,q) processes by computing mutual information, correlation dimensions, fnns (false nearest neighbour), the lyapunov exponents, and for both the stochastic difference equation given above and for the financial time series. we have observed that maximal lyapunov exponents are negative, therefore, it can be suggested that figarch (p,d,q) is not deterministic chaotic process.","","2016-01-29","2016-02-12","['adil yilmaz', 'gazanfer unal']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE
"1340",1602.00047,"a scalable blocked gibbs sampling algorithm for gaussian and poisson   regression models","stat.me","markov chain monte carlo (mcmc) methods are a popular technique in bayesian statistical modeling. they have long been used to obtain samples from posterior distributions, but recent research has focused on the scalability of these techniques for large problems. we do not develop new sampling methods but instead describe a blocked gibbs sampler which is sufficiently scalable to accomodate many interesting problems. the sampler we describe applies to a restricted subset of the generalized linear mixed-effects models (glmm's); this subset includes poisson and gaussian regression models. the blocked gibbs sampling steps jointly update a prior variance parameter along with all of the random effects underneath it. we also discuss extensions such as flexible prior distributions.","","2016-01-29","","['nicholas a. johnson', 'frank o. kuehnel', 'ali nasiri amini']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1341",1602.00221,"principal polynomial analysis","stat.ml","this paper presents a new framework for manifold learning based on a sequence of principal polynomials that capture the possibly nonlinear nature of the data. the proposed principal polynomial analysis (ppa) generalizes pca by modeling the directions of maximal variance by means of curves, instead of straight lines. contrarily to previous approaches, ppa reduces to performing simple univariate regressions, which makes it computationally feasible and robust. moreover, ppa shows a number of interesting analytical properties. first, ppa is a volume-preserving map, which in turn guarantees the existence of the inverse. second, such an inverse can be obtained in closed form. invertibility is an important advantage over other learning methods, because it permits to understand the identified features in the input domain where the data has physical meaning. moreover, it allows to evaluate the performance of dimensionality reduction in sensible (input-domain) units. volume preservation also allows an easy computation of information theoretic quantities, such as the reduction in multi-information after the transform. third, the analytical nature of ppa leads to a clear geometrical interpretation of the manifold: it allows the computation of frenet-serret frames (local features) and of generalized curvatures at any point of the space. and fourth, the analytical jacobian allows the computation of the metric induced by the data, thus generalizing the mahalanobis distance. these properties are demonstrated theoretically and illustrated experimentally. the performance of ppa is evaluated in dimensionality and redundancy reduction, in both synthetic and real datasets from the uci repository.","10.1142/s0129065714400073","2016-01-31","","['valero laparra', 'sandra jiménez', 'devis tuia', 'gustau camps-valls', 'jesús malo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1342",1602.00256,"some contra-arguments for the use of stable distributions in financial   modeling","stat.ap q-fin.mf","in the present paper, we discuss contra-arguments concerning the use of pareto-lev\'y distributions for modeling in finance. it appears that such probability laws do not provide sufficient number of outliers observed in real data. connection with the classical limit theorem for heavy-tailed distributions with such type of models is also questionable. the idea of alternative modeling is given.","","2016-01-31","","['lev b. klebanov', 'greg temnov', 'ashot v. kakosyan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1343",1602.00357,"deepcare: a deep dynamic memory model for predictive medicine","stat.ml cs.lg","personalized predictive medicine necessitates the modeling of patient illness and care processes, which inherently have long-term temporal dependencies. healthcare observations, recorded in electronic medical records, are episodic and irregular in time. we introduce deepcare, an end-to-end deep dynamic neural network that reads medical records, stores previous illness history, infers current illness states and predicts future medical outcomes. at the data level, deepcare represents care episodes as vectors in space, models patient health state trajectories through explicit memory of historical records. built on long short-term memory (lstm), deepcare introduces time parameterizations to handle irregular timed events by moderating the forgetting and consolidation of memory cells. deepcare also incorporates medical interventions that change the course of illness and shape future medical risk. moving up to the health state level, historical and present health states are then aggregated through multiscale temporal pooling, before passing through a neural network that estimates future outcomes. we demonstrate the efficacy of deepcare for disease progression modeling, intervention recommendation, and future risk prediction. on two important cohorts with heavy social and economic burden -- diabetes and mental health -- the results show improved modeling and risk prediction accuracy.","","2016-01-31","2017-04-10","['trang pham', 'truyen tran', 'dinh phung', 'svetha venkatesh']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1344",1602.00795,"gender, productivity, and prestige in computer science faculty hiring   networks","cs.si cs.cy physics.soc-ph stat.ap","women are dramatically underrepresented in computer science at all levels in academia and account for just 15% of tenure-track faculty. understanding the causes of this gender imbalance would inform both policies intended to rectify it and employment decisions by departments and individuals. progress in this direction, however, is complicated by the complexity and decentralized nature of faculty hiring and the non-independence of hires. using comprehensive data on both hiring outcomes and scholarly productivity for 2659 tenure-track faculty across 205 ph.d.-granting departments in north america, we investigate the multi-dimensional nature of gender inequality in computer science faculty hiring through a network model of the hiring process. overall, we find that hiring outcomes are most directly affected by (i) the relative prestige between hiring and placing institutions and (ii) the scholarly productivity of the candidates. after including these, and other features, the addition of gender did not significantly reduce modeling error. however, gender differences do exist, e.g., in scholarly productivity, postdoctoral training rates, and in career movements up the rankings of universities, suggesting that the effects of gender are indirectly incorporated into hiring decisions through gender's covariates. furthermore, we find evidence that more highly ranked departments recruit female faculty at higher than expected rates, which appears to inhibit similar efforts by lower ranked departments. these findings illustrate the subtle nature of gender inequality in faculty hiring networks and provide new insights to the underrepresentation of women in computer science.","","2016-02-02","","['samuel f. way', 'daniel b. larremore', 'aaron clauset']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1345",1602.01345,"a probabilistic modeling approach to hearing loss compensation","stat.ml","hearing aid (ha) algorithms need to be tuned (""fitted"") to match the impairment of each specific patient. the lack of a fundamental ha fitting theory is a strong contributing factor to an unsatisfying sound experience for about 20% of hearing aid patients. this paper proposes a probabilistic modeling approach to the design of ha algorithms. the proposed method relies on a generative probabilistic model for the hearing loss problem and provides for automated inference of the corresponding (1) signal processing algorithm, (2) the fitting solution as well as a principled (3) performance evaluation metric. all three tasks are realized as message passing algorithms in a factor graph representation of the generative model, which in principle allows for fast implementation on hearing aid or mobile device hardware. the methods are theoretically worked out and simulated with a custom-built factor graph toolbox for a specific hearing loss model.","10.1109/taslp.2016.2599275","2016-02-03","2016-09-06","['thijs van de laar', 'bert de vries']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1346",1602.01787,"efficient estimation of semiparametric transformation models for the   cumulative incidence of competing risks","stat.me","the cumulative incidence is the probability of failure from the cause of interest over a certain time period in the presence of other risks. a semiparametric regression model proposed by fine and gray (1999) has become the method of choice for formulating the effects of covariates on the cumulative incidence. its estimation, however, requires modeling of the censoring distribution and is not statistically efficient. in this paper, we present a broad class of semiparametric transformation models which extends the fine and gray model, and we allow for unknown causes of failure. we derive the nonparametric maximum likelihood estimators (npmles) and develop simple and fast numerical algorithms using the profile likelihood. we establish the consistency, asymptotic normality, and semiparametric efficiency of the npmles. in addition, we construct graphical and numerical procedures to evaluate and select models. finally, we demonstrate the advantages of the proposed methods over the existing ones through extensive simulation studies and an application to a major study on bone marrow transplantation.","","2016-02-04","2016-02-29","['lu mao', 'd. y. lin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1347",1602.02435,"a multi-resolution spatio-temporal model for brain activation and   connectivity in fmri data","stat.ap","functional magnetic resonance imaging (fmri) is a primary modality for studying brain activity. modeling spatial dependence of imaging data at different scales is one of the main challenges of contemporary neuroimaging, and it could allow for accurate testing for significance in neural activity. the high dimensionality of this type of data (on the order of hundreds of thousands of voxels) poses serious modeling challenges and considerable computational constraints. for the sake of feasibility, standard models typically reduce dimensionality by modeling covariance among regions of interest (rois) -- coarser or larger spatial units -- rather than among voxels. however, ignoring spatial dependence at different scales could drastically reduce our ability to detect activation patterns in the brain and hence produce misleading results. to overcome these problems, we introduce a multi-resolution spatio-temporal model and a computationally efficient methodology to estimate cognitive control related activation and whole-brain connectivity. the proposed model allows for testing voxel-specific activation while accounting for non-stationary local spatial dependence within anatomically defined rois, as well as regional dependence (between-rois). furthermore, the model allows for detection of interpretable connectivity patterns among rois using the graphical least absolute shrinkage selection operator (lasso). the model is used in a motor-task fmri study to investigate brain activation and connectivity patterns aimed at identifying associations between these patterns and regaining motor functionality following a stroke.","","2016-02-07","2016-06-15","['stefano castruccio', 'hernando ombao', 'marc g. genton']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1348",1602.02666,"a variational analysis of stochastic gradient algorithms","stat.ml cs.lg","stochastic gradient descent (sgd) is an important algorithm in machine learning. with constant learning rates, it is a stochastic process that, after an initial phase of convergence, generates samples from a stationary distribution. we show that sgd with constant rates can be effectively used as an approximate posterior inference algorithm for probabilistic modeling. specifically, we show how to adjust the tuning parameters of sgd such as to match the resulting stationary distribution to the posterior. this analysis rests on interpreting sgd as a continuous-time stochastic process and then minimizing the kullback-leibler divergence between its stationary distribution and the target posterior. (this is in the spirit of variational inference.) in more detail, we model sgd as a multivariate ornstein-uhlenbeck process and then use properties of this process to derive the optimal parameters. this theoretical framework also connects sgd to modern scalable inference algorithms; we analyze the recently proposed stochastic gradient fisher scoring under this perspective. we demonstrate that sgd with properly chosen constant rates gives a new way to optimize hyperparameters in probabilistic models.","","2016-02-08","","['stephan mandt', 'matthew d. hoffman', 'david m. blei']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1349",1602.02845,"online active linear regression via thresholding","stat.ml cs.lg","we consider the problem of online active learning to collect data for regression modeling. specifically, we consider a decision maker with a limited experimentation budget who must efficiently learn an underlying linear population model. our main contribution is a novel threshold-based algorithm for selection of most informative observations; we characterize its performance and fundamental lower bounds. we extend the algorithm and its guarantees to sparse linear regression in high-dimensional settings. simulations suggest the algorithm is remarkably robust: it provides significant benefits over passive random sampling in real-world datasets that exhibit high nonlinearity and high dimensionality --- significantly reducing both the mean and variance of the squared error.","","2016-02-08","2016-12-21","['carlos riquelme', 'ramesh johari', 'baosen zhang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1350",1602.02898,"modeling competition between two pharmaceutical drugs using innovation   diffusion models","stat.ap","the study of competition among brands in a common category is an interesting strategic issue for involved firms. sales monitoring and prediction of competitors' performance represent relevant tools for management. in the pharmaceutical market, the diffusion of product knowledge plays a special role, different from the role it plays in other competing fields. this latent feature naturally affects the evolution of drugs' performances in terms of the number of packages sold. in this paper, we propose an innovation diffusion model that takes the spread of knowledge into account. we are motivated by the need of modeling competition of two antidiabetic drugs in the italian market.","10.1214/15-aoas868","2016-02-09","","['renato guseo', 'cinzia mortarino']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1351",1602.02902,"a stochastic space-time model for intermittent precipitation occurrences","stat.ap","modeling a precipitation field is challenging due to its intermittent and highly scale-dependent nature. motivated by the features of high-frequency precipitation data from a network of rain gauges, we propose a threshold space-time $t$ random field (trf) model for 15-minute precipitation occurrences. this model is constructed through a space-time gaussian random field (grf) with random scaling varying along time or space and time. it can be viewed as a generalization of the purely spatial trf, and has a hierarchical representation that allows for bayesian interpretation. developing appropriate tools for evaluating precipitation models is a crucial part of the model-building process, and we focus on evaluating whether models can produce the observed conditional dry and rain probabilities given that some set of neighboring sites all have rain or all have no rain. these conditional probabilities show that the proposed space-time model has noticeable improvements in some characteristics of joint rainfall occurrences for the data we have considered.","10.1214/15-aoas875","2016-02-09","","['ying sun', 'michael l. stein']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1352",1602.02945,"the connected brain: causality, models and intrinsic dynamics","q-bio.nc math.ds stat.ap","recently, there have been several concerted international efforts - the brain initiative, european human brain project and the human connectome project, to name a few - that hope to revolutionize our understanding of the connected brain. over the past two decades, functional neuroimaging has emerged as the predominant technique in systems neuroscience. this is foreshadowed by an ever increasing number of publications on functional connectivity, causal modeling, connectomics, and multivariate analyses of distributed patterns of brain responses. in this article, we summarize pedagogically the (deep) history of brain mapping. we will highlight the theoretical advances made in the (dynamic) causal modelling of brain function - that may have escaped the wider audience of this article - and provide a brief overview of recent developments and interesting clinical applications. we hope that this article will engage the signal processing community by showcasing the inherently multidisciplinary nature of this important topic and the intriguing questions that are being addressed.","10.1109/msp.2015.2482121","2016-02-09","","['adeel razi', 'karl friston']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1353",1602.03105,"graphical model sketch","cs.ds cs.lg stat.ml","structured high-cardinality data arises in many domains, and poses a major challenge for both modeling and inference. graphical models are a popular approach to modeling structured data but they are unsuitable for high-cardinality variables. the count-min (cm) sketch is a popular approach to estimating probabilities in high-cardinality data but it does not scale well beyond a few variables. in this work, we bring together the ideas of graphical models and count sketches; and propose and analyze several approaches to estimating probabilities in structured high-cardinality streams of data. the key idea of our approximations is to use the structure of a graphical model and approximately estimate its factors by ""sketches"", which hash high-cardinality variables using random projections. our approximations are computationally efficient and their space complexity is independent of the cardinality of variables. our error bounds are multiplicative and significantly improve upon those of the cm sketch, a state-of-the-art approach to estimating probabilities in streams. we evaluate our approximations on synthetic and real-world problems, and report an order of magnitude improvements over the cm sketch.","","2016-02-09","2016-07-18","['branislav kveton', 'hung bui', 'mohammad ghavamzadeh', 'georgios theocharous', 's. muthukrishnan', 'siqi sun']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1354",1602.04003,"bayesian smoothing of dipoles in magneto-/electro-encephalography","math.na stat.ap stat.me","we describe a novel method for dynamic estimation of multi-dipole states from magneto/electro-encephalography (m/eeg) time series. the new approach builds on the recent development of particle filters for m/eeg; these algorithms approximate, with samples and weights, the posterior distribution of the neural sources at time t given the data up to time t. however, for off-line inference purposes it is preferable to work with the smoothing distribution, i.e. the distribution for the neural sources at time t conditioned on the whole time series. in this study, we use a monte carlo algorithm to approximate the smoothing distribution for a time-varying set of current dipoles. we show, using numerical simulations, that the estimates provided by the smoothing distribution are more accurate than those provided by the filtering distribution, particularly at the appearance of the source. we validate the proposed algorithm using an experimental dataset recorded from an epileptic patient. improved localization of the source onset can be particularly relevant in source modeling of epileptic patients, where the source onset brings information on the epileptogenic zone.","10.1088/0266-5611/32/4/045007","2016-02-12","","['valentina vivaldi', 'alberto sorrentino']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1355",1602.04391,"constrained multi-slot optimization for ranking recommendations","stat.ml math.oc stat.ap","ranking items to be recommended to users is one of the main problems in large scale social media applications. this problem can be set up as a multi-objective optimization problem to allow for trading off multiple, potentially conflicting objectives (that are driven by those items) against each other. most previous approaches to this problem optimize for a single slot without considering the interaction effect of these items on one another.   in this paper, we develop a constrained multi-slot optimization formulation, which allows for modeling interactions among the items on the different slots. we characterize the solution in terms of problem parameters and identify conditions under which an efficient solution is possible. the problem formulation results in a quadratically constrained quadratic program (qcqp). we provide an algorithm that gives us an efficient solution by relaxing the constraints of the qcqp minimally. through simulated experiments, we show the benefits of modeling interactions in a multi-slot ranking context, and the speed and accuracy of our qcqp approximate solver against other state of the art methods.","","2016-02-13","2017-05-16","['kinjal basu', 'shaunak chatterjee', 'ankan saha']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"1356",1602.04393,"semantic scan: detecting subtle, spatially localized events in text   streams","cs.ir stat.ml","early detection and precise characterization of emerging topics in text streams can be highly useful in applications such as timely and targeted public health interventions and discovering evolving regional business trends. many methods have been proposed for detecting emerging events in text streams using topic modeling. however, these methods have numerous shortcomings that make them unsuitable for rapid detection of locally emerging events on massive text streams. in this paper, we describe semantic scan (ss) that has been developed specifically to overcome these shortcomings in detecting new spatially compact events in text streams.   semantic scan integrates novel contrastive topic modeling with online document assignment and principled likelihood ratio-based spatial scanning to identify emerging events with unexpected patterns of keywords hidden in text streams. this enables more timely and accurate detection and characterization of anomalous, spatially localized emerging events. semantic scan does not require manual intervention or labeled training data, and is robust to noise in real-world text data since it identifies anomalous text patterns that occur in a cluster of new documents rather than an anomaly in a single new document.   we compare semantic scan to alternative state-of-the-art methods such as topics over time, online lda, and labeled lda on two real-world tasks: (i) a disease surveillance task monitoring free-text emergency department chief complaints in allegheny county, and (ii) an emerging business trend detection task based on yelp reviews. on both tasks, we find that semantic scan provides significantly better event detection and characterization accuracy than competing approaches, while providing up to an order of magnitude speedup.","","2016-02-13","","['abhinav maurya', 'kenton murray', 'yandong liu', 'chris dyer', 'william w. cohen', 'daniel b. neill']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1357",1602.04418,"identifiability assumptions and algorithm for directed graphical models   with feedback","stat.ml cs.lg","directed graphical models provide a useful framework for modeling causal or directional relationships for multivariate data. prior work has largely focused on identifiability and search algorithms for directed acyclic graphical (dag) models. in many applications, feedback naturally arises and directed graphical models that permit cycles occur. in this paper we address the issue of identifiability for general directed cyclic graphical (dcg) models satisfying the markov assumption. in particular, in addition to the faithfulness assumption which has already been introduced for cyclic models, we introduce two new identifiability assumptions, one based on selecting the model with the fewest edges and the other based on selecting the dcg model that entails the maximum number of d-separation rules. we provide theoretical results comparing these assumptions which show that: (1) selecting models with the largest number of d-separation rules is strictly weaker than the faithfulness assumption; (2) unlike for dag models, selecting models with the fewest edges does not necessarily result in a milder assumption than the faithfulness assumption. we also provide connections between our two new principles and minimality assumptions. we use our identifiability assumptions to develop search algorithms for small-scale dcg models. our simulation study supports our theoretical results, showing that the algorithms based on our two new principles generally out-perform algorithms based on the faithfulness assumption in terms of selecting the true skeleton for dcg models.","","2016-02-14","2016-07-06","['gunwoong park', 'garvesh raskutti']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1358",1602.0491,"bayesian generalized fused lasso modeling via neg distribution","stat.me stat.ml","the fused lasso penalizes a loss function by the $l_1$ norm for both the regression coefficients and their successive differences to encourage sparsity of both. in this paper, we propose a bayesian generalized fused lasso modeling based on a normal-exponential-gamma (neg) prior distribution. the neg prior is assumed into the difference of successive regression coefficients. the proposed method enables us to construct a more versatile sparse model than the ordinary fused lasso by using a flexible regularization term. we also propose a sparse fused algorithm to produce exact sparse solutions. simulation studies and real data analyses show that the proposed method has superior performance to the ordinary fused lasso.","10.1080/03610926.2018.1489056","2016-02-16","","['kaito shimamura', 'masao ueki', 'shuichi kawano', 'sadanori konishi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1359",1602.05162,"a bayes interpretation of stacking for m-complete and m-open settings","math.st stat.th","in m-open problems where no true model can be conceptualized, it is common to back off from modeling and merely seek good prediction. even in m-complete problems, taking a predictive approach can be very useful. stacking is a model averaging procedure that gives a composite predictor by combining individual predictors from a list of models using weights that optimize a cross-validation criterion. we show that the stacking weights also asymptotically minimize a posterior expected loss. hence we formally provide a bayesian justification for cross-validation. often the weights are constrained to be positive and sum to one. for greater generality, we omit the positivity constraint and relax the `sum to one' constraint.   a key question is `what predictors should be in the average?' we first verify that the stacking error depends only on the span of the models. then we propose using bootstrap samples from the data to generate empirical basis elements that can be used to form models. we use this in two computed examples to give stacking predictors that are (i) data driven, (ii) optimal with respect to the number of component predictors, and (iii) optimal with respect to the weight each predictor gets.","","2016-02-16","","['tri le', 'bertrand clarke']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1360",1602.06202,"nonparametric functional calibration of computer models","stat.me","standard methods in computer model calibration treat the calibration parameters as constant throughout the domain of control inputs. in many applications, systematic variation may cause the best values for the calibration parameters to change between different settings. when not accounted for in the code, this variation can make the computer model inadequate. in this article, we propose a framework for modeling the calibration parameters as functions of the control inputs to account for a computer model's incomplete system representation in this regard while simultaneously allowing for possible constraints imposed by prior expert opinion. we demonstrate how inappropriate modeling assumptions can mislead a researcher into thinking a calibrated model is in need of an empirical discrepancy term when it is only needed to allow for a functional dependence of the calibration parameters on the inputs. we apply our approach to plastic deformation of a visco-plastic self-consistent material in which the critical resolved shear stress is known to vary with temperature.","10.5705/ss.202015.0344","2016-02-19","2017-02-07","['d. andrew brown', 'sez atamturktur']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1361",1602.06604,"detection of cyber-physical faults and intrusions from physical   correlations","cs.sy cs.si physics.data-an physics.soc-ph stat.ap","cyber-physical systems are critical infrastructures that are crucial both to the reliable delivery of resources such as energy, and to the stable functioning of automatic and control architectures. these systems are composed of interdependent physical, control and communications networks described by disparate mathematical models creating scientific challenges that go well beyond the modeling and analysis of the individual networks. a key challenge in cyber-physical defense is a fast online detection and localization of faults and intrusions without prior knowledge of the failure type. we describe a set of techniques for the efficient identification of faults from correlations in physical signals, assuming only a minimal amount of available system information. the performance of our detection method is illustrated on data collected from a large building automation system.","","2016-02-21","2016-07-01","['andrey y. lokhov', 'nathan lemons', 'thomas c. mcandrew', 'aric hagberg', 'scott backhaus']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1362",1602.07109,"variational inference for on-line anomaly detection in high-dimensional   time series","stat.ml cs.lg","approximate variational inference has shown to be a powerful tool for modeling unknown complex probability distributions. recent advances in the field allow us to learn probabilistic models of sequences that actively exploit spatial and temporal structure. we apply a stochastic recurrent network (storn) to learn robot time series data. our evaluation demonstrates that we can robustly detect anomalies both off- and on-line.","","2016-02-23","2016-06-14","['maximilian soelch', 'justin bayer', 'marvin ludersdorfer', 'patrick van der smagt']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1363",1602.07337,"sparse estimation of multivariate poisson log-normal models from count   data","stat.me cs.lg","modeling data with multivariate count responses is a challenging problem due to the discrete nature of the responses. existing methods for univariate count responses cannot be easily extended to the multivariate case since the dependency among multiple responses needs to be properly accommodated. in this paper, we propose a multivariate poisson log-normal regression model for multivariate data with count responses. by simultaneously estimating the regression coefficients and inverse covariance matrix over the latent variables with an efficient monte carlo em algorithm, the proposed regression model takes advantages of association among multiple count responses to improve the model prediction performance. simulation studies and applications to real world data are conducted to systematically evaluate the performance of the proposed method in comparison with conventional methods.","","2016-02-21","2016-08-12","['hao wu', 'xinwei deng', 'naren ramakrishnan']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1364",1602.08066,"scalable semiparametric inference for the means of heavy-tailed   distributions","stat.ap","heavy tailed distributions present a tough setting for inference. they are also common in industrial applications, particularly with internet transaction datasets, and machine learners often analyze such data without considering the biases and risks associated with the misuse of standard tools. this paper outlines a procedure for inference about the mean of a (possibly conditional) heavy tailed distribution that combines nonparametric analysis for the bulk of the support with bayesian parametric modeling -- motivated from extreme value theory -- for the heavy tail. the procedure is fast and massively scalable. the resulting point estimators attain lowest-possible error rates and, unique among alternatives, we are able to provide accurate uncertainty quantification for these estimators. the work should find application in settings wherever correct inference is important and reward tails are heavy; we illustrate the framework in causal inference for a/b experiments involving hundreds of millions of users of ebay.com.","","2016-02-25","2016-10-13","['matt taddy', 'hedibert freitas lopes', 'matt gardner']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1365",1602.08355,"on short-term traffic flow forecasting and its reliability","stat.ap cs.oh","recent advances in time series, where deterministic and stochastic modelings as well as the storage and analysis of big data are useless, permit a new approach to short-term traffic flow forecasting and to its reliability, i.e., to the traffic volatility. several convincing computer simulations, which utilize concrete data, are presented and discussed.","","2016-02-25","","['hassane abouaïssa', 'michel fliess', 'cédric join']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1366",1602.08753,"stability and structural properties of gene regulation networks with   coregulation rules","stat.ml q-bio.mn q-bio.qm","coregulation of the expression of groups of genes has been extensively demonstrated empirically in bacterial and eukaryotic systems. such coregulation can arise through the use of shared regulatory motifs, which allow the coordinated expression of modules (and module groups) of functionally related genes across the genome. coregulation can also arise through the physical association of multi-gene complexes through chromosomal looping, which are then transcribed together. we present a general formalism for modeling coregulation rules in the framework of random boolean networks (rbn), and develop specific models for transcription factor networks with modular structure (including module groups, and multi-input modules (mim) with autoregulation) and multi-gene complexes (including hierarchical differentiation between multi-gene complex members). we develop a mean-field approach to analyse the stability of large networks incorporating coregulation, and show that autoregulated mim and hierarchical gene-complex models can achieve greater stability than networks without coregulation whose rules have matching activation frequency. we provide further analysis of the stability of small networks of both kinds through simulations. we also characterize several general properties of the transients and attractors in the hierarchical coregulation model, and show using simulations that the steady-state distribution factorizes hierarchically as a bayesian network in a markov jump process analogue of the rbn model.","","2016-02-28","2016-04-10","['jonathan h. warrell', 'musa m. mhlanga']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1367",1603.00074,"modeling the infectiousness of twitter hashtags","cs.si q-bio.pe stat.ap","this study applies dynamical and statistical modeling techniques to quantify the proliferation and popularity of trending hashtags on twitter. using time-series data reflecting actual tweets in new york city and san francisco, we present estimates for the dynamics (i.e., rates of infection and recovery) of several hundred trending hashtags using an epidemic modeling framework coupled with bayesian markov chain monte carlo (mcmc) methods. this methodological strategy is an extension of techniques traditionally used to model the spread of infectious disease. we demonstrate that in some models, hashtags can be grouped by infectiousness, possibly providing a method for quantifying the trendiness of a topic.","10.1016/j.physa.2016.08.038","2016-02-29","","['jonathan skaza', 'brian blais']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1368",1603.00788,"automatic differentiation variational inference","stat.ml cs.ai cs.lg stat.co","probabilistic modeling is iterative. a scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. however, fitting complex models to large data is a bottleneck in this process. deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. to this end, we develop automatic differentiation variational inference (advi). using our method, the scientist only provides a probabilistic model and a dataset, nothing else. advi automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models. advi supports a broad class of models-no conjugacy assumptions are required. we study advi across ten different models and apply it to a dataset with millions of observations. advi is integrated into stan, a probabilistic programming system; it is available for immediate use.","","2016-03-02","","['alp kucukelbir', 'dustin tran', 'rajesh ranganath', 'andrew gelman', 'david m. blei']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE
"1369",1603.01066,"what we look at in paintings: a comparison between experienced and   inexperienced art viewers","stat.ap","how do people look at art? are there any differences between how experienced and inexperienced art viewers look at a painting? we approach these questions by analyzing and modeling eye movement data from a cognitive art research experiment, where the eye movements of twenty test subjects, ten experienced and ten inexperienced art viewers, were recorded while they were looking at paintings.   eye movements consist of stops of the gaze as well as jumps between the stops. hence, the observed gaze stop locations can be thought as a spatial point pattern, which can be modeled by a spatio-temporal point process. we introduce some statistical tools to analyze the spatio-temporal eye movement data, and compare the eye movements of experienced and inexperienced art viewers. in addition, we develop a stochastic model, which is rather simple but fits quite well to the eye movement data, to further investigate the differences between the two groups through functional summary statistics.","10.1214/16-aoas921","2016-03-03","","['anna-kaisa ylitalo', 'aila särkkä', 'peter guttorp']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1370",1603.01376,"lasso estimation for gefcom2014 probabilistic electric load forecasting","stat.ap stat.ml","we present a methodology for probabilistic load forecasting that is based on lasso (least absolute shrinkage and selection operator) estimation. the model considered can be regarded as a bivariate time-varying threshold autoregressive(ar) process for the hourly electric load and temperature. the joint modeling approach incorporates the temperature effects directly, and reflects daily, weekly, and annual seasonal patterns and public holiday effects. we provide two empirical studies, one based on the probabilistic load forecasting track of the global energy forecasting competition 2014 (gefcom2014-l), and the other based on another recent probabilistic load forecasting competition that follows a setup similar to that of gefcom2014-l. in both empirical case studies, the proposed methodology outperforms two multiple linear regression based benchmarks from among the top eight entries to gefcom2014-l.","10.1016/j.ijforecast.2016.01.001","2016-03-04","","['florian ziel', 'bidong liu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"1371",1603.01424,"estimating non-simplified vine copulas using penalized splines","stat.me","vine copulas (or pair-copula constructions) have become an important tool for high-dimensional dependence modeling. typically, so called simplified vine copula models are estimated where bivariate conditional copulas are approximated by bivariate unconditional copulas. we present the first non-parametric estimator of a non-simplified vine copula that allows for varying conditional copulas using penalized hierarchical b-splines. throughout the vine copula, we test for the simplifying assumption in each edge, establishing a data-driven non-simplified vine copula estimator. to overcome the curse of dimensionality, we approximate conditional copulas with more than one conditioning argument by a conditional copula with the first principal component as conditioning argument. an extensive simulation study is conducted, showing a substantial improvement in the out-of-sample kullback-leibler divergence if the null hypothesis of a simplified vine copula can be rejected. we apply our method to the famous uranium data and present a classification of an eye state data set, demonstrating the potential benefit that can be achieved when conditional copulas are modeled.","10.1007/s11222-017-9737-7","2016-03-04","2016-12-14","['christian schellhase', 'fabian spanhel']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1372",1603.01476,"vine copula based likelihood estimation of dependence patterns in   multivariate event time data","stat.ap stat.me","in many studies multivariate event time data are generated from clusters having a possibly complex association pattern. flexible models are needed to capture this dependence. vine copulas serve this purpose. inference methods for vine copulas are available for complete data. event time data, however, are often subject to right-censoring. as a consequence, the existing inferential tools, e.g. likelihood estimation, need to be adapted. a two-stage estimation approach is proposed. first, the marginal distributions are modeled. second, the dependence structure modeled by a vine copula is estimated via likelihood maximization. due to the right-censoring single and double integrals show up in the copula likelihood expression such that numerical integration is needed for its evaluation. for the dependence modeling a sequential estimation approach that facilitates the computational challenges of the likelihood optimization is provided. a three-dimensional simulation study provides evidence for the good finite sample performance of the proposed method. using four-dimensional mastitis data, it is shown how an appropriate vine copula model can be selected for data at hand.","","2016-03-04","2017-07-22","['nicole barthel', 'candida geerdens', 'matthias killiches', 'paul janssen', 'claudia czado']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1373",1603.01851,"a semiparametric joint model for terminal trend of quality of life and   survival in palliative care research","stat.ap stat.me","palliative medicine is an interdisciplinary specialty focusing on improving quality of life (qol) for patients with serious illness and their families. palliative care programs are available or under development at over 80% of large us hospitals (300+ beds). palliative care clinical trials present unique analytic challenges relative to evaluating the palliative care treatment efficacy which is to improve patients diminishing qol as disease progresses towards end of life (eol). a unique feature of palliative care clinical trials is that patients will experience decreasing qol during the trial despite potentially beneficial treatment. often longitudinal qol and survival data are highly correlated which, in the face of censoring, makes it challenging to properly analyze and interpret longitudinal qol trajectory. to address these issues, we propose a novel semiparametric statistical approach to jointly model longitudinal qol and survival data. there are two sub-models in our approach: a semiparametric mixed effects model for longitudinal qol and a cox model for survival. we use regression splines method to estimate the nonparametric curves and aic to select knots. we assess the model through simulation and application to establish a novel modeling approach that could be applied in future palliative care treatment research trials.","","2016-03-06","2016-04-15","['zhigang li', 'h. r. frost', 'tor d. tosteson', 'lihui zhao', 'lei liu', 'kathleen lyons', 'huaihou chen', 'bernard cole', 'david currow', 'marie bakitas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1374",1603.01913,"a latent variable recurrent neural network for discourse relation   language models","cs.cl cs.lg cs.ne stat.ml","this paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and (possibly latent) discourse relations between adjacent sentences. a recurrent neural network generates individual words, thus reaping the benefits of discriminatively-trained vector representations. the discourse relations are represented with a latent variable, which can be predicted or marginalized, depending on the task. the resulting model can therefore employ a training objective that includes not only discourse relation classification, but also word prediction. as a result, it outperforms state-of-the-art alternatives for two tasks: implicit discourse relation classification in the penn discourse treebank, and dialog act classification in the switchboard corpus. furthermore, by marginalizing over latent discourse relations at test time, we obtain a discourse informed language model, which improves over a strong lstm baseline.","","2016-03-06","2016-04-05","['yangfeng ji', 'gholamreza haffari', 'jacob eisenstein']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1375",1603.02049,"prediction of functional arma processes with an application to traffic   data","stat.me","this work is devoted to functional arma$(p, q)$ processes and approximating vector models based on functional pca in the context of prediction. after deriving sufficient conditions for the existence of a stationary solution to both the functional and the vector model equations, the structure of the approximating vector model is investigated. the stationary vector process is used to predict the functional process. a bound for the difference between vector and functional best linear predictor is derived. the paper concludes by applying functional arma processes for the modeling and prediction of highway traffic data.","","2016-03-07","2016-09-10","['j. klepsch', 'c. klüppelberg', 't. wei']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1376",1603.02657,"data-driven probability concentration and sampling on manifold","math.pr math.st stat.th","a new methodology is proposed for generating realizations of a random vector with values in a finite-dimensional euclidean space that are statistically consistent with a data set of observations of this vector. the probability distribution of this random vector, while a-priori not known, is presumed to be concentrated on an unknown subset of the euclidean space. a random matrix is introduced whose columns are independent copies of the random vector and for which the number of columns is the number of data points in the data set. the approach is based on the use of (i) the multidimensional kernel-density estimation method for estimating the probability distribution of the random matrix, (ii) a mcmc method for generating realizations for the random matrix, (iii) the diffusion-maps approach for discovering and characterizing the geometry and the structure of the data set, and (iv) a reduced-order representation of the random matrix, which is constructed using the diffusion-maps vectors associated with the first eigenvalues of the transition matrix relative to the given data set. the convergence aspects of the proposed methodology are analyzed and a numerical validation is explored through three applications of increasing complexity. the proposed method is found to be robust to noise levels and data complexity as well as to the intrinsic dimension of data and the size of experimental data sets. both the methodology and the underlying mathematical framework presented in this paper contribute new capabilities and perspectives at the interface of uncertainty quantification, statistical data analysis, stochastic modeling and associated statistical inverse problems.","10.1016/j.jcp.2016.05.044","2016-03-08","","['christian soize', 'roger ghanem']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1377",1603.02745,"non-parametric latent modeling and network clustering","stat.me","the paper exposes a non-parametric approach to latent and co-latent modeling of bivariate data, based upon alternating minimization of the kullback-leibler divergence (em algorithm) for complete log-linear models. for categorical data, the iterative algorithm generates a soft clustering of both rows and columns of the contingency table. well-known results are systematically revisited, and some variants are presumably original. in particular, the consideration of square contingency tables induces a clustering algorithm for weighted networks, differing from spectral clustering or modularity maximization techniques. also, we present a co-clustering algorithm applicable to hmm models of general kind, distinct from the baum-welch algorithm. three case studies illustrate the theory.","","2016-03-08","","['françois bavaud']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1378",1603.02977,"frequency estimation in three-phase power systems with harmonic   contamination: a multistage quaternion kalman filtering approach","stat.ml stat.ap","motivated by the need for accurate frequency information, a novel algorithm for estimating the fundamental frequency and its rate of change in three-phase power systems is developed. this is achieved through two stages of kalman filtering. in the first stage a quaternion extended kalman filter, which provides a unified framework for joint modeling of voltage measurements from all the phases, is used to estimate the instantaneous phase increment of the three-phase voltages. the phase increment estimates are then used as observations of the extended kalman filter in the second stage that accounts for the dynamic behavior of the system frequency and simultaneously estimates the fundamental frequency and its rate of change. the framework is then extended to account for the presence of harmonics. finally, the concept is validated through simulation on both synthetic and real-world data.","","2016-03-08","","['sayed pouria talebi', 'danilo p. mandic']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"1379",1603.0345,"multisensor--multitarget bearing--only sensor registration","stat.me cs.sy","bearing--only estimation is one of the fundamental and challenging problems in target tracking. as in the case of radar tracking, the presence of offset or position biases can exacerbate the challenges in bearing--only estimation. modeling various sensor biases is not a trivial task and not much has been done in the literature specifically for bearing--only tracking. this paper addresses the modeling of offset biases in bearing--only sensors and the ensuing multitarget tracking with bias compensation. bias estimation is handled at the fusion node to which individual sensors report their local tracks in the form of associated measurement reports (amr) or angle-only tracks. the modeling is based on a multisensor approach that can effectively handle a time--varying number of targets in the surveillance region. the proposed algorithm leads to a maximum likelihood bias estimator. the corresponding cram\'er--rao lower bound to quantify the theoretical accuracy that can be achieved by the proposed method or any other algorithm is also derived. finally, simulation results on different distributed tracking scenarios are presented to demonstrate the capabilities of the proposed approach. in order to show that the proposed method can work even with false alarms and missed detections, simulation results on a centralized tracking scenario where the local sensors send all their measurements (not amrs or local tracks) are also presented.","","2016-03-09","","['ehsan taghavi', 'r. tharmarasa', 't. kirubarajan', 'mike mcdonald']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1380",1603.03819,"birth/birth-death processes and their computable transition   probabilities with biological applications","stat.co","birth-death processes track the size of a univariate population, but many biological systems involve interaction between populations, necessitating models for two or more populations simultaneously. a lack of efficient methods for evaluating finite-time transition probabilities of bivariate processes, however, has restricted statistical inference in these models. researchers rely on computationally expensive methods such as matrix exponentiation or monte carlo approximation, restricting likelihood-based inference to small systems, or indirect methods such as approximate bayesian computation. in this paper, we introduce the birth(death)/birth-death process, a tractable bivariate extension of the birth-death process. we develop an efficient and robust algorithm to calculate the transition probabilities of birth(death)/birth-death processes using a continued fraction representation of their laplace transforms. next, we identify several exemplary models arising in molecular epidemiology, macro-parasite evolution, and infectious disease modeling that fall within this class, and demonstrate advantages of our proposed method over existing approaches to inference in these models. notably, the ubiquitous stochastic susceptible-infectious-removed (sir) model falls within this class, and we emphasize that computable transition probabilities newly enable direct inference of parameters in the sir model. we also propose a very fast method for approximating the transition probabilities under the sir model via a novel branching process simplification, and compare it to the continued fraction representation method with application to the 17th century plague in eyam. although the two methods produce similar maximum a posteriori estimates, the branching process approximation fails to capture the correlation structure in the joint posterior distribution.","","2016-03-11","2017-08-07","['lam si tung ho', 'jason xu', 'forrest w. crawford', 'vladimir n. minin', 'marc a. suchard']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1381",1603.0395,"a copula model for non-gaussian multivariate spatial data","stat.ap stat.me","we propose a new copula model for replicated multivariate spatial data. unlike classical models that assume multivariate normality of the data, the proposed copula is based on the assumption that some factors exist that affect the joint spatial dependence of all measurements of each variable as well as the joint dependence among these variables. the model is parameterized in terms of a cross-covariance function that may be chosen from the many models proposed in the literature. in addition, there are additive factors in the model that allow tail dependence and reflection asymmetry of each variable measured at different locations and of different variables to be modeled. the proposed approach can therefore be seen as an extension of the linear model of coregionalization widely used for modeling multivariate spatial data. the likelihood of the model can be obtained in a simple form and therefore the likelihood estimation is quite fast. the model is not restricted to the set of data locations, and using the estimated copula, spatial data can be interpolated at locations where values of variables are unknown. we apply the proposed model to temperature and pressure data and compare its performance with the performance of a popular model from multivariate geostatistics.","","2016-03-12","2018-10-10","['pavel krupskii', 'marc g. genton']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1382",1603.05412,"online semi-parametric learning for inverse dynamics modeling","math.oc cs.lg stat.ml","this paper presents a semi-parametric algorithm for online learning of a robot inverse dynamics model. it combines the strength of the parametric and non-parametric modeling. the former exploits the rigid body dynamics equa- tion, while the latter exploits a suitable kernel function. we provide an extensive comparison with other methods from the literature using real data from the icub humanoid robot. in doing so we also compare two different techniques, namely cross validation and marginal likelihood optimization, for estimating the hyperparameters of the kernel function.","","2016-03-17","2016-10-09","['diego romeres', 'mattia zorzi', 'raffaello camoriano', 'alessandro chiuso']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1383",1603.058,"a comparison between deep neural nets and kernel acoustic models for   speech recognition","cs.lg stat.ml","we study large-scale kernel methods for acoustic modeling and compare to dnns on performance metrics related to both acoustic modeling and recognition. measuring perplexity and frame-level classification accuracy, kernel-based acoustic models are as effective as their dnn counterparts. however, on token-error-rates dnn models can be significantly better. we have discovered that this might be attributed to dnn's unique strength in reducing both the perplexity and the entropy of the predicted posterior probabilities. motivated by our findings, we propose a new technique, entropy regularized perplexity, for model selection. this technique can noticeably improve the recognition performance of both types of models, and reduces the gap between them. while effective on broadcast news, this technique could be also applicable to other tasks.","","2016-03-18","","['zhiyun lu', 'dong guo', 'alireza bagheri garakani', 'kuan liu', 'avner may', 'aurelien bellet', 'linxi fan', 'michael collins', 'brian kingsbury', 'michael picheny', 'fei sha']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1384",1603.05915,"msiq: joint modeling of multiple rna-seq samples for accurate isoform   quantification","stat.ap q-bio.gn q-bio.qm","next-generation rna sequencing (rna-seq) technology has been widely used to assess full-length rna isoform abundance in a high-throughput manner. rna-seq data offer insight into gene expression levels and transcriptome structures, enabling us to better understand the regulation of gene expression and fundamental biological processes. accurate isoform quantification from rna-seq data is challenging due to the information loss in sequencing experiments. a recent accumulation of multiple rna-seq data sets from the same tissue or cell type provides new opportunities to improve the accuracy of isoform quantification. however, existing statistical or computational methods for multiple rna-seq samples either pool the samples into one sample or assign equal weights to the samples when estimating isoform abundance. these methods ignore the possible heterogeneity in the quality of different samples and could result in biased and unrobust estimates. in this article, we develop a method, which we call ""joint modeling of multiple rna-seq samples for accurate isoform quantification"" (msiq), for more accurate and robust isoform quantification by integrating multiple rna-seq samples under a bayesian framework. our method aims to (1) identify a consistent group of samples with homogeneous quality and (2) improve isoform quantification accuracy by jointly modeling multiple rna-seq samples by allowing for higher weights on the consistent group. we show that msiq provides a consistent estimator of isoform abundance, and we demonstrate the accuracy and effectiveness of msiq compared with alternative methods through simulation studies on d. melanogaster genes. we justify msiq's advantages over existing approaches via application studies on real rna-seq data from human embryonic stem cells, brain tissues, and the hepg2 immortalized cell line.","","2016-03-18","2017-12-02","['wei vivian li', 'anqi zhao', 'shihua zhang', 'jingyi jessica li']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1385",1603.06045,"non-standard conditionally specified models for non-ignorable missing   data","stat.me","data analyses typically rely upon assumptions about missingness mechanisms that lead to observed versus missing data. when the data are missing not at random, direct assumptions about the missingness mechanism, and indirect assumptions about the distributions of observed and missing data, are typically untestable. we explore an approach, where the joint distribution of observed data and missing data is specified through non-standard conditional distributions. in this formulation, which traces back to a factorization of the joint distribution, apparently proposed by j.w. tukey, the modeling assumptions about the conditional factors are either testable or are designed to allow the incorporation of substantive knowledge about the problem at hand, thereby offering a possibly realistic portrayal of the data, both missing and observed. we apply tukey's conditional representation to exponential family models, and we propose a computationally tractable inferential strategy for this class of models. we illustrate the utility of this approach using high-throughput biological data with missing data that are not missing at random.","","2016-03-19","","['alexander m franks', 'edoardo m airoldi', 'donald b rubin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1386",1603.06277,"composing graphical models with neural networks for structured   representations and fast inference","stat.ml","we propose a general modeling and inference framework that composes probabilistic graphical models with deep learning methods and combines their respective strengths. our model family augments graphical structure in latent variables with neural network observation models. for inference, we extend variational autoencoders to use graphical model approximating distributions with recognition networks that output conjugate potentials. all components of these models are learned simultaneously with a single objective, giving a scalable algorithm that leverages stochastic variational inference, natural gradients, graphical model message passing, and the reparameterization trick. we illustrate this framework with several example models and an application to mouse behavioral phenotyping.","","2016-03-20","2017-07-07","['matthew j. johnson', 'david duvenaud', 'alexander b. wiltschko', 'sandeep r. datta', 'ryan p. adams']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1387",1603.06619,"multivariate peaks over thresholds models","math.pr stat.me","multivariate peaks over thresholds modeling based on generalized pareto distributions has up to now only been used in few and mostly 2-dimensional situations. this paper contributes theoretical understanding, physically based models, inference tools, and simulation methods to support routine use, with an aim at higher dimensions. we derive a general point process model for extreme episodes in data, and show how conditioning the distribution of extreme episodes on threshold exceedance gives four basic representations of the family of generalized pareto distributions. the first representation is constructed on the real scale of the observations. the second one starts with a model on a standard exponential scale which then is transformed to the real scale. the third and fourth are reformulations of a spectral representation proposed in a. ferreira and l. de haan [bernoulli 20 (2014) 1717--1737]. numerically tractable forms of densities and censored densities are found and give tools for flexible parametric likelihood inference. new simulation algorithms, explicit formulas for probabilities and conditional probabilities, and conditions which make the conditional distribution of weighted component sums generalized pareto are derived.","10.1007/s10687-017-0294-4","2016-03-21","2017-05-03","['holger rootzén', 'johan segers', 'jennifer l. wadsworth']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1388",1603.06915,"completely random measures for modeling power laws in sparse graphs","stat.ml math.st stat.me stat.th","network data appear in a number of applications, such as online social networks and biological networks, and there is growing interest in both developing models for networks as well as studying the properties of such data. since individual network datasets continue to grow in size, it is necessary to develop models that accurately represent the real-life scaling properties of networks. one behavior of interest is having a power law in the degree distribution. however, other types of power laws that have been observed empirically and considered for applications such as clustering and feature allocation models have not been studied as frequently in models for graph data. in this paper, we enumerate desirable asymptotic behavior that may be of interest for modeling graph data, including sparsity and several types of power laws. we outline a general framework for graph generative models using completely random measures; by contrast to the pioneering work of caron and fox (2015), we consider instantiating more of the existing atoms of the random measure as the dataset size increases rather than adding new atoms to the measure. we see that these two models can be complementary; they respectively yield interpretations as (1) time passing among existing members of a network and (2) new individuals joining a network. we detail a particular instance of this framework and show simulated results that suggest this model exhibits some desirable asymptotic power-law behavior.","","2016-03-22","","['diana cai', 'tamara broderick']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE
"1389",1603.07066,"phase-amplitude separation and modeling of spherical trajectories","stat.me","this paper studies the problem of separating phase-amplitude components in sample paths of a spherical process (longitudinal data on a unit two-sphere). such separation is essential for efficient modeling and statistical analysis of spherical longitudinal data in a manner that is invariant to any phase variability. the key idea is to represent each path or trajectory with a pair of variables, a starting point and a transported square-root velocity curve (tsrvc). a tsrvc is a curve in the tangent (vector) space at the starting point and has some important invariance properties under the l2 norm. the space of all such curves forms a vector bundle and the l2 norm, along with the standard riemannian metric on s2, provides a natural metric on this vector bundle. this invariant representation allows for separating phase and amplitude components in given data, using a template-based idea. furthermore, the metric property is useful in deriving computational procedures for clustering, mean computation, principal component analysis (pca), and modeling. this comprehensive framework is demonstrated using two datasets: a set of bird-migration trajectories and a set of hurricane paths in the atlantic ocean.","","2016-03-23","","['zhengwu zhang', 'eric klassen', 'anuj srivastava']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1390",1603.07624,"semantic properties of customer sentiment in tweets","cs.cl cs.ir cs.si stat.ml","an increasing number of people are using online social networking services (snss), and a significant amount of information related to experiences in consumption is shared in this new media form. text mining is an emerging technique for mining useful information from the web. we aim at discovering in particular tweets semantic patterns in consumers' discussions on social media. specifically, the purposes of this study are twofold: 1) finding similarity and dissimilarity between two sets of textual documents that include consumers' sentiment polarities, two forms of positive vs. negative opinions and 2) driving actual content from the textual data that has a semantic trend. the considered tweets include consumers opinions on us retail companies (e.g., amazon, walmart). cosine similarity and k-means clustering methods are used to achieve the former goal, and latent dirichlet allocation (lda), a popular topic modeling algorithm, is used for the latter purpose. this is the first study which discover semantic properties of textual data in consumption context beyond sentiment analysis. in addition to major findings, we apply lda (latent dirichlet allocations) to the same data and drew latent topics that represent consumers' positive opinions and negative opinions on social media.","10.1109/waina.2014.151","2016-03-24","","['eun hee ko', 'diego klabjan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1391",1603.07749,"pathway lasso: estimate and select sparse mediation pathways with high   dimensional mediators","stat.ml stat.ap stat.me","in many scientific studies, it becomes increasingly important to delineate the causal pathways through a large number of mediators, such as genetic and brain mediators. structural equation modeling (sem) is a popular technique to estimate the pathway effects, commonly expressed as products of coefficients. however, it becomes unstable to fit such models with high dimensional mediators, especially for a general setting where all the mediators are causally dependent but the exact causal relationships between them are unknown. this paper proposes a sparse mediation model using a regularized sem approach, where sparsity here means that a small number of mediators have nonzero mediation effects between a treatment and an outcome. to address the model selection challenge, we innovate by introducing a new penalty called pathway lasso. this penalty function is a convex relaxation of the non-convex product function, and it enables a computationally tractable optimization criterion to estimate and select many pathway effects simultaneously. we develop a fast admm-type algorithm to compute the model parameters, and we show that the iterative updates can be expressed in closed form. on both simulated data and a real fmri dataset, the proposed approach yields higher pathway selection accuracy and lower estimation bias than other competing methods.","","2016-03-24","","['yi zhao', 'xi luo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE
"1392",1603.07888,"dimension reduction for gaussian process emulation: an application to   the influence of bathymetry on tsunami heights","stat.co stat.ap","high accuracy complex computer models, or simulators, require large resources in time and memory to produce realistic results. statistical emulators are computationally cheap approximations of such simulators. they can be built to replace simulators for various purposes, such as the propagation of uncertainties from inputs to outputs or the calibration of some internal parameters against observations. however, when the input space is of high dimension, the construction of an emulator can become prohibitively expensive. in this paper, we introduce a joint framework merging emulation with dimension reduction in order to overcome this hurdle. the gradient-based kernel dimension reduction technique is chosen due to its ability to drastically decrease dimensionality with little loss in information. the gaussian process emulation technique is combined with this dimension reduction approach. our proposed approach provides an answer to the dimension reduction issue in emulation for a wide range of simulation problems that cannot be tackled using existing methods. the efficiency and accuracy of the proposed framework is demonstrated theoretically, and compared with other methods on an elliptic partial differential equation (pde) problem. we finally present a realistic application to tsunami modeling. the uncertainties in the bathymetry (seafloor elevation) are modeled as high-dimensional realizations of a spatial process using a geostatistical approach. our dimension-reduced emulation enables us to compute the impact of these uncertainties on resulting possible tsunami wave heights near-shore and on-shore. we observe a significant increase in the spread of uncertainties in the tsunami heights due to the contribution of the bathymetry uncertainties. these results highlight the need to include the effect of uncertainties in the bathymetry in tsunami early warnings and risk assessments.","","2016-03-25","2016-08-24","['xiaoyu liu', 'serge guillas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE
"1393",1603.08482,"estimating mixture models via mixtures of polynomials","stat.ml cs.lg","mixture modeling is a general technique for making any simple model more expressive through weighted combination. this generality and simplicity in part explains the success of the expectation maximization (em) algorithm, in which updates are easy to derive for a wide class of mixture models. however, the likelihood of a mixture model is non-convex, so em has no known global convergence guarantees. recently, method of moments approaches offer global guarantees for some mixture models, but they do not extend easily to the range of mixture models that exist. in this work, we present polymom, an unifying framework based on method of moments in which estimation procedures are easily derivable, just as in em. polymom is applicable when the moments of a single mixture component are polynomials of the parameters. our key observation is that the moments of the mixture model are a mixture of these polynomials, which allows us to cast estimation as a generalized moment problem. we solve its relaxations using semidefinite optimization, and then extract parameters using ideas from computer algebra. this framework allows us to draw insights and apply tools from convex optimization, computer algebra and the theory of moments to study problems in statistical estimation.","","2016-03-28","","['sida i. wang', 'arun tejasvi chaganty', 'percy liang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1394",1603.08813,"locally epistatic models for genome-wide prediction and association by   importance sampling","stat.ap q-bio.qm stat.ml","in statistical genetics an important task involves building predictive models for the genotype-phenotype relationships and thus attribute a proportion of the total phenotypic variance to the variation in genotypes. numerous models have been proposed to incorporate additive genetic effects into models for prediction or association. however, there is a scarcity of models that can adequately account for gene by gene or other forms of genetical interactions. in addition, there is an increased interest in using marker annotations in genome-wide prediction and association. in this paper, we discuss an hybrid modeling methodology which combines the parametric mixed modeling approach and the non-parametric rule ensembles. this approach gives us a flexible class of models that can be used to capture additive, locally epistatic genetic effects, gene x background interactions and allows us to incorporate one or more annotations into the genomic selection or association models. we use benchmark data sets covering a range of organisms and traits in addition to simulated data sets to illustrate the strengths of this approach. the improvement of model accuracies and association results suggest that a part of the ""missing heritability"" in complex traits can be captured by modeling local epistasis.","","2016-03-29","","['deniz akdemir', 'jean-luc jannink']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"1395",1603.09029,"adaptive maximization of pointwise submodular functions with budget   constraint","cs.ai cs.dm math.oc stat.ml","we study the worst-case adaptive optimization problem with budget constraint that is useful for modeling various practical applications in artificial intelligence and machine learning. we investigate the near-optimality of greedy algorithms for this problem with both modular and non-modular cost functions. in both cases, we prove that two simple greedy algorithms are not near-optimal but the best between them is near-optimal if the utility function satisfies pointwise submodularity and pointwise cost-sensitive submodularity respectively. this implies a combined algorithm that is near-optimal with respect to the optimal algorithm that uses half of the budget. we discuss applications of our theoretical results and also report experiments comparing the greedy algorithms on the active learning problem.","","2016-03-29","2017-05-22","['nguyen viet cuong', 'huan xu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1396",1603.09272,"bayesian inference in hierarchical models by combining independent   posteriors","stat.co stat.me stat.ml","hierarchical models are versatile tools for joint modeling of data sets arising from different, but related, sources. fully bayesian inference may, however, become computationally prohibitive if the source-specific data models are complex, or if the number of sources is very large. to facilitate computation, we propose an approach, where inference is first made independently for the parameters of each data set, whereupon the obtained posterior samples are used as observed data in a substitute hierarchical model, based on a scaled likelihood function. compared to direct inference in a full hierarchical model, the approach has the advantage of being able to speed up convergence by breaking down the initial large inference problem into smaller individual subproblems with better convergence properties. moreover it enables parallel processing of the possibly complex inferences of the source-specific parameters, which may otherwise create a computational bottleneck if processed jointly as part of a hierarchical model. the approach is illustrated with both simulated and real data.","","2016-03-30","2016-04-13","['ritabrata dutta', 'paul blomstedt', 'samuel kaski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE
"1397",1604.01064,"bayesian local extrema splines","stat.me","we consider the problem of shape restricted nonparametric regression on a closed set x ?\in r; where it is reasonable to assume the function has no more than h local extrema interior to x: following a bayesian approach we develop a nonparametric prior over a novel class of local extrema splines. this approach is shown to be consistent when modeling any continuously differentiable function within the class of functions considered, and is used to develop methods for hypothesis testing on the shape of the curve. sampling algorithms are developed, and the method is applied in simulation studies and data examples where the shape of the curve is of interest.","","2016-04-04","","['matthew w. wheeler', 'david b. dunson', 'amy h. herring']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1398",1604.01996,"copuladta: an r package for copula based bivariate beta-binomial models   for diagnostic test accuracy studies in a bayesian framework","stat.me","the current statistical procedures implemented in statistical software packages for pooling of diagnostic test accuracy data include hsroc regression and the bivariate random-effects meta-analysis model (brma). however, these models do not report the overall mean but rather the mean for a central study with random-effect equal to zero and have difficulties estimating the correlation between sensitivity and specificity when the number of studies in the meta-analysis is small and/or when the between-study variance is relatively large. this tutorial on advanced statistical methods for meta-analysis of diagnostic accuracy studies discusses and demonstrates bayesian modeling using copuladta package in r to fit different models to obtain the meta-analytic parameter estimates. the focus is on the joint modelling of sensitivity and specificity using copula based bivariate beta distribution. essentially, we extend the work of nikoloulopoulos by: i) presenting the bayesian approach which offers flexibility and ability to perform complex statistical modelling even with small data sets and ii) including covariate information, and iii) providing an easy to use code. the statistical methods are illustrated by re-analysing data of two published meta-analyses. modelling sensitivity and specificity using the bivariate beta distribution provides marginal as well as study-specific parameter estimates as opposed to using bivariate normal distribution (e.g., in brma) which only yields study-specific parameter estimates. moreover, copula based models offer greater flexibility in modelling different correlation structures in contrast to the normal distribution which allows for only one correlation structure.","","2016-04-07","","['victoria n nyaga', 'marc arbyn', 'marc aerts']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1399",1604.02027,"combinatorial topic models using small-variance asymptotics","cs.lg cs.cl stat.ml","topic models have emerged as fundamental tools in unsupervised machine learning. most modern topic modeling algorithms take a probabilistic view and derive inference algorithms based on latent dirichlet allocation (lda) or its variants. in contrast, we study topic modeling as a combinatorial optimization problem, and propose a new objective function derived from lda by passing to the small-variance limit. we minimize the derived objective by using ideas from combinatorial optimization, which results in a new, fast, and high-quality topic modeling algorithm. in particular, we show that our results are competitive with popular lda-based topic modeling approaches, and also discuss the (dis)similarities between our approach and its probabilistic counterparts.","","2016-04-07","2016-05-26","['ke jiang', 'suvrit sra', 'brian kulis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1400",1604.02123,"multilevel weighted support vector machine for classification on   healthcare data with missing values","stat.ml cs.lg stat.ap","this work is motivated by the needs of predictive analytics on healthcare data as represented by electronic medical records. such data is invariably problematic: noisy, with missing entries, with imbalance in classes of interests, leading to serious bias in predictive modeling. since standard data mining methods often produce poor performance measures, we argue for development of specialized techniques of data-preprocessing and classification. in this paper, we propose a new method to simultaneously classify large datasets and reduce the effects of missing values. it is based on a multilevel framework of the cost-sensitive svm and the expected maximization imputation method for missing values, which relies on iterated regression analyses. we compare classification results of multilevel svm-based algorithms on public benchmark datasets with imbalanced classes and missing values as well as real data in health applications, and show that our multilevel svm-based method produces fast, and more accurate and robust classification results.","10.1371/journal.pone.0155119","2016-04-07","","['talayeh razzaghi', 'oleg roderick', 'ilya safro', 'nicholas marko']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"1401",1604.02221,"box-cox symmetric distributions and applications to nutritional data","stat.ot stat.me","we introduce the box-cox symmetric class of distributions, which is useful for modeling positively skewed, possibly heavy-tailed, data. the new class of distributions includes the box-cox t, box-cox cole-gree, box-cox power exponential distributions, and the class of the log-symmetric distributions as special cases. it provides easy parameter interpretation, which makes it convenient for regression modeling purposes. additionally, it provides enough flexibility to handle outliers. the usefulness of the box-cox symmetric models is illustrated in applications to nutritional data.","10.1007/s10182-017-0291-6","2016-04-08","2017-03-07","['silvia l. p. ferrari', 'giovana fumes']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE
"1402",1604.02652,"hypergraphs in the characterization of regular vine copula structures","stat.me","vine copulas constitute a flexible way for modeling of dependences using only pair copulas as building blocks. the pair-copula constructions introduced by joe (1997) are able to encode more types of dependences in the same time since they can be expressed as a product of different types of bi-variate copulas. the regular-vine structures (r-vines), as pair copulas corresponding to a sequence of trees, have been introduced by bedford and cooke (2001, 2002) and further explored by kurowicka and cooke (2006). the complexity of these models strongly increases in larger dimensions. therefore the so called truncated r-vines were introduced in brechmann et al. (2012). in this paper we express the regular-vines using a special type of hypergraphs, which encodes the conditional independences.","","2016-04-10","","['edith kovács', 'tamás szántai']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1403",1604.03269,"on the connection between cherry-tree copulas and truncated r-vine   copulas","stat.me","vine copulas are a flexible way for modeling dependences using only pair-copulas as building blocks. however if the number of variables grows the problem gets fast intractable. for dealing with this problem brechmann at al. proposed the truncated r-vine copulas. the truncated r-vine copula has the very useful property that it can be constructed by using only pair-wise copulas, and conditional pair-wise copulas. in our earlier papers we introduced the concept of cherry-tree copulas. in this paper we characterize the relation between the cherry-tree copulas and the truncated r-vine copulas. both are based on exploiting of some conditional independences between the variables. we give a necessary and sufficient condition for a cherry-tree copula to be a truncated r-vine copula. we introduce a new perspective for truncated r-vine modeling. the new idea is finding first a good fitting cherry-tree copula of order $k$. then, if this is also a truncated r-vine copula we apply the backward algorithm introduced in this paper. this way the construction of a sequence of trees which leads to it becomes possible. so the cherry-tree copula can be expressed by pair-copulas and conditional pair-copulas. in the case when the fitted $k$ order cherry-tree copula is not a truncated r-vine copula we give an algorithm to transform it into truncated r-vine copula at level $k+1$. therefore this cherry-tree copula can also be expressed by pair-copulas.","","2016-04-12","2016-07-02","['edith kovács', 'tamás szántai']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1404",1604.03558,"error propagation through a network with non-uniform failure","stat.ap cs.si q-bio.pe","a central concern of network operators is to estimate the probability of an incident that affects a significant part and thus may yield to a breakdown. we answer this question by modeling how a failure of either a node or an edge will affect the rest of the network using percolation theory. our model is general in the sense that it only needs two inputs: the topology of the network and the chances of failure of its components. these chances may vary to represent different types of edges having different tendencies to fail. we illustrate the approach by an example, for which we can even obtain closed form expressions for the likelihood of an outbreak remaining bounded or spreading unlimitedly.","","2016-03-31","2018-02-21","['sandra könig']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1405",1604.03622,"kronecker stap and sar gmti","stat.ap","as a high resolution radar imaging modality, sar detects and localizes non-moving targets accurately, giving it an advantage over lower resolution gmti radars. moving target detection is more challenging due to target smearing and masking by clutter. space-time adaptive processing (stap) is often used on multiantenna sar to remove the stationary clutter and enhance the moving targets. in (greenewald et al., 2016) it was shown that the performance of stap can be improved by modeling the clutter covariance as a space vs. time kronecker product with low rank factors, providing robustness and reducing the number of training samples required. in this work, we present a massively parallel algorithm for implementing kronecker product stap, enabling application to very large sar datasets (such as the 2006 gotcha data collection) using gpus. finally, we develop an extension of kronecker stap that uses information from multiple passes to improve moving target detection.","10.1117/12.2223896","2016-04-12","","['kristjan greenewald', 'edmund zelnio', 'alfred hero']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1406",1604.04202,"representing sparse gaussian dags as sparse r-vines allowing for   non-gaussian dependence","stat.me","modeling dependence in high dimensional systems has become an increasingly important topic. most approaches rely on the assumption of a multivariate gaussian distribution such as statistical models on directed acyclic graphs (dags). they are based on modeling conditional independencies and are scalable to high dimensions. in contrast, vine copula models accommodate more elaborate features like tail dependence and asymmetry, as well as independent modeling of the marginals. this flexibility comes however at the cost of exponentially increasing complexity for model selection and estimation. we show a novel connection between dags with limited number of parents and truncated vine copulas under sufficient conditions. this motivates a more general procedure exploiting the fast model selection and estimation of sparse dags while allowing for non-gaussian dependence using vine copulas. we demonstrate in a simulation study and using a high dimensional data application that our approach outperforms standard methods for vine structure estimation.","","2016-04-14","2016-11-30","['dominik müller', 'claudia czado']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1407",1604.04899,"phase-aligned spectral filtering for decomposing spatiotemporal dynamics","stat.me","spatiotemporal dynamics is central to a wide range of applications from climatology, computer vision to neural sciences. from temporal observations taken on a high-dimensional vector of spatial locations, we seek to derive knowledge about such dynamics via data assimilation and modeling. it is assumed that the observed spatiotemporal data represent superimposed lower-rank smooth oscillations and movements from a generative dynamic system, mixed with higher-rank random noises. separating the signals from noises is essential for us to visualize, model and understand these lower-rank dynamic systems. it is also often the case that such a lower-rank dynamic system have multiple independent components, corresponding to different trends or functionalities of the system under study. in this paper, we present a novel filtering framework for identifying lower-rank dynamics and its components embedded in a high dimensional spatiotemporal system. it is based on an approach of structural decomposition and phase-aligned construction in the frequency domain. in both our simulated examples and real data applications, we illustrate that the proposed method is able to separate and identify meaningful lower-rank movements, while existing methods fail.","","2016-04-17","","['lu meng', 'tian zheng']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1408",1604.06518,"approximation vector machines for large-scale online learning","cs.lg stat.ml","one of the most challenging problems in kernel online learning is to bound the model size and to promote the model sparsity. sparse models not only improve computation and memory usage, but also enhance the generalization capacity, a principle that concurs with the law of parsimony. however, inappropriate sparsity modeling may also significantly degrade the performance. in this paper, we propose approximation vector machine (avm), a model that can simultaneously encourage the sparsity and safeguard its risk in compromising the performance. when an incoming instance arrives, we approximate this instance by one of its neighbors whose distance to it is less than a predefined threshold. our key intuition is that since the newly seen instance is expressed by its nearby neighbor the optimal performance can be analytically formulated and maintained. we develop theoretical foundations to support this intuition and further establish an analysis to characterize the gap between the approximation and optimal solutions. this gap crucially depends on the frequency of approximation and the predefined threshold. we perform the convergence analysis for a wide spectrum of loss functions including hinge, smooth hinge, and logistic for classification task, and $l_1$, $l_2$, and $\epsilon$-insensitive for regression task. we conducted extensive experiments for classification task in batch and online modes, and regression task in online mode over several benchmark datasets. the results show that our proposed avm achieved a comparable predictive performance with current state-of-the-art methods while simultaneously achieving significant computational speed-up due to the ability of the proposed avm in maintaining the model size.","","2016-04-21","2017-05-27","['trung le', 'tu dinh nguyen', 'vu nguyen', 'dinh phung']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1409",1604.0708,"the power inverse lindley distribution","stat.co","several probability distributions have been proposed in the literature, especially with the aim of obtaining models that are more flexible relative to the behaviors of the density and hazard rate functions. recently, a new generalization of the lindley distribution was proposed by ghitany et al. (2013), called power lindley distribution. another generaliza- tion was proposed by sharma et al. (2015), known as inverse lindley distribution. in this paper, a new distribution is proposed, which is obtained from these two generalizations and named power inverse lindley distribution. some properties of this new distribution and study of the behavior of maximum likelihood estimators are presented and discussed. it is also applied considering real data and compared with the fits obtained for already- known distributions. when applied, the power inverse lindley distribution was found to be a good alternative for modeling survival data.","","2016-04-24","","['k. v. p. barco', 'j. mazucheli', 'v. janeiro']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1410",1604.08772,"towards conceptual compression","stat.ml cs.cv cs.lg","we introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling. the system represents the state-of-the-art in latent variable models for both the imagenet and omniglot datasets. we show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'.","","2016-04-29","","['karol gregor', 'frederic besse', 'danilo jimenez rezende', 'ivo danihelka', 'daan wierstra']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1411",1604.08859,"the z-loss: a shift and scale invariant classification loss belonging to   the spherical family","cs.lg cs.ai stat.ml","despite being the standard loss function to train multi-class neural networks, the log-softmax has two potential limitations. first, it involves computations that scale linearly with the number of output classes, which can restrict the size of problems we are able to tackle with current hardware. second, it remains unclear how close it matches the task loss such as the top-k error rate or other non-differentiable evaluation metrics which we aim to optimize ultimately. in this paper, we introduce an alternative classification loss function, the z-loss, which is designed to address these two issues. unlike the log-softmax, it has the desirable property of belonging to the spherical loss family (vincent et al., 2015), a class of loss functions for which training can be performed very efficiently with a complexity independent of the number of output classes. we show experimentally that it significantly outperforms the other spherical loss functions previously investigated. furthermore, we show on a word language modeling task that it also outperforms the log-softmax with respect to certain ranking scores, such as top-k scores, suggesting that the z-loss has the flexibility to better match the task loss. these qualities thus makes the z-loss an appealing candidate to train very efficiently large output networks such as word-language models or other extreme classification problems. on the one billion word (chelba et al., 2014) dataset, we are able to train a model with the z-loss 40 times faster than the log-softmax and more than 4 times faster than the hierarchical softmax.","","2016-04-29","2016-05-27","['alexandre de brébisson', 'pascal vincent']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1412",1604.08943,"minimax optimal convex methods for poisson inverse problems under   $\ell_q$-ball sparsity","math.st stat.th","in this paper, we study the minimax rates and provide an implementable convex algorithm for poisson inverse problems under weak sparsity and physical constraints. in particular we assume the model $y_i \sim \mbox{poisson}(ta_i^{\top}f^*)$ for $1 \leq i \leq n$ where $t \in \mathbb{r}_+$ is the intensity, and we impose weak sparsity on $f^* \in \mathbb{r}^p$ by assuming $f^*$ lies in an $\ell_q$-ball when rotated according to an orthonormal basis $d \in \mathbb{r}^{p \times p}$. in addition, since we are modeling real physical systems we also impose positivity and flux-preserving constraints on the matrix $a = [a_1, a_2,...,a_n]^{\top}$ and the function $f^*$. we prove minimax lower bounds for this model which scale as $r_q (\frac{\log p}{t})^{1 - q/2}$ where it is noticeable that the rate depends on the intensity $t$ and not the sample size $n$. we also show that a $\ell_1$-based regularized least-squares estimator achieves this minimax lower bound, provided a suitable restricted eigenvalue condition is satisfied. finally we prove that provided $n \geq \tilde{k} \log p$ where $\tilde{k} = o(r_q (\frac{\log p}{t})^{- q/2})$ represents an approximate sparsity level, our restricted eigenvalue condition and physical constraints are satisfied for random bounded ensembles. we also provide numerical experiments that validate our mean-squared error bounds. our results address a number of open issues from prior work on poisson inverse problems that focuses on strictly sparse models and does not provide guarantees for convex implementable algorithms.","","2016-04-29","2017-12-17","['yuan li', 'garvesh raskutti']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1413",1605.01421,"siminf: an r package for data-driven stochastic disease spread   simulations","q-bio.pe stat.ap stat.co","we present the r package siminf which provides an efficient and very flexible framework to conduct data-driven epidemiological modeling in realistic large scale disease spread simulations. the framework integrates infection dynamics in subpopulations as continuous-time markov chains using the gillespie stochastic simulation algorithm and incorporates available data such as births, deaths and movements as scheduled events at predefined time-points. using c code for the numerical solvers and openmp to divide work over multiple processors ensures high performance when simulating a sample outcome. one of our design goal was to make siminf extendable and enable usage of the numerical solvers from other r extension packages in order to facilitate complex epidemiological research. in this paper, we provide a technical description of the framework and demonstrate its use on some basic examples. we also discuss how to specify and extend the framework with user-defined models.","","2016-05-04","2018-05-03","['stefan widgren', 'pavol bauer', 'robin eriksson', 'stefan engblom']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE
"1414",1605.01573,"observational-interventional priors for dose-response learning","stat.ml","controlled interventions provide the most direct source of information for learning causal effects. in particular, a dose-response curve can be learned by varying the treatment level and observing the corresponding outcomes. however, interventions can be expensive and time-consuming. observational data, where the treatment is not controlled by a known mechanism, is sometimes available. under some strong assumptions, observational data allows for the estimation of dose-response curves. estimating such curves nonparametrically is hard: sample sizes for controlled interventions may be small, while in the observational case a large number of measured confounders may need to be marginalized. in this paper, we introduce a hierarchical gaussian process prior that constructs a distribution over the dose-response curve by learning from observational data, and reshapes the distribution with a nonparametric affine transform learned from controlled interventions. this function composition from different sources is shown to speed-up learning, which we demonstrate with a thorough sensitivity analysis and an application to modeling the effect of therapy on cognitive skills of premature infants.","","2016-05-05","","['ricardo silva']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1415",1605.01684,"fractional brownian motion, the matern process, and stochastic modeling   of turbulent dispersion","stat.me physics.ao-ph physics.flu-dyn","stochastic process exhibiting power-law slopes in the frequency domain are frequently well modeled by fractional brownian motion (fbm). in particular, the spectral slope at high frequencies is associated with the degree of small-scale roughness or fractal dimension. however, a broad class of real-world signals have a high-frequency slope, like fbm, but a plateau in the vicinity of zero frequency. this low-frequency plateau, it is shown, implies that the temporal integral of the process exhibits diffusive behavior, dispersing from its initial location at a constant rate. such processes are not well modeled by fbm, which has a singularity at zero frequency corresponding to an unbounded rate of dispersion. a more appropriate stochastic model is a much lesser-known random process called the matern process, which is shown herein to be a damped version of fractional brownian motion. this article first provides a thorough introduction to fractional brownian motion, then examines the details of the matern process and its relationship to fbm. an algorithm for the simulation of the matern process in o(n log n) operations is given. unlike fbm, the matern process is found to provide an excellent match to modeling velocities from particle trajectories in an application to two-dimensional fluid turbulence.","10.5194/npg-24-481-2017","2016-05-05","2017-09-02","['j. m. lilly', 'a. m. sykulski', 'j. j early', 's. c. olhede']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1416",1605.02209,"revisiting simpson's paradox: a statistical misspecification perspective","stat.me","the primary objective of this paper is to revisit simpson's paradox using a statistical misspecification perspective. it is argued that the reversal of statistical associations is sometimes spurious, stemming from invalid probabilistic assumptions imposed on the data. the concept of statistical misspecification is used to formalize the vague term `spurious results' as `statistically untrustworthy' inference results. this perspective sheds new light on the paradox by distingusing between statistically trustworthy vs. untrustworthy association reversals. it turns out that in both cases there is nothing counterintuitive to explain or account for. this perspective is also used to revisit the causal `resolution' of the paradox in an attempt to delineate the modeling and inference issues raised by the statistical misspecification perspective. the main arguments are illustrated using both actual and hypothetical data from the literature, including yule's ""nonsense-correlations"" and the berkeley admissions study.","","2016-05-07","2016-05-13","['aris spanos']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1417",1605.02234,"a bayesian group sparse multi-task regression model for imaging genetics","stat.me stat.ap stat.ml","motivation: recent advances in technology for brain imaging and high-throughput genotyping have motivated studies examining the influence of genetic variation on brain structure. wang et al. (bioinformatics, 2012) have developed an approach for the analysis of imaging genomic studies using penalized multi-task regression with regularization based on a novel group $l_{2,1}$-norm penalty which encourages structured sparsity at both the gene level and snp level. while incorporating a number of useful features, the proposed method only furnishes a point estimate of the regression coefficients; techniques for conducting statistical inference are not provided. a new bayesian method is proposed here to overcome this limitation.   results: we develop a bayesian hierarchical modeling formulation where the posterior mode corresponds to the estimator proposed by wang et al. (bioinformatics, 2012), and an approach that allows for full posterior inference including the construction of interval estimates for the regression parameters. we show that the proposed hierarchical model can be expressed as a three-level gaussian scale mixture and this representation facilitates the use of a gibbs sampling algorithm for posterior simulation. simulation studies demonstrate that the interval estimates obtained using our approach achieve adequate coverage probabilities that outperform those obtained from the nonparametric bootstrap. our proposed methodology is applied to the analysis of neuroimaging and genetic data collected as part of the alzheimer's disease neuroimaging initiative (adni), and this analysis of the adni cohort demonstrates clearly the value added of incorporating interval estimation beyond only point estimation when relating snps to brain imaging endophenotypes.","","2016-05-07","2016-10-17","['keelin greenlaw', 'elena szefer', 'jinko graham', 'mary lesperance', 'farouk s. nathoo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE
"1418",1605.02351,"variance component score test for time-course gene set analysis of   longitudinal rna-seq data","stat.ap q-bio.gn stat.me","as gene expression measurement technology is shifting from microarrays to sequencing, the statistical tools available for their analysis must be adapted since rna-seq data are measured as counts. recently, it has been proposed to tackle the count nature of these data by modeling log-count reads per million as continuous variables, using nonparametric regression to account for their inherent heteroscedasticity. adopting such a framework, we propose tcgsaseq, a principled, model-free and efficient top-down method for detecting longitudinal changes in rna-seq gene sets. considering gene sets defined a priori, tcgsaseq identifies those whose expression vary over time, based on an original variance component score test accounting for both covariates and heteroscedasticity without assuming any specific parametric distribution for the transformed counts. we demonstrate that despite the presence of a nonparametric component, our test statistic has a simple form and limiting distribution, and both may be computed quickly. a permutation version of the test is additionally proposed for very small sample sizes. applied to both simulated data and two real datasets, the proposed method is shown to exhibit very good statistical properties, with an increase in stability and power when compared to state of the art methods roast, edger and deseq2, which can fail to control the type i error under certain realistic settings. we have made the method available for the community in the r package tcgsaseq.","","2016-05-08","2017-01-06","['denis agniel', 'boris p hejblum']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1419",1605.02869,"network modeling of short over-dispersed spike-counts: a hierarchical   parametric empirical bayes framework","q-bio.qm q-bio.nc stat.ml","accurate statistical models of neural spike responses can characterize the information carried by neural populations. yet, challenges in recording at the level of individual neurons commonly results in relatively limited samples of spike counts, which can lead to model overfitting. moreover, current models assume spike counts to be poisson-distributed, which ignores the fact that many neurons demonstrate over-dispersed spiking behavior. the negative binomial generalized linear model (nb-glm) provides a powerful tool for modeling over-dispersed spike counts. however, maximum likelihood based standard nb-glm leads to unstable and inaccurate parameter estimations. thus, we propose a hierarchical parametric empirical bayes method for estimating the parameters of the nb-glm. our method integrates generalized linear models (glms) and empirical bayes theory to: (1) effectively capture over-dispersion nature of spike counts from retinal ganglion neural responses; (2) significantly reduce mean square error of parameter estimations when compared to maximum likelihood based method for nb-glms; (3) provide an efficient alternative to fully bayesian inference with low computational cost for hierarchical models; and (4) give insightful findings on both neural interactions and spiking behaviors of real retina cells. we apply our approach to study both simulated data and experimental neural data from the retina. the simulation results indicate the new framework can efficiently and accurately retrieve the weights of functional connections among neural populations and predict mean spike counts. the results from the retinal datasets demonstrate the proposed method outperforms both standard poisson and negative binomial glms in terms of the predictive log-likelihood of held-out data.","","2016-05-10","2018-05-27","['qi she', 'beth jelfs', 'adam s. charles', 'rosa h. m. chan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1420",1605.03,"comparison of cross-validation methods for stochastic block models","stat.me","we introduce a novel cross-validation method that we call latincv and we compare this method to other model selection methods using data generated from a stochastic block model. comparing latincv to other cross-validation methods, we show that latincv performs similarly to a method described in \cite{hoff2008modeling} and that latincv has a significantly larger true model recovery accuracy than the ncv method of \cite{chen2014network}. we also show that the reason for this discrepancy is related to the larger variance of the ncv estimate. comparing latincv to alternative model selection methods, we show that latincv performs better than information criteria aic and bic, as well as the community detection method infomap and a routine that attempts to maximize modularity. the simulation study in this paper includes a range of network sizes and generative parameters for the stochastic block model that allow us to examine the relationship between model selection accuracy and the size and complexity of the model. overall, latincv performs more accurate model selection, and avoids overfitting better than any of the other model selection methods considered.","","2016-05-10","","['beau dabbs', 'brian junker']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1421",1605.03122,"kernel-based structural equation models for topology identification of   directed networks","stat.ml","structural equation models (sems) have been widely adopted for inference of causal interactions in complex networks. recent examples include unveiling topologies of hidden causal networks over which processes such as spreading diseases, or rumors propagate. the appeal of sems in these settings stems from their simplicity and tractability, since they typically assume linear dependencies among observable variables. acknowledging the limitations inherent to adopting linear models, the present paper advocates nonlinear sems, which account for (possible) nonlinear dependencies among network nodes. the advocated approach leverages kernels as a powerful encompassing framework for nonlinear modeling, and an efficient estimator with affordable tradeoffs is put forth. interestingly, pursuit of the novel kernel-based approach yields a convex regularized estimator that promotes edge sparsity, and is amenable to proximal-splitting optimization methods. to this end, solvers with complementary merits are developed by leveraging the alternating direction method of multipliers, and proximal gradient iterations. experiments conducted on simulated data demonstrate that the novel approach outperforms linear sems with respect to edge detection errors. furthermore, tests on a real gene expression dataset unveil interesting new edges that were not revealed by linear sems, which could shed more light on regulatory behavior of human genes.","10.1109/tsp.2017.2664039","2016-05-10","","['yanning shen', 'brian baingana', 'georgios b. giannakis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1422",1605.03306,"high dimensional thresholded regression and shrinkage effect","stat.me stat.ml","high-dimensional sparse modeling via regularization provides a powerful tool for analyzing large-scale data sets and obtaining meaningful, interpretable models. the use of nonconvex penalty functions shows advantage in selecting important features in high dimensions, but the global optimality of such methods still demands more understanding. in this paper, we consider sparse regression with hard-thresholding penalty, which we show to give rise to thresholded regression. this approach is motivated by its close connection with the $l_0$-regularization, which can be unrealistic to implement in practice but of appealing sampling properties, and its computational advantage. under some mild regularity conditions allowing possibly exponentially growing dimensionality, we establish the oracle inequalities of the resulting regularized estimator, as the global minimizer, under various prediction and variable selection losses, as well as the oracle risk inequalities of the hard-thresholded estimator followed by a further $l_2$-regularization. the risk properties exhibit interesting shrinkage effects under both estimation and prediction losses. we identify the optimal choice of the ridge parameter, which is shown to have simultaneous advantages to both the $l_2$-loss and prediction loss. these new results and phenomena are evidenced by simulation and real data examples.","10.1111/rssb.12037","2016-05-11","","['zemin zheng', 'yingying fan', 'jinchi lv']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1423",1605.0331,"asymptotic equivalence of regularization methods in thresholded   parameter space","stat.me math.st stat.ml stat.th","high-dimensional data analysis has motivated a spectrum of regularization methods for variable selection and sparse modeling, with two popular classes of convex ones and concave ones. a long debate has been on whether one class dominates the other, an important question both in theory and to practitioners. in this paper, we characterize the asymptotic equivalence of regularization methods, with general penalty functions, in a thresholded parameter space under the generalized linear model setting, where the dimensionality can grow up to exponentially with the sample size. to assess their performance, we establish the oracle inequalities, as in bickel, ritov and tsybakov (2009), of the global minimizer for these methods under various prediction and variable selection losses. these results reveal an interesting phase transition phenomenon. for polynomially growing dimensionality, the $l_1$-regularization method of lasso and concave methods are asymptotically equivalent, having the same convergence rates in the oracle inequalities. for exponentially growing dimensionality, concave methods are asymptotically equivalent but have faster convergence rates than the lasso. we also establish a stronger property of the oracle risk inequalities of the regularization methods, as well as the sampling properties of computable solutions. our new theoretical results are illustrated and justified by simulation and real data examples.","10.1080/01621459.2013.803972","2016-05-11","","['yingying fan', 'jinchi lv']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE
"1424",1605.03311,"the constrained dantzig selector with enhanced consistency","stat.me stat.ml","the dantzig selector has received popularity for many applications such as compressed sensing and sparse modeling, thanks to its computational efficiency as a linear programming problem and its nice sampling properties. existing results show that it can recover sparse signals mimicking the accuracy of the ideal procedure, up to a logarithmic factor of the dimensionality. such a factor has been shown to hold for many regularization methods. an important question is whether this factor can be reduced to a logarithmic factor of the sample size in ultra-high dimensions under mild regularity conditions. to provide an affirmative answer, in this paper we suggest the constrained dantzig selector, which has more flexible constraints and parameter space. we prove that the suggested method can achieve convergence rates within a logarithmic factor of the sample size of the oracle rates and improved sparsity, under a fairly weak assumption on the signal strength. such improvement is significant in ultra-high dimensions. this method can be implemented efficiently through sequential linear programming. numerical studies confirm that the sample size needed for a certain level of accuracy in these problems can be much reduced.","","2016-05-11","","['yinfei kong', 'zemin zheng', 'jinchi lv']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1425",1605.03313,"innovated scalable efficient estimation in ultra-large gaussian   graphical models","stat.me stat.ml","large-scale precision matrix estimation is of fundamental importance yet challenging in many contemporary applications for recovering gaussian graphical models. in this paper, we suggest a new approach of innovated scalable efficient estimation (isee) for estimating large precision matrix. motivated by the innovated transformation, we convert the original problem into that of large covariance matrix estimation. the suggested method combines the strengths of recent advances in high-dimensional sparse modeling and large covariance matrix estimation. compared to existing approaches, our method is scalable and can deal with much larger precision matrices with simple tuning. under mild regularity conditions, we establish that this procedure can recover the underlying graphical structure with significant probability and provide efficient estimation of link strengths. both computational and theoretical advantages of the procedure are evidenced through simulation and real data examples.","","2016-05-11","","['yingying fan', 'jinchi lv']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1426",1605.03335,"asymptotic properties for combined $l_1$ and concave regularization","stat.me stat.ml","two important goals of high-dimensional modeling are prediction and variable selection. in this article, we consider regularization with combined $l_1$ and concave penalties, and study the sampling properties of the global optimum of the suggested method in ultra-high dimensional settings. the $l_1$-penalty provides the minimum regularization needed for removing noise variables in order to achieve oracle prediction risk, while concave penalty imposes additional regularization to control model sparsity. in the linear model setting, we prove that the global optimum of our method enjoys the same oracle inequalities as the lasso estimator and admits an explicit bound on the false sign rate, which can be asymptotically vanishing. moreover, we establish oracle risk inequalities for the method and the sampling properties of computable solutions. numerical studies suggest that our method yields more stable estimates than using a concave penalty alone.","10.1093/biomet/ast047","2016-05-11","","['yingying fan', 'jinchi lv']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1427",1605.03702,"analytical model for outdoor millimeter wave channels using   geometry-based stochastic approach","stat.ap","the severe bandwidth shortage in conventional microwave bands has spurred the exploration of the millimeter wave (mmw) spectrum for the next revolution in wireless communications. however, there is still lack of proper channel modeling for the mmw wireless propagation, especially in the case of outdoor environments. in this paper, we develop a geometry-based stochastic channel model to statistically characterize the effect of all the first-order reflection paths between the transmitter and receiver. these first-order reflections are generated by the single-bounce of signals reflected from the walls of randomly distributed buildings. based on this geometric model, a closed-form expression for the power delay profile (pdp) contributed by all the first-order reflection paths is obtained and then used to evaluate their impact on the mmw outdoor propagation characteristics. numerical results are provided to validate the accuracy of the proposed model under various channel parameter settings. the findings in this paper provide a promising step towards more complex and practical mmw propagation channel modeling.","10.1109/tvt.2016.2566644","2016-05-12","","['nor aishah muhammad', 'peng wang', 'yonghui li', 'branka vucetic']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1428",1605.03795,"exponential machines","stat.ml cs.lg","modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). in this paper, we introduce exponential machines (exm), a predictor that models all interactions of every order. the key idea is to represent an exponentially large tensor of parameters in a factorized format called tensor train (tt). the tensor train format regularizes the model and lets you control the number of underlying parameters. to train the model, we develop a stochastic riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. we show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset movielens 100k.","","2016-05-12","2017-12-08","['alexander novikov', 'mikhail trofimov', 'ivan oseledets']",1,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1429",1605.04466,"generalized linear models for aggregated data","stat.ml cs.ai cs.lg","databases in domains such as healthcare are routinely released to the public in aggregated form. unfortunately, naive modeling with aggregated data may significantly diminish the accuracy of inferences at the individual level. this paper addresses the scenario where features are provided at the individual level, but the target variables are only available as histogram aggregates or order statistics. we consider a limiting case of generalized linear modeling when the target variables are only known up to permutation, and explore how this relates to permutation testing; a standard technique for assessing statistical dependency. based on this relationship, we propose a simple algorithm to estimate the model parameters and individual level inferences via alternating imputation and standard generalized linear model fitting. our results suggest the effectiveness of the proposed approach when, in the original data, permutation testing accurately ascertains the veracity of the linear relationship. the framework is extended to general histogram data with larger bins - with order statistics such as the median as a limiting case. our experimental results on simulated data and aggregated healthcare data suggest a diminishing returns property with respect to the granularity of the histogram - when a linear relationship holds in the original data, the targets can be predicted accurately given relatively coarse histograms.","","2016-05-14","","['avradeep bhowmik', 'joydeep ghosh', 'oluwasanmi koyejo']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1430",1605.04955,"probing the geometry of data with diffusion fr\'echet functions","stat.ml","many complex ecosystems, such as those formed by multiple microbial taxa, involve intricate interactions amongst various sub-communities. the most basic relationships are frequently modeled as co-occurrence networks in which the nodes represent the various players in the community and the weighted edges encode levels of interaction. in this setting, the composition of a community may be viewed as a probability distribution on the nodes of the network. this paper develops methods for modeling the organization of such data, as well as their euclidean counterparts, across spatial scales. using the notion of diffusion distance, we introduce diffusion frechet functions and diffusion frechet vectors associated with probability distributions on euclidean space and the vertex set of a weighted network, respectively. we prove that these functional statistics are stable with respect to the wasserstein distance between probability measures, thus yielding robust descriptors of their shapes. we apply the methodology to investigate bacterial communities in the human gut, seeking to characterize divergence from intestinal homeostasis in patients with clostridium difficile infection (cdi) and the effects of fecal microbiota transplantation, a treatment used in cdi patients that has proven to be significantly more effective than traditional treatment with antibiotics. the proposed method proves useful in deriving a biomarker that might help elucidate the mechanisms that drive these processes.","","2016-05-16","2017-03-07","['diego hernán díaz martínez', 'christine h. lee', 'peter t. kim', 'washington mio']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1431",1605.05382,"a harris process to model stochastic volatility","stat.ap math.st stat.th","we present a tractable non-independent increment process which provides a high modeling flexibility. the process lies on an extension of the so-called harris chains to continuous time being stationary and feller. we exhibit constructions, properties, and inference methods for the process. afterwards, we use the process to propose a stochastic volatility model with an arbitrary but fixed invariant distribution, which can be tailored to fit different applied scenarios. we study the model performance through simulation while illustrating its use in practice with empirical work. the model proves to be an interesting competitor to a number of short-range stochastic volatility models.","","2016-05-17","","['michelle anzarut', 'ramses h. mena']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE
"1432",1605.05602,"bayesian robust quantile regression","stat.me","traditional bayesian quantile regression relies on the asymmetric laplace distribution (ald) mainly because of its satisfactory empirical and theoretical performances. however, the ald displays medium tails and it is not suitable for data characterized by strong deviations from the gaussian hypothesis. in this paper, we propose an extension of the ald bayesian quantile regression framework to account for fat-tails using the skew exponential power (sep) distribution. beside having the $\tau$-level quantile as parameter, the sep distribution has an additional key parameter governing the decay of the tails, making it attractive for robust modeling of conditional quantiles at different confidence levels. linear and generalized additive models (gam) with penalized spline are considered to show the flexibility of the sep in the bayesian quantile regression context. lasso priors are considered in both cases to account for shrinking parameters problem when the parameters space becomes wide. to implement the bayesian inference we propose a new adaptive metropolis--hastings algorithm in the linear model and an adaptive metropolis within gibbs one in the gam framework. empirical evidence of the statistical properties of the proposed sep bayesian quantile regression method is provided through several example based on simulated and real dataset.","","2016-05-18","","['mauro bernardi', 'marco bottone', 'lea petrella']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1433",1605.05779,"conditional analysis for mixed covariates, with application to feed   intake of lactating sows","stat.ap","we propose a novel modeling framework to study the effect of covariates of various types on the conditional distribution of the response. the methodology accommodates flexible model structure, allows for joint estimation of the quantiles at all levels, and involves a computationally efficient estimation algorithm. extensive numerical investigation confirms good performance of the proposed method. the methodology is motivated by and applied to a lactating sow study, where the primary interest is to understand how the dynamic change of minute-by-minute temperature in the farrowing rooms within a day (functional covariate) is associated with low quantiles of feed intake of lactating sows, while accounting for other sow-specific information (vector covariate).","","2016-05-18","2019-05-30","['so young park', 'cai li', 'santa-maria mendoza', 'eric van heugten', 'ana-maria staicu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1434",1605.0642,"quantifying the accuracy of approximate diffusions and markov chains","math.st math.pr stat.co stat.ml stat.th","markov chains and diffusion processes are indispensable tools in machine learning and statistics that are used for inference, sampling, and modeling. with the growth of large-scale datasets, the computational cost associated with simulating these stochastic processes can be considerable, and many algorithms have been proposed to approximate the underlying markov chain or diffusion. a fundamental question is how the computational savings trade off against the statistical error incurred due to approximations. this paper develops general results that address this question. we bound the wasserstein distance between the equilibrium distributions of two diffusions as a function of their mixing rates and the deviation in their drifts. we show that this error bound is tight in simple gaussian settings. our general result on continuous diffusions can be discretized to provide insights into the computational-statistical trade-off of markov chains. as an illustration, we apply our framework to derive finite-sample error bounds of approximate unadjusted langevin dynamics. we characterize computation-constrained settings where, by using fast-to-compute approximate gradients in the langevin dynamics, we obtain more accurate samples compared to using the exact gradients. finally, as an additional application of our approach, we quantify the accuracy of approximate zig-zag sampling. our theoretical analyses are supported by simulation experiments.","","2016-05-20","2017-08-30","['jonathan h. huggins', 'james zou']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE
"1435",1605.06838,"causality on longitudinal data: stable specification search in   constrained structural equation modeling","stat.ml cs.ai","a typical problem in causal modeling is the instability of model structure learning, i.e., small changes in finite data can result in completely different optimal models. the present work introduces a novel causal modeling algorithm for longitudinal data, that is robust for finite samples based on recent advances in stability selection using subsampling and selection algorithms. our approach uses exploratory search but allows incorporation of prior knowledge, e.g., the absence of a particular causal relationship between two specific variables. we represent causal relationships using structural equation models. models are scored along two objectives: the model fit and the model complexity. since both objectives are often conflicting we apply a multi-objective evolutionary algorithm to search for pareto optimal models. to handle the instability of small finite data samples, we repeatedly subsample the data and select those substructures (from the optimal models) that are both stable and parsimonious. these substructures can be visualized through a causal graph. our more exploratory approach achieves at least comparable performance as, but often a significant improvement over state-of-the-art alternative approaches on a simulated data set with a known ground truth. we also present the results of our method on three real-world longitudinal data sets on chronic fatigue syndrome, alzheimer disease, and chronic kidney disease. the findings obtained with our approach are generally in line with results from more hypothesis-driven analyses in earlier studies and suggest some novel relationships that deserve further research.","","2016-05-22","2017-04-04","['ridho rahmadi', 'perry groot', 'marieke hc van rijn', 'jan ajg van den brand', 'marianne heins', 'hans knoop', 'tom heskes']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1436",1605.06866,"regularizing random points: complementary mat\'ern hard-core point   process","math.st stat.th","in this paper we present a tractable approach for regularizing randomly placed points, by splitting them into two subsets: the first is generated by means of the mat\'ern hard-core point process, while the remaining points constitute the complementary mat\'ern hard-core point process. we study the characteristics of these processes, deriving its pair-correlation functions, and the distribution of the distance to the nearest neighbour. the results have several applications in wireless communications, including the modeling of wireless sensor networks, where we investigate an example of regularizing such networks and illustrate its advantage in reducing the energy consumption of wireless nodes.","","2016-05-22","2016-05-25","['akram al-hourani', 'bill moran', 'sithamparanathan kandeepan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1437",1605.06886,"stochastic patching process","cs.ai stat.ml","stochastic partition models tailor a product space into a number of rectangular regions such that the data within each region exhibit certain types of homogeneity. due to constraints of partition strategy, existing models may cause unnecessary dissections in sparse regions when fitting data in dense regions. to alleviate this limitation, we propose a parsimonious partition model, named stochastic patching process (spp), to deal with multi-dimensional arrays. spp adopts an ""enclosing"" strategy to attach rectangular patches to dense regions. spp is self-consistent such that it can be extended to infinite arrays. we apply spp to relational modeling and the experimental results validate its merit compared to the state-of-the-arts.","","2016-05-22","2017-02-26","['xuhui fan', 'bin li', 'yi wang', 'yang wang', 'fang chen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1438",1605.07057,"bayesian model selection of stochastic block models","stat.ml cs.lg cs.si","a central problem in analyzing networks is partitioning them into modules or communities. one of the best tools for this is the stochastic block model, which clusters vertices into blocks with statistically homogeneous pattern of links. despite its flexibility and popularity, there has been a lack of principled statistical model selection criteria for the stochastic block model. here we propose a bayesian framework for choosing the number of blocks as well as comparing it to the more elaborate degree- corrected block models, ultimately leading to a universal model selection framework capable of comparing multiple modeling combinations. we will also investigate its connection to the minimum description length principle.","","2016-05-23","","['xiaoran yan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1439",1605.07072,"generalized stability approach for regularized graphical models","stat.me q-bio.mn stat.ap stat.co","selecting regularization parameters in penalized high-dimensional graphical models in a principled, data-driven, and computationally efficient manner continues to be one of the key challenges in high-dimensional statistics. we present substantial computational gains and conceptual generalizations of the stability approach to regularization selection (stars), a state-of-the-art graphical model selection scheme. using properties of the poisson-binomial distribution and convex non-asymptotic distributional modeling we propose lower and upper bounds on the stars graph regularization path which results in greatly reduced computational cost without compromising regularization selection. we also generalize the stars criterion from single edge to induced subgraph (graphlet) stability. we show that simultaneously requiring edge and graphlet stability leads to superior graph recovery performance independent of graph topology. these novel insights render gaussian graphical model selection a routine task on standard multi-core computers.","","2016-05-23","","['christian l. müller', 'richard bonneau', 'zachary kurtz']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"1440",1605.07127,"learning and policy search in stochastic dynamical systems with bayesian   neural networks","stat.ml cs.lg","we present an algorithm for model-based reinforcement learning that combines bayesian neural networks (bnns) with random roll-outs and stochastic optimization for policy learning. the bnns are trained by minimizing $\alpha$-divergences, allowing us to capture complicated statistical patterns in the transition dynamics, e.g. multi-modality and heteroskedasticity, which are usually missed by other common modeling approaches. we illustrate the performance of our method by solving a challenging benchmark where model-based approaches usually fail and by obtaining promising results in a real-world scenario for controlling a gas turbine.","","2016-05-23","2017-03-07","['stefan depeweg', 'josé miguel hernández-lobato', 'finale doshi-velez', 'steffen udluft']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1441",1605.07174,"kernel-based reconstruction of graph signals","stat.ml cs.lg","a number of applications in engineering, social sciences, physics, and biology involve inference over networks. in this context, graph signals are widely encountered as descriptors of vertex attributes or features in graph-structured data. estimating such signals in all vertices given noisy observations of their values on a subset of vertices has been extensively analyzed in the literature of signal processing on graphs (spog). this paper advocates kernel regression as a framework generalizing popular spog modeling and reconstruction and expanding their capabilities. formulating signal reconstruction as a regression task on reproducing kernel hilbert spaces of graph signals permeates benefits from statistical learning, offers fresh insights, and allows for estimators to leverage richer forms of prior information than existing alternatives. a number of spog notions such as bandlimitedness, graph filters, and the graph fourier transform are naturally accommodated in the kernel framework. additionally, this paper capitalizes on the so-called representer theorem to devise simpler versions of existing thikhonov regularized estimators, and offers a novel probabilistic interpretation of kernel methods on graphs based on graphical models. motivated by the challenges of selecting the bandwidth parameter in spog estimators or the kernel map in kernel-based methods, the present paper further proposes two multi-kernel approaches with complementary strengths. whereas the first enables estimation of the unknown bandwidth of bandlimited signals, the second allows for efficient graph filter selection. numerical tests with synthetic as well as real data demonstrate the merits of the proposed methods relative to state-of-the-art alternatives.","10.1109/tsp.2016.2620116","2016-05-23","","['daniel romero', 'meng ma', 'georgios b. giannakis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1442",1605.07422,"computing web-scale topic models using an asynchronous parameter server","cs.dc cs.ir cs.lg stat.ml","topic models such as latent dirichlet allocation (lda) have been widely used in information retrieval for tasks ranging from smoothing and feedback methods to tools for exploratory search and discovery. however, classical methods for inferring topic models do not scale up to the massive size of today's publicly available web-scale data sets. the state-of-the-art approaches rely on custom strategies, implementations and hardware to facilitate their asynchronous, communication-intensive workloads.   we present aps-lda, which integrates state-of-the-art topic modeling with cluster computing frameworks such as spark using a novel asynchronous parameter server. advantages of this integration include convenient usage of existing data processing pipelines and eliminating the need for disk writes as data can be kept in memory from start to finish. our goal is not to outperform highly customized implementations, but to propose a general high-performance topic modeling framework that can easily be used in today's data processing pipelines. we compare aps-lda to the existing spark lda implementations and show that our system can, on a 480-core cluster, process up to 135 times more data and 10 times more topics without sacrificing model quality.","10.1145/3077136.3084135","2016-05-24","2017-06-18","['rolf jagerman', 'carsten eickhoff', 'maarten de rijke']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1443",1605.07521,"a bivariate copula additive model for location, scale and shape","stat.me","rigby & stasinopoulos (2005) introduced generalized additive models for location, scale and shape (gamlss) where the response distribution is not restricted to belong to the exponential family and its parameters can be specified as functions of additive predictors that allows for several types of covariate effects (e.g., linear, non-linear, random and spatial effects). in many empirical situations, however, modeling simultaneously two or more responses conditional on some covariates can be of considerable relevance. in this article, we extend the scope of gamlss by introducing a bivariate copula additive model with continuous margins for location, scale and shape. the framework permits the copula dependence and marginal distribution parameters to be estimated simultaneously and, like in gamlss, each parameter to be modeled using an additive predictor. parameter estimation is achieved within a penalized likelihood framework using a trust region algorithm with integrated automatic multiple smoothing parameter selection. the proposed approach allows for straightforward inclusion of potentially any parametric continuous marginal distribution and copula function. the models can be easily used via the copulareg() function in the r package semiparbivprobit. the usefulness of the proposal is illustrated on two case studies (which use electricity price and demand data, and birth records) and on simulated data.","","2016-05-24","","['giampiero marra', 'rosalba radice']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1444",1605.07571,"sequential neural models with stochastic layers","stat.ml cs.lg","how can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? this paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. the clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. by retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the blizzard and timit speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.","","2016-05-24","2016-11-13","['marco fraccaro', 'søren kaae sønderby', 'ulrich paquet', 'ole winther']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1445",1605.07604,"posterior dispersion indices","stat.ml cs.ai stat.co","probabilistic modeling is cyclical: we specify a model, infer its posterior, and evaluate its performance. evaluation drives the cycle, as we revise our model based on how it performs. this requires a metric. traditionally, predictive accuracy prevails. yet, predictive accuracy does not tell the whole story. we propose to evaluate a model through posterior dispersion. the idea is to analyze how each datapoint fares in relation to posterior uncertainty around the hidden structure. we propose a family of posterior dispersion indices (pdi) that capture this idea. a pdi identifies rich patterns of model mismatch in three real data examples: voting preferences, supermarket shopping, and population genetics.","","2016-05-24","","['alp kucukelbir', 'david m. blei']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE
"1446",1605.07663,"estimating the malaria attributable fever fraction accounting for   parasites being killed by fever and measurement error","stat.ap stat.me","malaria is a parasitic disease that is a major health problem in many tropical regions. the most characteristic symptom of malaria is fever. the fraction of fevers that are attributable to malaria, the malaria attributable fever fraction (maff), is an important public health measure for assessing the effect of malaria control programs and other purposes. estimating the maff is not straightforward because there is no gold standard diagnosis of a malaria attributable fever; an individual can have malaria parasites in her blood and a fever, but the individual may have developed partial immunity that allows her to tolerate the parasites and the fever is being caused by another infection. we define the maff using the potential outcome framework for causal inference and show what assumptions underlie current estimation methods. current estimation methods rely on an assumption that the parasite density is correctly measured. however, this assumption does not generally hold because (i) fever kills some parasites and (ii) the measurement of parasite density has measurement error. in the presence of these problems, we show current estimation methods do not perform well. we propose a novel maximum likelihood estimation method based on exponential family g-modeling. under the assumption that the measurement error mechanism and the magnitude of the fever killing effect are known, we show that our proposed method provides approximately unbiased estimates of the maff in simulation studies. a sensitivity analysis can be used to assess the impact of different magnitudes of fever killing and different measurement error mechanisms. we apply our proposed method to estimate the maff in kilombero, tanzania.","","2016-05-24","","['kwonsang lee', 'dylan s. small']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1447",1605.07717,"deep structured energy based models for anomaly detection","cs.lg stat.ml","in this paper, we attack the anomaly detection problem by directly modeling the data distribution with deep architectures. we propose deep structured energy based models (dsebms), where the energy function is the output of a deterministic deep neural network with structure. we develop novel model architectures to integrate ebms with different types of data such as static data, sequential data, and spatial data, and apply appropriate model architectures to adapt to the data structure. our training algorithm is built upon the recent development of score matching \cite{sm}, which connects an ebm with a regularized autoencoder, eliminating the need for complicated sampling method. statistically sound decision criterion can be derived for anomaly detection purpose from the perspective of the energy landscape of the data distribution. we investigate two decision criteria for performing anomaly detection: the energy score and the reconstruction error. extensive empirical studies on benchmark tasks demonstrate that our proposed model consistently matches or outperforms all the competing methods.","","2016-05-24","2016-06-15","['shuangfei zhai', 'yu cheng', 'weining lu', 'zhongfei zhang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1448",1605.07919,"compression and conditional emulation of climate model output","stat.me","numerical climate model simulations run at high spatial and temporal resolutions generate massive quantities of data. as our computing capabilities continue to increase, storing all of the data is not sustainable, and thus it is important to develop methods for representing the full datasets by smaller compressed versions. we propose a statistical compression and decompression algorithm based on storing a set of summary statistics as well as a statistical model describing the conditional distribution of the full dataset given the summary statistics. the statistical model can be used to generate realizations representing the full dataset, along with characterizations of the uncertainties in the generated data. thus, the methods are capable of both compression and conditional emulation of the climate models. considerable attention is paid to accurately modeling the original dataset--one year of daily mean temperature data--particularly with regard to the inherent spatial nonstationarity in global fields, and to determining the statistics to be stored, so that the variation in the original data can be closely captured, while allowing for fast decompression and conditional emulation on modest computers.","10.1080/01621459.2017.1395339","2016-05-25","2018-02-19","['joseph guinness', 'dorit hammerling']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1449",1605.0795,"on fast convergence of proximal algorithms for sqrt-lasso optimization:   don't worry about its nonsmooth loss function","cs.lg math.oc stat.ml","many machine learning techniques sacrifice convenient computational structures to gain estimation robustness and modeling flexibility. however, by exploring the modeling structures, we find these ""sacrifices"" do not always require more computational efforts. to shed light on such a ""free-lunch"" phenomenon, we study the square-root-lasso (sqrt-lasso) type regression problem. specifically, we show that the nonsmooth loss functions of sqrt-lasso type regression ease tuning effort and gain adaptivity to inhomogeneous noise, but is not necessarily more challenging than lasso in computation. we can directly apply proximal algorithms (e.g. proximal gradient descent, proximal newton, and proximal quasi-newton algorithms) without worrying the nonsmoothness of the loss function. theoretically, we prove that the proximal algorithms combined with the pathwise optimization scheme enjoy fast convergence guarantees with high probability. numerical results are provided to support our theory.","","2016-05-25","2019-04-13","['xingguo li', 'haoming jiang', 'jarvis haupt', 'raman arora', 'han liu', 'mingyi hong', 'tuo zhao']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1450",1605.07999,"toward a general, scaleable framework for bayesian teaching with   applications to topic models","cs.lg cs.ai stat.ml","machines, not humans, are the world's dominant knowledge accumulators but humans remain the dominant decision makers. interpreting and disseminating the knowledge accumulated by machines requires expertise, time, and is prone to failure. the problem of how best to convey accumulated knowledge from computers to humans is a critical bottleneck in the broader application of machine learning. we propose an approach based on human teaching where the problem is formalized as selecting a small subset of the data that will, with high probability, lead the human user to the correct inference. this approach, though successful for modeling human learning in simple laboratory experiments, has failed to achieve broader relevance due to challenges in formulating general and scalable algorithms. we propose general-purpose teaching via pseudo-marginal sampling and demonstrate the algorithm by teaching topic models. simulation results show our sampling-based approach: effectively approximates the probability where ground-truth is possible via enumeration, results in data that are markedly different from those expected by random sampling, and speeds learning especially for small amounts of data. application to movie synopsis data illustrates differences between teaching and random sampling for teaching distributions and specific topics, and demonstrates gains in scalability and applicability to real-world problems.","","2016-05-25","","['baxter s. eaves', 'patrick shafto']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1451",1605.08454,"linear dynamical neural population models through nonlinear embeddings","q-bio.nc stat.ml","a body of recent work in modeling neural activity focuses on recovering low-dimensional latent features that capture the statistical structure of large-scale neural populations. most such approaches have focused on linear generative models, where inference is computationally tractable. here, we propose flds, a general class of nonlinear generative models that permits the firing rate of each neuron to vary as an arbitrary smooth function of a latent, linear dynamical state. this extra flexibility allows the model to capture a richer set of neural variability than a purely linear model, but retains an easily visualizable low-dimensional latent space. to fit this class of non-conjugate models we propose a variational inference scheme, along with a novel approximate posterior capable of capturing rich temporal correlations across time. we show that our techniques permit inference in a wide class of generative models.we also show in application to two neural datasets that, compared to state-of-the-art neural population models, flds captures a much larger proportion of neural variability with a small number of latent dimensions, providing superior predictive performance and interpretability.","","2016-05-26","2016-10-25","['yuanjun gao', 'evan archer', 'liam paninski', 'john p. cunningham']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1452",1605.08455,"suppressing background radiation using poisson principal component   analysis","cs.lg physics.data-an stat.ml","performance of nuclear threat detection systems based on gamma-ray spectrometry often strongly depends on the ability to identify the part of measured signal that can be attributed to background radiation. we have successfully applied a method based on principal component analysis (pca) to obtain a compact null-space model of background spectra using pca projection residuals to derive a source detection score. we have shown the method's utility in a threat detection system using mobile spectrometers in urban scenes (tandon et al 2012). while it is commonly assumed that measured photon counts follow a poisson process, standard pca makes a gaussian assumption about the data distribution, which may be a poor approximation when photon counts are low. this paper studies whether and in what conditions pca with a poisson-based loss function (poisson pca) can outperform standard gaussian pca in modeling background radiation to enable more sensitive and specific nuclear threat detection.","","2016-05-26","","['p. tandon', 'p. huggins', 'a. dubrawski', 's. labov', 'k. nelson']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1453",1605.08799,"estimation of interpretable eqtl effect sizes using a log of linear   model","stat.me","the study of expression quantitative trait loci (eqtl) is an important problem in genomics and biomedicine. while detection (testing) of eqtl associations has been widely studied, less work has been devoted to the estimation of eqtl effect size. to reduce false positives, detection methods frequently rely on linear modeling of rank-based normalized or log-transformed gene expression data. unfortunately, these approaches do not correspond to the simplest model of eqtl action, and thus yield estimates of eqtl association that can be uninterpretable and inaccurate. in this paper we propose a new, log-of-linear model for eqtl action, termed acme, that captures allelic contributions to cis-acting eqtls in an additive fashion, yielding effect size estimates that correspond to a biologically coherent model of cis-eqtls. we describe a non-linear least-squares algorithm to fit the model by maximum likelihood, and obtain corresponding $p$-values. we perform careful investigation of the model using a combination of simulated data and data from the genotype tissue expression (gtex) project. our results reveal little evidence for dominance effects, a parsimonious result that accords with a simple biological model for allele-specific expression and supports use of the acme model. we show that type-i error is well-controlled under our approach in a realistic setting, so that rank-based normalizations are unnecessary. furthermore, we show that such normalizations can be detrimental to power and estimation accuracy under the proposed model. we then provide summaries of acme effect sizes for whole-genome cis-eqtls in the gtex data.","","2016-05-27","2017-09-07","['john palowitch', 'andrey shabalin', 'yihui zhou', 'andrew b. nobel', 'fred a. wright']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1454",1605.08978,"quantile-based optimization under uncertainties using adaptive kriging   surrogate models","stat.co","uncertainties are inherent to real-world systems. taking them into account is crucial in industrial design problems and this might be achieved through reliability-based design optimization (rbdo) techniques. in this paper, we propose a quantile-based approach to solve rbdo problems. we first transform the safety constraints usually formulated as admissible probabilities of failure into constraints on quantiles of the performance criteria. in this formulation, the quantile level controls the degree of conservatism of the design. starting with the premise that industrial applications often involve high-fidelity and time-consuming computational models, the proposed approach makes use of kriging surrogate models (a.k.a. gaussian process modeling). thanks to the kriging variance (a measure of the local accuracy of the surrogate), we derive a procedure with two stages of enrichment of the design of computer experiments (doe) used to construct the surrogate model. the first stage globally reduces the kriging epistemic uncertainty and adds points in the vicinity of the limit-state surfaces describing the system performance to be attained. the second stage locally checks, and if necessary, improves the accuracy of the quantiles estimated along the optimization iterations. applications to three analytical examples and to the optimal design of a car body subsystem (minimal mass under mechanical safety constraints) show the accuracy and the remarkable efficiency brought by the proposed procedure.","","2016-05-29","","['m. moustapha', 'b. sudret', 'j. -m. bourinet', 'b. guillaume']",1,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1455",1605.09288,"generalized network psychometrics: combining network and latent variable   models","math.st stat.me stat.th","we introduce the network model as a formal psychometric model, conceptualizing the covariance between psychometric indicators as resulting from pairwise interactions between observable variables in a network structure. this contrasts with standard psychometric models, in which the covariance between test items arises from the influence of one or more common latent variables. here, we present two generalizations of the network model that encompass latent variable structures, establishing network modeling as parts of the more general framework of structural equation modeling (sem). in the first generalization, we model the covariance structure of latent variables as a network. we term this framework latent network modeling (lnm) and show that, with lnm, a unique structure of conditional independence relationships between latent variables can be obtained in an explorative manner. in the second generalization, the residual variance-covariance structure of indicators is modeled as a network. we term this generalization residual network modeling (rnm) and show that, within this framework, identifiable models can be obtained in which local independence is structurally violated. these generalizations allow for a general modeling framework that can be used to fit, and compare, sem models, network models, and the rnm and lnm generalizations. this methodology has been implemented in the free-to-use software package lvnet, which contains confirmatory model testing as well as two exploratory search algorithms: stepwise search algorithms for low-dimensional datasets and penalized maximum likelihood estimation for larger datasets. we show in simulation studies that these search algorithms performs adequately in identifying the structure of the relevant residual or latent networks. we further demonstrate the utility of these generalizations in an empirical example on a personality inventory dataset.","10.1007/s11336-017-9557-x","2016-05-30","2017-09-11","['sacha epskamp', 'mijke rhemtulla', 'denny borsboom']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1456",1605.09459,"scalable and flexible multiview max-var canonical correlation analysis","stat.ml","generalized canonical correlation analysis (gcca) aims at finding latent low-dimensional common structure from multiple views (feature vectors in different domains) of the same entities. unlike principal component analysis (pca) that handles a single view, (g)cca is able to integrate information from different feature spaces. here we focus on max-var gcca, a popular formulation which has recently gained renewed interest in multilingual processing and speech modeling. the classic max-var gcca problem can be solved optimally via eigen-decomposition of a matrix that compounds the (whitened) correlation matrices of the views; but this solution has serious scalability issues, and is not directly amenable to incorporating pertinent structural constraints such as non-negativity and sparsity on the canonical components. we posit regularized max-var gcca as a non-convex optimization problem and propose an alternating optimization (ao)-based algorithm to handle it. our algorithm alternates between {\em inexact} solutions of a regularized least squares subproblem and a manifold-constrained non-convex subproblem, thereby achieving substantial memory and computational savings. an important benefit of our design is that it can easily handle structure-promoting regularization. we show that the algorithm globally converges to a critical point at a sublinear rate, and approaches a global optimal solution at a linear rate when no regularization is considered. judiciously designed simulations and large-scale word embedding tasks are employed to showcase the effectiveness of the proposed algorithm.","10.1109/tsp.2017.2698365","2016-05-30","2017-05-04","['xiao fu', 'kejun huang', 'mingyi hong', 'nicholas d. sidiropoulos', 'anthony man-cho so']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1457",1605.09511,"the natural selection of bad science","physics.soc-ph stat.ap","poor research design and data analysis encourage false-positive findings. such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. the persistence of poor methods results partly from incentives that favor them, leading to the natural selection of bad science. this dynamic requires no conscious strategizing---no deliberate cheating nor loafing---by scientists, only that publication is a principle factor for career advancement. some normative methods of analysis have almost certainly been selected to further publication instead of discovery. in order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. we support this argument with empirical evidence and computational modeling. we first present a 60-year meta-analysis of statistical power in the behavioral sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. to demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. as in the real world, successful labs produce more ""progeny"", such that their methods are more often copied and their students are more likely to start labs of their own. selection for high output leads to poorer methods and increasingly high false discovery rates. we additionally show that replication slows but does not stop the process of methodological deterioration. improving the quality of research requires change at the institutional level.","10.1098/rsos.160384","2016-05-31","","['paul e. smaldino', 'richard mcelreath']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1458",1605.09522,"kernel mean embedding of distributions: a review and beyond","stat.ml cs.lg","a hilbert space embedding of a distribution---in short, a kernel mean embedding---has recently emerged as a powerful tool for machine learning and inference. the basic idea behind this framework is to map distributions into a reproducing kernel hilbert space (rkhs) in which the whole arsenal of kernel methods can be extended to probability measures. it can be viewed as a generalization of the original ""feature map"" common to support vector machines (svms) and other kernel methods. while initially closely associated with the latter, it has meanwhile found application in fields ranging from kernel machines and probabilistic modeling to statistical inference, causal discovery, and deep learning. the goal of this survey is to give a comprehensive review of existing work and recent advances in this research area, and to discuss the most challenging issues and open problems that could lead to new research directions. the survey begins with a brief introduction to the rkhs and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the hilbert space embedding of marginal distributions, theoretical guarantees, and a review of its applications. the embedding of distributions enables us to apply rkhs methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, and learning on distributional data. next, we discuss the hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. the conditional mean embedding enables us to perform sum, product, and bayes' rules---which are ubiquitous in graphical model, probabilistic inference, and reinforcement learning---in a non-parametric way. we then discuss relationships between this framework and other related areas. lastly, we give some suggestions on future research directions.","10.1561/2200000060","2016-05-31","2017-01-25","['krikamol muandet', 'kenji fukumizu', 'bharath sriperumbudur', 'bernhard schölkopf']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1459",1606.00068,"quantifying the probable approximation error of probabilistic inference   programs","cs.ai cs.lg stat.ml","this paper introduces a new technique for quantifying the approximation error of a broad class of probabilistic inference programs, including ones based on both variational and monte carlo approaches. the key idea is to derive a subjective bound on the symmetrized kl divergence between the distribution achieved by an approximate inference program and its true target distribution. the bound's validity (and subjectivity) rests on the accuracy of two auxiliary probabilistic programs: (i) a ""reference"" inference program that defines a gold standard of accuracy and (ii) a ""meta-inference"" program that answers the question ""what internal random choices did the original approximate inference program probably make given that it produced a particular result?"" the paper includes empirical results on inference problems drawn from linear regression, dirichlet process mixture modeling, hmms, and bayesian networks. the experiments show that the technique is robust to the quality of the reference inference program and that it can detect implementation bugs that are not apparent from predictive performance.","","2016-05-31","","['marco f cusumano-towner', 'vikash k mansinghka']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1460",1606.00235,"clustering with phylogenetic tools in astrophysics","astro-ph.im math.st q-bio.qm stat.ml stat.th","phylogenetic approaches are finding more and more applications outside the field of biology. astrophysics is no exception since an overwhelming amount of multivariate data has appeared in the last twenty years or so. in particular, the diversification of galaxies throughout the evolution of the universe quite naturally invokes phylogenetic approaches. we have demonstrated that maximum parsimony brings useful astrophysical results, and we now proceed toward the analyses of large datasets for galaxies. in this talk i present how we solve the major difficulties for this goal: the choice of the parameters, their discretization, and the analysis of a high number of objects with an unsupervised np-hard classification technique like cladistics. 1. introduction how do the galaxy form, and when? how did the galaxy evolve and transform themselves to create the diversity we observe? what are the progenitors to present-day galaxies? to answer these big questions, observations throughout the universe and the physical modelisation are obvious tools. but between these, there is a key process, without which it would be impossible to extract some digestible information from the complexity of these systems. this is classification. one century ago, galaxies were discovered by hubble. from images obtained in the visible range of wavelengths, he synthetised his observations through the usual process: classification. with only one parameter (the shape) that is qualitative and determined with the eye, he found four categories: ellipticals, spirals, barred spirals and irregulars. this is the famous hubble classification. he later hypothetized relationships between these classes, building the hubble tuning fork. the hubble classification has been refined, notably by de vaucouleurs, and is still used as the only global classification of galaxies. even though the physical relationships proposed by hubble are not retained any more, the hubble tuning fork is nearly always used to represent the classification of the galaxy diversity under its new name the hubble sequence (e.g. delgado-serrano, 2012). its success is impressive and can be understood by its simplicity, even its beauty, and by the many correlations found between the morphology of galaxies and their other properties. and one must admit that there is no alternative up to now, even though both the hubble classification and diagram have been recognised to be unsatisfactory. among the most obvious flaws of this classification, one must mention its monovariate, qualitative, subjective and old-fashioned nature, as well as the difficulty to characterise the morphology of distant galaxies. the first two most significant multivariate studies were by watanabe et al. (1985) and whitmore (1984). since the year 2005, the number of studies attempting to go beyond the hubble classification has increased largely. why, despite of this, the hubble classification and its sequence are still alive and no alternative have yet emerged (sandage, 2005)? my feeling is that the results of the multivariate analyses are not easily integrated into a one-century old practice of modeling the observations. in addition, extragalactic objects like galaxies, stellar clusters or stars do evolve. astronomy now provides data on very distant objects, raising the question of the relationships between those and our present day nearby galaxies. clearly, this is a phylogenetic problem. astrocladistics 1 aims at exploring the use of phylogenetic tools in astrophysics (fraix-burnet et al., 2006a,b). we have proved that maximum parsimony (or cladistics) can be applied in astrophysics and provides a new exploration tool of the data (fraix-burnet et al., 2009, 2012, cardone \& fraix-burnet, 2013). as far as the classification of galaxies is concerned, a larger number of objects must now be analysed. in this paper, i","","2016-06-01","","['didier fraix-burnet']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"1461",1606.00242,"ctsmr - continuous time stochastic modeling in r","stat.co","ctsmr is an r package providing a general framework for identifying and estimating partially observed continuous-discrete time gray-box models. the estimation is based on maximum likelihood principles and kalman filtering efficiently implemented in fortran. this paper briefly demonstrates how to construct a continuous time stochastic model using multivariate time series data, and how to estimate the embedded parameters. the setup provides a unique framework for statistical modeling of physical phenomena, and the approach is often called grey box modeling. finally three examples are provided to demonstrate the capabilities of ctsmr.","","2016-06-01","","['rune juhl', 'jan kloppenborg møller', 'henrik madsen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1462",1606.00411,"temporal topic modeling to assess associations between news trends and   infectious disease outbreaks","cs.si cs.cl cs.ir stat.ml","in retrospective assessments, internet news reports have been shown to capture early reports of unknown infectious disease transmission prior to official laboratory confirmation. in general, media interest and reporting peaks and wanes during the course of an outbreak. in this study, we quantify the extent to which media interest during infectious disease outbreaks is indicative of trends of reported incidence. we introduce an approach that uses supervised temporal topic models to transform large corpora of news articles into temporal topic trends. the key advantages of this approach include, applicability to a wide range of diseases, and ability to capture disease dynamics - including seasonality, abrupt peaks and troughs. we evaluated the method using data from multiple infectious disease outbreaks reported in the united states of america (u.s.), china and india. we noted that temporal topic trends extracted from disease-related news reports successfully captured the dynamics of multiple outbreaks such as whooping cough in u.s. (2012), dengue outbreaks in india (2013) and china (2014). our observations also suggest that efficient modeling of temporal topic trends using time-series regression techniques can estimate disease case counts with increased precision before official reports by health organizations.","","2016-06-01","","['saurav ghosh', 'prithwish chakraborty', 'elaine o. nsoesie', 'emily cohn', 'sumiko r. mekaru', 'john s. brownstein', 'naren ramakrishnan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1463",1606.00546,"forecasting wind power - modeling periodic and non-linear effects under   conditional heteroscedasticity","stat.ap stat.co stat.ml stat.ot","in this article we present an approach that enables joint wind speed and wind power forecasts for a wind park. we combine a multivariate seasonal time varying threshold autoregressive moving average (tvarma) model with a power threshold generalized autoregressive conditional heteroscedastic (power-tgarch) model. the modeling framework incorporates diurnal and annual periodicity modeling by periodic b-splines, conditional heteroscedasticity and a complex autoregressive structure with non-linear impacts. in contrast to usually time-consuming estimation approaches as likelihood estimation, we apply a high-dimensional shrinkage technique. we utilize an iteratively re-weighted least absolute shrinkage and selection operator (lasso) technique. it allows for conditional heteroscedasticity, provides fast computing times and guarantees a parsimonious and regularized specification, even though the parameter space may be vast. we are able to show that our approach provides accurate forecasts of wind power at a turbine-specific level for forecasting horizons of up to 48 h (short- to medium-term forecasts).","10.1016/j.apenergy.2016.05.111","2016-06-02","","['florian ziel', 'carsten croonenbroeck', 'daniel ambach']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE
"1464",1606.00583,"$c_p$ criterion for semiparametric approach in causal inference","stat.me","for marginal structural models, which recently play an important role in causal inference, we consider a model selection problem in the framework of a semiparametric approach using inverse-probability-weighted estimation or doubly robust estimation. in this framework, the modeling target is a potential outcome which may be a missing value, and so we cannot apply the aic nor its extended version to this problem. in other words, there is no analytical information criterion obtained according to its classical derivation for this problem. hence, we define a mean squared error appropriate for treating the potential outcome, and then we derive its asymptotic unbiased estimator as a $c_{p}$ criterion from an asymptotics for the semiparametric approach and using an ignorable treatment assignment condition. in simulation study, it is shown that the proposed criterion exceeds a conventionally derived existing criterion in the squared error and model selection frequency. specifically, in all simulation settings, the proposed criterion provides clearly smaller squared errors and higher frequencies selecting the true or nearly true model. moreover, in real data analysis, we check that there is a clear difference between the selections by the two criteria.","","2016-06-02","","['takamichi baba', 'yoshiyuki ninomiya']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1465",1606.00776,"multiresolution recurrent neural networks: an application to dialogue   response generation","cs.cl cs.ai cs.lg cs.ne stat.ml","we introduce the multiresolution recurrent neural network, which extends the sequence-to-sequence framework to model natural language generation as two parallel discrete stochastic processes: a sequence of high-level coarse tokens, and a sequence of natural language tokens. there are many ways to estimate or learn the high-level coarse tokens, but we argue that a simple extraction procedure is sufficient to capture a wealth of high-level discourse semantics. such procedure allows training the multiresolution recurrent neural network by maximizing the exact joint log-likelihood over both sequences. in contrast to the standard log- likelihood objective w.r.t. natural language tokens (word perplexity), optimizing the joint log-likelihood biases the model towards modeling high-level abstractions. we apply the proposed model to the task of dialogue response generation in two challenging domains: the ubuntu technical support domain, and twitter conversations. on ubuntu, the model outperforms competing approaches by a substantial margin, achieving state-of-the-art results according to both automatic evaluation metrics and a human evaluation study. on twitter, the model appears to generate more relevant and on-topic responses according to automatic evaluation metrics. finally, our experiments demonstrate that the proposed model is more adept at overcoming the sparsity of natural language and is better able to capture long-term structure.","","2016-06-02","2016-06-13","['iulian vlad serban', 'tim klinger', 'gerald tesauro', 'kartik talamadupula', 'bowen zhou', 'yoshua bengio', 'aaron courville']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1466",1606.00931,"deepsurv: personalized treatment recommender system using a cox   proportional hazards deep neural network","stat.ml cs.ne","medical practitioners use survival models to explore and understand the relationships between patients' covariates (e.g. clinical and genetic features) and the effectiveness of various treatment options. standard survival models like the linear cox proportional hazards model require extensive feature engineering or prior medical knowledge to model treatment interaction at an individual level. while nonlinear survival methods, such as neural networks and survival forests, can inherently model these high-level interaction terms, they have yet to be shown as effective treatment recommender systems. we introduce deepsurv, a cox proportional hazards deep neural network and state-of-the-art survival method for modeling interactions between a patient's covariates and treatment effectiveness in order to provide personalized treatment recommendations. we perform a number of experiments training deepsurv on simulated and real survival data. we demonstrate that deepsurv performs as well as or better than other state-of-the-art survival models and validate that deepsurv successfully models increasingly complex relationships between a patient's covariates and their risk of failure. we then show how deepsurv models the relationship between a patient's features and effectiveness of different treatment options to show how deepsurv can be used to provide individual treatment recommendations. finally, we train deepsurv on real clinical studies to demonstrate how it's personalized treatment recommendations would increase the survival time of a set of patients. the predictive and modeling capabilities of deepsurv will enable medical researchers to use deep neural networks as a tool in their exploration, understanding, and prediction of the effects of a patient's characteristics on their risk of failure.","10.1186/s12874-018-0482-1","2016-06-02","2017-08-08","['jared katzman', 'uri shaham', 'jonathan bates', 'alexander cloninger', 'tingting jiang', 'yuval kluger']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1467",1606.0098,"fast bayesian whole-brain fmri analysis with spatial 3d priors","stat.co stat.ap stat.me","spatial whole-brain bayesian modeling of task-related functional magnetic resonance imaging (fmri) is a great computational challenge. most of the currently proposed methods therefore do inference in subregions of the brain separately or do approximate inference without comparison to the true posterior distribution. a popular such method, which is now the standard method for bayesian single subject analysis in the spm software, is introduced in penny et al. (2005b). the method processes the data slice-by-slice and uses an approximate variational bayes (vb) estimation algorithm that enforces posterior independence between activity coefficients in different voxels. we introduce a fast and practical markov chain monte carlo (mcmc) scheme for exact inference in the same model, both slice-wise and for the whole brain using a 3d prior on activity coefficients. the algorithm exploits sparsity and uses modern techniques for efficient sampling from high-dimensional gaussian distributions, leading to speed-ups without which mcmc would not be a practical option. using mcmc, we are for the first time able to evaluate the approximate vb posterior against the exact mcmc posterior, and show that vb can lead to spurious activation. in addition, we develop an improved vb method that drops the assumption of independent voxels a posteriori. this algorithm is shown to be much faster than both mcmc and the original vb for large datasets, with negligible error compared to the mcmc posterior.","10.1016/j.neuroimage.2016.11.040","2016-06-03","2016-09-29","['per sidén', 'anders eklund', 'david bolin', 'mattias villani']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"1468",1606.00991,"dynamic modeling with conditional quantile trajectories for longitudinal   snippet data, with application to cognitive decline of alzheimer's patients","stat.me","longitudinal data are often plagued with sparsity of time points where measurements are available. the functional data analysis perspective has been shown to provide an effective and flexible approach to address this problem for the case where measurements are sparse but their times are randomly distributed over an interval. here we focus on a different scenario where available data can be characterized as snippets, which are very short stretches of longitudinal measurements. for each subject the stretch of available data is much shorter than the time frame of interest, a common occurrence in accelerated longitudinal studies. an added challenge is introduced if a time proxy that is basic for usual longitudinal modeling is not available. this situation arises in the case of alzheimer's disease and comparable scenarios, where one is interested in time dynamics of declining performance, but the time of disease onset is unknown and the chronological age does not provide a meaningful time reference for longitudinal modeling. our main methodological contribution is to address this problem with a novel approach. key quantities for our approach are conditional quantile trajectories for monotonic processes that emerge as solutions of a dynamic system, and for which we obtain uniformly consistent estimates. these trajectories are shown to be useful to describe processes that quantify deterioration over time, such as hippocampal volumes in alzheimer's patients.","","2016-06-03","2017-02-09","['matthew dawson', 'hans-georg müller']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1469",1606.01746,"unsupervised classification of children's bodies using currents","stat.me cs.cv stat.ap","object classification according to their shape and size is of key importance in many scientific fields. this work focuses on the case where the size and shape of an object is characterized by a current}. a current is a mathematical object which has been proved relevant to the modeling of geometrical data, like submanifolds, through integration of vector fields along them. as a consequence of the choice of a vector-valued reproducing kernel hilbert space (rkhs) as a test space for integrating manifolds, it is possible to consider that shapes are embedded in this hilbert space. a vector-valued rkhs is a hilbert space of vector fields; therefore, it is possible to compute a mean of shapes, or to calculate a distance between two manifolds. this embedding enables us to consider size-and-shape classification algorithms.   these algorithms are applied to a 3d database obtained from an anthropometric survey of the spanish child population with a potential application to online sales of children's wear.","","2016-06-06","","['sonia barahona', 'ximo gual-arnau', 'maria victoria ibáñez', 'amelia simó']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1470",1606.01855,"bayesian poisson tucker decomposition for learning the structure of   international relations","stat.ml cs.ai cs.lg cs.si stat.ap","we introduce bayesian poisson tucker decomposition (bptd) for modeling country--country interaction event data. these data consist of interaction events of the form ""country $i$ took action $a$ toward country $j$ at time $t$."" bptd discovers overlapping country--community memberships, including the number of latent communities. in addition, it discovers directed community--community interaction networks that are specific to ""topics"" of action types and temporal ""regimes."" we show that bptd yields an efficient mcmc inference algorithm and achieves better predictive performance than related models. we also demonstrate that it discovers interpretable latent structure that agrees with our knowledge of international relations.","","2016-06-06","","['aaron schein', 'mingyuan zhou', 'david m. blei', 'hanna wallach']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"1471",1606.02077,"regret bounds for non-decomposable metrics with missing labels","cs.lg stat.ml","we consider the problem of recommending relevant labels (items) for a given data point (user). in particular, we are interested in the practically important setting where the evaluation is with respect to non-decomposable (over labels) performance metrics like the $f_1$ measure, and the training data has missing labels. to this end, we propose a generic framework that given a performance metric $\psi$, can devise a regularized objective function and a threshold such that all the values in the predicted score vector above and only above the threshold are selected to be positive. we show that the regret or generalization error in the given metric $\psi$ is bounded ultimately by estimation error of certain underlying parameters. in particular, we derive regret bounds under three popular settings: a) collaborative filtering, b) multilabel classification, and c) pu (positive-unlabeled) learning. for each of the above problems, we can obtain precise non-asymptotic regret bound which is small even when a large fraction of labels is missing. our empirical results on synthetic and benchmark datasets demonstrate that by explicitly modeling for missing labels and optimizing the desired performance metric, our algorithm indeed achieves significantly better performance (like $f_1$ score) when compared to methods that do not model missing label information carefully.","","2016-06-07","","['prateek jain', 'nagarajan natarajan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1472",1606.02359,"structure learning in graphical modeling","stat.me stat.ml","a graphical model is a statistical model that is associated to a graph whose nodes correspond to variables of interest. the edges of the graph reflect allowed conditional dependencies among the variables. graphical models admit computationally convenient factorization properties and have long been a valuable tool for tractable modeling of multivariate distributions. more recently, applications such as reconstructing gene regulatory networks from gene expression data have driven major advances in structure learning, that is, estimating the graph underlying a model. we review some of these advances and discuss methods such as the graphical lasso and neighborhood selection for undirected graphical models (or markov random fields), and the pc algorithm and score-based search methods for directed graphical models (or bayesian networks). we further review extensions that account for effects of latent variables and heterogeneous data sources.","","2016-06-07","","['mathias drton', 'marloes h. maathuis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1473",1606.02381,"nonparametric bayes models for mixed-scale longitudinal surveys","stat.ap","modeling and computation for multivariate longitudinal surveys have proven challenging, particularly when data are not all continuous and gaussian but contain discrete measurements. in many social science surveys, study participants are selected via complex survey designs such as stratified random sampling, leading to discrepancies between the sample and population, which are further compounded by missing data and loss to follow up. survey weights are typically constructed to address these issues, but it is not clear how to include them in models. motivated by data on sexual development, we propose a novel nonparametric approach for mixed-scale longitudinal data in surveys. in the proposed approach, the mixed-scale multivariate response is expressed through an underlying continuous variable with dynamic latent factors inducing time-varying associations. bias from the survey design is adjusted for in posterior computation relying on a markov chain monte carlo algorithm. the approach is assessed in simulation studies, and applied to the national longitudinal study of adolescent to adult health.","","2016-06-07","","['tsuyoshi kunihama', 'carolyn t. halpern', 'amy h. herring']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1474",1606.02536,"the forecasting of menstruation based on a state-space modeling of basal   body temperature time series","stat.ap","women's basal body temperature (bbt) follows a periodic pattern that is associated with the events in their menstrual cycle. although daily bbt time series contain potentially useful information for estimating the underlying menstrual phase and for predicting the length of current menstrual cycle, few models have been constructed for bbt time series. here, we propose a state-space model that includes menstrual phase as a latent state variable to explain fluctuations in bbt and menstrual cycle length. conditional distributions for the menstrual phase were obtained by using sequential bayesian filtering techniques. a predictive distribution for the upcoming onset of menstruation was then derived based on the conditional distributions and the model, leading to a novel statistical framework that provided a sequentially updated prediction of the day of onset of menstruation. we applied this framework to a real dataset comprising women's self-reported bbt and days of menstruation, comparing the prediction accuracy of our proposed method with that of conventional calendar calculation. we found that our proposed method provided a better prediction of the day of onset of menstruation. potential extensions of this framework may provide the basis of modeling and predicting other events that are associated with the menstrual cycle.","10.1002/sim.7345","2016-06-08","","['keiichi fukaya', 'ai kawamori', 'yutaka osada', 'masumi kitazawa', 'makio ishiguro']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1475",1606.0296,"sequence-to-sequence learning as beam-search optimization","cs.cl cs.lg cs.ne stat.ml","sequence-to-sequence (seq2seq) modeling has rapidly become an important general-purpose nlp tool that has proven effective for many text-generation and sequence-labeling tasks. seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. in this work, we introduce a model and beam-search training scheme, based on the work of daume iii and marcu (2005), that extends seq2seq to learn global sequence scores. this structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. we show that our system outperforms a highly-optimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation.","","2016-06-09","2016-11-09","['sam wiseman', 'alexander m. rush']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1476",1606.02979,"generative topic embedding: a continuous representation of documents   (extended version with proofs)","cs.cl cs.ai cs.ir cs.lg stat.ml","word embedding maps words into a low-dimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. on the other hand, topic modeling maps documents onto a low-dimensional topic space, by utilizing the global word collocation patterns in the same document. these two types of patterns are complementary. in this paper, we propose a generative topic embedding model to combine the two types of patterns. in our model, topics are represented by embedding vectors, and are shared across documents. the probability of each word is influenced by both its local context and its topic. a variational inference method yields the topic embeddings as well as the topic mixing proportions for each document. jointly they represent the document in a low-dimensional continuous space. in two document classification tasks, our method performs better than eight existing methods, with fewer features. in addition, we illustrate with an example that our method can generate coherent topics even based on only one document.","","2016-06-09","2016-08-08","['shaohua li', 'tat-seng chua', 'jun zhu', 'chunyan miao']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1477",1606.03295,"simultaneous inference for misaligned multivariate functional data","stat.ap","we consider inference for misaligned multivariate functional data that represents the same underlying curve, but where the functional samples have systematic differences in shape. in this paper we introduce a new class of generally applicable models where warping effects are modeled through nonlinear transformation of latent gaussian variables and systematic shape differences are modeled by gaussian processes. to model cross-covariance between sample coordinates we introduce a class of low-dimensional cross-covariance structures suitable for modeling multivariate functional data. we present a method for doing maximum-likelihood estimation in the models and apply the method to three data sets. the first data set is from a motion tracking system where the spatial positions of a large number of body-markers are tracked in three-dimensions over time. the second data set consists of height and weight measurements for danish boys. the third data set consists of three-dimensional spatial hand paths from a controlled obstacle-avoidance experiment. we use the developed method to estimate the cross-covariance structure, and use a classification setup to demonstrate that the method outperforms state-of-the-art methods for handling misaligned curve data.","","2016-06-10","2017-12-08","['niels lundtorp olsen', 'bo markussen', 'lars lau rakêt']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1478",1606.03623,"drug response prediction by inferring pathway-response associations with   kernelized bayesian matrix factorization","stat.ml cs.lg q-bio.qm","a key goal of computational personalized medicine is to systematically utilize genomic and other molecular features of samples to predict drug responses for a previously unseen sample. such predictions are valuable for developing hypotheses for selecting therapies tailored for individual patients. this is especially valuable in oncology, where molecular and genetic heterogeneity of the cells has a major impact on the response. however, the prediction task is extremely challenging, raising the need for methods that can effectively model and predict drug responses. in this study, we propose a novel formulation of multi-task matrix factorization that allows selective data integration for predicting drug responses. to solve the modeling task, we extend the state-of-the-art kernelized bayesian matrix factorization (kbmf) method with component-wise multiple kernel learning. in addition, our approach exploits the known pathway information in a novel and biologically meaningful fashion to learn the drug response associations. our method quantitatively outperforms the state of the art on predicting drug responses in two publicly available cancer data sets as well as on a synthetic data set. in addition, we validated our model predictions with lab experiments using an in-house cancer cell line panel. we finally show the practical applicability of the proposed method by utilizing prior knowledge to infer pathway-drug response associations, opening up the opportunity for elucidating drug action mechanisms. we demonstrate that pathway-response associations can be learned by the proposed model for the well known egfr and mek inhibitors.","10.1093/bioinformatics/btw433.","2016-06-11","","['muhammad ammad-ud-din', 'suleiman a. khan', 'disha malani', 'astrid murumägi', 'olli kallioniemi', 'tero aittokallio', 'samuel kaski']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1479",1606.03685,"efficient klms and krls algorithms: a random fourier feature perspective","cs.lg stat.ml","we present a new framework for online least squares algorithms for nonlinear modeling in rkh spaces (rkhs). instead of implicitly mapping the data to a rkhs (e.g., kernel trick), we map the data to a finite dimensional euclidean space, using random features of the kernel's fourier transform. the advantage is that, the inner product of the mapped data approximates the kernel function. the resulting ""linear"" algorithm does not require any form of sparsification, since, in contrast to all existing algorithms, the solution's size remains fixed and does not increase with the iteration steps. as a result, the obtained algorithms are computationally significantly more efficient compared to previously derived variants, while, at the same time, they converge at similar speeds and to similar error floors.","","2016-06-12","","['pantelis bouboulis', 'spyridon pougkakiotis', 'sergios theodoridis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1480",1606.03998,"inference on subspheres model for directional data","math.st stat.th","modeling deformations of a real object is an important task in computer vision, biomedical engineering and biomechanics. in this paper, we focus on a situation where a three-dimensional object is rotationally deformed about a fixed axis, and assume that many independent observations are available. such a problem is generalized to an estimation of concentric, co-dimension 1, subspheres of a polysphere. we formulate least-square estimators as generalized fr\'{e}chet means, and evaluate the consistency and asymptotic normality.","","2016-06-13","","['sungkyu jung']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1481",1606.0408,"matching networks for one shot learning","cs.lg stat.ml","learning from a few examples remains a key challenge in machine learning. despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. in this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. we then define one-shot learning problems on vision (using omniglot, imagenet) and language tasks. our algorithm improves one-shot accuracy on imagenet from 87.6% to 93.2% and from 88.0% to 93.8% on omniglot compared to competing approaches. we also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the penn treebank.","","2016-06-13","2017-12-29","['oriol vinyals', 'charles blundell', 'timothy lillicrap', 'koray kavukcuoglu', 'daan wierstra']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1482",1606.04146,"inference for instrumental variables: a randomization inference approach","stat.me stat.ap","the method of instrumental variables (iv) provides a framework to study causal effects in both randomized experiments with noncompliance and in observational studies where natural circumstances produce as-if random nudges to accept treatment. traditionally, inference for iv relied on asymptotic approximations of the distribution of the wald estimator or two-stage least squares, often with structural modeling assumptions and/or moment conditions. in this paper, we utilize the randomization inference approach to iv inference. first, we outline the exact method, which uses the randomized assignment of treatment in experiments as a basis for inference, but lacks a closed-form solution and may be computationally infeasible in many applications. we then provide an alternative to the exact method, the almost exact method, which is computationally feasible but retains the advantages of the exact method. we also review asymptotic methods of inference, including those associated with two-stage least squares, and analytically compare them to randomization inference methods. we also perform additional comparisons using a set of simulations. we conclude with three different applications from the social sciences.","","2016-06-13","2018-02-06","['hyunseung kang', 'laura peck', 'luke keele']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1483",1606.04317,"calibration of phone likelihoods in automatic speech recognition","stat.ml cs.lg cs.sd","in this paper we study the probabilistic properties of the posteriors in a speech recognition system that uses a deep neural network (dnn) for acoustic modeling. we do this by reducing kaldi's dnn shared pdf-id posteriors to phone likelihoods, and using test set forced alignments to evaluate these using a calibration sensitive metric. individual frame posteriors are in principle well-calibrated, because the dnn is trained using cross entropy as the objective function, which is a proper scoring rule. when entire phones are assessed, we observe that it is best to average the log likelihoods over the duration of the phone. further scaling of the average log likelihoods by the logarithm of the duration slightly improves the calibration, and this improvement is retained when tested on independent test data.","","2016-06-14","","['david a. van leeuwen', 'joost van doremalen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1484",1606.04393,"deep learning with darwin: evolutionary synthesis of deep neural   networks","cs.cv cs.lg cs.ne stat.ml","taking inspiration from biological evolution, we explore the idea of ""can deep neural networks evolve naturally over successive generations into highly efficient deep neural networks?"" by introducing the notion of synthesizing new highly efficient, yet powerful deep neural networks over successive generations via an evolutionary process from ancestor deep neural networks. the architectural traits of ancestor deep neural networks are encoded using synaptic probability models, which can be viewed as the `dna' of these networks. new descendant networks with differing network architectures are synthesized based on these synaptic probability models from the ancestor networks and computational environmental factor models, in a random manner to mimic heredity, natural selection, and random mutation. these offspring networks are then trained into fully functional networks, like one would train a newborn, and have more efficient, more diverse network architectures than their ancestor networks, while achieving powerful modeling capabilities. experimental results for the task of visual saliency demonstrated that the synthesized `evolved' offspring networks can achieve state-of-the-art performance while having network architectures that are significantly more efficient (with a staggering $\sim$48-fold decrease in synapses by the fourth generation) compared to the original ancestor network.","","2016-06-14","2017-02-06","['mohammad javad shafiee', 'akshaya mishra', 'alexander wong']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1485",1606.05196,"a goldilocks principle for modeling radial velocity noise","astro-ph.ep stat.ap","the doppler measurements of stars are diluted and distorted by stellar activity noise. different choices of noise models and statistical methods have led to much controversy in the confirmation of exoplanet candidates obtained through analysing radial velocity data. to quantify the limitation of various models and methods, we compare different noise models and signal detection criteria for various simulated and real data sets in the bayesian framework. according to our analyses, the white noise model tend to interpret noise as signal, leading to false positives. on the other hand, the red noise models are likely to interprete signal as noise, resulting in false negatives. we find that the bayesian information criterion combined with a bayes factor threshold of 150 can efficiently rule out false positives and confirm true detections. we further propose a goldilocks principle aimed at modeling radial velocity noise to avoid too many false positives and too many false negatives. we propose that the noise model with rhk-dependent jitter is used in combination with the moving average model to detect planetary signals for m dwarfs. our work may also shed light on the noise modeling for hotter stars, and provide a valid approach for finding similar principles in other disciplines.","10.1093/mnras/stw1478","2016-06-16","","['fabo feng', 'm. tuomi', 'h. r. a. jones', 'r. p. butler', 's. vogt']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1486",1606.054,"complex systems: features, similarity and connectivity","physics.soc-ph physics.data-an stat.ml","the increasing interest in complex networks research has been a consequence of several intrinsic features of this area, such as the generality of the approach to represent and model virtually any discrete system, and the incorporation of concepts and methods deriving from many areas, from statistical physics to sociology, which are often used in an independent way. yet, for this same reason, it would be desirable to integrate these various aspects into a more coherent and organic framework, which would imply in several benefits normally allowed by the systematization in science, including the identification of new types of problems and the cross-fertilization between fields. more specifically, the identification of the main areas to which the concepts frequently used in complex networks can be applied paves the way to adopting and applying a larger set of concepts and methods deriving from those respective areas. among the several areas that have been used in complex networks research, pattern recognition, optimization, linear algebra, and time series analysis seem to play a more basic and recurrent role. in the present manuscript, we propose a systematic way to integrate the concepts from these diverse areas regarding complex networks research. in order to do so, we start by grouping the multidisciplinary concepts into three main groups, namely features, similarity, and network connectivity. then we show that several of the analysis and modeling approaches to complex networks can be thought as a composition of maps between these three groups, with emphasis on nine main types of mappings, which are presented and illustrated. such a systematization of principles and approaches also provides an opportunity to review some of the most closely related works in the literature, which is also developed in this article.","","2016-06-16","","['cesar h. comin', 'thomas k. dm. peron', 'filipi n. silva', 'diego r. amancio', 'francisco a. rodrigues', 'luciano da f. costa']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1487",1606.05658,"the basis function approach for modeling autocorrelation in ecological   data","stat.ap","analyzing ecological data often requires modeling the autocorrelation created by spatial and temporal processes. many of the statistical methods used to account for autocorrelation can be viewed as regression models that include basis functions. understanding the concept of basis functions enables ecologists to modify commonly used ecological models to account for autocorrelation, which can improve inference and predictive accuracy. understanding the properties of basis functions is essential for evaluating the fit of spatial or time-series models, detecting a hidden form of multicollinearity, and analyzing large data sets. we present important concepts and properties related to basis functions and illustrate several tools and techniques ecologists can use when modeling autocorrelation in ecological data.","","2016-06-17","","['trevor j. hefley', 'kristin m. broms', 'brian m. brost', 'frances e. buderman', 'shannon l. kay', 'henry r. scharf', 'john r. tipton', 'perry j. williams', 'mevin b. hooten']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1488",1606.05925,"graph based manifold regularized deep neural networks for automatic   speech recognition","stat.ml cs.cl cs.lg","deep neural networks (dnns) have been successfully applied to a wide variety of acoustic modeling tasks in recent years. these include the applications of dnns either in a discriminative feature extraction or in a hybrid acoustic modeling scenario. despite the rapid progress in this area, a number of challenges remain in training dnns. this paper presents an effective way of training dnns using a manifold learning based regularization framework. in this framework, the parameters of the network are optimized to preserve underlying manifold based relationships between speech feature vectors while minimizing a measure of loss between network outputs and targets. this is achieved by incorporating manifold based locality constraints in the objective criterion of dnns. empirical evidence is provided to demonstrate that training a network with manifold constraints preserves structural compactness in the hidden layers of the network. manifold regularization is applied to train bottleneck dnns for feature extraction in hidden markov model (hmm) based speech recognition. the experiments in this work are conducted on the aurora-2 spoken digits and the aurora-4 read news large vocabulary continuous speech recognition tasks. the performance is measured in terms of word error rate (wer) on these tasks. it is shown that the manifold regularized dnns result in up to 37% reduction in wer relative to standard dnns.","","2016-06-19","","['vikrant singh tomar', 'richard c. rose']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1489",1606.06645,"sensitivity to serial dependency of input processes: a robust approach","stat.me","procedures in assessing the impact of serial dependency on performance analysis are usually built on parametrically specified models. in this paper, we propose a robust, nonparametric approach to carry out this assessment, by computing the worst-case deviation of the performance measure due to arbitrary dependence. the approach is based on optimizations, posited on the model space, that have constraints specifying the level of dependency measured by a nonparametric distance to some nominal i.i.d. input model. we study approximation methods for these optimizations via simulation and analysis-of-variance (anova). numerical experiments demonstrate how the proposed approach can discover the hidden impacts of dependency beyond those revealed by conventional parametric modeling and correlation studies.","","2016-06-21","","['henry lam']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1490",1606.06912,"spatial bayesian latent factor regression modeling of coordinate-based   meta-analysis data","stat.me","now over 20 years old, functional mri (fmri) has a large and growing literature that is best synthesised with meta-analytic tools. as most authors do not share image data, only the peak activation coordinates (foci) reported in the paper are available for coordinate-based meta-analysis (cbma). neuroimaging meta-analysis is used to 1) identify areas of consistent activation; and 2) build a predictive model of task type or cognitive process for new studies (reverse inference). to simultaneously address these aims, we propose a bayesian point process hierarchical model for cbma. we model the foci from each study as a doubly stochastic poisson process, where the study-specific log intensity function is characterised as a linear combination of a high-dimensional basis set. a sparse representation of the intensities is guaranteed through latent factor modeling of the basis coefficients. within our framework, it is also possible to account for the effect of study-level covariates (meta-regression), significantly expanding the capabilities of the current neuroimaging meta-analysis methods available. we apply our methodology to synthetic data and a neuroimaging meta-analysis dataset.","","2016-06-22","","['silvia montagna', 'tor wager', 'lisa feldman-barrett', 'timothy d. johnson', 'thomas e. nichols']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1491",1606.06997,"on the uniqueness and stability of dictionaries for sparse   representation of noisy signals","stat.ml cs.it math.it","learning optimal dictionaries for sparse coding has exposed characteristic sparse features of many natural signals. however, universal guarantees of the stability of such features in the presence of noise are lacking. here, we provide very general conditions guaranteeing when dictionaries yielding the sparsest encodings are unique and stable with respect to measurement or modeling error. we demonstrate that some or all original dictionary elements are recoverable from noisy data even if the dictionary fails to satisfy the spark condition, its size is overestimated, or only a polynomial number of distinct sparse supports appear in the data. importantly, we derive these guarantees without requiring any constraints on the recovered dictionary beyond a natural upper bound on its size. our results also yield an effective procedure sufficient to affirm if a proposed solution to the dictionary learning problem is unique within bounds commensurate with the noise. we suggest applications to data analysis, engineering, and neuroscience and close with some remaining challenges left open by our work.","","2016-06-22","2019-05-14","['charles j. garfinkle', 'christopher j. hillar']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1492",1606.07251,"algorithmic composition of melodies with deep recurrent neural networks","stat.ml cs.lg","a big challenge in algorithmic composition is to devise a model that is both easily trainable and able to reproduce the long-range temporal dependencies typical of music. here we investigate how artificial neural networks can be trained on a large corpus of melodies and turned into automated music composers able to generate new melodies coherent with the style they have been trained on. we employ gated recurrent unit networks that have been shown to be particularly efficient in learning complex sequential activations with arbitrary long time lags. our model processes rhythm and melody in parallel while modeling the relation between these two features. using such an approach, we were able to generate interesting complete melodies or suggest possible continuations of a melody fragment that is coherent with the characteristics of the fragment itself.","10.13140/rg.2.1.2436.5683","2016-06-23","","['florian colombo', 'samuel p. muscinelli', 'alexander seeholzer', 'johanni brea', 'wulfram gerstner']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1493",1606.0784,"modeling group dynamics using probabilistic tensor decompositions","stat.ml","we propose a probabilistic modeling framework for learning the dynamic patterns in the collective behaviors of social agents and developing profiles for different behavioral groups, using data collected from multiple information sources. the proposed model is based on a hierarchical bayesian process, in which each observation is a finite mixture of an set of latent groups and the mixture proportions (i.e., group probabilities) are drawn randomly. each group is associated with some distributions over a finite set of outcomes. moreover, as time evolves, the structure of these groups also changes; we model the change in the group structure by a hidden markov model (hmm) with a fixed transition probability. we present an efficient inference method based on tensor decompositions and the expectation-maximization (em) algorithm for parameter estimation.","","2016-06-24","","['lin li', 'ananthram swami', 'anna scaglione']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1494",1606.07986,"flexible discrete space models of animal movement","stat.ap","movement drives the spread of infectious disease, gene flow, and other critical ecological processes. to study these processes we need models for movement that capture complex behavior that changes over time and space in response to biotic and abiotic factors. penalized likelihood approaches, such as penalized semiparametric spline expansions and lasso regression, allow inference on complex models without overfitting. continuous-time markov chains (ctmcs) have been recently introduced as a flexible discrete-space model for animal movement. modeling with ctmcs involves discretizing an animal's path to the resolution of a raster grid. the resulting stochastic process model can easily incorporate environmental and other covariates, represented as raster layers, that affect directional bias and overall movement rate. we introduce a weighted likelihood approach that allows for modeling movement using ctmcs, with path uncertainty due to missing data modeled by imputing continuous-time paths between telemetry locations. the framework we introduce allows for inference on ctmc movement models using existing software for fitting poisson regression models, including penalized versions of poisson regression. the result is a flexible, powerful, and accessible framework for modeling a wide range of animal movement behavior.","","2016-06-25","","['ephraim m. hanks', 'david a. hughes']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1495",1606.08063,"enhancing transparency and control when drawing data-driven inferences   about individuals","stat.ml cs.cy","recent studies have shown that information disclosed on social network sites (such as facebook) can be used to predict personal characteristics with surprisingly high accuracy. in this paper we examine a method to give online users transparency into why certain inferences are made about them by statistical models, and control to inhibit those inferences by hiding (""cloaking"") certain personal information from inference. we use this method to examine whether such transparency and control would be a reasonable goal by assessing how difficult it would be for users to actually inhibit inferences. applying the method to data from a large collection of real users on facebook, we show that a user must cloak only a small portion of her facebook likes in order to inhibit inferences about their personal characteristics. however, we also show that in response a firm could change its modeling of users to make cloaking more difficult.","","2016-06-26","","['daizhuo chen', 'samuel p. fraiberger', 'robert moakler', 'foster provost']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1496",1606.08098,"california reservoir drought sensitivity and exhaustion risk using   statistical graphical models","stat.ap","the ongoing california drought has highlighted the potential vulnerability of state water management infrastructure to multi-year dry intervals. due to the high complexity of the network, dynamic storage changes across the california reservoir system have been difficult to model using either conventional statistical or physical approaches. here, we analyze the interactions of monthly volumes in a network of 55 large california reservoirs, over a period of 136 months from 2004 to 2015, and we develop a latent-variable graphical model of their joint fluctuations. we achieve reliable and tractable modeling of the system because the model structure allows unique recovery of the best-in-class model via convex optimization with control of the number of free parameters. we extract a statewide `latent' influencing factor which turns out to be highly correlated with both the palmer drought severity index (pdsi, $\rho \approx 0.86$) and hydroelectric production ($\rho \approx 0.71$). further, the model allows us to determine system health measures such as exhaustion probability and per-reservoir drought sensitivity. we find that as pdsi approaches -6, there is a probability greater than 50\% of simultaneous exhaustion of multiple large reservoirs.","","2016-06-26","","['armeen taeb', 'john t. reager', 'michael turmon', 'venkat chandrasekaran']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1497",1606.08105,"the dependent random measures with independent increments in mixture   models","stat.ml","when observations are organized into groups where commonalties exist amongst them, the dependent random measures can be an ideal choice for modeling. one of the propositions of the dependent random measures is that the atoms of the posterior distribution are shared amongst groups, and hence groups can borrow information from each other. when normalized dependent random measures prior with independent increments are applied, we can derive appropriate exchangeable probability partition function (eppf), and subsequently also deduce its inference algorithm given any mixture model likelihood. we provide all necessary derivation and solution to this framework. for demonstration, we used mixture of gaussians likelihood in combination with a dependent structure constructed by linear combinations of crms. our experiments show superior performance when using this framework, where the inferred values including the mixing weights and the number of clusters both respond appropriately to the number of completely random measure used.","","2016-06-26","","['cheng luo', 'richard yi da xu', 'yang xiang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1498",1606.08292,"dynamics and sparsity in latent threshold factor models: a study in   multivariate eeg signal processing","stat.ap","we discuss bayesian analysis of multivariate time series with dynamic factor models that exploit time-adaptive sparsity in model parametrizations via the latent threshold approach. one central focus is on the transfer responses of multiple interrelated series to underlying, dynamic latent factor processes. structured priors on model hyper-parameters are key to the efficacy of dynamic latent thresholding, and mcmc-based computation enables model fitting and analysis. a detailed case study of electroencephalographic (eeg) data from experimental psychiatry highlights the use of latent threshold extensions of time-varying vector autoregressive and factor models. this study explores a class of dynamic transfer response factor models, extending prior bayesian modeling of multiple eeg series and highlighting the practical utility of the latent thresholding concept in multivariate, non-stationary time series analysis.","","2016-06-27","","['jouchi nakajima', 'mike west']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1499",1606.084,"robust and rate-optimal gibbs posterior inference on the boundary of a   noisy image","stat.me","detection of an image boundary when the pixel intensities are measured with noise is an important problem in image segmentation, with numerous applications in medical imaging and engineering. from a statistical point of view, the challenge is that likelihood-based methods require modeling the pixel intensities inside and outside the image boundary, even though these are typically of no practical interest. since misspecification of the pixel intensity models can negatively affect inference on the image boundary, it would be desirable to avoid this modeling step altogether. towards this, we develop a robust gibbs approach that constructs a posterior distribution for the image boundary directly, without modeling the pixel intensities. we prove that, for a suitable prior on the image boundary, the gibbs posterior concentrates asymptotically at the minimax optimal rate, adaptive to the boundary smoothness. monte carlo computation of the gibbs posterior is straightforward, and simulation experiments show that the corresponding inference is more accurate than that based on existing bayesian methodology.","","2016-06-27","2018-06-01","['nicholas syring', 'ryan martin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1500",1606.08842,"active ranking from pairwise comparisons and when parametric assumptions   don't help","cs.lg cs.ai cs.it math.it stat.ml","we consider sequential or active ranking of a set of n items based on noisy pairwise comparisons. items are ranked according to the probability that a given item beats a randomly chosen item, and ranking refers to partitioning the items into sets of pre-specified sizes according to their scores. this notion of ranking includes as special cases the identification of the top-k items and the total ordering of the items. we first analyze a sequential ranking algorithm that counts the number of comparisons won, and uses these counts to decide whether to stop, or to compare another pair of items, chosen based on confidence intervals specified by the data collected up to that point. we prove that this algorithm succeeds in recovering the ranking using a number of comparisons that is optimal up to logarithmic factors. this guarantee does not require any structural properties of the underlying pairwise probability matrix, unlike a significant body of past work on pairwise ranking based on parametric models such as the thurstone or bradley-terry-luce models. it has been a long-standing open question as to whether or not imposing these parametric assumptions allows for improved ranking algorithms. for stochastic comparison models, in which the pairwise probabilities are bounded away from zero, our second contribution is to resolve this issue by proving a lower bound for parametric models. this shows, perhaps surprisingly, that these popular parametric modeling choices offer at most logarithmic gains for stochastic comparisons.","","2016-06-28","2016-09-23","['reinhard heckel', 'nihar b. shah', 'kannan ramchandran', 'martin j. wainwright']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1501",1606.08925,"a fused latent and graphical model for multivariate binary data","stat.me","we consider modeling, inference, and computation for analyzing multivariate binary data. we propose a new model that consists of a low dimensional latent variable component and a sparse graphical component. our study is motivated by analysis of item response data in cognitive assessment and has applications to many disciplines where item response data are collected. standard approaches to item response data in cognitive assessment adopt the multidimensional item response theory (irt) models. however, human cognition is typically a complicated process and thus may not be adequately described by just a few factors. consequently, a low-dimensional latent factor model, such as the multidimensional irt models, is often insufficient to capture the structure of the data. the proposed model adds a sparse graphical component that captures the remaining ad hoc dependence. it reduces to a multidimensional irt model when the graphical component becomes degenerate. model selection and parameter estimation are carried out simultaneously through construction of a pseudo-likelihood function and properly chosen penalty terms. the convexity of the pseudo-likelihood function allows us to develop an efficient algorithm, while the penalty terms generate a low-dimensional latent component and a sparse graphical structure. desirable theoretical properties are established under suitable regularity conditions. the method is applied to the revised eysenck's personality questionnaire, revealing its usefulness in item analysis. simulation results are reported that show the new method works well in practical situations.","","2016-06-28","","['yunxiao chen', 'xiaoou li', 'jingchen liu', 'zhiliang ying']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1502",1606.09585,"hierarchical animal movement models for population-level inference","stat.me","new methods for modeling animal movement based on telemetry data are developed regularly. with advances in telemetry capabilities, animal movement models are becoming increasingly sophisticated. despite a need for population-level inference, animal movement models are still predominantly developed for individual-level inference. most efforts to upscale the inference to the population-level are either post hoc or complicated enough that only the developer can implement the model. hierarchical bayesian models provide an ideal platform for the development of population-level animal movement models but can be challenging to fit due to computational limitations or extensive tuning required. we propose a two-stage procedure for fitting hierarchical animal movement models to telemetry data. the two-stage approach is statistically rigorous and allows one to fit individual-level movement models separately, then resample them using a secondary mcmc algorithm. the primary advantages of the two-stage approach are that the first stage is easily parallelizable and the second stage is completely unsupervised, allowing for a completely automated fitting procedure in many cases. we demonstrate the two-stage procedure with two applications of animal movement models. the first application involves a spatial point process approach to modeling telemetry data and the second involves a more complicated continuous-time discrete-space animal movement model. we fit these models to simulated data and real telemetry data arising from a population of monitored canada lynx in colorado, usa.","","2016-06-30","","['mevin b. hooten', 'frances e. buderman', 'brian m. brost', 'ephraim m. hanks', 'jacob s. ivan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1503",1607.00136,"missing data estimation in high-dimensional datasets: a swarm   intelligence-deep neural network approach","cs.ai cs.lg stat.ml","in this paper, we examine the problem of missing data in high-dimensional datasets by taking into consideration the missing completely at random and missing at random mechanisms, as well as thearbitrary missing pattern. additionally, this paper employs a methodology based on deep learning and swarm intelligence algorithms in order to provide reliable estimates for missing data. the deep learning technique is used to extract features from the input data via an unsupervised learning approach by modeling the data distribution based on the input. this deep learning technique is then used as part of the objective function for the swarm intelligence technique in order to estimate the missing data after a supervised fine-tuning phase by minimizing an error function based on the interrelationship and correlation between features in the dataset. the investigated methodology in this paper therefore has longer running times, however, the promising potential outcomes justify the trade-off. also, basic knowledge of statistics is presumed.","","2016-07-01","","['collins leke', 'tshilidzi marwala']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1504",1607.00515,"the multiple quantile graphical model","stat.me","we introduce the multiple quantile graphical model (mqgm), which extends the neighborhood selection approach of meinshausen and buhlmann for learning sparse graphical models. the latter is defined by the basic subproblem of modeling the conditional mean of one variable as a sparse function of all others. our approach models a set of conditional quantiles of one variable as a sparse function of all others, and hence offers a much richer, more expressive class of conditional distribution estimates. we establish that, under suitable regularity conditions, the mqgm identifies the exact conditional independencies with probability tending to one as the problem size grows, even outside of the usual homoskedastic gaussian data model. we develop an efficient algorithm for fitting the mqgm using the alternating direction method of multipliers. we also describe a strategy for sampling from the joint distribution that underlies the mqgm estimate. lastly, we present detailed experiments that demonstrate the flexibility and effectiveness of the mqgm in modeling hetereoskedastic non-gaussian data.","","2016-07-02","2016-10-27","['alnur ali', 'j. zico kolter', 'ryan j. tibshirani']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1505",1607.01367,"a tutorial on regularized partial correlation networks","stat.ap stat.me","recent years have seen an emergence of network modeling applied to moods, attitudes, and problems in the realm of psychology. in this framework, psychological variables are understood to directly affect each other rather than being caused by an unobserved latent entity. in this tutorial, we introduce the reader to estimating the most popular network model for psychological data: the partial correlation network. we describe how regularization techniques can be used to efficiently estimate a parsimonious and interpretable network structure in psychological data. we show how to perform these analyses in r and demonstrate the method in an empirical example on post-traumatic stress disorder data. in addition, we discuss the effect of the hyperparameter that needs to be manually set by the researcher, how to handle non-normal data, how to determine the required sample size for a network analysis, and provide a checklist with potential solutions for problems that can arise when estimating regularized partial correlation networks.","10.1037/met0000167","2016-07-05","2017-12-01","['sacha epskamp', 'eiko i. fried']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1506",1607.01624,"bayesian nonparametrics for sparse dynamic networks","stat.ml","we propose a bayesian nonparametric prior for time-varying networks. to each node of the network is associated a positive parameter, modeling the sociability of that node. sociabilities are assumed to evolve over time, and are modeled via a dynamic point process model. the model is able to (a) capture smooth evolution of the interaction between nodes, allowing edges to appear/disappear over time (b) capture long term evolution of the sociabilities of the nodes (c) and yield sparse graphs, where the number of edges grows subquadratically with the number of nodes. the evolution of the sociabilities is described by a tractable time-varying gamma process. we provide some theoretical insights into the model and apply it to three real world datasets.","","2016-07-06","","['konstantina palla', 'francois caron', 'yee whye teh']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1507",1607.01664,"personalized optimization for computer experiments with environmental   inputs","stat.co","optimization problems with both control variables and environmental variables arise in many fields. this paper introduces a framework of personalized optimization to han- dle such problems. unlike traditional robust optimization, personalized optimization devotes to finding a series of optimal control variables for different values of environmental variables. therefore, the solution from personalized optimization consists of optimal surfaces defined on the domain of the environmental variables. when the environmental variables can be observed or measured, personalized optimization yields more reasonable and better solution- s than robust optimization. the implementation of personalized optimization for complex computer models is discussed. based on statistical modeling of computer experiments, we provide two algorithms to sequentially design input values for approximating the optimal surfaces. numerical examples show the effectiveness of our algorithms.","","2016-07-06","","['shifeng xiong']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1508",1607.01668,"tensor decomposition for signal processing and machine learning","stat.ml cs.lg cs.na math.na","tensors or {\em multi-way arrays} are functions of three or more indices $(i,j,k,\cdots)$ -- similar to matrices (two-way arrays), which are functions of two indices $(r,c)$ for (row,column). tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining and machine learning. this overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. as such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth {\em and depth} that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. some background in applied optimization is useful but not strictly required. the material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning.","10.1109/tsp.2017.2690524","2016-07-06","2016-12-14","['nicholas d. sidiropoulos', 'lieven de lathauwer', 'xiao fu', 'kejun huang', 'evangelos e. papalexakis', 'christos faloutsos']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1509",1607.01718,"graphons, mergeons, and so on!","stat.ml cs.ds math.st stat.th","in this work we develop a theory of hierarchical clustering for graphs. our modeling assumption is that graphs are sampled from a graphon, which is a powerful and general model for generating graphs and analyzing large networks. graphons are a far richer class of graph models than stochastic blockmodels, the primary setting for recent progress in the statistical theory of graph clustering. we define what it means for an algorithm to produce the ""correct"" clustering, give sufficient conditions in which a method is statistically consistent, and provide an explicit algorithm satisfying these properties.","","2016-07-06","2017-05-22","['justin eldridge', 'mikhail belkin', 'yusu wang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"1510",1607.02566,"robust causal inference with continuous instruments using the local   instrumental variable curve","stat.me","instrumental variables are commonly used to estimate effects of a treatment afflicted by unmeasured confounding, and in practice instruments are often continuous (e.g., measures of distance, or treatment preference). however, available methods for continuous instruments have important limitations: they either require restrictive parametric assumptions for identification, or else rely on modeling both the outcome and treatment process well (and require modeling effect modification by all adjustment covariates). in this work we develop the first semiparametric doubly robust estimators of the local instrumental variable effect curve, i.e., the effect among those who would take treatment for instrument values above some threshold and not below. in addition to being robust to misspecification of either the instrument or treatment/outcome processes, our approach also incorporates information about the instrument mechanism and allows for flexible data-adaptive estimation of effect modification. we discuss asymptotic properties under weak conditions, and use the methods to study infant mortality effects of neonatal intensive care units with high versus low technical capacity, using travel time as an instrument.","","2016-07-09","2018-07-04","['edward h. kennedy', 'scott a. lorch', 'dylan s. small']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1511",1607.02802,"mapping distributional to model-theoretic semantic spaces: a baseline","cs.cl cs.ai stat.ml","word embeddings have been shown to be useful across state-of-the-art systems in many natural language processing tasks, ranging from question answering systems to dependency parsing. (herbelot and vecchi, 2015) explored word embeddings and their utility for modeling language semantics. in particular, they presented an approach to automatically map a standard distributional semantic space onto a set-theoretic model using partial least squares regression. we show in this paper that a simple baseline achieves a +51% relative improvement compared to their model on one of the two datasets they used, and yields competitive results on the second dataset.","","2016-07-10","","['franck dernoncourt']",1,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1512",1607.03026,"retrospective causal inference with machine learning ensembles: an   application to anti-recidivism policies in colombia","stat.ml","we present new methods to estimate causal effects retrospectively from micro data with the assistance of a machine learning ensemble. this approach overcomes two important limitations in conventional methods like regression modeling or matching: (i) ambiguity about the pertinent retrospective counterfactuals and (ii) potential misspecification, overfitting, and otherwise bias-prone or inefficient use of a large identifying covariate set in the estimation of causal effects. our method targets the analysis toward a well defined ``retrospective intervention effect'' (rie) based on hypothetical population interventions and applies a machine learning ensemble that allows data to guide us, in a controlled fashion, on how to use a large identifying covariate set. we illustrate with an analysis of policy options for reducing ex-combatant recidivism in colombia.","","2016-07-11","","['cyrus samii', 'laura paler', 'sarah zukerman daly']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1513",1607.03202,"rapid prediction of player retention in free-to-play mobile games","stat.ml cs.si stat.ap","predicting and improving player retention is crucial to the success of mobile free-to-play games. this paper explores the problem of rapid retention prediction in this context. heuristic modeling approaches are introduced as a way of building simple rules for predicting short-term retention. compared to common classification algorithms, our heuristic-based approach achieves reasonable and comparable performance using information from the first session, day, and week of player activity.","","2016-07-11","","['anders drachen', 'eric thurston lundquist', 'yungjen kung', 'pranav simha rao', 'diego klabjan', 'rafet sifa', 'julian runge']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"1514",1607.03313,"predicting the evolution of stationary graph signals","stat.ml cs.lg","an emerging way of tackling the dimensionality issues arising in the modeling of a multivariate process is to assume that the inherent data structure can be captured by a graph. nevertheless, though state-of-the-art graph-based methods have been successful for many learning tasks, they do not consider time-evolving signals and thus are not suitable for prediction. based on the recently introduced joint stationarity framework for time-vertex processes, this letter considers multivariate models that exploit the graph topology so as to facilitate the prediction. the resulting method yields similar accuracy to the joint (time-graph) mean-squared error estimator but at lower complexity, and outperforms purely time-based methods.","","2016-07-12","","['andreas loukas', 'nathanael perraudin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1515",1607.03615,"multiple-instance logistic regression with lasso penalty","stat.ml stat.ap","in this work, we consider a manufactory process which can be described by a multiple-instance logistic regression model. in order to compute the maximum likelihood estimation of the unknown coefficient, an expectation-maximization algorithm is proposed, and the proposed modeling approach can be extended to identify the important covariates by adding the coefficient penalty term into the likelihood function. in addition to essential technical details, we demonstrate the usefulness of the proposed method by simulations and real examples.","","2016-07-13","","['ray-bing chen', 'kuang-hung cheng', 'sheng-mao chang', 'shuen-lin jeng', 'ping-yang chen', 'chun-hao yang', 'chi-chun hsia']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"1516",1607.05073,"higher-order block term decomposition for spatially folded fmri data","cs.na stat.ml","the growing use of neuroimaging technologies generates a massive amount of biomedical data that exhibit high dimensionality. tensor-based analysis of brain imaging data has been proved quite effective in exploiting their multiway nature. the advantages of tensorial methods over matrix-based approaches have also been demonstrated in the characterization of functional magnetic resonance imaging (fmri) data, where the spatial (voxel) dimensions are commonly grouped (unfolded) as a single way/mode of the 3-rd order array, the other two ways corresponding to time and subjects. however, such methods are known to be ineffective in more demanding scenarios, such as the ones with strong noise and/or significant overlapping of activated regions. this paper aims at investigating the possible gains from a better exploitation of the spatial dimension, through a higher- (4 or 5) order tensor modeling of the fmri signal. in this context, and in order to increase the degrees of freedom of the modeling process, a higher-order block term decomposition (btd) is applied, for the first time in fmri analysis. its effectiveness is demonstrated via extensive simulation results.","","2016-07-15","","['christos chatzichristos', 'eleftherios kofidis', 'giannis kopsinis', 'sergios theodoridis']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1517",1607.06011,"on the modeling of error functions as high dimensional landscapes for   weight initialization in learning networks","cs.lg cs.cv physics.data-an stat.ml","next generation deep neural networks for classification hosted on embedded platforms will rely on fast, efficient, and accurate learning algorithms. initialization of weights in learning networks has a great impact on the classification accuracy. in this paper we focus on deriving good initial weights by modeling the error function of a deep neural network as a high-dimensional landscape. we observe that due to the inherent complexity in its algebraic structure, such an error function may conform to general results of the statistics of large systems. to this end we apply some results from random matrix theory to analyse these functions. we model the error function in terms of a hamiltonian in n-dimensions and derive some theoretical results about its general behavior. these results are further used to make better initial guesses of weights for the learning algorithm.","","2016-07-20","","['n/a julius', 'gopinath mahale', 'sumana t.', 'c. s. adityakrishna']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1518",1607.0628,"explaining classification models built on high-dimensional sparse data","stat.ml cs.lg","predictive modeling applications increasingly use data representing people's behavior, opinions, and interactions. fine-grained behavior data often has different structure from traditional data, being very high-dimensional and sparse. models built from these data are quite difficult to interpret, since they contain many thousands or even many millions of features. listing features with large model coefficients is not sufficient, because the model coefficients do not incorporate information on feature presence, which is key when analysing sparse data. in this paper we introduce two alternatives for explaining predictive models by listing important features. we evaluate these alternatives in terms of explanation ""bang for the buck,"", i.e., how many examples' inferences are explained for a given number of features listed. the bottom line: (i) the proposed alternatives have double the bang-for-the-buck as compared to just listing the high-coefficient features, and (ii) interestingly, although they come from different sources and motivations, the two new alternatives provide strikingly similar rankings of important features.","","2016-07-21","2016-07-26","['julie moeyersoms', ""brian d'alessandro"", 'foster provost', 'david martens']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1519",1607.06333,"uncovering causality from multivariate hawkes integrated cumulants","stat.ml cs.lg","we design a new nonparametric method that allows one to estimate the matrix of integrated kernels of a multivariate hawkes process. this matrix not only encodes the mutual influences of each nodes of the process, but also disentangles the causality relationships between them. our approach is the first that leads to an estimation of this matrix without any parametric modeling and estimation of the kernels themselves. a consequence is that it can give an estimation of causality relationships between nodes (or users), based on their activity timestamps (on a social network for instance), without knowing or estimating the shape of the activities lifetime. for that purpose, we introduce a moment matching method that fits the third-order integrated cumulants of the process. we show on numerical experiments that our approach is indeed very robust to the shape of the kernels, and gives appealing results on the memetracker database.","","2016-07-21","2017-05-29","['massil achab', 'emmanuel bacry', 'stéphane gaïffas', 'iacopo mastromatteo', 'jean-francois muzy']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1520",1607.07515,"single stage prediction with embedded topic modeling of online reviews   for mobile app management","stat.ap cs.ir cs.se","mobile apps are one of the building blocks of the mobile digital economy. a differentiating feature of mobile apps to traditional enterprise software is online reviews, which are available on app marketplaces and represent a valuable source of consumer feedback on the app. we create a supervised topic modeling approach for app developers to use mobile reviews as useful sources of quality and customer feedback, thereby complementing traditional software testing. the approach is based on a constrained matrix factorization that leverages the relationship between term frequency and a given response variable in addition to co-occurrences between terms to recover topics that are both predictive of consumer sentiment and useful for understanding the underlying textual themes. the factorization is combined with ordinal regression to provide guidance from online reviews on a single app's performance as well as systematically compare different apps over time for benchmarking of features and consumer sentiment. we apply our approach using a dataset of over 100,000 mobile reviews over several years for three of the most popular online travel agent apps from the itunes and google play marketplaces.","","2016-07-25","2018-02-19","['shawn mankad', 'shengli hu', 'anandasivam gopal']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1521",1607.07521,"propensity score weighting for causal inference with multi-stage   clustered data","stat.me","propensity score weighting is a tool for causal inference to adjust for measured confounders. survey data are often collected under complex sampling designs such as multistage cluster sampling, which presents challenges for propensity score modeling and estimation. in addition, for clustered data, there may also be unobserved cluster effects related to both the treatment and the outcome. when such unmeasured confounders exist and are omitted in the propensity score model, the subsequent propensity score adjustment will be biased. we propose a calibrated propensity score weighting adjustment for multi-stage clustered data in the presence of unmeasured cluster-level confounders. the propensity score is calibrated to balance design-weighted covariate distributions and cluster effects between treatment groups. in particular, we consider a growing number of calibration constraints increasing with the number of clusters, which is necessary for removing asymptotic bias that is associated with the unobserved cluster-level confounders. we show that our estimator is robust in the sense that the estimator is consistent without correct specification of the propensity score model. we extend the results to the multiple treatments case. in simulation studies we show that the proposed estimator is superior to other competitors. we estimate the effect of school body mass index screening on prevalence of overweight and obesity for elementary schools in pennsylvania.","","2016-07-25","","['shu yang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1522",1607.07549,"concentration of measure for radial distributions and consequences for   statistical modeling","math.st stat.th","motivated by problems in high-dimensional statistics such as mixture modeling for classification and clustering, we consider the behavior of radial densities as the dimension increases. we establish a form of concentration of measure, and even a convergence in distribution, under additional assumptions. this extends the well-known behavior of the normal distribution (its concentration around the sphere of radius square-root of the dimension) to other radial densities. we draw some possible consequences for statistical modeling in high-dimensions, including a possible universality property of gaussian mixtures.","","2016-07-26","2016-09-11","['ery arias-castro', 'xiao pu']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1523",1607.07573,"variational mixture models with gamma or inverse-gamma components","stat.ml","mixture models with gamma and or inverse-gamma distributed mixture components are useful for medical image tissue segmentation or as post-hoc models for regression coefficients obtained from linear regression within a generalised linear modeling framework (glm), used in this case to separate stochastic (gaussian) noise from some kind of positive or negative ""activation"" (modeled as gamma or inverse-gamma distributed). to date, the most common choice in this context it is gaussian/gamma mixture models learned through a maximum likelihood (ml) approach; we recently extended such algorithm for mixture models with inverse-gamma components. here, we introduce a fully analytical variational bayes (vb) learning framework for both gamma and/or inverse-gamma components. we use synthetic and resting state fmri data to compare the performance of the ml and vb algorithms in terms of area under the curve and computational cost. we observed that the ml gaussian/gamma model is very expensive specially when considering high resolution images; furthermore, these solutions are highly variable and they occasionally can overestimate the activations severely. the bayesian gauss-gamma is in general the fastest algorithm but provides too dense solutions. the maximum likelihood gaussian/inverse-gamma is also very fast but provides in general very sparse solutions. the variational gaussian/inverse-gamma mixture model is the most robust and its cost is acceptable even for high resolution images. further, the presented methodology represents an essential building block that can be directly used in more complex inference tasks, specially designed to analyse mri-fmri data; such models include for example analytical variational mixture models with adaptive spatial regularization or better source models for new spatial blind source separation approaches.","","2016-07-26","","['a. llera', 'd. vidaurre', 'r. h. r. pruim', 'c. f. beckmann']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1524",1607.08033,"efficient nonparametric estimation and inference for the volatility   function","stat.me","during the last decades there has been increasing interest in modeling the volatility of financial data. several parametric models have been proposed to this aim, starting from arch, garch and their variants, but often it is hard to evaluate which one is the most suitable for the analyzed financial data. in this paper we focus on nonparametric analysis of the volatility function for mixing processes. our approach encompasses many parametric frameworks and supplies several tools which can be used to give evidence against or in favor of a specific parametric model: nonparametric function estimation, confidence bands and test for symmetry. another contribution of this paper is to give an alternative representation of the garch(1,1) model in terms of a nonparametric-arch(1) model, which avoids the use of the lagged volatility, so that a more precise and more informative news impact function can be estimated by our procedure. we prove the consistency of the proposed method and investigate its empirical performance on synthetic and real datasets. surprisingly, for finite sample size, the simulation results show a better performance of our nonparametric estimator compared with the mle estimator of a garch(1,1) model, even in the case of correct specification of the model.","","2016-07-27","","['francesco giordano', 'maria lucia parrella']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1525",1608.00092,"deepsoft: a vision for a deep model of software","cs.se stat.ml","although software analytics has experienced rapid growth as a research area, it has not yet reached its full potential for wide industrial adoption. most of the existing work in software analytics still relies heavily on costly manual feature engineering processes, and they mainly address the traditional classification problems, as opposed to predicting future events. we present a vision for \emph{deepsoft}, an \emph{end-to-end} generic framework for modeling software and its development process to predict future risks and recommend interventions. deepsoft, partly inspired by human memory, is built upon the powerful deep learning-based long short term memory architecture that is capable of learning long-term temporal dependencies that occur in software evolution. such deep learned patterns of software can be used to address a range of challenging problems such as code and task recommendation and prediction. deepsoft provides a new approach for research into modeling of source code, risk prediction and mitigation, developer modeling, and automatically generating code patches from bug reports.","","2016-07-30","","['hoa khanh dam', 'truyen tran', 'john grundy', 'aditya ghose']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1526",1608.00218,"hyperparameter transfer learning through surrogate alignment for   efficient deep neural network training","cs.lg cs.cv cs.ne stat.ml","recently, several optimization methods have been successfully applied to the hyperparameter optimization of deep neural networks (dnns). the methods work by modeling the joint distribution of hyperparameter values and corresponding error. those methods become less practical when applied to modern dnns whose training may take a few days and thus one cannot collect sufficient observations to accurately model the distribution. to address this challenging issue, we propose a method that learns to transfer optimal hyperparameter values for a small source dataset to hyperparameter values with comparable performance on a dataset of interest. as opposed to existing transfer learning methods, our proposed method does not use hand-designed features. instead, it uses surrogates to model the hyperparameter-error distributions of the two datasets and trains a neural network to learn the transfer function. extensive experiments on three cv benchmark datasets clearly demonstrate the efficiency of our method.","","2016-07-31","","['ilija ilievski', 'jiashi feng']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1527",1608.00264,"frequency of frequencies distributions and size dependent exchangeable   random partitions","stat.me math.st stat.ap stat.th","motivated by the fundamental problem of modeling the frequency of frequencies (fof) distribution, this paper introduces the concept of a cluster structure to define a probability function that governs the joint distribution of a random count and its exchangeable random partitions. a cluster structure, naturally arising from a completely random measure mixed poisson process, allows the probability distribution of the random partitions of a subset of a population to be dependent on the population size, a distinct and motivated feature that makes it more flexible than a partition structure. this allows it to model an entire fof distribution whose structural properties change as the population size varies. a fof vector can be simulated by drawing an infinite number of poisson random variables, or by a stick-breaking construction with a finite random number of steps. a generalized negative binomial process model is proposed to generate a cluster structure, where in the prior the number of clusters is finite and poisson distributed, and the cluster sizes follow a truncated negative binomial distribution. we propose a simple gibbs sampling algorithm to extrapolate the fof vector of a population given the fof vector of a sample taken without replacement from the population. we illustrate our results and demonstrate the advantages of the proposed models through the analysis of real text, genomic, and survey data.","","2016-07-31","","['mingyuan zhou', 'stefano favaro', 'stephen g walker']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE
"1528",1608.00621,"efficient multiple incremental computation for kernel ridge regression   with bayesian uncertainty modeling","cs.lg stat.ml","this study presents an efficient incremental/decremental approach for big streams based on kernel ridge regression (krr), a frequently used data analysis in cloud centers. to avoid reanalyzing the whole dataset whenever sensors receive new training data, typical incremental krr used a single-instance mechanism for updating an existing system. however, this inevitably increased redundant computational time, not to mention applicability to big streams. to this end, the proposed mechanism supports incremental/decremental processing for both single and multiple samples (i.e., batch processing). a large scale of data can be divided into batches, processed by a machine, without sacrificing the accuracy. moreover, incremental/decremental analyses in empirical and intrinsic space are also proposed in this study to handle different types of data either with a large number of samples or high feature dimensions, whereas typical methods focused only on one type. at the end of this study, we further the proposed mechanism to statistical kernelized bayesian regression, so that uncertainty modeling with incremental/decremental computation becomes applicable. experimental results showed that computational time was significantly reduced, better than the original nonincremental design and the typical single incremental method. furthermore, the accuracy of the proposed method remained the same as the baselines. this implied that the system enhanced efficiency without sacrificing the accuracy. these findings proved that the proposed method was appropriate for variable streaming data analysis, thereby demonstrating the effectiveness of the proposed method.","10.1016/j.future.2017.08.053","2016-08-01","2017-11-08","['bo-wei chen', 'nik nailah binti abdullah', 'sangoh park']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1529",1608.00629,"sparsity oriented importance learning for high-dimensional linear   regression","stat.me","with now well-recognized non-negligible model selection uncertainty, data analysts should no longer be satisfied with the output of a single final model from a model selection process, regardless of its sophistication. to improve reliability and reproducibility in model choice, one constructive approach is to make good use of a sound variable importance measure. although interesting importance measures are available and increasingly used in data analysis, little theoretical justification has been done. in this paper, we propose a new variable importance measure, sparsity oriented importance learning (soil), for high-dimensional regression from a sparse linear modeling perspective by taking into account the variable selection uncertainty via the use of a sensible model weighting. the soil method is theoretically shown to have the inclusion/exclusion property: when the model weights are properly around the true model, the soil importance can well separate the variables in the true model from the rest. in particular, even if the signal is weak, soil rarely gives variables not in the true model significantly higher important values than those in the true model. extensive simulations in several illustrative settings and real data examples with guided simulations show desirable properties of the soil importance in contrast to other importance measures.","","2016-08-01","","['chenglong ye', 'yi yang', 'yuhong yang']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1530",1608.00778,"exponential family embeddings","stat.ml cs.lg","word embeddings are a powerful approach for capturing semantic similarity among terms in a vocabulary. in this paper, we develop exponential family embeddings, a class of methods that extends the idea of word embeddings to other types of high-dimensional data. as examples, we studied neural data with real-valued observations, count data from a market basket analysis, and ratings data from a movie recommendation system. the main idea is to model each observation conditioned on a set of other observations. this set is called the context, and the way the context is defined is a modeling choice that depends on the problem. in language the context is the surrounding words; in neuroscience the context is close-by neurons; in market basket data the context is other items in the shopping cart. each type of embedding model defines the context, the exponential family of conditional distributions, and how the latent embedding vectors are shared across data. we infer the embeddings with a scalable algorithm based on stochastic gradient descent. on all three applications - neural activity of zebrafish, users' shopping behavior, and movie ratings - we found exponential family embedding models to be more effective than other types of dimension reduction. they better reconstruct held-out data and find interesting qualitative structure.","","2016-08-02","2016-11-21","['maja r. rudolph', 'francisco j. r. ruiz', 'stephan mandt', 'david m. blei']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1531",1608.01264,"fast and simple optimization for poisson likelihood models","cs.lg math.oc stat.ml","poisson likelihood models have been prevalently used in imaging, social networks, and time series analysis. we propose fast, simple, theoretically-grounded, and versatile, optimization algorithms for poisson likelihood modeling. the poisson log-likelihood is concave but not lipschitz-continuous. since almost all gradient-based optimization algorithms rely on lipschitz-continuity, optimizing poisson likelihood models with a guarantee of convergence can be challenging, especially for large-scale problems.   we present a new perspective allowing to efficiently optimize a wide range of penalized poisson likelihood objectives. we show that an appropriate saddle point reformulation enjoys a favorable geometry and a smooth structure. therefore, we can design a new gradient-based optimization algorithm with $o(1/t)$ convergence rate, in contrast to the usual $o(1/\sqrt{t})$ rate of non-smooth minimization alternatives. furthermore, in order to tackle problems with large samples, we also develop a randomized block-decomposition variant that enjoys the same convergence rate yet more efficient iteration cost.   experimental results on several point process applications including social network estimation and temporal recommendation show that the proposed algorithm and its randomized block variant outperform existing methods both on synthetic and real-world datasets.","","2016-08-03","","['niao he', 'zaid harchaoui', 'yichen wang', 'le song']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1532",1608.01958,"iterative importance sampling algorithms for parameter estimation","math.na stat.co","in parameter estimation problems one computes a posterior distribution over uncertain parameters defined jointly by a prior distribution, a model, and noisy data. markov chain monte carlo (mcmc) is often used for the numerical solution of such problems. an alternative to mcmc is importance sampling, which can exhibit near perfect scaling with the number of cores on high performance computing systems because samples are drawn independently. however, finding a suitable proposal distribution is a challenging task. several sampling algorithms have been proposed over the past years that take an iterative approach to constructing a proposal distribution. we investigate the applicability of such algorithms by applying them to two realistic and challenging test problems, one in subsurface flow, and one in combustion modeling. more specifically, we implement importance sampling algorithms that iterate over the mean and covariance matrix of gaussian or multivariate t-proposal distributions. our implementation leverages massively parallel computers, and we present strategies to initialize the iterations using ""coarse"" mcmc runs or gaussian mixture models.","","2016-08-05","2017-11-14","['matthias morzfeld', 'marcus s. day', 'ray w. grout', 'george shu heng pau', 'stefan a. finsterle', 'john b. bell']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1533",1608.02715,"a deep language model for software code","cs.se stat.ml","existing language models such as n-grams for software code often fail to capture a long context where dependent code elements scatter far apart. in this paper, we propose a novel approach to build a language model for software code to address this particular issue. our language model, partly inspired by human memory, is built upon the powerful deep learning-based long short term memory architecture that is capable of learning long-term dependencies which occur frequently in software code. results from our intrinsic evaluation on a corpus of java projects have demonstrated the effectiveness of our language model. this work contributes to realizing our vision for deepsoft, an end-to-end, generic deep learning-based framework for modeling software and its development process.","","2016-08-09","","['hoa khanh dam', 'truyen tran', 'trang pham']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1534",1608.03023,"stochastic rank-1 bandits","cs.lg stat.ml","we propose stochastic rank-$1$ bandits, a class of online learning problems where at each step a learning agent chooses a pair of row and column arms, and receives the product of their values as a reward. the main challenge of the problem is that the individual values of the row and column are unobserved. we assume that these values are stochastic and drawn independently. we propose a computationally-efficient algorithm for solving our problem, which we call rank1elim. we derive a $o((k + l) (1 / \delta) \log n)$ upper bound on its $n$-step regret, where $k$ is the number of rows, $l$ is the number of columns, and $\delta$ is the minimum of the row and column gaps; under the assumption that the mean row and column rewards are bounded away from zero. to the best of our knowledge, we present the first bandit algorithm that finds the maximum entry of a rank-$1$ matrix whose regret is linear in $k + l$, $1 / \delta$, and $\log n$. we also derive a nearly matching lower bound. finally, we evaluate rank1elim empirically on multiple problems. we observe that it leverages the structure of our problems and can learn near-optimal solutions even if our modeling assumptions are mildly violated.","","2016-08-09","2017-03-08","['sumeet katariya', 'branislav kveton', 'csaba szepesvari', 'claire vernade', 'zheng wen']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1535",1608.03191,"efficient model-based clustering with coalescents: application to   multiple outcomes using medical records data","stat.ap","we present a sequential monte carlo sampler for coalescent based bayesian hierarchical clustering. the model is appropriate for multivariate non-\iid data and our approach offers a substantial reduction in computational cost when compared to the original sampler. we also propose a quadratic complexity approximation that in practice shows almost no loss in performance compared to its counterpart. our formulation leads to a greedy algorithm that exhibits performance improvement over other greedy algorithms, particularly in small data sets. we incorporate the coalescent into a hierarchical regression model that allows joint modeling of multiple correlated outcomes. the approach does not require {\em a priori} knowledge of either the degree or structure of the correlation and, as a byproduct, generates additional models for a subset of the composite outcomes. we demonstrate the utility of the approach by predicting multiple different types of outcomes using medical records data from a cohort of diabetic patients.","","2016-08-10","","['ricardo henao', 'joseph e. lucas']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1536",1608.03333,"temporal learning and sequence modeling for a job recommender system","cs.lg stat.ml","we present our solution to the job recommendation task for recsys challenge 2016. the main contribution of our work is to combine temporal learning with sequence modeling to capture complex user-item activity patterns to improve job recommendations. first, we propose a time-based ranking model applied to historical observations and a hybrid matrix factorization over time re-weighted interactions. second, we exploit sequence properties in user-items activities and develop a rnn-based recommendation model. our solution achieved 5$^{th}$ place in the challenge among more than 100 participants. notably, the strong performance of our rnn approach shows a promising new direction in employing sequence modeling for recommendation systems.","10.1145/2987538.2987540","2016-08-10","","['kuan liu', 'xing shi', 'anoop kumar', 'linhong zhu', 'prem natarajan']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1537",1608.03769,"spatial modeling, with application to complex survey data: discussion of   ""model-based geostatistics for prevalence mapping in low-resource settings"",   by diggle and giorgi","stat.me","prevalence mapping in low resource settings is an increasingly important endeavor to guide policy making and to spatially and temporally characterize the burden of disease. we will focus our discussion on consideration of the complex design when analyzing survey data, and on spatial modeling. with respect to the former, we consider two approaches: direct use of the weights, and a model-based approach using a spatial model to acknowledge clustering. for the latter we consider continuously indexed markovian gaussian random field models.","","2016-08-12","","['jon wakefield', 'daniel simpson', 'jessica godwin']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1538",1608.04236,"generative and discriminative voxel modeling with convolutional neural   networks","cs.cv cs.hc cs.lg stat.ml","when working with three-dimensional data, choice of representation is key. we explore voxel-based models, and present evidence for the viability of voxellated representations in applications including shape modeling and object classification. our key contributions are methods for training voxel-based variational autoencoders, a user interface for exploring the latent space learned by the autoencoder, and a deep convolutional neural network architecture for object classification. we address challenges unique to voxel-based representations, and empirically evaluate our models on the modelnet benchmark, where we demonstrate a 51.5% relative improvement in the state of the art for object classification.","","2016-08-15","2016-08-16","['andrew brock', 'theodore lim', 'j. m. ritchie', 'nick weston']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1539",1608.04331,"consistency constraints for overlapping data clustering","cs.lg stat.ml","we examine overlapping clustering schemes with functorial constraints, in the spirit of carlsson--memoli. this avoids issues arising from the chaining required by partition-based methods. our principal result shows that any clustering functor is naturally constrained to refine single-linkage clusters and be refined by maximal-linkage clusters. we work in the context of metric spaces with non-expansive maps, which is appropriate for modeling data processing which does not increase information content.","","2016-08-15","","['jared culbertson', 'dan p. guralnik', 'jakob hansen', 'peter f. stiller']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1540",1608.04664,"variational gaussian process auto-encoder for ordinal prediction of   facial action units","stat.ml cs.cv","we address the task of simultaneous feature fusion and modeling of discrete ordinal outputs. we propose a novel gaussian process(gp) auto-encoder modeling approach. in particular, we introduce gp encoders to project multiple observed features onto a latent space, while gp decoders are responsible for reconstructing the original features. inference is performed in a novel variational framework, where the recovered latent representations are further constrained by the ordinal output labels. in this way, we seamlessly integrate the ordinal structure in the learned manifold, while attaining robust fusion of the input features. we demonstrate the representation abilities of our model on benchmark datasets from machine learning and affect analysis. we further evaluate the model on the tasks of feature fusion and joint ordinal prediction of facial action units. our experiments demonstrate the benefits of the proposed approach compared to the state of the art.","","2016-08-16","2016-09-05","['stefanos eleftheriadis', 'ognjen rudovic', 'marc p. deisenroth', 'maja pantic']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1541",1608.04972,"a three spatial dimension wave latent force model for describing   excitation sources and electric potentials produced by deep brain stimulation","q-bio.nc stat.ml","deep brain stimulation (dbs) is a surgical treatment for parkinson's disease. static models based on quasi-static approximation are common approaches for dbs modeling. while this simplification has been validated for bioelectric sources, its application to rapid stimulation pulses, which contain more high-frequency power, may not be appropriate, as dbs therapeutic results depend on stimulus parameters such as frequency and pulse width, which are related to time variations of the electric field. we propose an alternative hybrid approach based on probabilistic models and differential equations, by using gaussian processes and wave equation. our model avoids quasi-static approximation, moreover, it is able to describe dynamic behavior of dbs. therefore, the proposed model may be used to obtain a more realistic phenomenon description. the proposed model can also solve inverse problems, i.e. to recover the corresponding source of excitation, given electric potential distribution. the electric potential produced by a time-varying source was predicted using proposed model. for static sources, the electric potential produced by different electrode configurations were modeled. four different sources of excitation were recovered by solving the inverse problem. we compare our outcomes with the electric potential obtained by solving poisson's equation using the finite element method (fem). our approach is able to take into account time variations of the source and the produced field. also, inverse problem can be addressed using the proposed model. the electric potential calculated with the proposed model is close to the potential obtained by solving poisson's equation using fem.","","2016-08-17","","['pablo a. alvarado', 'mauricio a. álvarez', 'álvaro a. orozco']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1542",1608.05142,"generic inference on quantile and quantile effect functions for discrete   outcomes","stat.me econ.em","quantile and quantile effect functions are important tools for descriptive and causal analyses due to their natural and intuitive interpretation. existing inference methods for these functions do not apply to discrete random variables. this paper offers a simple, practical construction of simultaneous confidence bands for quantile and quantile effect functions of possibly discrete random variables. it is based on a natural transformation of simultaneous confidence bands for distribution functions, which are readily available for many problems. the construction is generic and does not depend on the nature of the underlying problem. it works in conjunction with parametric, semiparametric, and nonparametric modeling methods for observed and counterfactual distributions, and does not depend on the sampling scheme. we apply our method to characterize the distributional impact of insurance coverage on health care utilization and obtain the distributional decomposition of the racial test score gap. we find that universal insurance coverage increases the number of doctor visits across the entire distribution, and that the racial test score gap is small at early ages but grows with age due to socio economic factors affecting child development especially at the top of the distribution. these are new, interesting empirical findings that complement previous analyses that focused on mean effects only. in both applications, the outcomes of interest are discrete rendering existing inference methods invalid for obtaining uniform confidence bands for observed and counterfactual quantile functions and for their difference -- the quantile effects functions.","","2016-08-17","2018-08-30","['victor chernozhukov', 'iván fernández-val', 'blaise melly', 'kaspar wüthrich']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1543",1608.05347,"probabilistic data analysis with probabilistic programming","cs.ai cs.lg stat.ml","probabilistic techniques are central to data analysis, but different approaches can be difficult to apply, combine, and compare. this paper introduces composable generative population models (cgpms), a computational abstraction that extends directed graphical models and can be used to describe and compose a broad class of probabilistic data analysis techniques. examples include hierarchical bayesian models, multivariate kernel methods, discriminative machine learning, clustering algorithms, dimensionality reduction, and arbitrary probabilistic programs. we also demonstrate the integration of cgpms into bayesdb, a probabilistic programming platform that can express data analysis tasks using a modeling language and a structured query language. the practical value is illustrated in two ways. first, cgpms are used in an analysis that identifies satellite data records which probably violate kepler's third law, by composing causal probabilistic programs with non-parametric bayes in under 50 lines of probabilistic code. second, for several representative data analysis tasks, we report on lines of code and accuracy measurements of various cgpms, plus comparisons with standard baseline solutions from python and matlab libraries.","","2016-08-18","","['feras saad', 'vikash mansinghka']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1544",1608.05493,"network volume anomaly detection and identification in large-scale   networks based on online time-structured traffic tensor tracking","cs.ni stat.ml","this paper addresses network anomography, that is, the problem of inferring network-level anomalies from indirect link measurements. this problem is cast as a low-rank subspace tracking problem for normal flows under incomplete observations, and an outlier detection problem for abnormal flows. since traffic data is large-scale time-structured data accompanied with noise and outliers under partial observations, an efficient modeling method is essential. to this end, this paper proposes an online subspace tracking of a hankelized time-structured traffic tensor for normal flows based on the candecomp/parafac decomposition exploiting the recursive least squares (rls) algorithm. we estimate abnormal flows as outlier sparse flows via sparsity maximization in the underlying under-constrained linear-inverse problem. a major advantage is that our algorithm estimates normal flows by low-dimensional matrices with time-directional features as well as the spatial correlation of multiple links without using the past observed measurements and the past model parameters. extensive numerical evaluations show that the proposed algorithm achieves faster convergence per iteration of model approximation, and better volume anomaly detection performance compared to state-of-the-art algorithms.","10.1109/tnsm.2016.2598788","2016-08-19","","['hiroyuki kasai', 'wolfgang kellerer', 'martin kleinsteuber']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1545",1608.05987,"the kumaraswamy generalized marshall-olkin-g family of distributions","math.st stat.th","another new family of continuous probability distribution is proposed by using generalized marshal-olkin distribution as the base line distribution in the kumaraswamy-g distribution. this family includes (cordeiro and de castro, 2011) and (jayakumar and mathew, 2008) families special case besides a under of other distributions. the probability density function (pdf) and the survival function (sf) are expressed as series to observe as a mixture of the generalized marshal-olkin distribution. series expansions pdf of order statistics are also obtained. moments, moment generating function, r\'enyi entropies, quantile function, random sample generation and asymptotes are also investigated. parameter estimation by method of maximum likelihood and method of moment are also presented. finally the proposed model is compared to the generalized marshall-olkin kumaraswamy extended family (handique and chakraborty, 2015) by considering four examples of real life data modeling.","","2016-08-21","","['laba handique', 'subrata chakraborty']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1546",1608.06238,"single-shot adaptive measurement for quantum-enhanced metrology","quant-ph stat.ml","quantum-enhanced metrology aims to estimate an unknown parameter such that the precision scales better than the shot-noise bound. single-shot adaptive quantum-enhanced metrology (aqem) is a promising approach that uses feedback to tweak the quantum process according to previous measurement outcomes. techniques and formalism for the adaptive case are quite different from the usual non-adaptive quantum metrology approach due to the causal relationship between measurements and outcomes. we construct a formal framework for aqem by modeling the procedure as a decision-making process, and we derive the imprecision and the cram\'{e}r-rao lower bound with explicit dependence on the feedback policy. we also explain the reinforcement learning approach for generating quantum control policies, which is adopted due to the optimal policy being non-trivial to devise. applying a learning algorithm based on differential evolution enables us to attain imprecision for adaptive interferometric phase estimation, which turns out to be sql when non-entangled particles are used in the scheme.","10.1117/12.2237355","2016-08-22","","['pantita palittapongarnpim', 'peter wittek', 'barry c. sanders']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1547",1608.06663,"efficient posterior inference on the volatility of a jump diffusion   process","stat.me math.pr","jump diffusion processes are widely used to model asset prices over time, mainly for their ability to capture complex discontinuous behavior, but inference on the model parameters remains a challenge. here our goal is posterior inference on the volatility coefficient of the diffusion part of the process based on discrete samples. a bayesian approach requires specification of a model for the jump part of the process, prior distributions for the corresponding parameters, and computation of the joint posterior. since the volatility coefficient is our only interest, it would be desirable to avoid the modeling and computational costs associated with the jump part of the process. towards this, we consider a {\em purposely misspecified model} that ignores the jump part entirely. we work out precisely the asymptotic behavior of the bayesian posterior under the misspecified model, propose some simple modifications to correct for the effects of misspecification, and demonstrate that our modified posterior inference on the volatility is efficient in the sense that its asymptotic variance equals the no-jumps model cram\'er--rao bound.","","2016-08-23","2017-02-21","['ryan martin', 'cheng ouyang', 'francois domagni']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1548",1608.07618,"multiresolution network models","stat.me stat.ap","many existing statistical and machine learning tools for social network analysis focus on a single level of analysis. methods designed for clustering optimize a global partition of the graph, whereas projection based approaches (e.g. the latent space model in the statistics literature) represent in rich detail the roles of individuals. many pertinent questions in sociology and economics, however, span multiple scales of analysis. further, many questions involve comparisons across disconnected graphs that will, inevitably be of different sizes, either due to missing data or the inherent heterogeneity in real-world networks. we propose a class of network models that represent network structure on multiple scales and facilitate comparison across graphs with different numbers of individuals. these models differentially invest modeling effort within subgraphs of high density, often termed communities, while maintaining a parsimonious structure between said subgraphs. we show that our model class is projective, highlighting an ongoing discussion in the social network modeling literature on the dependence of inference paradigms on the size of the observed graph. we illustrate the utility of our method using data on household relations from karnataka, india.","","2016-08-26","2018-07-05","['bailey k. fosdick', 'tyler h. mccormick', 'thomas brendan murphy', 'tin lok james ng', 'ted westling']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1549",1608.07636,"learning temporal dependence from time-series data with latent variables","cs.lg stat.ml","we consider the setting where a collection of time series, modeled as random processes, evolve in a causal manner, and one is interested in learning the graph governing the relationships of these processes. a special case of wide interest and applicability is the setting where the noise is gaussian and relationships are markov and linear. we study this setting with two additional features: firstly, each random process has a hidden (latent) state, which we use to model the internal memory possessed by the variables (similar to hidden markov models). secondly, each variable can depend on its latent memory state through a random lag (rather than a fixed lag), thus modeling memory recall with differing lags at distinct times. under this setting, we develop an estimator and prove that under a genericity assumption, the parameters of the model can be learned consistently. we also propose a practical adaption of this estimator, which demonstrates significant performance gains in both synthetic and real-world datasets.","","2016-08-26","","['hossein hosseini', 'sreeram kannan', 'baosen zhang', 'radha poovendran']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1550",1608.07839,"non-linear wavelet regression and branch & bound optimization for the   full identification of bivariate operator fractional brownian motion","math.pr math.st stat.th","self-similarity is widely considered the reference framework for modeling the scaling properties of real-world data. however, most theoretical studies and their practical use have remained univariate. operator fractional brownian motion (ofbm) was recently proposed as a multivariate model for self-similarity. yet it has remained seldom used in applications because of serious issues that appear in the joint estimation of its numerous parameters. while the univariate fractional brownian motion requires the estimation of two parameters only, its mere bivariate extension already involves 7 parameters which are very different in nature. the present contribution proposes a method for the full identification of bivariate ofbm (i.e., the joint estimation of all parameters) through an original formulation as a non-linear wavelet regression coupled with a custom-made branch & bound numerical scheme. the estimation performance (consistency and asymptotic normality) is mathematically established and numerically assessed by means of monte carlo experiments. the impact of the parameters defining ofbm on the estimation performance as well as the associated computational costs are also thoroughly investigated.","10.1109/tsp.2016.2551695","2016-08-28","","['jordan frecon', 'gustavo didier', 'nelly pustelnik', 'patrice abry']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1551",1608.07934,"relevant based structure learning for feature selection","cs.lg stat.ml","feature selection is an important task in many problems occurring in pattern recognition, bioinformatics, machine learning and data mining applications. the feature selection approach enables us to reduce the computation burden and the falling accuracy effect of dealing with huge number of features in typical learning problems. there is a variety of techniques for feature selection in supervised learning problems based on different selection metrics. in this paper, we propose a novel unified framework for feature selection built on the graphical models and information theoretic tools. the proposed approach exploits the structure learning among features to select more relevant and less redundant features to the predictive modeling problem according to a primary novel likelihood based criterion. in line with the selection of the optimal subset of features through the proposed method, it provides us the bayesian network classifier without the additional cost of model training on the selected subset of features. the optimal properties of our method are established through empirical studies and computational complexity analysis. furthermore the proposed approach is evaluated on a bunch of benchmark datasets based on the well-known classification algorithms. extensive experiments confirm the significant improvement of the proposed approach compared to the earlier works.","10.1016/j.engappai.2016.06.001","2016-08-29","","['hadi zare', 'mojtaba niazi']",1,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1552",705.0304,"mod\'elisations prospectives de l'occupation du sol. le cas d'une   montagne m\'editerran\'eenne","stat.ap stat.me","the authors apply three methods of prospective modelling to high resolution georeferenced land cover data in a mediterranean mountain area: gis approach, non linear parametric model and neuronal network. land cover prediction to the latest known date is used to validate the models. in the frame of spatial-temporal dynamics in open systems results are encouraging and comparable. correct prediction scores are about 73 %. the results analysis focuses on geographic location, land cover categories and parametric distance to reality of the residues. crossing the three models show the high degree of convergence and a relative similitude of the results obtained by the two statistic approaches compared to the gis supervised model. steps under work are the application of the models to other test areas and the identification of respective advantages to develop an integrated model.","","2007-05-02","2007-05-09","['martin paegelow', 'nathalie villa', 'laurence cornez', 'frédéric ferraty', 'louis ferré', 'pascal sarda']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1553",705.2605,"sample eigenvalue based detection of high dimensional signals in white   noise using relatively few samples","math.st stat.th","we present a mathematically justifiable, computationally simple, sample eigenvalue based procedure for estimating the number of high-dimensional signals in white noise using relatively few samples. the main motivation for considering a sample eigenvalue based scheme is the computational simplicity and the robustness to eigenvector modelling errors which are can adversely impact the performance of estimators that exploit information in the sample eigenvectors.   there is, however, a price we pay by discarding the information in the sample eigenvectors; we highlight a fundamental asymptotic limit of sample eigenvalue based detection of weak/closely spaced high-dimensional signals from a limited sample size. this motivates our heuristic definition of the effective number of identifiable signals which is equal to the number of ""signal"" eigenvalues of the population covariance matrix which exceed the noise variance by a factor strictly greater than 1+sqrt(dimensionality of the system/sample size). the fundamental asymptotic limit brings into sharp focus why, when there are too few samples available so that the effective number of signals is less than the actual number of signals, underestimation of the model order is unavoidable (in an asymptotic sense) when using any sample eigenvalue based detection scheme, including the one proposed herein. the analysis reveals why adding more sensors can only exacerbate the situation. numerical simulations are used to demonstrate that the proposed estimator consistently estimates the true number of signals in the dimension fixed, large sample size limit and the effective number of identifiable signals in the large dimension, large sample size limit.","","2007-05-17","","['n. raj rao', 'alan edelman']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1554",706.0787,"construction of bayesian deformable models via stochastic approximation   algorithm: a convergence study","stat.co","the problem of the definition and the estimation of generative models based on deformable templates from raw data is of particular importance for modelling non aligned data affected by various types of geometrical variability. this is especially true in shape modelling in the computer vision community or in probabilistic atlas building for computational anatomy (ca). a first coherent statistical framework modelling the geometrical variability as hidden variables has been given by allassonni\`ere, amit and trouv\'e (jrss 2006). setting the problem in a bayesian context they proved the consistency of the map estimator and provided a simple iterative deterministic algorithm with an em flavour leading to some reasonable approximations of the map estimator under low noise conditions. in this paper we present a stochastic algorithm for approximating the map estimator in the spirit of the saem algorithm. we prove its convergence to a critical point of the observed likelihood with an illustration on images of handwritten digits.","","2007-06-06","2009-01-16","['stéphanie allassonnière', 'estelle kuhn', 'alain trouvé']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1555",707.3794,"binary models for marginal independence","math.st stat.th","log-linear models are a classical tool for the analysis of contingency tables. in particular, the subclass of graphical log-linear models provides a general framework for modelling conditional independences. however, with the exception of special structures, marginal independence hypotheses cannot be accommodated by these traditional models. focusing on binary variables, we present a model class that provides a framework for modelling marginal independences in contingency tables. the approach taken is graphical and draws on analogies to multivariate gaussian models for marginal independence. for the graphical model representation we use bi-directed graphs, which are in the tradition of path diagrams. we show how the models can be parameterized in a simple fashion, and how maximum likelihood estimation can be performed using a version of the iterated conditional fitting algorithm. finally we consider combining these models with symmetry restrictions.","10.1111/j.1467-9868.2007.00636.x","2007-07-25","","['mathias drton', 'thomas s. richardson']",0,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1556",707.4242,"importance tempering","stat.co stat.ap","simulated tempering (st) is an established markov chain monte carlo (mcmc) method for sampling from a multimodal density $\pi(\theta)$. typically, st involves introducing an auxiliary variable $k$ taking values in a finite subset of $[0,1]$ and indexing a set of tempered distributions, say $\pi_k(\theta) \propto \pi(\theta)^k$. in this case, small values of $k$ encourage better mixing, but samples from $\pi$ are only obtained when the joint chain for $(\theta,k)$ reaches $k=1$. however, the entire chain can be used to estimate expectations under $\pi$ of functions of interest, provided that importance sampling (is) weights are calculated. unfortunately this method, which we call importance tempering (it), can disappoint. this is partly because the most immediately obvious implementation is na\""ive and can lead to high variance estimators. we derive a new optimal method for combining multiple is estimators and prove that the resulting estimator has a highly desirable property related to the notion of effective sample size. we briefly report on the success of the optimal combination in two modelling scenarios requiring reversible-jump mcmc, where the na\""ive approach fails.","","2007-07-28","2008-11-03","['robert b. gramacy', 'richard j. samworth', 'ruth king']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE
"1557",708.1071,"statistical thinking: from tukey to vardi and beyond","math.st stat.th","data miners (minors?) and neural networkers tend to eschew modelling, misled perhaps by misinterpretation of strongly expressed views of john tukey. i discuss vardi's views of these issues as well as other aspects of vardi's work in emision tomography and in sampling bias.","10.1214/074921707000000210","2007-08-08","","['larry shepp']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1558",708.1321,"graphical methods for efficient likelihood inference in gaussian   covariance models","math.st stat.th","in graphical modelling, a bi-directed graph encodes marginal independences among random variables that are identified with the vertices of the graph. we show how to transform a bi-directed graph into a maximal ancestral graph that (i) represents the same independence structure as the original bi-directed graph, and (ii) minimizes the number of arrowheads among all ancestral graphs satisfying (i). here the number of arrowheads of an ancestral graph is the number of directed edges plus twice the number of bi-directed edges. in gaussian models, this construction can be used for more efficient iterative maximization of the likelihood function and to determine when maximum likelihood estimates are equal to empirical counterparts.","","2007-08-09","2008-03-25","['mathias drton', 'thomas s. richardson']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1559",708.1566,"markov chain modelling for reliability estimation of engineering systems   at different scales - some considerations","stat.ap stat.me","the concepts of probability, statistics and stochastic theory are being successfully used in structural engineering. markov chain modelling is a simple stochastic process model that has found its application in both describing stochastic evolution of system and in system reliability estimation. the recent developments in markov chain monte carlo and the possible integration of bayesian theory within markov chain theory have enhanced its application possibilities. however, the application possibility can be furthered to range over wider scales of application (perhaps from nano- to macro-) by considering the developments in physics (in particular quantum physics). this paper tries to present the results of quantum physics that would help in interpretation of transition probability matrix. however, care has to be taken in the choice of densities in computing the transition probability matrix. the paper is based on available literature, and the aim is only to make an attempt to show how markov chain can be used to model systems at various scales.","","2007-08-11","","['k. balaji rao']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1560",708.1965,"tail asymptotics and estimation for elliptical distributions","math.pr math.st stat.th","let (x,y) be a bivariate elliptical random vector with associated random radius in the gumbel max-domain of attraction. in this paper we obtain a second order asymptotic expansion of the joint survival probability p(x > x, y> y) for x,y large. further, based on the asymptotic bounds we discuss some aspects of the statistical modelling of joint survival probabilities and the survival conditional excess probability.","","2007-08-14","2008-05-15","['enkelejd hashorva']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1561",709.0447,"local mixture models of exponential families","math.st stat.th","exponential families are the workhorses of parametric modelling theory. one reason for their popularity is their associated inference theory, which is very clean, both from a theoretical and a computational point of view. one way in which this set of tools can be enriched in a natural and interpretable way is through mixing. this paper develops and applies the idea of local mixture modelling to exponential families. it shows that the highly interpretable and flexible models which result have enough structure to retain the attractive inferential properties of exponential families. in particular, results on identification, parameter orthogonality and log-concavity of the likelihood are proved.","10.3150/07-bej6170","2007-09-04","","['karim anaya-izquierdo', 'paul marriott']",0,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1562",709.2936,"bayesian classification and regression with high dimensional features","stat.ml","this thesis responds to the challenges of using a large number, such as thousands, of features in regression and classification problems.   there are two situations where such high dimensional features arise. one is when high dimensional measurements are available, for example, gene expression data produced by microarray techniques. for computational or other reasons, people may select only a small subset of features when modelling such data, by looking at how relevant the features are to predicting the response, based on some measure such as correlation with the response in the training data. although it is used very commonly, this procedure will make the response appear more predictable than it actually is. in chapter 2, we propose a bayesian method to avoid this selection bias, with application to naive bayes models and mixture models.   high dimensional features also arise when we consider high-order interactions. the number of parameters will increase exponentially with the order considered. in chapter 3, we propose a method for compressing a group of parameters into a single one, by exploiting the fact that many predictor variables derived from high-order interactions have the same values for all the training cases. the number of compressed parameters may have converged before considering the highest possible order. we apply this compression method to logistic sequence prediction models and logistic classification models.   we use both simulated data and real data to test our methods in both chapters.","","2007-09-18","","['longhai li']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1563",709.3427,"mutual information for the selection of relevant variables in   spectrometric nonlinear modelling","cs.lg cs.ne stat.ap","data from spectrophotometers form vectors of a large number of exploitable variables. building quantitative models using these variables most often requires using a smaller set of variables than the initial one. indeed, a too large number of input variables to a model results in a too large number of parameters, leading to overfitting and poor generalization abilities. in this paper, we suggest the use of the mutual information measure to select variables from the initial set. the mutual information measures the information content in input variables with respect to the model output, without making any assumption on the model that will be used; it is thus suitable for nonlinear modelling. in addition, it leads to the selection of variables among the initial set, and not to linear or nonlinear combinations of them. without decreasing the model performances compared to other variable projection methods, it allows therefore a greater interpretability of the results.","10.1016/j.chemolab.2005.06.010","2007-09-21","","['fabrice rossi', 'amaury lendasse', 'damien françois', 'vincent wertz', 'michel verleysen']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1564",710.3742,"bayesian online changepoint detection","stat.ml","changepoints are abrupt variations in the generative parameters of a data sequence. online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. while frequentist methods have yielded online filtering and prediction techniques, most bayesian papers have focused on the retrospective segmentation problem. here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. we compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. our implementation is highly modular so that the algorithm may be applied to a variety of types of data. we illustrate this modularity by demonstrating the algorithm on three different real-world data sets.","","2007-10-19","","['ryan prescott adams', 'david j. c. mackay']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1565",801.1966,"symmetry of models versus models of symmetry","math.st stat.th","a model for a subject's beliefs about a phenomenon may exhibit symmetry, in the sense that it is invariant under certain transformations. on the other hand, such a belief model may be intended to represent that the subject believes or knows that the phenomenon under study exhibits symmetry. we defend the view that these are fundamentally different things, even though the difference cannot be captured by bayesian belief models. in fact, the failure to distinguish between both situations leads to laplace's so-called principle of insufficient reason, which has been criticised extensively in the literature.   we show that there are belief models (imprecise probability models, coherent lower previsions) that generalise and include the bayesian belief models, but where this fundamental difference can be captured. this leads to two notions of symmetry for such belief models: weak invariance (representing symmetry of beliefs) and strong invariance (modelling beliefs of symmetry). we discuss various mathematical as well as more philosophical aspects of these notions. we also discuss a few examples to show the relevance of our findings both to probabilistic modelling and to statistical inference, and to the notion of exchangeability in particular.","","2008-01-13","","['gert de cooman', 'enrique miranda']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1566",801.2231,"spatial modelling for mixed-state observations","math.st stat.th","in several application fields like daily pluviometry data modelling, or motion analysis from image sequences, observations contain two components of different nature. a first part is made with discrete values accounting for some symbolic information and a second part records a continuous (real-valued) measurement. we call such type of observations ""mixed-state observations"". this paper introduces spatial models suited for the analysis of these kinds of data. we consider multi-parameter auto-models whose local conditional distributions belong to a mixed state exponential family. specific examples with exponential distributions are detailed, and we present some experimental results for modelling motion measurements from video sequences.","10.1214/08-ejs173","2008-01-15","2008-03-27","['cécile hardouin', 'jian-feng yao']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1567",802.0214,"multivariate stochastic volatility with bayesian dynamic linear models","q-fin.st stat.ap stat.me","this paper develops a bayesian procedure for estimation and forecasting of the volatility of multivariate time series. the foundation of this work is the matrix-variate dynamic linear model, for the volatility of which we adopt a multiplicative stochastic evolution, using wishart and singular multivariate beta distributions. a diagonal matrix of discount factors is employed in order to discount the variances element by element and therefore allowing a flexible and pragmatic variance modelling approach. diagnostic tests and sequential model monitoring are discussed in some detail. the proposed estimation theory is applied to a four-dimensional time series, comprising spot prices of aluminium, copper, lead and zinc of the london metal exchange. the empirical findings suggest that the proposed bayesian procedure can be effectively applied to financial data, overcoming many of the disadvantages of existing volatility models.","10.1016/j.jspi.2007.03.057","2008-02-01","","['k. triantafyllopoulos']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1568",802.1521,"stochastic algorithm for parameter estimation for dense deformable   template mixture model","stat.co math.st stat.th","estimating probabilistic deformable template models is a new approach in the fields of computer vision and probabilistic atlases in computational anatomy. a first coherent statistical framework modelling the variability as a hidden random variable has been given by allassonni\`ere, amit and trouv\'e in [1] in simple and mixture of deformable template models. a consistent stochastic algorithm has been introduced in [2] to face the problem encountered in [1] for the convergence of the estimation algorithm for the one component model in the presence of noise. we propose here to go on in this direction of using some ""saem-like"" algorithm to approximate the map estimator in the general bayesian setting of mixture of deformable template model. we also prove the convergence of this algorithm toward a critical point of the penalised likelihood of the observations and illustrate this with handwritten digit images.","","2008-02-11","2009-01-16","['stéphanie allassonnière', 'estelle kuhn']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE
"1569",803.086,"l\'{e}vy-based growth models","math.st stat.th","in the present paper, we give a condensed review, for the nonspecialist reader, of a new modelling framework for spatio-temporal processes, based on l\'{e}vy theory. we show the potential of the approach in stochastic geometry and spatial statistics by studying l\'{e}vy-based growth modelling of planar objects. the growth models considered are spatio-temporal stochastic processes on the circle. as a by product, flexible new models for space--time covariance functions on the circle are provided. an application of the l\'{e}vy-based growth models to tumour growth is discussed.","10.3150/07-bej6130","2008-03-06","","['kristjana ýr jónsdóttir', 'jürgen schmiegel', 'eva b. vedel jensen']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1570",804.4738,"structural shrinkage of nonparametric spectral estimators for   multivariate time series","math.st stat.th","in this paper we investigate the performance of periodogram based estimators of the spectral density matrix of possibly high-dimensional time series. we suggest and study shrinkage as a remedy against numerical instabilities due to deteriorating condition numbers of (kernel) smoothed periodogram matrices. moreover, shrinking the empirical eigenvalues in the frequency domain towards one another also improves at the same time the mean squared error (mse) of these widely used nonparametric spectral estimators. compared to some existing time domain approaches, restricted to i.i.d. data, in the frequency domain it is necessary to take the size of the smoothing span as ""effective or local sample size"" into account. while b\""{o}hm and von sachs (2007) proposes a multiple of the identity matrix as optimal shrinkage target in the absence of knowledge about the multidimensional structure of the data, here we consider ""structural"" shrinkage. we assume that the spectral structure of the data is induced by underlying factors. however, in contrast to actual factor modelling suffering from the need to choose the number of factors, we suggest a model-free approach. our final estimator is the asymptotically mse-optimal linear combination of the smoothed periodogram and the parametric estimator based on an underfitting (and hence deliberately misspecified) factor model. we complete our theoretical considerations by some extensive simulation studies. in the situation of data generated from a higher-order factor model, we compare all four types of involved estimators (including the one of b\""{o}hm and von sachs (2007)).","10.1214/08-ejs236","2008-04-30","2008-08-13","['hilmar böhm', 'rainer von sachs']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1571",805.2096,"garch modelling in continuous time for irregularly spaced time series   data","q-fin.st math.st stat.th","the discrete-time garch methodology which has had such a profound influence on the modelling of heteroscedasticity in time series is intuitively well motivated in capturing many `stylized facts' concerning financial series, and is now almost routinely used in a wide range of situations, often including some where the data are not observed at equally spaced intervals of time. however, such data is more appropriately analyzed with a continuous-time model which preserves the essential features of the successful garch paradigm. one possible such extension is the diffusion limit of nelson, but this is problematic in that the discrete-time garch model and its continuous-time diffusion limit are not statistically equivalent. as an alternative, kl\""{u}ppelberg et al. recently introduced a continuous-time version of the garch (the `cogarch' process) which is constructed directly from a background driving l\'{e}vy process. the present paper shows how to fit this model to irregularly spaced time series data using discrete-time garch methodology, by approximating the cogarch with an embedded sequence of discrete-time garch series which converges to the continuous-time model in a strong sense (in probability, in the skorokhod metric), as the discrete approximating grid grows finer. this property is also especially useful in certain other applications, such as options pricing. the way is then open to using, for the cogarch, similar statistical techniques to those already worked out for garch models and to illustrate this, an empirical investigation using stock index data is carried out.","10.3150/07-bej6189","2008-05-14","","['ross a. maller', 'gernot müller', 'alex szimayer']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1572",808.3466,"on sequential monte carlo, partial rejection control and approximate   bayesian computation","stat.co math.st stat.th","we present a sequential monte carlo sampler variant of the partial rejection control algorithm, and show that this variant can be considered as a sequential monte carlo sampler with a modified mutation kernel. we prove that the new sampler can reduce the variance of the incremental importance weights when compared with standard sequential monte carlo samplers. we provide a study of theoretical properties of the new algorithm, and make connections with some existing algorithms. finally, the sampler is adapted for application under the challenging ""likelihood free,"" approximate bayesian computation modelling framework, where we demonstrate superior performance over existing likelihood-free samplers.","","2008-08-26","2009-11-11","['g. w. peters', 'y. fan', 's. a. sisson']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE
"1573",808.4031,"hybrid data regression modelling in measurement","stat.ap stat.me","measurement involves the determination of quantitative estimates of physical quantities from experiment, along with estimates of their associated uncertainties. herewith an experimental system model is the key to extracting information from the experimental data. the measurement information obtained depends directly on the quality of the model. with this concern novel regression modelling techniques have been fashioned by data integration from computer-simulation and physical designed experiments. these techniques have allowed attaining the advanced level of model completeness, parsimony, and precision via approximation of the exact unknown model by mathematical product of available theoretical and appropriate empirical functions. the purpose of this approximation is to represent adequately the true model on the considered region of factor space with all advantages of theoretical modelling. this allows a further focus on the measurement science of issue. pneumatic gauge hybrid data candidate model building, solving and validation reviled that such adequate models permit to attain minimum discrepancy from empirical evidence.","","2008-08-29","","['vladimir b. bokov']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1574",808.4111,"relative entropy and statistics","cs.it math.it math.st stat.th","formalising the confrontation of opinions (models) to observations (data) is the task of inferential statistics. information theory provides us with a basic functional, the relative entropy (or kullback-leibler divergence), an asymmetrical measure of dissimilarity between the empirical and the theoretical distributions. the formal properties of the relative entropy turn out to be able to capture every aspect of inferential statistics, as illustrated here, for simplicity, on dices (= i.i.d. process with finitely many outcomes): refutability (strict or probabilistic): the asymmetry data / models; small deviations: rejecting a single hypothesis; competition between hypotheses and model selection; maximum likelihood: model inference and its limits; maximum entropy: reconstructing partially observed data; em-algorithm; flow data and gravity modelling; determining the order of a markov chain.","","2008-08-29","2010-04-03","['françois bavaud']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1575",809.2932,"stability selection","stat.me","estimation of structure, such as in variable selection, graphical modelling or cluster analysis is notoriously difficult, especially for high-dimensional data. we introduce stability selection. it is based on subsampling in combination with (high-dimensional) selection algorithms. as such, the method is extremely general and has a very wide range of applicability. stability selection provides finite sample control for some error rates of false discoveries and hence a transparent principle to choose a proper amount of regularisation for structure estimation. variable selection and structure estimation improve markedly for a range of selection methods if stability selection is applied. we prove for randomised lasso that stability selection will be variable selection consistent even if the necessary conditions needed for consistency of the original lasso method are violated. we demonstrate stability selection for variable selection and gaussian graphical modelling, using real and simulated data.","","2008-09-17","2009-05-16","['nicolai meinshausen', 'peter buehlmann']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1576",812.2548,"copulas for markovian dependence","math.pr math.st stat.th","copulas have been popular to model dependence for multivariate distributions, but have not been used much in modelling temporal dependence of univariate time series. this paper demonstrates some difficulties with using copulas even for markov processes: some tractable copulas such as mixtures between copulas of complete co- and countermonotonicity and independence (fr\'{e}chet copulas) are shown to imply quite a restricted type of markov process and archimedean copulas are shown to be incompatible with markov chains. we also investigate markov chains that are spreadable or, equivalently, conditionally i.i.d.","10.3150/09-bej214","2008-12-13","2010-10-08","['andreas n. lagerås']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1577",901.1518,"second-order refined peaks-over-threshold modelling for heavy-tailed   distributions","math.st stat.th","modelling excesses over a high threshold using the pareto or generalized pareto distribution (pd/gpd) is the most popular approach in extreme value statistics. this method typically requires high thresholds in order for the (g)pd to fit well and in such a case applies only to a small upper fraction of the data. the extension of the (g)pd proposed in this paper is able to describe the excess distribution for lower thresholds in case of heavy tailed distributions. this yields a statistical model that can be fitted to a larger portion of the data. moreover, estimates of tail parameters display stability for a larger range of thresholds. our findings are supported by asymptotic results, simulations and a case study.","","2009-01-12","","['jan beirlant', 'elisabeth joossens', 'johan segers']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1578",902.1598,"p-order rounded integer-valued autoregressive (rinar(p)) process","stat.me math.st stat.th","an extension of the rinar(1) process for modelling discrete-time dependent counting processes is considered. the model rinar(p) investigated here is a direct and natural extension of the real ar(p) model. compared to classical inar(p) models based on the thinning operator, the new models have several advantages: simple innovation structure ; autoregressive coefficients with arbitrary signs ; possible negative values for time series ; possible negative values for the autocorrelation function. the conditions for the stationarity and ergodicity, of the rinar(p) model, are given. for parameter estimation, we consider the least squares estimator and we prove its consistency under suitable identifiability condition. simulation experiments as well as analysis of real data sets are carried out to assess the performance of the model.","","2009-02-10","","['m. kachour']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1579",903.2651,"perfect simulation of spatial point processes using dominated coupling   from the past with application to a multiscale area-interaction point process","stat.me stat.ap stat.co","we consider perfect simulation algorithms for locally stable point processes based on dominated coupling from the past. a version of the algorithm is developed which is feasible for processes which are neither purely attractive nor purely repulsive. such processes include multiscale area-interaction processes, which are capable of modelling point patterns whose clustering structure varies across scales. we prove correctness of the algorithm and existence of these processes. an application to the redwood seedlings data is discussed.","","2009-03-15","","['graeme k. ambler', 'bernard w. silverman']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"1580",904.0317,"integrating remote sensing, gis and prediction models to monitor the   deforestation and erosion in peten reserve, guatemala","stat.ap","this contribution provides a strategy for studying and modelling the deforestation and soil deterioration in the natural forest reserve of peten, guatemala, using a poor spatial database. a multispectral image processing of spot and tm landsat data permits to understand the behaviour of the past land cover dynamics; a multi-temporal analysis of normalized difference vegetation and hydric stress index, most informative rgb (according to statistical criteria) and principal components, points out the importance and the direction of environmental impacts. we gain from the remote sensing images new environmental criteria (distance from roads, oil pipe-line, dem, etc.) which influence the spatial allocation of predicted land cover probabilities. we are comparing the results of different prospective approaches (markov chains, multi criteria evaluation and cellular automata; neural networks) analysing the residues for improving the final model of future deforestation risk.","","2009-04-02","","['roberto bruno', 'marco follador', 'martin paegelow', 'fernanda renno', 'nathalie villa']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1581",905.2441,"on the utility of graphics cards to perform massively parallel   simulation of advanced monte carlo methods","stat.co","we present a case-study on the utility of graphics cards to perform massively parallel simulation of advanced monte carlo methods. graphics cards, containing multiple graphics processing units (gpus), are self-contained parallel computational devices that can be housed in conventional desktop and laptop computers. for certain classes of monte carlo algorithms they offer massively parallel simulation, with the added advantage over conventional distributed multi-core processors that they are cheap, easily accessible, easy to maintain, easy to code, dedicated local devices with low power consumption. on a canonical set of stochastic simulation examples including population-based markov chain monte carlo methods and sequential monte carlo methods, we find speedups from 35 to 500 fold over conventional single-threaded computer code. our findings suggest that gpus have the potential to facilitate the growth of statistical modelling into complex data rich domains through the availability of cheap and accessible many-core computation. we believe the speedup we observe should motivate wider use of parallelizable simulation methods and greater methodological attention to their design.","10.1198/jcgs.2010.10039","2009-05-14","2009-07-15","['anthony lee', 'christopher yau', 'michael b. giles', 'arnaud doucet', 'christopher c. holmes']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1582",906.253,"observed universality of phase transitions in high-dimensional geometry,   with implications for modern data analysis and signal processing","math.st cs.it math.it physics.data-an stat.co stat.th","we review connections between phase transitions in high-dimensional combinatorial geometry and phase transitions occurring in modern high-dimensional data analysis and signal processing. in data analysis, such transitions arise as abrupt breakdown of linear model selection, robust data fitting or compressed sensing reconstructions, when the complexity of the model or the number of outliers increases beyond a threshold. in combinatorial geometry these transitions appear as abrupt changes in the properties of face counts of convex polytopes when the dimensions are varied. the thresholds in these very different problems appear in the same critical locations after appropriate calibration of variables.   these thresholds are important in each subject area: for linear modelling, they place hard limits on the degree to which the now-ubiquitous high-throughput data analysis can be successful; for robustness, they place hard limits on the degree to which standard robust fitting methods can tolerate outliers before breaking down; for compressed sensing, they define the sharp boundary of the undersampling/sparsity tradeoff in undersampling theorems.   existing derivations of phase transitions in combinatorial geometry assume the underlying matrices have independent and identically distributed (iid) gaussian elements. in applications, however, it often seems that gaussianity is not required. we conducted an extensive computational experiment and formal inferential analysis to test the hypothesis that these phase transitions are {\it universal} across a range of underlying matrix ensembles. the experimental results are consistent with an asymptotic large-$n$ universality across matrix ensembles; finite-sample universality can be rejected.","10.1098/rsta.2009.0152","2009-06-14","","['david l. donoho', 'jared tanner']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE
"1583",907.322,"inter genre similarity modelling for automatic music genre   classification","cs.sd cs.ai stat.ml","music genre classification is an essential tool for music information retrieval systems and it has been finding critical applications in various media platforms. two important problems of the automatic music genre classification are feature extraction and classifier design. this paper investigates inter-genre similarity modelling (igs) to improve the performance of automatic music genre classification. inter-genre similarity information is extracted over the mis-classified feature population. once the inter-genre similarity is modelled, elimination of the inter-genre similarity reduces the inter-genre confusion and improves the identification rates. inter-genre similarity modelling is further improved with iterative igs modelling(iigs) and score modelling for igs elimination(smigs). experimental results with promising classification improvements are provided.","","2009-07-18","","['ulas bagci', 'engin erzin']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1584",907.5151,"locally stationary long memory estimation","math.st stat.th","there exists a wide literature on modelling strongly dependent time series using a longmemory parameter d, including more recent work on semiparametric wavelet estimation. as a generalization of these latter approaches, in this work we allow the long-memory parameter d to be varying over time. we embed our approach into the framework of locally stationary processes. we show weak consistency and a central limit theorem for our log-regression wavelet estimator of the time-dependent d in a gaussian context. both simulations and a real data example complete our work on providing a fairly general approach.","","2009-07-29","2010-05-31","['françois roueff', 'rainer von sachs']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1585",908.005,"online learning for matrix factorization and sparse coding","stat.ml cs.lg math.oc","sparse coding--that is, modelling data vectors as sparse linear combinations of basis elements--is widely used in machine learning, neuroscience, signal processing, and statistics. this paper focuses on the large-scale matrix factorization problem that consists of learning the basis set, adapting it to specific data. variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. in this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large datasets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. a proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large datasets.","","2009-08-01","2010-02-11","['julien mairal', 'francis bach', 'jean ponce', 'guillermo sapiro']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1586",908.2359,"online em algorithm for hidden markov models","stat.co stat.ml","online (also called ""recursive"" or ""adaptive"") estimation of fixed model parameters in hidden markov models is a topic of much interest in times series modelling. in this work, we propose an online parameter estimation algorithm that combines two key ideas. the first one, which is deeply rooted in the expectation-maximization (em) methodology consists in reparameterizing the problem using complete-data sufficient statistics. the second ingredient consists in exploiting a purely recursive form of smoothing in hmms based on an auxiliary recursion. although the proposed online em algorithm resembles a classical stochastic approximation (or robbins-monro) algorithm, it is sufficiently different to resist conventional analysis of convergence. we thus provide limited results which identify the potential limiting points of the recursion as well as the large-sample behavior of the quantities involved in the algorithm. the performance of the proposed algorithm is numerically evaluated through simulations in the case of a noisily observed markov chain. in this case, the algorithm reaches estimation results that are comparable to that of the maximum likelihood estimator for large sample sizes.","","2009-08-17","2011-02-15","['olivier cappé']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE
"1587",909.5299,"parameter estimation for multivariate diffusion systems","stat.me","diffusion processes are widely used for modelling real-world phenomena. except for select cases however, analytical expressions do not exist for a diffusion process' transitional probabilities. it is proposed that the cumulant truncation procedure can be applied to predict the evolution of the cumulants of the system. these predictions may be subsequently used within the saddlepoint procedure to approximate the transitional probabilities. an approximation to the likelihood of the diffusion system is then easily derived. the method is applicable for a wide-range of diffusion systems - including multivariate, irreducible diffusion systems that existing estimation schemes struggle with. not only is the accuracy of the saddlepoint comparable with the hermite expansion - a popular approximation to a diffusion system's transitional density - it also appears to be less susceptible to increasing lags between successive samplings of the diffusion process. furthermore, the saddlepoint is more stable in regions of the parameter space that are far from the maximum likelihood estimates. hence, the saddlepoint method can be naturally incorporated within a markov chain monte carlo (mcmc) routine in order to provide reliable estimates and credibility intervals of the diffusion model's parameters. the method is applied to fit the heston model to daily observations of the s&p 500 and vix indices from december 2009 to november 2010.","","2009-09-29","2012-09-13","['melvin m. varughese']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1588",910.4443,"stochastic epidemic models: a survey","math.pr math.st stat.ap stat.me stat.th","this paper is a survey paper on stochastic epidemic models. a simple stochastic epidemic model is defined and exact and asymptotic model properties (relying on a large community) are presented. the purpose of modelling is illustrated by studying effects of vaccination and also in terms of inference procedures for important parameters, such as the basic reproduction number and the critical vaccination coverage. several generalizations towards realism, e.g. multitype and household epidemic models, are also presented, as is a model for endemic diseases.","","2009-10-23","","['tom britton']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE
"1589",910.5185,"nonparametric methods for volatility density estimation","stat.me math.st q-fin.st stat.th","stochastic volatility modelling of financial processes has become increasingly popular. the proposed models usually contain a stationary volatility process. we will motivate and review several nonparametric methods for estimation of the density of the volatility process. both models based on discretely sampled continuous time processes and discrete time models will be discussed.   the key insight for the analysis is a transformation of the volatility density estimation problem to a deconvolution model for which standard methods exist. three type of nonparametric density estimators are reviewed: the fourier-type deconvolution kernel density estimator, a wavelet deconvolution density estimator and a penalized projection estimator. the performance of these estimators will be compared. key words: stochastic volatility models, deconvolution, density estimation, kernel estimator, wavelets, minimum contrast estimation, mixing","","2009-10-27","","['bert van es', 'peter spreij', 'harry van zanten']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1590",911.1472,"variable second-order inclusion probabilities as a tool to predict the   sampling variance","stat.ap stat.me","a generalization of gy's theory for the variance of the fundamental sampling error is reviewed. practical situations where the generalized model potentially leads to more accurate variance estimates are identified as: clustering of particles, differences in densities or sizes of the particles or repulsive inter-particle forces. two general approaches for estimating an input parameter for the generalized model are discussed. the first approach consists of modelling based on physical properties of particles such as size, density and electrostatic forces between particles. the second approach uses image analysis of actual samples. further research into both methods is proposed and a suggestion is made to use line-intercept sampling combined with markov chain modelling in the second approach.   it is concluded that although, at the moment, it is too early for a routine application of the generalized theory, the generalization has the potential of providing more accurate variance estimates than are possible in the theory of gy. therefore, further research into the development and expansion of the generalized theory is worthwhile.","","2009-11-07","","['bastiaan geelhoed']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1591",911.327,"nonparametric bayesian inference on bivariate extremes","math.st stat.th","the tail of a bivariate distribution function in the domain of attraction of a bivariate extreme-value distribution may be approximated by the one of its extreme-value attractor. the extreme-value attractor has margins that belong to a three-parameter family and a dependence structure which is characterised by a spectral measure, that is a probability measure on the unit interval with mean equal to one half. as an alternative to parametric modelling of the spectral measure, we propose an infinite-dimensional model which is at the same time manageable and still dense within the class of spectral measures. inference is done in a bayesian framework, using the censored-likelihood approach. in particular, we construct a prior distribution on the class of spectral measures and develop a trans-dimensional markov chain monte carlo algorithm for numerical computations. the method provides a bivariate predictive density which can be used for predicting the extreme outcomes of the bivariate distribution. in a practical perspective, this is useful for computing rare event probabilities and extreme conditional quantiles. the methodology is validated by simulations and applied to a data-set of danish fire insurance claims.","","2009-11-17","2012-05-10","['simon guillotte', 'francois perron', 'johan segers']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1592",911.4046,"super-linear convergence of dual augmented-lagrangian algorithm for   sparsity regularized estimation","stat.ml cs.lg stat.me","we analyze the convergence behaviour of a recently proposed algorithm for regularized estimation called dual augmented lagrangian (dal). our analysis is based on a new interpretation of dal as a proximal minimization algorithm. we theoretically show under some conditions that dal converges super-linearly in a non-asymptotic and global sense. due to a special modelling of sparse estimation problems in the context of machine learning, the assumptions we make are milder and more natural than those made in conventional analysis of augmented lagrangian algorithms. in addition, the new interpretation enables us to generalize dal to wide varieties of sparse estimation problems. we experimentally confirm our analysis in a large scale $\ell_1$-regularized logistic regression problem and extensively compare the efficiency of dal algorithm to previously proposed algorithms on both synthetic and benchmark datasets.","","2009-11-20","2011-01-02","['ryota tomioka', 'taiji suzuki', 'masashi sugiyama']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1593",911.5242,"the ilium forward modelling algorithm for multivariate parameter   estimation and its application to derive stellar parameters from gaia   spectrophotometry","astro-ph.im astro-ph.ga astro-ph.sr cs.ne stat.ml","i introduce an algorithm for estimating parameters from multidimensional data based on forward modelling. in contrast to many machine learning approaches it avoids fitting an inverse model and the problems associated with this. the algorithm makes explicit use of the sensitivities of the data to the parameters, with the goal of better treating parameters which only have a weak impact on the data. the forward modelling approach provides uncertainty (full covariance) estimates in the predicted parameters as well as a goodness-of-fit for observations. i demonstrate the algorithm, ilium, with the estimation of stellar astrophysical parameters (aps) from simulations of the low resolution spectrophotometry to be obtained by gaia. the ap accuracy is competitive with that obtained by a support vector machine. for example, for zero extinction stars covering a wide range of metallicity, surface gravity and temperature, ilium can estimate teff to an accuracy of 0.3% at g=15 and to 4% for (lower signal-to-noise ratio) spectra at g=20. [fe/h] and logg can be estimated to accuracies of 0.1-0.4dex for stars with g<=18.5. if extinction varies a priori over a wide range (av=0-10mag), then teff and av can be estimated quite accurately (3-4% and 0.1-0.2mag respectively at g=15), but there is a strong and ubiquitous degeneracy in these parameters which limits our ability to estimate either accurately at faint magnitudes. using the forward model we can map these degeneracies (in advance), and thus provide a complete probability distribution over solutions. (abridged)","10.1111/j.1365-2966.2009.16125.x","2009-11-27","2010-01-04","['c. a. l. bailer-jones']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1594",912.4729,"likelihood-free bayesian inference for alpha-stable models","stat.co","$\alpha$-stable distributions are utilised as models for heavy-tailed noise in many areas of statistics, finance and signal processing engineering.   however, in general, neither univariate nor multivariate $\alpha$-stable models admit closed form densities which can be evaluated pointwise. this complicates the inferential procedure.   as a result, $\alpha$-stable models are practically limited to the univariate setting under the bayesian paradigm, and to bivariate models under the classical framework.   in this article we develop a novel bayesian approach to modelling univariate and multivariate $\alpha$-stable distributions based on recent advances in ""likelihood-free"" inference.   we present an evaluation of the performance of this procedure in 1, 2 and 3 dimensions, and provide an analysis of real daily currency exchange rate data. the proposed approach provides a feasible inferential methodology at a moderate computational cost.","","2009-12-23","","['g. w. peters', 's. a. sisson', 'y. fan']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1595",1001.4656,"on bayesian data analysis","stat.me","this introduction to bayesian statistics presents the main concepts as well as the principal reasons advocated in favour of a bayesian modelling. we cover the various approaches to prior determination as well as the basis asymptotic arguments in favour of using bayes estimators. the testing aspects of bayesian inference are also examined in details.","","2010-01-26","2010-02-09","['christian p. robert', 'judith rousseau']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1596",1003.0243,"perfect simulation using dominated coupling from the past with   application to area-interaction point processes and wavelet thresholding","stat.me stat.co","we consider perfect simulation algorithms for locally stable point processes based on dominated coupling from the past, and apply these methods in two different contexts. a new version of the algorithm is developed which is feasible for processes which are neither purely attractive nor purely repulsive. such processes include multiscale area-interaction processes, which are capable of modelling point patterns whose clustering structure varies across scales. the other topic considered is nonparametric regression using wavelets, where we use a suitable area-interaction process on the discrete space of indices of wavelet coefficients to model the notion that if one wavelet coefficient is non-zero then it is more likely that neighbouring coefficients will be also. a method based on perfect simulation within this model shows promising results compared to the standard methods which threshold coefficients independently.","","2010-02-28","","['graeme k. ambler', 'bernard w. silverman']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"1597",1003.3988,"colouring and breaking sticks: random distributions and heterogeneous   clustering","stat.me stat.co","we begin by reviewing some probabilistic results about the dirichlet process and its close relatives, focussing on their implications for statistical modelling and analysis. we then introduce a class of simple mixture models in which clusters are of different `colours', with statistical characteristics that are constant within colours, but different between colours. thus cluster identities are exchangeable only within colours. the basic form of our model is a variant on the familiar dirichlet process, and we find that much of the standard modelling and computational machinery associated with the dirichlet process may be readily adapted to our generalisation. the methodology is illustrated with an application to the partially-parametric clustering of gene expression profiles.","","2010-03-21","","['peter j. green']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"1598",1004.1341,"algebraic comparison of partial lists in bioinformatics","stat.ml q-bio.qm","the outcome of a functional genomics pipeline is usually a partial list of genomic features, ranked by their relevance in modelling biological phenotype in terms of a classification or regression model. due to resampling protocols or just within a meta-analysis comparison, instead of one list it is often the case that sets of alternative feature lists (possibly of different lengths) are obtained. here we introduce a method, based on the algebraic theory of symmetric groups, for studying the variability between lists (""list stability"") in the case of lists of unequal length. we provide algorithms evaluating stability for lists embedded in the full feature set or just limited to the features occurring in the partial lists. the method is demonstrated first on synthetic data in a gene filtering task and then for finding gene profiles on a recent prostate cancer dataset.","10.1371/journal.pone.0036540","2010-04-08","","['giuseppe jurman', 'samantha riccadonna', 'roberto visintainer', 'cesare furlanello']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1599",1004.3871,"practical estimation of high dimensional stochastic differential   mixed-effects models","stat.co math.ds stat.me","stochastic differential equations (sdes) are established tools to model physical phenomena whose dynamics are affected by random noise. by estimating parameters of an sde intrinsic randomness of a system around its drift can be identified and separated from the drift itself. when it is of interest to model dynamics within a given population, i.e. to model simultaneously the performance of several experiments or subjects, mixed-effects modelling allows for the distinction of between and within experiment variability. a framework to model dynamics within a population using sdes is proposed, representing simultaneously several sources of variation: variability between experiments using a mixed-effects approach and stochasticity in the individual dynamics using sdes. these ""stochastic differential mixed-effects models"" have applications in e.g. pharmacokinetics/pharmacodynamics and biomedical modelling. a parameter estimation method is proposed and computational guidelines for an efficient implementation are given. finally the method is evaluated using simulations from standard models like the two-dimensional ornstein-uhlenbeck (ou) and the square root models.","10.1016/j.csda.2010.10.003","2010-04-22","2010-10-03","['umberto picchini', 'susanne ditlevsen']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"1600",1004.4116,"assessing molecular variability in cancer genomes","q-bio.pe stat.co","the dynamics of tumour evolution are not well understood. in this paper we provide a statistical framework for evaluating the molecular variation observed in different parts of a colorectal tumour. a multi-sample version of the ewens sampling formula forms the basis for our modelling of the data, and we provide a simulation procedure for use in obtaining reference distributions for the statistics of interest. we also describe the large-sample asymptotics of the joint distributions of the variation observed in different parts of the tumour. while actual data should be evaluated with reference to the simulation procedure, the asymptotics serve to provide theoretical guidelines, for instance with reference to the choice of possible statistics.","","2010-04-13","","['a. d. barbour', 'simon tavaré']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1601",1006.294,"lasso isotone for high dimensional additive isotonic regression","stat.me stat.co stat.ml","additive isotonic regression attempts to determine the relationship between a multi-dimensional observation variable and a response, under the constraint that the estimate is the additive sum of univariate component effects that are monotonically increasing. in this article, we present a new method for such regression called lasso isotone (liso). liso adapts ideas from sparse linear modelling to additive isotonic regression. thus, it is viable in many situations with high dimensional predictor variables, where selection of significant versus insignificant variables are required. we suggest an algorithm involving a modification of the backfitting algorithm cpav. we give a numerical convergence result, and finally examine some of its properties through simulations. we also suggest some possible extensions that improve performance, and allow calculation to be carried out when the direction of the monotonicity is unknown.","","2010-06-15","","['zhou fang', 'nicolai meinshausen']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE
"1602",1007.0296,"a bayesian view of the poisson-dirichlet process","math.st cs.lg math.pr stat.th","the two parameter poisson-dirichlet process (pdp), a generalisation of the dirichlet process, is increasingly being used for probabilistic modelling in discrete areas such as language technology, bioinformatics, and image analysis. there is a rich literature about the pdp and its derivative distributions such as the chinese restaurant process (crp). this article reviews some of the basic theory and then the major results needed for bayesian modelling of discrete problems including details of priors, posteriors and computation.   the pdp allows one to build distributions over countable partitions. the pdp has two other remarkable properties: first it is partially conjugate to itself, which allows one to build hierarchies of pdps, and second using a marginalised relative the crp, one gets fragmentation and clustering properties that lets one layer partitions to build trees. this article presents the basic theory for understanding the notion of partitions and distributions over them, the pdp and the crp, and the important properties of conjugacy, fragmentation and clustering, as well as some key related properties such as consistency and convergence. this article also presents a bayesian interpretation of the poisson-dirichlet process based on an improper and infinite dimensional dirichlet distribution. this means we can understand the process as just another dirichlet and thus all its sampling properties emerge naturally.   the theory of pdps is usually presented for continuous distributions (more generally referred to as non-atomic distributions), however, when applied to discrete distributions its remarkable conjugacy property emerges. this context and basic results are also presented, as well as techniques for computing the second order stirling numbers that occur in the posteriors for discrete distributions.","","2010-07-02","2012-02-15","['wray buntine', 'marcus hutter']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1603",1007.0394,"non-uniform state space reconstruction and coupling detection","nlin.cd cs.it math.it physics.data-an q-bio.nc stat.me","we investigate the state space reconstruction from multiple time series derived from continuous and discrete systems and propose a method for building embedding vectors progressively using information measure criteria regarding past, current and future states. the embedding scheme can be adapted for different purposes, such as mixed modelling, cross-prediction and granger causality. in particular we apply this method in order to detect and evaluate information transfer in coupled systems. as a practical application, we investigate in records of scalp epileptic eeg the information flow across brain areas.","10.1103/physreve.82.016207","2010-07-02","","['ioannis vlachos', 'dimitris kugiumtzis']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1604",1007.4622,"adaptive wavelet estimation of the diffusion coefficient under additive   error measurements","math.st stat.th","we study nonparametric estimation of the diffusion coefficient from discrete data, when the observations are blurred by additional noise. such issues have been developed over the last 10 years in several application fields and in particular in high frequency financial data modelling, however mainly from a parametric and semiparametric point of view. this paper addresses the nonparametric estimation of the path of the (possibly stochastic) diffusion coefficient in a relatively general setting. by developing pre-averaging techniques combined with wavelet thresholding, we construct adaptive estimators that achieve a nearly optimal rate within a large scale of smoothness constraints of besov type. since the diffusion coefficient is usually genuinely random, we propose a new criterion to assess the quality of estimation; we retrieve the usual minimax theory when this approach is restricted to a deterministic diffusion coefficient. in particular, we take advantage of recent results of reiss [33] of asymptotic equivalence between a gaussian diffusion with additive noise and gaussian white noise model, in order to prove a sharp lower bound.","","2010-07-27","2011-12-29","['marc hoffmann', 'axel munk', 'johannes schmidt-hieber']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1605",1007.5388,"reference priors of nuisance parameters in bayesian sequential   population analysis","math.st stat.th","prior distributions elicited for modelling the natural fluctuations or the uncertainty on parameters of bayesian fishery population models, can be chosen among a vast range of statistical laws. since the statistical framework is defined by observational processes, observational parameters enter into the estimation and must be considered random, similarly to parameters or states of interest like population levels or real catches. the former are thus perceived as nuisance parameters whose values are intrinsically linked to the considered experiment, which also require noninformative priors. in fishery research jeffreys methodology has been presented by millar (2002) as a practical way to elicit such priors. however they can present wrong properties in multiparameter contexts. therefore we suggest to use the elicitation method proposed by berger and bernardo to avoid paradoxical results raised by jeffreys priors. these benchmark priors are derived here in the framework of sequential population analysis.","","2010-07-30","2010-10-11","['nicolas bousquet']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1606",1008.1636,"censoring outdegree compromises inferences of social network peer   effects and autocorrelation","stat.me","i examine the consequences of modelling contagious influence in a social network with incomplete edge information, namely in the situation where each individual may name a limited number of friends, so that extra outbound ties are censored. in particular, i consider a prototypical time series configuration where a property of the ""ego"" is affected in a causal fashion by the properties of their ""alters"" at a previous time point, both in the total number of alters as well as the deviation from a central value. this is considered with three potential methods for naming one's friends: a strict upper limit on the number of declarations, a flexible limit, and an instruction where a person names a prespecified fraction of their friends. i find that one of two effects is present in the estimation of these effects: either that the size of the effect is inflated in magnitude, or that the estimators instead are centered about zero rather than related to the true effect. the degree of heterogeneity in friend count is one of the major factors into whether such an analysis can be salvaged by post-hoc adjustments.","","2010-08-10","2011-01-06","['andrew c. thomas']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1607",1008.287,"efficient and robust estimation for a class of generalized linear   longitudinal mixed models","stat.me stat.ap","we propose a versatile and computationally efficient estimating equation method for a class of hierarchical multiplicative generalized linear mixed models with additive dispersion components, based on explicit modelling of the covariance structure. the class combines longitudinal and random effects models and retains a marginal as well as a conditional interpretation. the estimation procedure combines that of generalized estimating equations for the regression with residual maximum likelihood estimation for the association parameters. this avoids the multidimensional integral of the conventional generalized linear mixed models likelihood and allows an extension of the robust empirical sandwich estimator for use with both association and regression parameters. the method is applied to a set of otolith data, used for age determination of fish.","","2010-08-17","","['rené holst', 'bent jørgensen']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1608",1009.169,"probabilistic models over ordered partitions with application in   learning to rank","cs.ir stat.ml","this paper addresses the general problem of modelling and learning rank data with ties. we propose a probabilistic generative model, that models the process as permutations over partitions. this results in super-exponential combinatorial state space with unknown numbers of partitions and unknown ordering among them. we approach the problem from the discrete choice theory, where subsets are chosen in a stagewise manner, reducing the state space per each stage significantly. further, we show that with suitable parameterisation, we can still learn the models in linear time. we evaluate the proposed models on the problem of learning to rank with the data from the recently held yahoo! challenge, and demonstrate that the models are competitive against well-known rivals.","","2010-09-09","2010-10-04","['tran the truyen', 'dinh q. phung', 'svetha venkatesh']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1609",1010.1069,"cooperative distributed sequential spectrum sensing","cs.it math.it stat.ap","we consider cooperative spectrum sensing for cognitive radios. we develop an energy efficient detector with low detection delay using sequential hypothesis testing. sequential probability ratio test (sprt) is used at both the local nodes and the fusion center. we also analyse the performance of this algorithm and compare with the simulations. modelling uncertainties in the distribution parameters are considered. slow fading with and without perfect channel state information at the cognitive radios is taken into account.","","2010-10-06","2012-11-23","['jithin k s', 'vinod sharma', 'raghav gopalarathnam']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1610",1010.4406,"impact of insurance for operational risk: is it worthwhile to insure or   be insured for severe losses?","q-fin.rm q-fin.st stat.ap stat.co stat.me","under the basel ii standards, the operational risk (oprisk) advanced measurement approach allows a provision for reduction of capital as a result of insurance mitigation of up to 20%. this paper studies the behaviour of different insurance policies in the context of capital reduction for a range of possible extreme loss models and insurance policy scenarios in a multi-period, multiple risk settings. a loss distributional approach (lda) for modelling of the annual loss process, involving homogeneous compound poisson processes for the annual losses, with heavy tailed severity models comprised of alpha-stable severities is considered. there has been little analysis of such models to date and it is believed, insurance models will play more of a role in oprisk mitigation and capital reduction in future. the first question of interest is when would it be equitable for a bank or financial institution to purchase insurance for heavy tailed oprisk losses under different insurance policy scenarios? the second question then pertains to solvency ii and addresses what the insurers capital would be for such operational risk scenarios under different policy offerings. in addition we consider the insurers perspective with respect to fair premium as a percentage above the expected annual claim for each insurance policy. the intention being to address questions related to var reduction under basel ii, scr under solvency ii and fair insurance premiums in oprisk for different extreme loss scenarios. in the process we provide closed form solutions for the distribution of loss process and claims process in an lda structure as well as closed form analytic solutions for the expected shortfall, scr and mcr under basel ii and solvency ii. we also provide closed form analytic solutions for the annual loss distribution of multiple risks including insurance mitigation.","","2010-10-21","2010-11-02","['gareth w. peters', 'aaron d. byrnes', 'pavel v. shevchenko']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"1611",1011.1379,"model selection by loss rank for classification and unsupervised   learning","stat.me stat.ml","hutter (2007) recently introduced the loss rank principle (lorp) as a generalpurpose principle for model selection. the lorp enjoys many attractive properties and deserves further investigations. the lorp has been well-studied for regression framework in hutter and tran (2010). in this paper, we study the lorp for classification framework, and develop it further for model selection problems in unsupervised learning where the main interest is to describe the associations between input measurements, like cluster analysis or graphical modelling. theoretical properties and simulation studies are presented.","","2010-11-05","","['minh-ngoc tran', 'marcus hutter']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1612",1012.3407,"translating biomarkers between multi-way time-series experiments","stat.ml","translating potential disease biomarkers between multi-species 'omics' experiments is a new direction in biomedical research. the existing methods are limited to simple experimental setups such as basic healthy-diseased comparisons. most of these methods also require an a priori matching of the variables (e.g., genes or metabolites) between the species. however, many experiments have a complicated multi-way experimental design often involving irregularly-sampled time-series measurements, and for instance metabolites do not always have known matchings between organisms. we introduce a bayesian modelling framework for translating between multiple species the results from 'omics' experiments having a complex multi-way, time-series experimental design. the underlying assumption is that the unknown matching can be inferred from the response of the variables to multiple covariates including time.","","2010-12-15","","['ilkka huopaniemi', 'tommi suvitaival', 'matej orešič', 'samuel kaski']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1613",1102.5228,"some covariance models based on normal scale mixtures","math.st stat.th","modelling spatio-temporal processes has become an important issue in current research. since gaussian processes are essentially determined by their second order structure, broad classes of covariance functions are of interest. here, a new class is described that merges and generalizes various models presented in the literature, in particular models in gneiting (j. amer. statist. assoc. 97 (2002) 590--600) and stein (nonstationary spatial covariance functions (2005) univ. chicago). furthermore, new models and a multivariate extension are introduced.","10.3150/09-bej226","2011-02-25","","['martin schlather']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1614",1102.5606,"ornstein-uhlenbeck type processes with heavy distribution tails","math.pr math.st stat.th","we consider a transformed ornstein-uhlenbeck process model that can be a good candidate for modelling real-life processes characterized by a combination of time-reverting behaviour with heavy distribution tails. we begin with presenting the results of an exploratory statistical analysis of the log prices of a major australian public company, demonstrating several key features typical of such time series. motivated by these findings, we suggest a simple transformed ornstein-uhlenbeck process model and analyze its properties showing that the model is capable of replicating our empirical findings. we also discuss three different estimators for the drift coefficient in the underlying (unobservable) ornstein-uhlenbeck process which is the key descriptor of dependence in the process.","","2011-02-28","","['k. borovkov', 'g. decrouez']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1615",1104.0896,"on identifying significant edges in graphical models of molecular   networks","stat.ml stat.me","objective: modelling the associations from high-throughput experimental molecular data has provided unprecedented insights into biological pathways and signalling mechanisms. graphical models and networks have especially proven to be useful abstractions in this regard. ad-hoc thresholds are often used in conjunction with structure learning algorithms to determine significant associations. the present study overcomes this limitation by proposing a statistically-motivated approach for identifying significant associations in a network.   methods and materials: a new method that identifies significant associations in graphical models by estimating the threshold minimising the $l_{\mathrm{1}}$ norm between the cumulative distribution function (cdf) of the observed edge confidences and those of its asymptotic counterpart is proposed. the effectiveness of the proposed method is demonstrated on popular synthetic data sets as well as publicly available experimental molecular data corresponding to gene and protein expression profiles.   results: the improved performance of the proposed approach is demonstrated across the synthetic data sets using sensitivity, specificity and accuracy as performance metrics. the results are also demonstrated across varying sample sizes and three different structure learning algorithms with widely varying assumptions. in all cases, the proposed approach has specificity and accuracy close to 1, while sensitivity increases linearly in the logarithm of the sample size. the estimated threshold systematically outperforms common ad-hoc ones in terms of sensitivity while maintaining comparable levels of specificity and accuracy. networks from experimental data sets are reconstructed accurately with respect to the results from the original papers.","","2011-04-05","2013-04-23","['marco scutari', 'radhakrishnan nagarajan']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1616",1104.1608,"lattices of graphical gaussian models with symmetries","math.st stat.th","in order to make graphical gaussian models a viable modelling tool when the number of variables outgrows the number of observations, model classes which place equality restrictions on concentrations or partial correlations have previously been introduced in the literature. the models can be represented by vertex and edge coloured graphs. the need for model selection methods makes it imperative to understand the structure of model classes. we identify four model classes that form complete lattices of models with respect to model inclusion, which qualifies them for an edwards-havr\'anek model selection procedure. two classes turn out most suitable for a corresponding model search. we obtain an explicit search algorithm for one of them and provide a model search example for the other.","10.3390/sym3030653","2011-04-08","2011-09-19","['helene gehrmann']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1617",1104.3503,"resid: a practical stochastic model for software reliability","stat.ap cs.se","a new approach called resid is proposed in this paper for estimating reliability of a software allowing for imperfect debugging. unlike earlier approaches based on counting number of bugs or modelling inter-failure time gaps, resid focuses on the probability of ""bugginess"" of different parts of a program buggy. this perspective allows an easy way to incorporate the structure of the software under test, as well as imperfect debugging. one main design objective behind resid is ease of implementation in practical scenarios.","","2011-04-18","","['arnab chakraborty']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1618",1104.4422,"the multivariate watson distribution: maximum-likelihood estimation and   other aspects","stat.co math.ca","this paper studies fundamental aspects of modelling data using multivariate watson distributions. although these distributions are natural for modelling axially symmetric data (i.e., unit vectors where $\pm \x$ are equivalent), for high-dimensions using them can be difficult. why so? largely because for watson distributions even basic tasks such as maximum-likelihood are numerically challenging. to tackle the numerical difficulties some approximations have been derived---but these are either grossly inaccurate in high-dimensions (\emph{directional statistics}, mardia & jupp. 2000) or when reasonably accurate (\emph{j. machine learning research, w. & c.p., v2}, bijral \emph{et al.}, 2007, pp. 35--42), they lack theoretical justification. we derive new approximations to the maximum-likelihood estimates; our approximations are theoretically well-defined, numerically accurate, and easy to compute. we build on our parameter estimation and discuss mixture-modelling with watson distributions; here we uncover a hitherto unknown connection to the ""diametrical clustering"" algorithm of dhillon \emph{et al.} (\emph{bioinformatics}, 19(13), 2003, pp. 1612--1619).","","2011-04-22","2012-05-25","['suvrit sra', 'dmitrii karp']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1619",1105.6075,"marginal log-linear parameters for graphical markov models","stat.me","marginal log-linear (mll) models provide a flexible approach to multivariate discrete data. mll parametrizations under linear constraints induce a wide variety of models, including models defined by conditional independences. we introduce a sub-class of mll models which correspond to acyclic directed mixed graphs (admgs) under the usual global markov property. we characterize for precisely which graphs the resulting parametrization is variation independent. the mll approach provides the first description of admg models in terms of a minimal list of constraints. the parametrization is also easily adapted to sparse modelling techniques, which we illustrate using several examples of real data.","10.1111/rssb.12020","2011-05-30","2012-10-31","['robin j. evans', 'thomas s. richardson']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1620",1106.4509,"machine learning markets","cs.ai cs.ma cs.ne q-fin.tr stat.ml","prediction markets show considerable promise for developing flexible mechanisms for machine learning. here, machine learning markets for multivariate systems are defined, and a utility-based framework is established for their analysis. this differs from the usual approach of defining static betting functions. it is shown that such markets can implement model combination methods used in machine learning, such as product of expert and mixture of expert approaches as equilibrium pricing models, by varying agent utility functions. they can also implement models composed of local potentials, and message passing methods. prediction markets also allow for more flexible combinations, by combining multiple different utility functions. conversely, the market mechanisms implement inference in the relevant probabilistic models. this means that market mechanism can be utilized for implementing parallelized model building and inference for probabilistic modelling.","","2011-06-22","","['amos storkey']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1621",1107.1811,"modelling outliers and structural breaks in dynamic linear models with a   novel use of a heavy tailed prior for the variances: an alternative to the   inverted gamma","stat.me","modelling outliers and structural breaks in dynamic linear models with a novel use of a heavy tailed prior for the variances: an alternative to the inverted gamma","","2011-07-09","2013-01-24","['jairo fuquene', 'maria perez', 'luis pericchi']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1622",1107.2205,"sequential monte carlo em for multivariate probit models","stat.me stat.co","multivariate probit models (mpm) have the appealing feature of capturing some of the dependence structure between the components of multidimensional binary responses. the key for the dependence modelling is the covariance matrix of an underlying latent multivariate gaussian. most approaches to mle in multivariate probit regression rely on mcem algorithms to avoid computationally intensive evaluations of multivariate normal orthant probabilities. as an alternative to the much used gibbs sampler a new smc sampler for truncated multivariate normals is proposed. the algorithm proceeds in two stages where samples are first drawn from truncated multivariate student $t$ distributions and then further evolved towards a gaussian. the sampler is then embedded in a mcem algorithm. the sequential nature of smc methods can be exploited to design a fully sequential version of the em, where the samples are simply updated from one iteration to the next rather than resampled from scratch. recycling the samples in this manner significantly reduces the computational cost. an alternative view of the standard conditional maximisation step provides the basis for an iterative procedure to fully perform the maximisation needed in the em algorithm. the identifiability of mpm is also thoroughly discussed. in particular, the likelihood invariance can be embedded in the em algorithm to ensure that constrained and unconstrained maximisation are equivalent. a simple iterative procedure is then derived for either maximisation which takes effectively no computational time. the method is validated by applying it to the widely analysed six cities dataset and on a higher dimensional simulated example. previous approaches to the six cities overly restrict the parameter space but, by considering the correct invariance, the maximum likelihood is quite naturally improved when treating the full unrestricted model.","10.1016/j.csda.2013.10.019","2011-07-12","2013-11-14","['giusi moffa', 'jack kuipers']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"1623",1107.2699,"linear latent force models using gaussian processes","stat.ml cs.ai","purely data driven approaches for machine learning present difficulties when data is scarce relative to the complexity of the model or when the model is forced to extrapolate. on the other hand, purely mechanistic approaches need to identify and specify all the interactions in the problem at hand (which may not be feasible) and still leave the issue of how to parameterize the system. in this paper, we present a hybrid approach using gaussian processes and differential equations to combine data driven modelling with a physical model of the system. we show how different, physically-inspired, kernel functions can be developed through sensible, simple, mechanistic assumptions about the underlying system. the versatility of our approach is illustrated with three case studies from motion capture, computational biology and geostatistics.","","2011-07-13","2020-03-13","['mauricio a. álvarez', 'david luengo', 'neil d. lawrence']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1624",1107.3904,"asymptotics of the discrete log-concave maximum likelihood estimator and   related applications","stat.me","the assumption of log-concavity is a flexible and appealing nonparametric shape constraint in distribution modelling. in this work, we study the log-concave maximum likelihood estimator (mle) of a probability mass function (pmf). we show that the mle is strongly consistent and derive its pointwise asymptotic theory under both the well- and misspecified setting. our asymptotic results are used to calculate confidence intervals for the true log-concave pmf. both the mle and the associated confidence intervals may be easily computed using the r package logcondiscr. we illustrate our theoretical results using recent data from the h1n1 pandemic in ontario, canada.","","2011-07-20","2012-10-14","['fadoua balabdaoui', 'hanna jankowski', 'kaspar rufibach', 'marios pavlides']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1625",1107.4464,"max-stable processes for modelling extremes observed in space and time","stat.me","max-stable processes have proved to be useful for the statistical modelling of spatial extremes. several representations of max-stable random fields have been proposed in the literature. for statistical inference it is often assumed that there is no temporal dependence, i.e., the observations at spatial locations are independent in time. we use two representations of stationary max-stable spatial random fields and extend the concepts to the space-time domain. in a first approach, we extend the idea of constructing max-stable random fields as limits of normalized and rescaled pointwise maxima of independent gaussian random fields, which was introduced by kabluchko, schlather and de haan [2009], who construct max-stable random fields associated to a class of variograms. we use a similar approach based on a well-known result by h\""usler and reiss and apply specific spatio-temporal covariance models for the underlying gaussian random field, which satisfy weak regularity assumptions. furthermore, we extend smith's storm profile model to a space-time setting and provide explicit expressions for the bivariate distribution functions.   the tail dependence coefficient is an important measure of extremal dependence. we show how the spatio-temporal covariance function underlying the gaussian random field can be interpreted in terms of the tail dependence coefficient. within this context, we examine different concepts for constructing spatio-temporal covariance models and analyse several specific examples, including gneiting's class of nonseparable stationary covariance functions.","","2011-07-22","","['richard a. davis', 'claudia klüppelberg', 'christina steinkohl']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1626",1108.0184,"modelling chaotic data","math.st stat.th","this paper extends the subjects dicussed in the data analysis and dynamical systems courses by looking at the subject of modelling data. this task is nontrivial as the underlying process could be non-linear. in the paper some common methods, including global and local polynomial fitting, are discussed in terms of their applicability, level of computation and accuracy. one example method, measure based reconstruction, has been investigated in greater detail and experimentation is carried out to evaluate the method. in this project we shall be looking at the different ways one can model chaotic time series. the reason we are going to look at a range of methods is that different methods are ""good"" for different applications. as the ""goodness"" of a model is subjective to the task one wishes to do, we will investigate a selected models and compare the prediction to see how one goes about testing a model.","","2011-07-31","","['vincent mellor']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1627",1108.1884,"estimation of parameters in dna mixture analysis","stat.me","in cowell et al. (2007), a bayesian network for analysis of mixed traces of dna was presented using gamma distributions for modelling peak sizes in the electropherogram. it was demonstrated that the analysis was sensitive to the choice of a variance factor and hence this should be adapted to any new trace analysed. in the present paper we discuss how the variance parameter can be estimated by maximum likelihood to achieve this. the unknown proportions of dna from each contributor can similarly be estimated by maximum likelihood jointly with the variance parameter. furthermore we discuss how to incorporate prior knowledge about the parameters in a bayesian analysis. the proposed estimation methods are illustrated through a few examples of applications for calculating evidential value in casework and for mixture deconvolution.","10.1080/02664763.2013.817549","2011-08-09","2013-06-20","['therese graversen', 'steffen lauritzen']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1628",1108.284,"generalised elastic nets","q-bio.nc cs.lg stat.ml","the elastic net was introduced as a heuristic algorithm for combinatorial optimisation and has been applied, among other problems, to biological modelling. it has an energy function which trades off a fitness term against a tension term. in the original formulation of the algorithm the tension term was implicitly based on a first-order derivative. in this paper we generalise the elastic net model to an arbitrary quadratic tension term, e.g. derived from a discretised differential operator, and give an efficient learning algorithm. we refer to these as generalised elastic nets (gens). we give a theoretical analysis of the tension term for 1d nets with periodic boundary conditions, and show that the model is sensitive to the choice of finite difference scheme that represents the discretised derivative. we illustrate some of these issues in the context of cortical map models, by relating the choice of tension term to a cortical interaction function. in particular, we prove that this interaction takes the form of a mexican hat for the original elastic net, and of progressively more oscillatory mexican hats for higher-order derivatives. the results apply not only to generalised elastic nets but also to other methods using discrete differential penalties, and are expected to be useful in other areas, such as data analysis, computer graphics and optimisation problems.","","2011-08-13","","['miguel á. carreira-perpiñán', 'geoffrey j. goodhill']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1629",1108.4912,"the importance of prior choice in model selection: a density dependence   example","stat.me","we perform a bayesian analysis on abundance data for ten species of north american duck, using the results to investigate the evidence in favour of biologically motivated hypotheses about the causes and mechanisms of density dependence in these species. we explore the capabilities of our methods to detect density dependent effects, both by simulation and through analyzes of real data. the effect of the prior choice on predictive accuracy is also examined. we conclude that our priors, which are motivated by considering the dynamics of the system of interest, offer clear advances over the priors used by previous authors for the duck data sets. we use this analysis as a motivating example to demonstrate the importance of careful parameter prior selection if we are to perform a balanced model selection procedure. we also present some simple guidelines that can be followed in a wide variety of modelling frameworks where vague parameter prior choice is not a viable option. these will produce parameter priors that not only greatly reduce bias in selecting certain models, but improve the predictive ability of the resulting model-averaged predictor.","","2011-08-24","","['james d. lawrence', 'dr. robert b. gramacy', 'dr. len thomas', 'prof. stephen t. buckland']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1630",1109.0863,"identifying differentially expressed transcripts from rna-seq data with   biological variation","q-bio.gn stat.ap","motivation: high-throughput sequencing enables expression analysis at the level of individual transcripts. the analysis of transcriptome expression levels and differential expression estimation requires a probabilistic approach to properly account for ambiguity caused by shared exons and finite read sampling as well as the intrinsic biological variance of transcript expression.   results: we present bitseq (bayesian inference of transcripts from sequencing data), a bayesian approach for estimation of transcript expression level from rna-seq experiments. inferred relative expression is represented by markov chain monte carlo (mcmc) samples from the posterior probability distribution of a generative model of the read data. we propose a novel method for differential expression analysis across replicates which propagates uncertainty from the sample-level model while modelling biological variance using an expression-level-dependent prior. we demonstrate the advantages of our method using simulated data as well as an rna-seq dataset with technical and biological replication for both studied conditions.   availability: the implementation of the transcriptome expression estimation and differential expression analysis, bitseq, has been written in c++.","10.1093/bioinformatics/bts260","2011-09-05","2012-03-05","['peter glaus', 'antti honkela', 'magnus rattray']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1631",1109.4706,"on the fitting of mixtures of multivariate skew t-distributions via the   em algorithm","stat.me","we show how the expectation-maximization (em) algorithm can be applied exactly for the fitting of mixtures of general multivariate skew t (mst) distributions, eliminating the need for computationally expensive monte carlo estimation. finite mixtures of mst distributions have proven to be useful in modelling heterogeneous data with asymmetric and heavy tail behaviour. recently, they have been exploited as an effective tool for modelling flow cytometric data. however, without restrictions on the the characterizations of the component skew t-distributions, monte carlo methods have been used to fit these models. in this paper, we show how the em algorithm can be implemented for the iterative computation of the maximum likelihood estimates of the model parameters without resorting to monte carlo methods for mixtures with unrestricted mst components. the fast calculation of semi-infinite integrals on the e-step of the em algorithm is effected by noting that they can be put in the form of moments of the truncated multivariate t-distribution, which subsequently can be expressed in terms of the non-truncated form of the t-distribution function for which fast algorithms are available. we demonstrate the usefulness of the proposed methodology by some applications to three real data sets.","","2011-09-22","2012-09-05","['s. x. lee', 'g. j. mclachlan']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1632",1109.5894,"learning item trees for probabilistic modelling of implicit feedback","cs.lg stat.ml","user preferences for items can be inferred from either explicit feedback, such as item ratings, or implicit feedback, such as rental histories. research in collaborative filtering has concentrated on explicit feedback, resulting in the development of accurate and scalable models. however, since explicit feedback is often difficult to collect it is important to develop effective models that take advantage of the more widely available implicit feedback. we introduce a probabilistic approach to collaborative filtering with implicit feedback based on modelling the user's item selection process. in the interests of scalability, we restrict our attention to tree-structured distributions over items and develop a principled and efficient algorithm for learning item trees from data. we also identify a problem with a widely used protocol for evaluating implicit feedback models and propose a way of addressing it using a small quantity of explicit feedback data.","","2011-09-27","","['andriy mnih', 'yee whye teh']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1633",1109.6042,"some fundamental properties of a multivariate von mises distribution","math.st stat.th","in application areas like bioinformatics multivariate distributions on angles are encountered which show significant clustering. one approach to statistical modelling of such situations is to use mixtures of unimodal distributions. in the literature (mardia et al., 2011), the multivariate von mises distribution, also known as the multivariate sine distribution, has been suggested for components of such models, but work in the area has been hampered by the fact that no good criteria for the von mises distribution to be unimodal were available. in this article we study the question about when a multivariate von mises distribution is unimodal. we give sufficient criteria for this to be the case and show examples of distributions with multiple modes when these criteria are violated. in addition, we propose a method to generate samples from the von mises distribution in the case of high concentration.","10.1080/03610926.2012.670353","2011-09-27","2013-07-05","['kanti v. mardia', 'jochen voss']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1634",1109.6804,"comparing probabilistic models for melodic sequences","stat.ml","modelling the real world complexity of music is a challenge for machine learning. we address the task of modeling melodic sequences from the same music genre. we perform a comparative analysis of two probabilistic models; a dirichlet variable length markov model (dirichlet-vmm) and a time convolutional restricted boltzmann machine (tc-rbm). we show that the tc-rbm learns descriptive music features, such as underlying chords and typical melody transitions and dynamics. we assess the models for future prediction and compare their performance to a vmm, which is the current state of the art in melody generation. we show that both models perform significantly better than the vmm, with the dirichlet-vmm marginally outperforming the tc-rbm. finally, we evaluate the short order statistics of the models, using the kullback-leibler divergence between test sequences and model samples, and show that our proposed methods match the statistics of the music genre significantly better than the vmm.","","2011-09-30","","['athina spiliopoulou', 'amos storkey']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1635",1110.2894,"two algorithms for fitting constrained marginal models","stat.co stat.me","we study in detail the two main algorithms which have been considered for fitting constrained marginal models to discrete data, one based on lagrange multipliers and the other on a regression model. we show that the updates produced by the two methods are identical, but that the lagrangian method is more efficient in the case of identically distributed observations. we provide a generalization of the regression algorithm for modelling the effect of exogenous individual-level covariates, a context in which the use of the lagrangian algorithm would be infeasible for even moderate sample sizes. an extension of the method to likelihood-based estimation under $l_1$-penalties is also considered.","10.1016/j.csda.2013.02.001","2011-10-13","2012-12-24","['robin j. evans', 'antonio forcina']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"1636",1110.44,"functional uniform priors for nonlinear modelling","stat.co stat.me","this paper considers the topic of finding prior distributions when a major component of the statistical model depends on a nonlinear function. using results on how to construct uniform distributions in general metric spaces, we propose a prior distribution that is uniform in the space of functional shapes of the underlying nonlinear function and then back-transform to obtain a prior distribution for the original model parameters. the primary application considered in this article is nonlinear regression, but the idea might be of interest beyond this case. for nonlinear regression the so constructed priors have the advantage that they are parametrization invariant and do not violate the likelihood principle, as opposed to uniform distributions on the parameters or the jeffrey's prior, respectively. the utility of the proposed priors is demonstrated in the context of nonlinear regression modelling in clinical dose-finding trials, through a real data example and simulation. in addition the proposed priors are used for calculation of an optimal bayesian design.","10.1111/j.1541-0420.2012.01747.x","2011-10-19","","['björn bornkamp']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"1637",1110.4713,"kernel topic models","cs.lg stat.ml","latent dirichlet allocation models discrete data as a mixture of discrete distributions, using dirichlet beliefs over the mixture weights. we study a variation of this concept, in which the documents' mixture weight beliefs are replaced with squashed gaussian distributions. this allows documents to be associated with elements of a hilbert space, admitting kernel topic models (ktm), modelling temporal, spatial, hierarchical, social and other structure between documents. the main challenge is efficient approximate inference on the latent gaussian. we present an approximate algorithm cast around a laplace approximation in a transformed basis. the ktm can also be interpreted as a type of gaussian process latent variable model, or as a topic model conditional on document features, uncovering links between earlier work in these areas.","","2011-10-21","","['philipp hennig', 'david stern', 'ralf herbrich', 'thore graepel']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1638",1110.6054,"lgcp an r package for inference with spatio-temporal log-gaussian cox   processes","stat.co","this paper introduces an r package for spatio-temporal prediction and forecasting for log-gaussian cox processes. the main computational tool for these models is markov chain monte carlo and the new package, lgcp, therefore also provides an extensible suite of functions for implementing mcmc algorithms for processes of this type. the modelling framework and details of inferential procedures are first presented before a tour of lgcp functionality is given via a walk-through data-analysis. topics covered include reading in and converting data, estimation of the key components and parameters of the model, specifying output and simulation quantities, computation of monte carlo expectations, post-processing and simulation of data sets.","","2011-10-27","","['benjamin m. taylor', 'tilman m. davies', 'barry s. rowlingson', 'peter j. diggle']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1639",1111.2411,"an individual-based model of infection spread in an urban environment","q-bio.pe stat.ap","an individual-based model of the infectious disease spread among the urban population is considered. a system of stochastic equations, which describes changes in quantities of four population groups, susceptible, exposed, infected individuals and individuals in the state of remission, is built. the system of equations of the model is supplemented with correlations, which consider disease heaviness and duration for every infected individual. an algorithm and a modelling program based on monte-carlo methods which allows to investigate the group number dynamics is developed.","","2011-11-10","","['vasiliy leonenko']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1640",1111.6899,"extended generalised pareto models for tail estimation","stat.me stat.ap","the most popular approach in extreme value statistics is the modelling of threshold exceedances using the asymptotically motivated generalised pareto distribution. this approach involves the selection of a high threshold above which the model fits the data well. sometimes, few observations of a measurement process might be recorded in applications and so selecting a high quantile of the sample as the threshold leads to almost no exceedances. in this paper we propose extensions of the generalised pareto distribution that incorporate an additional shape parameter while keeping the tail behaviour unaffected. the inclusion of this parameter offers additional structure for the main body of the distribution, improves the stability of the modified scale, tail index and return level estimates to threshold choice and allows a lower threshold to be selected. we illustrate the benefits of the proposed models with a simulation study and two case studies.","10.1016/j.jspi.2012.07.001","2011-11-29","","['ioannis papastathopoulos', 'jonathan a. tawn']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1641",1112.0152,"a skew-t-normal multi-level reduced-rank functional pca model with   applications to replicated `omics time series data sets","stat.me","a powerful study design in the fields of genomics and metabolomics is the 'replicated time course experiment' where individual time series are observed for a sample of biological units, such as human patients, termed replicates. standard practice for analysing these data sets is to fit each variable (e.g. gene transcript) independently with a functional mixed-effects model to account for between-replicate variance. however, such an independence assumption is biologically implausible given that the variables are known to be highly correlated.   in this article we present a skew-t-normal multi-level reduced-rank functional principal components analysis (fpca) model for simultaneously modelling the between-variable and between-replicate variance. the reduced-rank fpca model is computationally efficient and, analogously with a standard pca for vectorial data, provides a low dimensional representation that can be used to identify the major patterns of temporal variation. using an example case study exploring the genetic response to bcg infection we demonstrate that these low dimensional representations are eminently biologically interpretable. we also show using a simulation study that modelling all variables simultaneously greatly reduces the estimation error compared to the independence assumption.","","2011-12-01","","['maurice berk', 'giovanni montana']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1642",1112.4204,"bayesian approaches to copula modelling","stat.me","copula models have become one of the most widely used tools in the applied modelling of multivariate data. similarly, bayesian methods are increasingly used to obtain efficient likelihood-based inference. however, to date, there has been only limited use of bayesian approaches in the formulation and estimation of copula models. this article aims to address this shortcoming in two ways. first, to introduce copula models and aspects of copula theory that are especially relevant for a bayesian analysis. second, to outline bayesian approaches to formulating and estimating copula models, and their advantages over alternative methods. copulas covered include archimedean, copulas constructed by inversion, and vine copulas; along with their interpretation as transformations. a number of parameterisations of a correlation matrix of a gaussian copula are considered, along with hierarchical priors that allow for bayesian selection and model averaging for each parameterisation. markov chain monte carlo sampling schemes for fitting gaussian and d-vine copulas, with and without selection, are given in detail. the relationship between the prior for the parameters of a d-vine, and the prior for a correlation matrix of a gaussian copula, is discussed. last, it is shown how to compute bayesian inference when the data are discrete-valued using data augmentation. this approach generalises popular bayesian methods for the estimation of models for multivariate binary and other ordinal data to more general copula models. bayesian data augmentation has substantial advantages over other methods of estimation for this class of models.","10.1093/acprof:oso/9780199695607.001.0001","2011-12-18","","['michael stanley smith']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1643",1112.5389,"bayesian analysis of hierarchical multi-fidelity codes","math.st stat.th","this paper deals with the gaussian process based approximation of a code which can be run at different levels of accuracy. this method, which is a particular case of co-kriging, allows us to improve a surrogate model of a complex computer code using fast approximations of it. in particular, we focus on the case of a large number of code levels on the one hand and on a bayesian approach when we have two levels on the other hand. the main results of this paper are a new approach to estimate the model parameters which provides a closed form expression for an important parameter of the model (the scale factor), a reduction of the numerical complexity by simplifying the covariance matrix inversion, and a new bayesian modelling that gives an explicit representation of the joint distribution of the parameters and that is not computationally expensive. a thermodynamic example is used to illustrate the comparison between 2-level and 3-level co-kriging.","","2011-12-22","2012-09-24","['loic le gratiet']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1644",1201.0155,"carma processes driven by non-gaussian noise","math.pr math.st stat.th","we present an outline of the theory of certain l\'evy-driven, multivariate stochastic processes, where the processes are represented by rational transfer functions (continuous-time autoregressive moving average or carma models) and their applications in non-gaussian time series modelling. we discuss in detail their definition, their spectral representation, the equivalence to linear state space models and further properties like the second order structure and the tail behaviour under a heavy-tailed input. furthermore, we study the estimation of the parameters using quasi-maximum likelihood estimates for the auto-regressive and moving average parameters, as well as how to estimate the driving l\'evy process.","","2011-12-30","","['robert stelzer']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1645",1201.0633,"general bound of overfitting for mlp regression models","math.st stat.th","multilayer perceptrons (mlp) with one hidden layer have been used for a long time to deal with non-linear regression. however, in some task, mlp's are too powerful models and a small mean square error (mse) may be more due to overfitting than to actual modelling. if the noise of the regression model is gaussian, the overfitting of the model is totally determined by the behavior of the likelihood ratio test statistic (lrts), however in numerous cases the assumption of normality of the noise is arbitrary if not false. in this paper, we present an universal bound for the overfitting of such model under weak assumptions, this bound is valid without gaussian or identifiability assumptions. the main application of this bound is to give a hint about determining the true architecture of the mlp model when the number of data goes to infinite. as an illustration, we use this theoretical result to propose and compare effective criteria to find the true architecture of an mlp.","10.1016/j.neucom.2011.11.028","2012-01-03","","['joseph rynkiewicz']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1646",1201.1461,"a class of infinitely divisible multivariate and matrix gamma   distributions and cone-valued generalised gamma convolutions","math.pr math.st stat.th","classes of multivariate and cone valued infinitely divisible gamma distributions are introduced. particular emphasis is put on the cone-valued case, due to the relevance of infinitely divisible distributions on the positive semi-definite matrices in applications. the cone-valued class of generalised gamma convolutions is studied. in particular, a characterisation in terms of an it\^o-wiener integral with respect to an infinitely divisible random measure associated to the jumps of a l\'evy process is established.   a new example of an infinitely divisible positive definite gamma random matrix is introduced. it has properties which make it appealing for modelling under an infinite divisibility framework. an interesting relation of the moments of the l\'evy measure and the wishart distribution is highlighted which we suppose to be important when considering the limiting distribution of the eigenvalues .","","2012-01-06","","['victor pérez-abreu', 'robert stelzer']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1647",1201.2899,"parameter estimation using empirical likelihood combined with market   information","stat.me q-fin.pr","during the last decade levy processes with jumps have received increasing popularity for modelling market behaviour for both derviative pricing and risk management purposes. chan et al. (2009) introduced the use of empirical likelihood methods to estimate the parameters of various diffusion processes via their characteristic functions which are readily avaiable in most cases. return series from the market are used for estimation. in addition to the return series, there are many derivatives actively traded in the market whose prices also contain information about parameters of the underlying process. this observation motivates us, in this paper, to combine the return series and the associated derivative prices observed at the market so as to provide a more reflective estimation with respect to the market movement and achieve a gain of effciency. the usual asymptotic properties, including consistency and asymptotic normality, are established under suitable regularity conditions. simulation and case studies are performed to demonstrate the feasibility and effectiveness of the proposed method.","","2012-01-13","","['steven kou', 'tony sit', 'zhiliang ying']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1648",1201.3245,"space-time modelling of extreme events","stat.me","max-stable processes are the natural analogues of the generalized extreme-value distribution for the modelling of extreme events in space and time. under suitable conditions, these processes are asymptotically justified models for maxima of independent replications of random fields, and they are also suitable for the modelling of joint individual extreme measurements over high thresholds. this paper extends a model of schlather (2001) to the space-time framework, and shows how a pairwise censored likelihood can be used for consistent estimation under mild mixing conditions. estimator efficiency is also assessed and the choice of pairs to be included in the pairwise likelihood is discussed based on computations for simple time series models. the ideas are illustrated by an application to hourly precipitation data over switzerland.","10.1111/rssb.12035","2012-01-16","","['raphaël huser', 'a. c. davison']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1649",1201.338,"on the relationship between odes and dbns","stat.ap q-bio.qm","recently, li et al. (bioinformatics 27(19), 2686-91, 2011) proposed a method, called differential equation-based local dynamic bayesian network (deldbn), for reverse engineering gene regulatory networks from time-course data. we commend the authors for an interesting paper that draws attention to the close relationship between dynamic bayesian networks (dbns) and differential equations (des). their central claim is that modifying a dbn to model euler approximations to the gradient rather than expression levels themselves is beneficial for network inference. the empirical evidence provided is based on time-course data with equally-spaced observations. however, as we discuss below, in the particular case of equally-spaced observations, euler approximations and conventional dbns lead to equivalent statistical models that, absent artefacts due to the estimation procedure, yield networks with identical inter-gene edge sets. here, we discuss further the relationship between des and conventional dbns and present new empirical results on unequally spaced data which demonstrate that modelling euler approximations in a dbn can lead to improved network reconstruction.","","2012-01-16","2012-03-02","['chris. j. oates', 'steven. m. hill', 'sach mukherjee']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1650",1201.3935,"reliability-based design optimization of imperfect shells using adaptive   kriging meta-models","stat.ap","the optimal and robust design of structures has gained much attention in the past ten years due to the ever increasing need for manufacturers to build robust systems at the lowest cost. reliability-based design optimization (rbdo) allows the analyst to minimize some cost function while ensuring some minimal performances cast as admissible probabilities of failure for a set of performance functions. in order to address real-world problems in which the performance is assessed through computational models (e.g. large scale finite element models) meta-modelling techniques have been developed in the past decade. this paper introduces adaptive kriging surrogate models to solve the rbdo problem. the latter is cast in an augmented space that ""sums up"" the range of the design space and the aleatory uncertainty in the design parameters and the environmental conditions. thus the surrogate model is used (i) for evaluating robust estimates of the probabilities of failure (and for enhancing the computational experimental design by adaptive sampling) in order to achieve the requested accuracy and (ii) for applying the gradient-based optimization algorithm. the approach is applied to the optimal design of imperfect stiffened cylinder shells used in submarine engineering. for this application the performance of the structure is related to buckling which is addressed here by means of the asymptotic numerical method.","","2012-01-18","2012-04-24","['vincent dubourg', 'jean-marc bourinet', 'bruno sudret']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1651",1201.5568,"dynamic trees for streaming and massive data contexts","stat.me stat.ml","data collection at a massive scale is becoming ubiquitous in a wide variety of settings, from vast offline databases to streaming real-time information. learning algorithms deployed in such contexts must rely on single-pass inference, where the data history is never revisited. in streaming contexts, learning must also be temporally adaptive to remain up-to-date against unforeseen changes in the data generating mechanism. although rapidly growing, the online bayesian inference literature remains challenged by massive data and transient, evolving data streams. non-parametric modelling techniques can prove particularly ill-suited, as the complexity of the model is allowed to increase with the sample size. in this work, we take steps to overcome these challenges by porting standard streaming techniques, like data discarding and downweighting, into a fully bayesian framework via the use of informative priors and active learning heuristics. we showcase our methods by augmenting a modern non-parametric modelling framework, dynamic trees, and illustrate its performance on a number of practical examples. the end product is a powerful streaming regression and classification tool, whose performance compares favourably to the state-of-the-art.","","2012-01-26","","['christoforos anagnostopoulos', 'robert b. gramacy']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1652",1202.0709,"mcmc methods for functions: modifying old algorithms to make them faster","stat.co stat.me","many problems arising in applications result in the need to probe a probability distribution for functions. examples include bayesian nonparametric statistics and conditioned diffusion processes. standard mcmc algorithms typically become arbitrarily slow under the mesh refinement dictated by nonparametric description of the unknown function. we describe an approach to modifying a whole range of mcmc methods, applicable whenever the target measure has density with respect to a gaussian process or gaussian random field reference measure, which ensures that their speed of convergence is robust under mesh refinement. gaussian processes or random fields are fields whose marginal distributions, when evaluated at any finite set of $n$ points, are $\mathbb{r}^n$-valued gaussians. the algorithmic approach that we describe is applicable not only when the desired probability measure has density with respect to a gaussian process or gaussian random field reference measure, but also to some useful non-gaussian reference measures constructed through random truncation. in the applications of interest the data is often sparse and the prior specification is an essential part of the overall modelling strategy. these gaussian-based reference measures are a very flexible modelling tool, finding wide-ranging application. examples are shown in density estimation, data assimilation in fluid mechanics, subsurface geophysics and image registration. the key design principle is to formulate the mcmc method so that it is, in principle, applicable for functions; this may be achieved by use of proposals based on carefully chosen time-discretizations of stochastic dynamical systems which exactly preserve the gaussian reference measure. taking this approach leads to many new algorithms which can be implemented via minor modification of existing algorithms, yet which show enormous speed-up on a wide range of applied problems.","10.1214/13-sts421","2012-02-03","2013-10-10","['s. l. cotter', 'g. o. roberts', 'a. m. stuart', 'd. white']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"1653",1203.0106,"sparsity-promoting bayesian dynamic linear models","stat.me stat.co stat.ml","sparsity-promoting priors have become increasingly popular over recent years due to an increased number of regression and classification applications involving a large number of predictors. in time series applications where observations are collected over time, it is often unrealistic to assume that the underlying sparsity pattern is fixed. we propose here an original class of flexible bayesian linear models for dynamic sparsity modelling. the proposed class of models expands upon the existing bayesian literature on sparse regression using generalized multivariate hyperbolic distributions. the properties of the models are explored through both analytic results and simulation studies. we demonstrate the model on a financial application where it is shown that it accurately represents the patterns seen in the analysis of stock and derivative data, and is able to detect major events by filtering an artificial portfolio of assets.","","2012-03-01","","['françois caron', 'luke bornn', 'arnaud doucet']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE
"1654",1203.1515,"multiple change point estimation in stationary ergodic time series","stat.ml cs.it math.it math.st stat.th","given a heterogeneous time-series sample, the objective is to find points in time (called change points) where the probability distribution generating the data has changed. the data are assumed to have been generated by arbitrary unknown stationary ergodic distributions. no modelling, independence or mixing assumptions are made. a novel, computationally efficient, nonparametric method is proposed, and is shown to be asymptotically consistent in this general framework. the theoretical results are complemented with experimental evaluations.","","2012-03-07","2015-05-11","['azadeh khaleghi', 'daniil ryabko']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"1655",1203.3083,"clustering in networks with the collapsed stochastic block model","stat.co","an efficient mcmc algorithm is presented to cluster the nodes of a network such that nodes with similar role in the network are clustered together. this is known as block-modelling or block-clustering. the model is the stochastic blockmodel (sbm) with block parameters integrated out. the resulting marginal distribution defines a posterior over the number of clusters and cluster memberships. sampling from this posterior is simpler than from the original sbm as transdimensional mcmc can be avoided. the algorithm is based on the allocation sampler. it requires a prior to be placed on the number of clusters, thereby allowing the number of clusters to be directly estimated by the algorithm, rather than being given as an input parameter. synthetic and real data are used to test the speed and accuracy of the model and algorithm, including the ability to estimate the number of clusters. the algorithm can scale to networks with up to ten thousand nodes and tens of millions of edges.","","2012-03-14","2012-11-08","['aaron f. mcdaid', 'thomas brendan murphy', 'nial friel', 'neil j hurley']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1656",1203.3366,"enhancing bayesian risk prediction for epidemics using contact tracing","stat.me q-bio.pe","contact tracing data collected from disease outbreaks has received relatively little attention in the epidemic modelling literature because it is thought to be unreliable: infection sources might be wrongly attributed, or data might be missing due to resource contraints in the questionnaire exercise. nevertheless, these data might provide a rich source of information on disease transmission rate. this paper presents novel methodology for combining contact tracing data with rate-based contact network data to improve posterior precision, and therefore predictive accuracy. we present an advancement in bayesian inference for epidemics that assimilates these data, and is robust to partial contact tracing. using a simulation study based on the british poultry industry, we show how the presence of contact tracing data improves posterior predictive accuracy, and can directly inform a more effective control strategy.","","2012-03-15","","['chris jewell', 'gareth roberts']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1657",1203.5446,"a bayesian model committee approach to forecasting global solar   radiation","stat.ap cs.lg","this paper proposes to use a rather new modelling approach in the realm of solar radiation forecasting. in this work, two forecasting models: autoregressive moving average (arma) and neural network (nn) models are combined to form a model committee. the bayesian inference is used to affect a probability to each model in the committee. hence, each model's predictions are weighted by their respective probability. the models are fitted to one year of hourly global horizontal irradiance (ghi) measurements. another year (the test set) is used for making genuine one hour ahead (h+1) out-of-sample forecast comparisons. the proposed approach is benchmarked against the persistence model. the very first results show an improvement brought by this approach.","","2012-03-24","","['philippe lauret', 'auline rodler', 'marc muselli', 'mathieu david', 'hadja diagne', 'cyril voyant']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1658",1204.4611,"applications of the likelihood theory in finance: modelling and pricing","math.st stat.th","this paper discusses the connection between mathematical finance and statistical modelling which turns out to be more than a formal mathematical correspondence. we like to figure out how common results and notions in statistics and their meaning can be translated to the world of mathematical finance and vice versa. a lot of similarities can be expressed in terms of lecam's theory for statistical experiments which is the theory of the behaviour of likelihood processes. for positive prices the arbitrage free financial assets fit into filtered experiments. it is shown that they are given by filtered likelihood ratio processes. from the statistical point of view, martingale measures, completeness and pricing formulas are revisited. the pricing formulas for various options are connected with the power functions of tests. for instance the black-scholes price of a european option has an interpretation as bayes risk of a neyman pearson test. under contiguity the convergence of financial experiments and option prices are obtained. in particular, the approximation of ito type price processes by discrete models and the convergence of associated option prices is studied. the result relies on the central limit theorem for statistical experiments, which is well known in statistics in connection with local asymptotic normal (lan) families. as application certain continuous time option prices can be approximated by related discrete time pricing formulas.","","2012-04-20","","['arnold janssen', 'martin tietje']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1659",1204.5581,"statistical inference for max-stable processes in space and time","stat.me","max-stable processes have proved to be useful for the statistical modelling of spatial extremes. several representations of max-stable random fields have been proposed in the literature. one such representation is based on a limit of normalized and scaled pointwise maxima of stationary gaussian processes that was first introduced by kabluchko, schlather and de haan (2009).   this paper deals with statistical inference for max-stable space-time processes that are defined in an analogous fashion. we describe pairwise likelihood estimation, where the pairwise density of the process is used to estimate the model parameters and prove strong consistency and asymptotic normality of the parameter estimates for an increasing space-time dimension, i.e., as the joint number of spatial locations and time points tends to infinity. a simulation study shows that the proposed method works well for these models.","","2012-04-25","","['richard a. davis', 'claudia klüppelberg', 'christina steinkohl']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1660",1204.5963,"on a reliable peer-review process","cs.gt stat.ap stat.ot","we propose an enhanced peer-review process where the reviewers are encouraged to truthfully disclose their reviews. we start by modelling that process using a bayesian model where the uncertainty regarding the quality of the manuscript is taken into account. after that, we introduce a scoring function to evaluate the reported reviews. under mild assumptions, we show that reviewers strictly maximize their expected scores by telling the truth. we also show how those scores can be used in order to reach consensus.","","2012-04-26","2013-06-26","['arthur carvalho', 'kate larson']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE
"1661",1205.1997,"model-based clustering in networks with stochastic community finding","stat.co cs.si physics.soc-ph","in the model-based clustering of networks, blockmodelling may be used to identify roles in the network. we identify a special case of the stochastic block model (sbm) where we constrain the cluster-cluster interactions such that the density inside the clusters of nodes is expected to be greater than the density between clusters. this corresponds to the intuition behind community-finding methods, where nodes tend to clustered together if they link to each other. we call this model stochastic community finding (scf) and present an efficient mcmc algorithm which can cluster the nodes, given the network. the algorithm is evaluated on synthetic data and is applied to a social network of interactions at a karate club and at a monastery, demonstrating how the scf finds the 'ground truth' clustering where sometimes the sbm does not. the scf is only one possible form of constraint or specialization that may be applied to the sbm. in a more supervised context, it may be appropriate to use other specializations to guide the sbm.","","2012-05-09","2012-10-28","['aaron f. mcdaid', 'brendan thomas murphy', 'nial friel', 'neil j. hurley']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1662",1205.4159,"theory of dependent hierarchical normalized random measures","cs.lg math.st stat.ml stat.th","this paper presents theory for normalized random measures (nrms), normalized generalized gammas (nggs), a particular kind of nrm, and dependent hierarchical nrms which allow networks of dependent nrms to be analysed. these have been used, for instance, for time-dependent topic modelling. in this paper, we first introduce some mathematical background of completely random measures (crms) and their construction from poisson processes, and then introduce nrms and nggs. slice sampling is also introduced for posterior inference. the dependency operators in poisson processes and for the corresponding crms and nrms is then introduced and posterior inference for the ngg presented. finally, we give dependency and composition results when applying these operators to nrms so they can be used in a network with hierarchical and dependent relations.","","2012-05-18","2012-05-25","['changyou chen', 'wray buntine', 'nan ding']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE
"1663",1206.1268,"parameter estimation through ignorance","physics.data-an math-ph math.mp stat.ap","dynamical modelling lies at the heart of our understanding of physical systems. its role in science is deeper than mere operational forecasting, in that it allows us to evaluate the adequacy of the mathematical structure of our models. despite the importance of model parameters, there is no general method of parameter estimation outside linear systems. a new relatively simple method of parameter estimation for nonlinear systems is presented, based on variations in the accuracy of probability forecasts. it is illustrated on the logistic map, the henon map and the 12-d lorenz96 flow, and its ability to outperform linear least squares in these systems is explored at various noise levels and sampling rates. as expected, it is more effective when the forecast error distributions are non-gaussian. the new method selects parameter values by minimizing a proper, local skill score for continuous probability forecasts as a function of the parameter values. this new approach is easier to implement in practice than alternative nonlinear methods based on the geometry of attractors or the ability of the model to shadow the observations. new direct measures of inadequacy in the model, the ""implied ignorance"" and the information deficit are introduced.","10.1103/physreve.86.016213","2012-06-06","","['hailiang du', 'leonard a. smith']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1664",1206.2054,"maximum a posteriori covariance estimation using a power inverse wishart   prior","stat.me","the estimation of the covariance matrix is an initial step in many multivariate statistical methods such as principal components analysis and factor analysis, but in many practical applications the dimensionality of the sample space is large compared to the number of samples, and the usual maximum likelihood estimate is poor. typically, improvements are obtained by modelling or regularization. from a practical point of view, these methods are often computationally heavy and rely on approximations. as a fast substitute, we propose an easily calculable maximum a posteriori (map) estimator based on a new class of prior distributions generalizing the inverse wishart prior, discuss its properties, and demonstrate the estimator on simulated and real data.","","2012-06-10","","['søren feodor nielsen', 'jon sporring']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1665",1206.3833,"a bayesian spatio-temporal model of panel design data: airborne particle   number concentration in brisbane, australia","stat.ap physics.ao-ph physics.data-an","this paper outlines a methodology for semi-parametric spatio-temporal modelling of data which is dense in time but sparse in space, obtained from a split panel design, the most feasible approach to covering space and time with limited equipment. the data are hourly averaged particle number concentration (pnc) and were collected, as part of the ultrafine particles from transport emissions and child health (uptech) project. two weeks of continuous measurements were taken at each of a number of government primary schools in the brisbane metropolitan area. the monitoring equipment was taken to each school sequentially. the school data are augmented by data from long term monitoring stations at three locations in brisbane, australia.   fitting the model helps describe the spatial and temporal variability at a subset of the uptech schools and the long-term monitoring sites. the temporal variation is modelled hierarchically with penalised random walk terms, one common to all sites and a term accounting for the remaining temporal trend at each site. parameter estimates and their uncertainty are computed in a computationally efficient approximate bayesian inference environment, r-inla.   the temporal part of the model explains daily and weekly cycles in pnc at the schools, which can be used to estimate the exposure of school children to ultrafine particles (ufps) emitted by vehicles. at each school and long-term monitoring site, peaks in pnc can be attributed to the morning and afternoon rush hour traffic and new particle formation events. the spatial component of the model describes the school to school variation in mean pnc at each school and within each school ground. it is shown how the spatial model can be expanded to identify spatial patterns at the city scale with the inclusion of more spatial locations.","10.1002/env.2597","2012-06-18","2013-02-06","['sam clifford', 'sama low choy', 'mandana mazaheri', 'farhad salimi', 'lidia morawska', 'kerrie mengsersen']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1666",1206.461,"manifold relevance determination","cs.lg cs.cv stat.ml","in this paper we present a fully bayesian latent variable model which exploits conditional nonlinear(in)-dependence structures to learn an efficient latent representation. the latent space is factorized to represent shared and private information from multiple views of the data. in contrast to previous approaches, we introduce a relaxation to the discrete segmentation and allow for a ""softly"" shared latent space. further, bayesian techniques allow us to automatically estimate the dimensionality of the latent spaces. the model is capable of capturing structure underlying extremely high dimensional spaces. this is illustrated by modelling unprocessed images with tenths of thousands of pixels. this also allows us to directly generate novel images from the trained model by sampling from the discovered latent spaces. we also demonstrate the model by prediction of human pose in an ambiguous setting. our bayesian framework allows us to perform disambiguation in a principled manner by including latent space priors which incorporate the dynamic nature of the data.","","2012-06-18","","['andreas damianou', 'carl ek', 'michalis titsias', 'neil lawrence']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1667",1206.6839,"fitting graphical interaction models to multivariate time series","stat.me","graphical interaction models have become an important tool for analysing multivariate time series. in these models, the interrelationships among the components of a time series are described by undirected graphs in which the vertices depict the components while the edges indictate possible dependencies between the components. current methods for the identification of the graphical structure are based on nonparametric spectral stimation, which prevents application of common model selection strategies. in this paper, we present a parametric approach for graphical interaction modelling of multivariate stationary time series. the proposed models generalize covariance selection models to the time series setting and are formulated in terms of inverse covariances. we show that these models correspond to vector autoregressive models under conditional independence constraints encoded by undirected graphs. furthermore, we discuss maximum likelihood estimation based on whittle's approximation to the log-likelihood function and propose an iterative method for solving the resulting likelihood equations. the concepts are illustrated by an example.","","2012-06-27","","['michael eichler']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1668",1206.6868,"bayesian random fields: the bethe-laplace approximation","cs.lg stat.ml","while learning the maximum likelihood value of parameters of an undirected graphical model is hard, modelling the posterior distribution over parameters given data is harder. yet, undirected models are ubiquitous in computer vision and text modelling (e.g. conditional random fields). but where bayesian approaches for directed models have been very successful, a proper bayesian treatment of undirected models in still in its infant stages. we propose a new method for approximating the posterior of the parameters given data based on the laplace approximation. this approximation requires the computation of the covariance matrix over features which we compute using the linear response approximation based in turn on loopy belief propagation. we develop the theory for conditional and 'unconditional' random fields with or without hidden variables. in the conditional setting we introduce a new variant of bagging suitable for structured domains. here we run the loopy max-product algorithm on a 'super-graph' composed of graphs for individual models sampled from the posterior and connected by constraints. experiments on real world data validate the proposed methods.","","2012-06-27","","['max welling', 'sridevi parise']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1669",1207.036,"a sarimax coupled modelling applied to individual load curves intraday   forecasting","stat.ap","a dynamic coupled modelling is investigated to take temperature into account in the individual energy consumption forecasting. the objective is both to avoid the inherent complexity of exhaustive sarimax models and to take advantage of the usual linear relation between energy consumption and temperature for thermosensitive customers. we first recall some issues related to individual load curves forecasting. then, we propose and study the properties of a dynamic coupled modelling taking temperature into account as an exogenous contribution and its application to the intraday prediction of energy consumption. finally, these theoretical results are illustrated on a real individual load curve. the authors discuss the relevance of such an approach and anticipate that it could form a substantial alternative to the commonly used methods for energy consumption forecasting of individual customers.","","2012-07-02","","['sophie bercu', 'frédéric proïa']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1670",1207.0558,"bayesian semi-parametric forecasting of ultrafine particle number   concentration with penalised splines and autoregressive errors","stat.ap physics.ao-ph physics.data-an stat.me","observational time series data often exhibit both cyclic temporal trends and autocorrelation and may also depend on covariates. as such, there is a need for flexible regression models that are able to capture these trends and model any residual autocorrelation simultaneously. modelling the autocorrelation in the residuals leads to more realistic forecasts than an assumption of independence. in this paper we propose a method which combines spline-based semi-parametric regression modelling with the modelling of auto-regressive errors.   the method is applied to a simulated data set in order to show its efficacy and to ultrafine particle number concentration in helsinki, finland, to show its use in real world problems.","","2012-07-02","2012-09-25","['sam clifford', 'bjarke mølgaard', 'sama low choy', 'jukka corander', 'kaarle hämeri', 'kerrie mengersen', 'tareq hussein']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1671",1207.1727,"mixtures of shifted asymmetric laplace distributions","stat.me stat.co stat.ml","a mixture of shifted asymmetric laplace distributions is introduced and used for clustering and classification. a variant of the em algorithm is developed for parameter estimation by exploiting the relationship with the general inverse gaussian distribution. this approach is mathematically elegant and relatively computationally straightforward. our novel mixture modelling approach is demonstrated on both simulated and real data to illustrate clustering and classification applications. in these analyses, our mixture of shifted asymmetric laplace distributions performs favourably when compared to the popular gaussian approach. this work, which marks an important step in the non-gaussian model-based clustering and classification direction, concludes with discussion as well as suggestions for future work.","10.1109/tpami.2013.216","2012-07-06","2012-12-21","['brian c. franczak', 'ryan p. browne', 'paul d. mcnicholas']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE
"1672",1207.1916,"how good are matlab, octave and scilab for computational modelling?","cs.ms stat.co","in this article we test the accuracy of three platforms used in computational modelling: matlab, octave and scilab, running on i386 architecture and three operating systems (windows, ubuntu and mac os). we submitted them to numerical tests using standard data sets and using the functions provided by each platform. a monte carlo study was conducted in some of the datasets in order to verify the stability of the results with respect to small departures from the original input. we propose a set of operations which include the computation of matrix determinants and eigenvalues, whose results are known. we also used data provided by nist (national institute of standards and technology), a protocol which includes the computation of basic univariate statistics (mean, standard deviation and first-lag correlation), linear regression and extremes of probability distributions. the assessment was made comparing the results computed by the platforms with certified values, that is, known results, computing the number of correct significant digits.","","2012-07-08","","['eliana s. de almeida', 'antonio c. medeiros', 'alejandro c. frery']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1673",1207.411,"the minimum information principle for discriminative learning","cs.lg stat.ml","exponential models of distributions are widely used in machine learning for classiffication and modelling. it is well known that they can be interpreted as maximum entropy models under empirical expectation constraints. in this work, we argue that for classiffication tasks, mutual information is a more suitable information theoretic measure to be optimized. we show how the principle of minimum mutual information generalizes that of maximum entropy, and provides a comprehensive framework for building discriminative classiffiers. a game theoretic interpretation of our approach is then given, and several generalization bounds provided. we present iterative algorithms for solving the minimum information problem and its convex dual, and demonstrate their performance on various classiffication tasks. the results show that minimum information classiffiers outperform the corresponding maximum entropy models.","","2012-07-11","","['amir globerson', 'naftali tishby']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1674",1207.4125,"applying discrete pca in data analysis","cs.lg stat.ml","methods for analysis of principal components in discrete data have existed for some time under various names such as grade of membership modelling, probabilistic latent semantic analysis, and genotype inference with admixture. in this paper we explore a number of extensions to the common theory, and present some application of these methods to some common statistical tasks. we show that these methods can be interpreted as a discrete version of ica. we develop a hierarchical version yielding components at different levels of detail, and additional techniques for gibbs sampling. we compare the algorithms on a text prediction task using support vector machines, and to information retrieval.","","2012-07-11","","['wray l. buntine', 'aleks jakulin']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1675",1207.4145,"joint discovery of haplotype blocks and complex trait associations from   snp sequences","q-bio.gn cs.ce stat.me","haplotypes, the global patterns of dna sequence variation, have important implications for identifying complex traits. recently, blocks of limited haplotype diversity have been discovered in human chromosomes, intensifying the research on modelling the block structure as well as the transitions or co-occurrence of the alleles in these blocks as a way to compress the variability and infer the associations more robustly. the haplotype block structure analysis is typically complicated by the fact that the phase information for each snp is missing, i.e., the observed allele pairs are not given in a consistent order across the sequence. the techniques for circumventing this require additional information, such as family data, or a more complex sequencing procedure. in this paper we present a hierarchical statistical model and the associated learning and inference algorithms that simultaneously deal with the allele ambiguity per locus, missing data, block estimation, and the complex trait association. while the blo structure may differ from the structures inferred by other methods, which use the pedigree information or previously known alleles, the parameters we estimate, including the learned block structure and the estimated block transitions per locus, define a good model of variability in the set. the method is completely datadriven and can detect chron's disease from the snp data taken from the human chromosome 5q31 with the detection rate of 80% and a small error variance.","","2012-07-11","","['nebojsa jojic', 'vladimir jojic', 'david heckerman']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1676",1208.3659,"thermal effects for shaft-pre-stress on rotor dynamic system","stat.ap","this paper outlines study behaviour of rotating shaft with high speed under thermal effects. the method of obtaining the frequency response functions of a rotor system with study whirl effect in this revision the raw data obtained from the experimental results (using smart office program) are curve-fitted by theoretical data regenerated from some of the experimental data and simulating it using finite element (ansys 12). (fe) models using the eigen analysis capability were used to simulate the vibration. the results were compared with experimental data show analysis data with acceptable accuracy and performance. the rotating effect causes un-symmetry in the system matrices, resulting in complexity in decoupling the mathematical models of the system for the purpose of modal analysis. different method is therefore required, which can handle general system matrices rather than symmetrical matrices, which is normal for passive structures. mathematical model of the system from the test data can be assembled. the frequency response functions are extracted, campbell diagram are draw and simulated. (fe) is used to carry out such as simulation since it has good capability for eigen analysis and also good graphical facility.   keywords: thermal effects, modelling, campbell diagram, whirl, rotor dynamics.","","2012-08-16","","['hisham a. h. al-khazali', 'mohamad r. askari']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1677",1208.4462,"accept & reject statement-based uncertainty models","math.pr math.st stat.me stat.th","we develop a framework for modelling and reasoning with uncertainty based on accept and reject statements about gambles. it generalises the frameworks found in the literature based on statements of acceptability, desirability, or favourability and clarifies their relative position. next to the statement-based formulation, we also provide a translation in terms of preference relations, discuss---as a bridge to existing frameworks---a number of simplified variants, and show the relationship with prevision-based uncertainty models. we furthermore provide an application to modelling symmetry judgements.","10.1016/j.ijar.2014.12.003","2012-08-22","2015-01-23","['erik quaeghebeur', 'gert de cooman', 'filip hermans']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1678",1208.5311,"assessing the health of richibucto estuary with the latent health factor   index","stat.ap q-bio.qm","the ability to quantitatively assess the health of an ecosystem is often of great interest to those tasked with monitoring and conserving ecosystems. for decades, research in this area has relied upon multimetric indices of various forms. although indices may be numbers, many are constructed based on procedures that are highly qualitative in nature, thus limiting the quantitative rigour of the practical interpretations made from these indices. the statistical modelling approach to construct the latent health factor index (lhfi) was recently developed to express ecological data, collected to construct conventional multimetric health indices, in a rigorous quantitative model that integrates qualitative features of ecosystem health and preconceived ecological relationships among such features. this hierarchical modelling approach allows (a) statistical inference of health for observed sites and (b) prediction of health for unobserved sites, all accompanied by formal uncertainty statements. thus far, the lhfi approach has been demonstrated and validated on freshwater ecosystems. the goal of this paper is to adapt this approach to modelling estuarine ecosystem health, particularly that of the previously unassessed system in richibucto in new brunswick, canada. field data correspond to biotic health metrics that constitute the azti marine biotic index (ambi) and abiotic predictors preconceived to influence biota. we also briefly discuss related lhfi research involving additional metrics that form the infaunal trophic index (iti). our paper is the first to construct a scientifically sensible model to rigorously identify the collective explanatory capacity of salinity, distance downstream, channel depth, and silt-clay content --- all regarded a priori as qualitatively important abiotic drivers --- towards site health in the richibucto ecosystem.","10.1371/journal.pone.0065697","2012-08-27","2013-06-24","['margaret wu', 'grace s. chiu', 'lin lu']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1679",1208.5376,"conditional simulation of max-stable processes","stat.me stat.ap","since many environmental processes such as heat waves or precipitation are spatial in extent, it is likely that a single extreme event affects several locations and the areal modelling of extremes is therefore essential if the spatial dependence of extremes has to be appropriately taken into account. this paper proposes a framework for conditional simulations of max-stable processes and give closed forms for brown-resnick and schlather processes. we test the method on simulated data and give an application to extreme rainfall around zurich and extreme temperature in switzerland. results show that the proposed framework provides accurate conditional simulations and can handle real-sized problems.","","2012-08-27","","['clément dombry', 'frédéric éyi-minko', 'mathieu ribatet']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1680",1209.1988,"computational information geometry: theory and practice","math.st stat.co stat.th","this paper lays the foundations for a unified framework for numerically and computationally applying methods drawn from a range of currently distinct geometrical approaches to statistical modelling. in so doing, it extends information geometry from a manifold based approach to one where the simplex is the fundamental geometrical object, thereby allowing applications to models which do not have a fixed dimension or support. finally, it starts to build a computational framework which will act as a proxy for the 'space of all distributions' that can be used, in particular, to investigate model selection and model uncertainty. a varied set of substantive running examples is used to illustrate theoretical and practical aspects of the discussion. further developments are briefly indicated.","","2012-09-10","","['karim anaya-izquierdo', 'frank critchley', 'paul marriott', 'paul w. vos']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE
"1681",1209.373,"a bayesian method for the analysis of deterministic and stochastic time   series","astro-ph.im astro-ph.sr physics.data-an stat.ml","i introduce a general, bayesian method for modelling univariate time series data assumed to be drawn from a continuous, stochastic process. the method accommodates arbitrary temporal sampling, and takes into account measurement uncertainties for arbitrary error models (not just gaussian) on both the time and signal variables. any model for the deterministic component of the variation of the signal with time is supported, as is any model of the stochastic component on the signal and time variables. models illustrated here are constant and sinusoidal models for the signal mean combined with a gaussian stochastic component, as well as a purely stochastic model, the ornstein-uhlenbeck process. the posterior probability distribution over model parameters is determined via monte carlo sampling. models are compared using the ""cross-validation likelihood"", in which the posterior-averaged likelihood for different partitions of the data are combined. in principle this is more robust to changes in the prior than is the evidence (the prior-averaged likelihood). the method is demonstrated by applying it to the light curves of 11 ultra cool dwarf stars, claimed by a previous study to show statistically significant variability. this is reassessed here by calculating the cross-validation likelihood for various time series models, including a null hypothesis of no variability beyond the error bars. 10 of 11 light curves are confirmed as being significantly variable, and one of these seems to be periodic, with two plausible periods identified. another object is best described by the ornstein-uhlenbeck process, a conclusion which is obviously limited to the set of models actually tested.","10.1051/0004-6361/201220109","2012-09-17","2012-10-23","['c. a. l. bailer-jones']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1682",1209.5561,"supervised blockmodelling","cs.lg cs.si stat.ml","collective classification models attempt to improve classification performance by taking into account the class labels of related instances. however, they tend not to learn patterns of interactions between classes and/or make the assumption that instances of the same class link to each other (assortativity assumption). blockmodels provide a solution to these issues, being capable of modelling assortative and disassortative interactions, and learning the pattern of interactions in the form of a summary network. the supervised blockmodel provides good classification performance using link structure alone, whilst simultaneously providing an interpretable summary of network interactions to allow a better understanding of the data. this work explores three variants of supervised blockmodels of varying complexity and tests them on four structurally different real world networks.","","2012-09-25","","['leto peel']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1683",1210.0685,"local stability and robustness of sparse dictionary learning in the   presence of noise","stat.ml cs.lg","a popular approach within the signal processing and machine learning communities consists in modelling signals as sparse linear combinations of atoms selected from a learned dictionary. while this paradigm has led to numerous empirical successes in various fields ranging from image to audio processing, there have only been a few theoretical arguments supporting these evidences. in particular, sparse coding, or sparse dictionary learning, relies on a non-convex procedure whose local minima have not been fully analyzed yet. in this paper, we consider a probabilistic model of sparse signals, and show that, with high probability, sparse coding admits a local minimum around the reference dictionary generating the signals. our study takes into account the case of over-complete dictionaries and noisy signals, thus extending previous work limited to noiseless settings and/or under-complete dictionaries. the analysis we conduct is non-asymptotic and makes it possible to understand how the key quantities of the problem, such as the coherence or the level of noise, can scale with respect to the dimension of the signals, the number of atoms, the sparsity and the number of observations.","","2012-10-02","","['rodolphe jenatton', 'rémi gribonval', 'francis bach']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1684",1210.2503,"gaussian process modelling of multiple short time series","stat.ml q-bio.qm stat.me","we present techniques for effective gaussian process (gp) modelling of multiple short time series. these problems are common when applying gp models independently to each gene in a gene expression time series data set. such sets typically contain very few time points. naive application of common gp modelling techniques can lead to severe over-fitting or under-fitting in a significant fraction of the fitted models, depending on the details of the data set. we propose avoiding over-fitting by constraining the gp length-scale to values that focus most of the energy spectrum to frequencies below the nyquist frequency corresponding to the sampling frequency in the data set. under-fitting can be avoided by more informative priors on observation noise. combining these methods allows applying gp methods reliably automatically to large numbers of independent instances of short time series. this is illustrated with experiments with both synthetic data and real gene expression data.","","2012-10-09","","['hande topa', 'antti honkela']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1685",1210.306,"markov kernels and the conditional extreme value model","math.st stat.th","the classical approach to multivariate extreme value modelling assumes that the joint distribution belongs to a multivariate domain of attraction. this requires each marginal distribution be individually attracted to a univariate extreme value distribution. an apparently more flexible extremal model for multivariate data was proposed by heffernan and tawn under which not all the components are required to belong to an extremal domain of attraction but assumes instead the existence of an asymptotic approximation to the conditional distribution of the random vector given one of the components is extreme. combined with the knowledge that the conditioning component belongs to a univariate domain of attraction, this leads to an approximation of the probability of certain risk regions. the original focus on conditional distributions had technical drawbacks but is natural in several contexts. we place this approach in the context of the more general approach using convergence of measures and multivariate regular variation on cones.","","2012-10-10","","['sidney resnick', 'david zeber']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1686",1210.3087,"therapeutic hypothermia: quantification of the transition of core body   temperature using the flexible mixture bent-cable model for longitudinal data","stat.me q-bio.to","by reducing core body temperature, t_c, induced hypothermia is a therapeutic tool to prevent brain damage resulting from physical trauma. however, all physiological systems begin to slow down due to hypothermia that in turn can result in increased risk of mortality. therefore, quantification of the transition of t_c to early hypothermia is of great clinical interest. conceptually, t_c may exhibit an either gradual or abrupt transition. bent-cable regression is an appealing statistical tool to model such data due to the model's flexibility and greatly interpretable regression coefficients. it handles more flexibly models that traditionally have been handled by low-order polynomial models (for gradual transition) or piecewise linear changepoint models (for abrupt change). we consider a rat model for humans to quantify the temporal trend of t_c to primarily address the question: what is the critical time point associated with a breakdown in the compensatory mechanisms following the start of hypothermia therapy? to this end, we develop a bayesian modelling framework for bent-cable regression of longitudinal data to simultaneously account for gradual and abrupt transitions. our analysis reveals that: (a) about 39% of rats exhibit a gradual transition in t_c; (b) the critical time point is approximately the same regardless of transition type; (c) both transition types show a significant increase of t_c followed by a significant decrease.","10.1111/anzs.12047","2012-10-10","2013-04-14","['shahedul a khan', 'grace s chiu', 'joel a dubin']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1687",1210.3831,"graphical modelling in genetics and systems biology","stat.me q-bio.mn stat.ml","graphical modelling has a long history in statistics as a tool for the analysis of multivariate data, starting from wright's path analysis and gibbs' applications to statistical physics at the beginning of the last century. in its modern form, it was pioneered by lauritzen and wermuth and pearl in the 1980s, and has since found applications in fields as diverse as bioinformatics, customer satisfaction surveys and weather forecasts.   genetics and systems biology are unique among these fields in the dimension of the data sets they study, which often contain several hundreds of variables and only a few tens or hundreds of observations. this raises problems in both computational complexity and the statistical significance of the resulting networks, collectively known as the ""curse of dimensionality"". furthermore, the data themselves are difficult to model correctly due to the limited understanding of the underlying mechanisms. in the following, we will illustrate how such challenges affect practical graphical modelling and some possible solutions.","","2012-10-14","2014-01-12","['marco scutari']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1688",1210.3851,"an introduction to particle integration methods: with applications to   risk and insurance","q-fin.cp math.st q-fin.rm stat.th","interacting particle methods are increasingly used to sample from complex and high-dimensional distributions. these stochastic particle integration techniques can be interpreted as an universal acceptance-rejection sequential particle sampler equipped with adaptive and interacting recycling mechanisms. practically, the particles evolve randomly around the space independently and to each particle is associated a positive potential function. periodically, particles with high potentials duplicate at the expense of low potential particle which die. this natural genetic type selection scheme appears in numerous applications in applied probability, physics, bayesian statistics, signal processing, biology, and information engineering. it is the intention of this paper to introduce them to risk modeling. from a purely mathematical point of view, these stochastic samplers can be interpreted as feynman-kac particle integration methods. these functional models are natural mathematical extensions of the traditional change of probability measures, commonly used to design an importance sampling strategy. in this article, we provide a brief introduction to the stochastic modeling and the theoretical analysis of these particle algorithms. then we conclude with an illustration of a subset of such methods to resolve important risk measure and capital estimation in risk and insurance modelling.","","2012-10-14","2012-10-29","['p. del moral', 'g. w. peters', 'ch. vergé']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1689",1210.4844,"plackett-luce regression: a new bayesian model for polychotomous data","stat.me stat.ap stat.co","multinomial logistic regression is one of the most popular models for modelling the effect of explanatory variables on a subject choice between a set of specified options. this model has found numerous applications in machine learning, psychology or economy. bayesian inference in this model is non trivial and requires, either to resort to a metropolishastings algorithm, or rejection sampling within a gibbs sampler. in this paper, we propose an alternative model to multinomial logistic regression. the model builds on the plackett-luce model, a popular model for multiple comparisons. we show that the introduction of a suitable set of auxiliary variables leads to an expectation-maximization algorithm to find maximum a posteriori estimates of the parameters. we further provide a full bayesian treatment by deriving a gibbs sampler, which only requires to sample from highly standard distributions. we also propose a variational approximate inference scheme. all are very simple to implement. one property of our plackett-luce regression model is that it learns a sparse set of feature weights. we compare our method to sparse bayesian multinomial logistic regression and show that it is competitive, especially in presence of polychotomous data.","","2012-10-16","","['cedric archambeau', 'francois caron']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"1690",1210.7485,"approximation multivariate distribution with pair copula using the   orthonormal polynomial and legendre multiwavelets basis functions","stat.co","in this paper, we concentrate on new methodologies for copulas introduced and developed by joe, cooke, bedford, kurowica, daneshkhah and others on the new class of graphical models called vines as a way of constructing higher dimensional distributions. we develop the approximation method presented by bedford et al (2012) at which they show that any $n$-dimensional copula density can be approximated arbitrarily well pointwise using a finite parameter set of 2-dimensional copulas in a vine or pair-copula construction. our constructive approach involves the use of minimum information copulas that can be specified to any required degree of precision based on the available data or experts' judgements. by using this method, we are able to use a fixed finite dimensional family of copulas to be employed in a vine construction, with the promise of a uniform level of approximation.   the basic idea behind this method is to use a two-dimensional ordinary polynomial series to approximate any log-density of a bivariate copula function by truncating the series at an appropriate point. we present an alternative approximation of the multivariate distribution of interest by considering orthonormal polynomial and legendre multiwavelets as the basis functions. we show the derived approximations are more precise and computationally faster with better properties than the one proposed by bedford et al. (2012). we then apply our method to modelling a dataset of norwegian financial data that was previously analysed in the series of papers, and finally compare our results by them.","","2012-10-28","","['alireza daneshkhah', 'golamali parham', 'omid chatrabgoun', 'm. jokar']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1691",1211.0174,"laplace approximation for logistic gaussian process density estimation   and regression","stat.co stat.me stat.ml","logistic gaussian process (lgp) priors provide a flexible alternative for modelling unknown densities. the smoothness properties of the density estimates can be controlled through the prior covariance structure of the lgp, but the challenge is the analytically intractable inference. in this paper, we present approximate bayesian inference for lgp density estimation in a grid using laplace's method to integrate over the non-gaussian posterior distribution of latent function values and to determine the covariance function parameters with type-ii maximum a posteriori (map) estimation. we demonstrate that laplace's method with map is sufficiently fast for practical interactive visualisation of 1d and 2d densities. our experiments with simulated and real 1d data sets show that the estimation accuracy is close to a markov chain monte carlo approximation and state-of-the-art hierarchical infinite gaussian mixture models. we also construct a reduced-rank approximation to speed up the computations for dense 2d grids, and demonstrate density regression with the proposed laplace approach.","","2012-11-01","2013-10-07","['jaakko riihimäki', 'aki vehtari']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE
"1692",1211.0358,"deep gaussian processes","stat.ml cs.lg math.pr","in this paper we introduce deep gaussian process (gp) models. deep gps are a deep belief network based on gaussian process mappings. the data is modeled as the output of a multivariate gp. the inputs to that gaussian process are then governed by another gp. a single layer model is equivalent to a standard gp or the gp latent variable model (gp-lvm). we perform inference in the model by approximate variational marginalization. this results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. our fully bayesian treatment allows for the application of deep models even when data is scarce. model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.","","2012-11-01","2013-03-22","['andreas c. damianou', 'neil d. lawrence']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1693",1211.0906,"algorithm runtime prediction: methods & evaluation","cs.ai cs.lg cs.pf stat.ml","perhaps surprisingly, it is possible to predict how long an algorithm will take to run on a previously unseen input, using machine learning techniques to build a model of the algorithm's runtime as a function of problem-specific instance features. such models have important applications to algorithm analysis, portfolio-based algorithm selection, and the automatic configuration of parameterized algorithms. over the past decade, a wide variety of techniques have been studied for building such models. here, we describe extensions and improvements of existing models, new families of models, and -- perhaps most importantly -- a much more thorough treatment of algorithm parameters as model inputs. we also comprehensively describe new and existing features for predicting algorithm runtime for propositional satisfiability (sat), travelling salesperson (tsp) and mixed integer programming (mip) problems. we evaluate these innovations through the largest empirical analysis of its kind, comparing to a wide range of runtime modelling techniques from the literature. our experiments consider 11 algorithms and 35 instance distributions; they also span a very wide range of sat, mip, and tsp instances, with the least structured having been generated uniformly at random and the most structured having emerged from real industrial applications. overall, we demonstrate that our new models yield substantially better runtime predictions than previous approaches in terms of their generalization to new problem instances, to new algorithms from a parameterized space, and to both simultaneously.","","2012-11-05","2013-10-26","['frank hutter', 'lin xu', 'holger h. hoos', 'kevin leyton-brown']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1694",1211.1328,"random walk kernels and learning curves for gaussian process regression   on random graphs","stat.ml cond-mat.dis-nn cond-mat.stat-mech cs.lg","we consider learning on graphs, guided by kernels that encode similarity between vertices. our focus is on random walk kernels, the analogues of squared exponential kernels in euclidean spaces. we show that on large, locally treelike, graphs these have some counter-intuitive properties, specifically in the limit of large kernel lengthscales. we consider using these kernels as covariance matrices of e.g.\ gaussian processes (gps). in this situation one typically scales the prior globally to normalise the average of the prior variance across vertices. we demonstrate that, in contrast to the euclidean case, this generically leads to significant variation in the prior variance across vertices, which is undesirable from the probabilistic modelling point of view. we suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for gaussian process regression. numerical calculations as well as novel theoretical predictions for the learning curves using belief propagation make it clear that one obtains distinctly different probabilistic models depending on the choice of normalisation. our method for predicting the learning curves using belief propagation is significantly more accurate than previous approximations and should become exact in the limit of large random graphs.","","2012-11-06","2013-09-30","['matthew urry', 'peter sollich']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1695",1211.1717,"bayesian learning and predictability in a stochastic nonlinear dynamical   model","stat.ap","bayesian inference methods are applied within a bayesian hierarchical modelling framework to the problems of joint state and parameter estimation, and of state forecasting. we explore and demonstrate the ideas in the context of a simple nonlinear marine biogeochemical model. a novel approach is proposed to the formulation of the stochastic process model, in which ecophysiological properties of plankton communities are represented by autoregressive stochastic processes. this approach captures the effects of changes in plankton communities over time, and it allows the incorporation of literature metadata on individual species into prior distributions for process model parameters. the approach is applied to a case study at ocean station papa, using particle markov chain monte carlo computational techniques. the results suggest that, by drawing on objective prior information, it is possible to extract useful information about model state and a subset of parameters, and even to make useful long-term forecasts, based on sparse and noisy observations.","10.1890/12-0312.1","2012-11-07","","['john parslow', 'noel cressie', 'edward p. campbell', 'emlyn jones', 'lawrence murray']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1696",1211.3602,"on mixtures of skew normal and skew t-distributions","stat.me","finite mixture of skew distributions have emerged as an effective tool in modelling heterogeneous data with asymmetric features. with various proposals appearing rapidly in the recent years, which are similar but not identical, the connections between them and their relative performance becomes rather unclear. this paper aims to provide a concise overview of these developments by presenting a systematic classification of the existing skew distributions into four types, thereby clarifying their close relationships. this also aids in understanding the link between some of the proposed expectation-maximization (em) based algorithms for the computation of the maximum likelihood estimates of the parameters of the models. the final part of this paper presents an illustration of the performance of these mixture models in clustering a real dataset, relative to other non-elliptically contoured clustering methods and associated algorithms for their implementation.","10.1007/s11634-013-0132-8","2012-11-15","2013-05-28","['sharon x. lee', 'geoffrey j. mclachlan']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1697",1211.529,"emmix-uskew: an r package for fitting mixtures of multivariate skew   t-distributions via the em algorithm","stat.co stat.me","this paper describes an algorithm for fitting finite mixtures of unrestricted multivariate skew t (fm-umst) distributions. the package emmix-uskew implements a closed-form expectation-maximization (em) algorithm for computing the maximum likelihood (ml) estimates of the parameters for the (unrestricted) fm-mst model in r. emmix-uskew also supports visualization of fitted contours in two and three dimensions, and random sample generation from a specified fm-umst distribution.   finite mixtures of skew t-distributions have proven to be useful in modelling heterogeneous data with asymmetric and heavy tail behaviour, for example, datasets from flow cytometry. in recent years, various versions of mixtures with multivariate skew t (mst) distributions have been proposed. however, these models adopted some restricted characterizations of the component mst distributions so that the e-step of the em algorithm can be evaluated in closed form. this paper focuses on mixtures with unrestricted mst components, and describes an iterative algorithm for the computation of the ml estimates of its model parameters.   the usefulness of the proposed algorithm is demonstrated in three applications to real data sets. the first example illustrates the use of the main function fmmst in the package by fitting a mst distribution to a bivariate unimodal flow cytometric sample. the second example fits a mixture of mst distributions to the australian institute of sport (ais) data, and demonstrate that emmix-uskew can provide better clustering results than mixtures with restricted mst components. in the third example, emmix-uskew is applied to classify cells in a trivariate flow cytometric dataset. comparisons with other available methods suggests that the emmix-uskew result achieved a lower misclassification rate with respect to the labels given by benchmark gating analysis.","","2012-11-22","2013-03-27","['sharon x. lee', 'geoffrey j. mclachlan']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"1698",1211.562,"pair-copula bayesian networks","stat.me","pair-copula bayesian networks (pcbns) are a novel class of multivariate statistical models, which combine the distributional flexibility of pair-copula constructions (pccs) with the parsimony of conditional independence models associated with directed acyclic graphs (dag). we are first to provide generic algorithms for random sampling and likelihood inference in arbitrary pcbns as well as for selecting orderings of the parents of the vertices in the underlying graphs. model selection of the dag is facilitated using a version of the well-known pc algorithm which is based on a novel test for conditional independence of random variables tailored to the pcc framework. a simulation study shows the pc algorithm's high aptitude for structure estimation in non-gaussian pcbns. the proposed methods are finally applied to modelling financial return data.","","2012-11-23","","['alexander bauer', 'claudia czado']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1699",1301.1318,"efficient eigen-updating for spectral graph clustering","stat.ml","partitioning a graph into groups of vertices such that those within each group are more densely connected than vertices assigned to different groups, known as graph clustering, is often used to gain insight into the organisation of large scale networks and for visualisation purposes. whereas a large number of dedicated techniques have been recently proposed for static graphs, the design of on-line graph clustering methods tailored for evolving networks is a challenging problem, and much less documented in the literature. motivated by the broad variety of applications concerned, ranging from the study of biological networks to the analysis of networks of scientific references through the exploration of communications networks such as the world wide web, it is the main purpose of this paper to introduce a novel, computationally efficient, approach to graph clustering in the evolutionary context. namely, the method promoted in this article can be viewed as an incremental eigenvalue solution for the spectral clustering method described by ng. et al. (2001). the incremental eigenvalue solution is a general technique for finding the approximate eigenvectors of a symmetric matrix given a change. as well as outlining the approach in detail, we present a theoretical bound on the quality of the approximate eigenvectors using perturbation theory. we then derive a novel spectral clustering algorithm called incremental approximate spectral clustering (iasc). the iasc algorithm is simple to implement and its efficacy is demonstrated on both synthetic and real datasets modelling the evolution of a hiv epidemic, a citation network and the purchase history graph of an e-commerce website.","","2013-01-07","2014-01-27","['charanpal dhanjal', 'romaric gaudel', 'stéphan clémençon']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1700",1301.1817,"a toolbox for fitting complex spatial point process models using   integrated nested laplace approximation (inla)","stat.ap","this paper develops methodology that provides a toolbox for routinely fitting complex models to realistic spatial point pattern data. we consider models that are based on log-gaussian cox processes and include local interaction in these by considering constructed covariates. this enables us to use integrated nested laplace approximation and to considerably speed up the inferential task. in addition, methods for model comparison and model assessment facilitate the modelling process. the performance of the approach is assessed in a simulation study. to demonstrate the versatility of the approach, models are fitted to two rather different examples, a large rainforest data set with covariates and a point pattern with multiple marks.","10.1214/11-aoas530","2013-01-09","","['janine b. illian', 'sigrunn h. sørbye', 'håvard rue']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1701",1301.2318,"statistical modeling in continuous speech recognition (csr)(invited   talk)","cs.lg cs.ai stat.ml","automatic continuous speech recognition (csr) is sufficiently mature that a variety of real world applications are now possible including large vocabulary transcription and interactive spoken dialogues. this paper reviews the evolution of the statistical modelling techniques which underlie current-day systems, specifically hidden markov models (hmms) and n-grams. starting from a description of the speech signal and its parameterisation, the various modelling assumptions and their consequences are discussed. it then describes various techniques by which the effects of these assumptions can be mitigated. despite the progress that has been made, the limitations of current modelling techniques are still evident. the paper therefore concludes with a brief review of some of the more fundamental modelling work now in progress.","","2013-01-10","","['steve young']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1702",1301.3558,"model selection for gaussian mixture models","stat.me math.st stat.ml stat.th","this paper is concerned with an important issue in finite mixture modelling, the selection of the number of mixing components. we propose a new penalized likelihood method for model selection of finite multivariate gaussian mixture models. the proposed method is shown to be statistically consistent in determining of the number of components. a modified em algorithm is developed to simultaneously select the number of components and to estimate the mixing weights, i.e. the mixing probabilities, and unknown parameters of gaussian distributions. simulations and a real data analysis are presented to illustrate the performance of the proposed method.","","2013-01-15","","['tao huang', 'heng peng', 'kun zhang']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE
"1703",1301.3581,"d-optimal factorial designs under generalized linear models","math.st stat.th","generalized linear models (glms) have been used widely for modelling the mean response both for discrete and continuous random variables with an emphasis on categorical response. recently yang, mandal and majumdar (2013) considered full factorial and fractional factorial locally d-optimal designs for binary response and two-level experimental factors. in this paper, we extend their results to a general setup with response belonging to a single-parameter exponential family and for multi-level predictors.","","2013-01-15","2013-05-03","['jie yang', 'abhyuday mandal']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1704",1301.3863,"yggdrasil - a statistical package for learning split models","cs.ai cs.ms stat.me","there are two main objectives of this paper. the first is to present a statistical framework for models with context specific independence structures, i.e., conditional independences holding only for sepcific values of the conditioning variables. this framework is constituted by the class of split models. split models are extension of graphical models for contigency tables and allow for a more sophisticiated modelling than graphical models. the treatment of split models include estimation, representation and a markov property for reading off those independencies holding in a specific context. the second objective is to present a software package named yggdrasil which is designed for statistical inference in split models, i.e., for learning such models on the basis of data.","","2013-01-16","","['soren hojsgaard']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1705",1301.4144,"non-parametric bayesian modelling of digital gene expression data","q-bio.qm q-bio.gn stat.ap stat.ml","next-generation sequencing technologies provide a revolutionary tool for generating gene expression data. starting with a fixed rna sample, they construct a library of millions of differentially abundant short sequence tags or ""reads"", which constitute a fundamentally discrete measure of the level of gene expression. a common limitation in experiments using these technologies is the low number or even absence of biological replicates, which complicates the statistical analysis of digital gene expression data. analysis of this type of data has often been based on modified tests originally devised for analysing microarrays; both these and even de novo methods for the analysis of rna-seq data are plagued by the common problem of low replication. we propose a novel, non-parametric bayesian approach for the analysis of digital gene expression data. we begin with a hierarchical model for modelling over-dispersed count data and a blocked gibbs sampling algorithm for inferring the posterior distribution of model parameters conditional on these counts. the algorithm compensates for the problem of low numbers of biological replicates by clustering together genes with tag counts that are likely sampled from a common distribution and using this augmented sample for estimating the parameters of this distribution. the number of clusters is not decided a priori, but it is inferred along with the remaining model parameters. we demonstrate the ability of this approach to model biological data with high fidelity by applying the algorithm on a public dataset obtained from cancerous and non-cancerous neural tissues.","10.4172/jcsb.1000131","2013-01-17","","['dimitrios v. vavoulis', 'julian gough']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"1706",1301.5007,"ergodicity and scaling limit of a constrained multivariate hawkes   process","stat.ap q-fin.cp q-fin.tr","we introduce a multivariate hawkes process with constraints on its conditional density. it is a multivariate point process with conditional intensity similar to that of a multivariate hawkes process but certain events are forbidden with respect to boundary conditions on a multidimensional constraint variable, whose evolution is driven by the point process. we study this process in the special case where the fertility function is exponential so that the process is entirely described by an underlying markov chain, which includes the constraint variable. some conditions on the parameters are established to ensure the ergodicity of the chain. moreover, scaling limits are derived for the integrated point process. this study is primarily motivated by the stochastic modelling of a limit order book for high frequency financial data analysis.","","2013-01-18","2014-02-13","['ban zheng', 'françois roueff', 'frédéric abergel']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1707",1302.0893,"probabilistic quantitative precipitation forecasting using ensemble   model output statistics","stat.ap physics.ao-ph","statistical post-processing of dynamical forecast ensembles is an essential component of weather forecasting. in this article, we present a post-processing method that generates full predictive probability distributions for precipitation accumulations based on ensemble model output statistics (emos). we model precipitation amounts by a generalized extreme value distribution that is left-censored at zero. this distribution permits modelling precipitation on the original scale without prior transformation of the data. a closed form expression for its continuous rank probability score can be derived and permits computationally efficient model fitting. we discuss an extension of our approach that incorporates further statistics characterizing the spatial variability of precipitation amounts in the vicinity of the location of interest. the proposed emos method is applied to daily 18-h forecasts of 6-h accumulated precipitation over germany in 2011 using the cosmo-de ensemble prediction system operated by the german meteorological service. it yields calibrated and sharp predictive distributions and compares favourably with extended logistic regression and bayesian model averaging which are state of the art approaches for precipitation post-processing. the incorporation of neighbourhood information further improves predictive performance and turns out to be a useful strategy to account for displacement errors of the dynamical forecasts in a probabilistic forecasting framework.","10.1002/qj.2183","2013-02-04","","['michael scheuerer']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1708",1302.1571,"score and information for recursive exponential models with incomplete   data","stat.me cs.ai","recursive graphical models usually underlie the statistical modelling concerning probabilistic expert systems based on bayesian networks. this paper defines a version of these models, denoted as recursive exponential models, which have evolved by the desire to impose sophisticated domain knowledge onto local fragments of a model. besides the structural knowledge, as specified by a given model, the statistical modelling may also include expert opinion about the values of parameters in the model. it is shown how to translate imprecise expert knowledge into approximately conjugate prior distributions. based on possibly incomplete data, the score and the observed information are derived for these models. this accounts for both the traditional score and observed information, derived as derivatives of the log-likelihood, and the posterior score and observed information, derived as derivatives of the log-posterior distribution. throughout the paper the specialization into recursive graphical models is accounted for by a simple example.","","2013-02-06","","['bo thiesson']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1709",1302.2373,"parsimonious skew mixture models for model-based clustering and   classification","stat.me","in recent work, robust mixture modelling approaches using skewed distributions have been explored to accommodate asymmetric data. we introduce parsimony by developing skew-t and skew-normal analogues of the popular gpcm family that employ an eigenvalue decomposition of a positive-semidefinite matrix. the methods developed in this paper are compared to existing models in both an unsupervised and semi-supervised classification framework. parameter estimation is carried out using the expectation-maximization algorithm and models are selected using the bayesian information criterion. the efficacy of these extensions is illustrated on simulated and benchmark clustering data sets.","10.1016/j.csda.2013.07.008","2013-02-10","","['irene vrbik', 'paul d. mcnicholas']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1710",1302.4245,"gaussian process kernels for pattern discovery and extrapolation","stat.ml cs.ai stat.me","gaussian processes are rich distributions over functions, which provide a bayesian nonparametric approach to smoothing and interpolation. we introduce simple closed form kernels that can be used with gaussian processes to discover patterns and enable extrapolation. these kernels are derived by modelling a spectral density -- the fourier transform of a kernel -- with a gaussian mixture. the proposed kernels support a broad class of stationary covariances, but gaussian process inference remains simple and analytic. we demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric co2 trends and airline passenger data. we also show that we can reconstruct standard covariances within our framework.","","2013-02-18","2013-12-31","['andrew gordon wilson', 'ryan prescott adams']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1711",1302.4404,"analysis of forensic dna mixtures with artefacts","stat.me stat.ap","dna is now routinely used in criminal investigations and court cases, although dna samples taken at crime scenes are of varying quality and therefore present challenging problems for their interpretation. we present a statistical model for the quantitative peak information obtained from an electropherogram (epg) of a forensic dna sample and illustrate its potential use for the analysis of criminal cases. in contrast to most previously used methods, we directly model the peak height information and incorporates important artefacts associated with the production of the epg. our model has a number of unknown parameters, and we show that these can be estimated by the method of maximum likelihood in the presence of multiple unknown contributors, and their approximate standard errors calculated; the computations exploit a bayesian network representation of the model. a case example from a uk trial, as reported in the literature, is used to illustrate the efficacy and use of the model, both in finding likelihood ratios to quantify the strength of evidence, and in the deconvolution of mixtures for the purpose of finding likely profiles of one or more unknown contributors to a dna sample. our model is readily extended to simultaneous analysis of more than one mixture as illustrated in a case example. we show that combination of evidence from several samples may give an evidential strength close to that of a single source trace and thus modelling of peak height information provides for a potentially very efficient mixture analysis.","","2013-02-18","2013-12-10","['r. g. cowell', 't. graversen', 's. lauritzen', 'j. mortera']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1712",1302.5714,"bayes linear variance structure learning for inspection of large scale   physical systems","stat.me","modelling of inspection data for large scale physical systems is critical to assessment of their integrity. we present a general method for inference about system state and associated model variance structure from spatially distributed time series which are typically short, irregular, incomplete and not directly observable. bayes linear analysis simplifies parameter estimation and avoids often-unrealistic distributional assumptions. second-order exchangeability judgements facilitate variance learning for sparse inspection time-series. the model is applied to inspection data for minimum wall thickness from corroding pipe-work networks on a full-scale offshore platform, and shown to give materially different forecasts of remnant life compared to an equivalent model neglecting variance learning.","","2013-02-22","","['david randell', 'michael goldstein', 'philip jonathan']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1713",1302.5849,"pathways-driven sparse regression identifies pathways and genes   associated with high-density lipoprotein cholesterol in two asian cohorts","stat.me stat.ap","standard approaches to analysing data in genome-wide association studies (gwas) ignore any potential functional relationships between genetic markers. in contrast gene pathways analysis uses prior information on functional structure within the genome to identify pathways associated with a trait of interest. in a second step, important single nucleotide polymorphisms (snps) or genes may be identified within associated pathways. most pathways methods begin by testing snps one at a time, and so fail to capitalise on the potential advantages inherent in a multi-snp, joint modelling approach. here we describe a dual-level, sparse regression model for the simultaneous identification of pathways, genes and snps associated with a quantitative trait. our method takes account of various factors specific to the joint modelling of pathways with genome-wide data, including widespread correlation between genetic predictors, and the fact that variants may overlap multiple pathways. we use a resampling strategy that exploits finite sample variability to provide robust rankings for pathways, snps and genes. we test our method through simulation, and use it to perform pathways-driven snp selection in a search for pathways, genes and snps associated with variation in serum high-density lipoprotein cholesterol (hdlc) levels in two separate gwas cohorts of asian adults. by comparing results from both cohorts we identify a number of candidate pathways including those associated with cardiomyopathy, and t cell receptor and ppar signalling. highlighted genes include those associated with the l-type calcium channel, adenylate cyclase, integrin, laminin, mapk signalling and immune function.","","2013-02-23","","['m. silver', 'p. chen', 'l. ruoying', 'c. y. cheng', 't. y. wong', 'e. tai', 'y. y. teo', 'g. montana']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1714",1302.5856,"a press statistic for two-block partial least squares regression","stat.me","predictive modelling of multivariate data where both the covariates and responses are high-dimensional is becoming an increasingly popular task in many data mining applications. partial least squares (pls) regression often turns out to be a useful model in these situations since it performs dimensionality reduction by assuming the existence of a small number of latent factors that may explain the linear dependence between input and output. in practice, the number of latent factors to be retained, which controls the complexity of the model and its predictive ability, has to be carefully selected. typically this is done by cross validating a performance measure, such as the predictive error. although cross validation works well in many practical settings, it can be computationally expensive. various extensions to pls have also been proposed for regularising the pls solution and performing simultaneous dimensionality reduction and variable selection, but these come at the expense of additional complexity parameters that also need to be tuned by cross-validation. in this paper we derive a computationally efficient alternative to leave-one-out cross validation (loocv), a predicted sum of squares (press) statistic for two-block pls. we show that the press is nearly identical to loocv but has the computational expense of only a single pls model fit. examples of the press for selecting the number of latent factors and regularisation parameters are provided.","10.1109/ukci.2010.5625583","2013-02-23","","['brian mcwilliams', 'giovanni montana']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1715",1303.1219,"analysis of partially observed networks via exponential-family random   network models","stat.me","exponential-family random network (ern) models specify a joint representation of both the dyads of a network and nodal characteristics. this class of models allow the nodal characteristics to be modelled as stochastic processes, expanding the range and realism of exponential-family approaches to network modelling. in this paper we develop a theory of inference for ern models when only part of the network is observed, as well as specific methodology for missing data, including non-ignorable mechanisms for network-based sampling designs and for latent class models. in particular, we consider data collected via contact tracing, of considerable importance to infectious disease epidemiology and public health.","","2013-03-05","","['ian e. fellows', 'mark s. handcock']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1716",1303.2316,"capturing patterns via parsimonious t mixture models","stat.me stat.ap","this paper exploits a simplified version of the mixture of multivariate t-factor analyzers (mtfa) for robust mixture modelling and clustering of high-dimensional data that frequently contain a number of outliers. two classes of eight parsimonious t mixture models are introduced and computation of maximum likelihood estimates of parameters is achieved using the alternating expectation conditional maximization (aecm) algorithm. the usefulness of the methodology is illustrated through applications of image compression and compact facial representation.","","2013-03-10","","['tsung-i lin', 'paul d. mcnicholas', 'hsiu j. ho']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1717",1303.2739,"machine learning for bioclimatic modelling","cs.lg stat.ap","many machine learning (ml) approaches are widely used to generate bioclimatic models for prediction of geographic range of organism as a function of climate. applications such as prediction of range shift in organism, range of invasive species influenced by climate change are important parameters in understanding the impact of climate change. however, success of machine learning-based approaches depends on a number of factors. while it can be safely said that no particular ml technique can be effective in all applications and success of a technique is predominantly dependent on the application or the type of the problem, it is useful to understand their behavior to ensure informed choice of techniques. this paper presents a comprehensive review of machine learning-based bioclimatic model generation and analyses the factors influencing success of such models. considering the wide use of statistical techniques, in our discussion we also include conventional statistical techniques used in bioclimatic modelling.","","2013-03-11","","['maumita bhattacharya']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1718",1303.291,"understanding operational risk capital approximations: first and second   orders","q-fin.rm math.st stat.th","we set the context for capital approximation within the framework of the basel ii / iii regulatory capital accords. this is particularly topical as the basel iii accord is shortly due to take effect. in this regard, we provide a summary of the role of capital adequacy in the new accord, highlighting along the way the significant loss events that have been attributed to the operational risk class that was introduced in the basel ii and iii accords. then we provide a semi-tutorial discussion on the modelling aspects of capital estimation under a loss distributional approach (lda). our emphasis is to focus on the important loss processes with regard to those that contribute most to capital, the so called high consequence, low frequency loss processes. this leads us to provide a tutorial overview of heavy tailed loss process modelling in oprisk under basel iii, with discussion on the implications of such tail assumptions for the severity model in an lda structure. this provides practitioners with a clear understanding of the features that they may wish to consider when developing oprisk severity models in practice. from this discussion on heavy tailed severity models, we then develop an understanding of the impact such models have on the right tail asymptotics of the compound loss process and we provide detailed presentation of what are known as first and second order tail approximations for the resulting heavy tailed loss process. from this we develop a tutorial on three key families of risk measures and their equivalent second order asymptotic approximations: value-at-risk (basel iii industry standard); expected shortfall (es) and the spectral risk measure. these then form the capital approximations.","","2013-03-12","","['gareth w. peters', 'rodrigo s. targino', 'pavel v. shevchenko']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1719",1303.6223,"random intersection trees","stat.ml stat.co stat.me","finding interactions between variables in large and high-dimensional datasets is often a serious computational challenge. most approaches build up interaction sets incrementally, adding variables in a greedy fashion. the drawback is that potentially informative high-order interactions may be overlooked. here, we propose at an alternative approach for classification problems with binary predictor variables, called random intersection trees. it works by starting with a maximal interaction that includes all variables, and then gradually removing variables if they fail to appear in randomly chosen observations of a class of interest. we show that informative interactions are retained with high probability, and the computational complexity of our procedure is of order $p^\kappa$ for a value of $\kappa$ that can reach values as low as 1 for very sparse data; in many more general settings, it will still beat the exponent $s$ obtained when using a brute force search constrained to order $s$ interactions. in addition, by using some new ideas based on min-wise hash schemes, we are able to further reduce the computational cost. interactions found by our algorithm can be used for predictive modelling in various forms, but they are also often of interest in their own right as useful characterisations of what distinguishes a certain class from others.","","2013-03-25","","['rajen dinesh shah', 'nicolai meinshausen']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE
"1720",1303.6746,"exploiting correlation and budget constraints in bayesian multi-armed   bandit optimization","stat.ml cs.lg","we address the problem of finding the maximizer of a nonlinear smooth function, that can only be evaluated point-wise, subject to constraints on the number of permitted function evaluations. this problem is also known as fixed-budget best arm identification in the multi-armed bandit literature. we introduce a bayesian approach for this problem and show that it empirically outperforms both the existing frequentist counterpart and other bayesian optimization methods. the bayesian approach places emphasis on detailed modelling, including the modelling of correlations among the arms. as a result, it can perform well in situations where the number of arms is much larger than the number of allowed function evaluation, whereas the frequentist counterpart is inapplicable. this feature enables us to develop and deploy practical applications, such as automatic machine learning toolboxes. the paper presents comprehensive comparisons of the proposed approach, thompson sampling, classical bayesian optimization techniques, more recent bayesian bandit approaches, and state-of-the-art best arm identification methods. this is the first comparison of many of these methods in the literature and allows us to examine the relative merits of their different features.","","2013-03-27","2013-11-11","['matthew w. hoffman', 'bobak shahriari', 'nando de freitas']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1721",1303.709,"gaussian process models for periodicity detection","math.st stat.th","we consider the problem of detecting and quantifying the periodic component of a function given noise-corrupted observations of a limited number of input/output tuples. our approach is based on gaussian process regression which provides a flexible non-parametric framework for modelling periodic data. we introduce a novel decomposition of the covariance function as the sum of periodic and aperiodic kernels. this decomposition allows for the creation of sub-models which capture the periodic nature of the signal and its complement. to quantify the periodicity of the signal, we derive a periodicity ratio which reflects the uncertainty in the fitted sub-models. although the method can be applied to many kernels, we give a special emphasis to the mat\'ern family, from the expression of the reproducing kernel hilbert space inner product to the implementation of the associated periodic kernels in a gaussian process toolkit. the proposed method is illustrated by considering the detection of periodically expressed genes in the arabidopsis genome.","","2013-03-28","2016-08-19","['nicolas durrande', 'james hensman', 'magnus rattray', 'neil d. lawrence']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1722",1304.2069,"robustification of elliott's on-line em algorithm for hmms","stat.me q-fin.st stat.co","in this paper, we establish a robustification of an on-line algorithm for modelling asset prices within a hidden markov model (hmm). in this hmm framework, parameters of the model are guided by a markov chain in discrete time, parameters of the asset returns are therefore able to switch between different regimes. the parameters are estimated through an on-line algorithm, which utilizes incoming information from the market and leads to adaptive optimal estimates. we robustify this algorithm step by step against additive outliers appearing in the observed asset prices with the rationale to better handle possible peaks or missings in asset returns.","","2013-04-07","","['christina erlwein', 'peter ruckdeschel']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"1723",1304.4302,"a framework for modelling probabilistic uncertainty in rainfall scenario   analysis","stat.me math.pr","predicting future probable values of model parameters, is an essential pre-requisite for assessing model decision reliability in an uncertain environment. scenario analysis is a methodology for modelling uncertainty in water resources management modelling. uncertainty if not considered appropriately in decision making will decrease reliability of decisions, especially in long-term planning. one of the challenges in scenario analysis is how scenarios are made. one of the most approved methods is statistical modelling based on auto-regressive models. stream flow future scenarios in developed basins that human has made changes to the natural flow process could not be generated directly by arma modelling. in this case, making scenarios for monthly rainfall and using it in a water resources system model makes more sense. rainfall is an ephemeral process which has zero values in some months which introduces some limitations in making use of monthly arma model. therefore, a two stage modelling approach is adopted here which in the first stage yearly modelling is done. within this yearly model three ranges are identified: dry, normal and wet. in the normal range yearly arma modelling is used. dry and wet range are considered as random processes and are modeled by frequency analysis. monthly distribution of rainfall, which is extracted from available data from a moving average are considered to be deterministic and fixed in time. each rainfall scenario is composed of a yearly arma process super-imposed by dry and wet events according to the frequency analysis. this modelling framework is applied to available data from three rain-gauge stations in iran. results show this modelling approach has better consistency with observed data in comparison with making use of arma modelling alone.","","2013-04-15","","['seyed hamed alemohammad', 'reza ardakanian', 'akbar karimi']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1724",1304.6949,"exploring a new class of non-stationary spatial gaussian random fields   with varying local anisotropy","stat.me","gaussian random fields (grfs) constitute an important part of spatial modelling, but can be computationally infeasible for general covariance structures. an efficient approach is to specify grfs via stochastic partial differential equations (spdes) and derive gaussian markov random field (gmrf) approximations of the solutions. we consider the construction of a class of non-stationary grfs with varying local anisotropy, where the local anisotropy is introduced by allowing the coefficients in the spde to vary with position. this is done by using a form of diffusion equation driven by gaussian white noise with a spatially varying diffusion matrix. this allows for the introduction of parameters that control the grf by parametrizing the diffusion matrix. these parameters and the grf may be considered to be part of a hierarchical model and the parameters estimated in a bayesian framework. the results show that the use of an spde with non-constant coefficients is a promising way of creating non-stationary spatial gmrfs that allow for physical interpretability of the parameters, although there are several remaining challenges that would need to be solved before these models can be put to general practical use.","","2013-04-25","2014-04-25","['geir-arne fuglstad', 'finn lindgren', 'daniel simpson', 'håvard rue']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1725",1305.1002,"efficient estimation of the number of neighbours in probabilistic k   nearest neighbour classification","cs.lg stat.ml","probabilistic k-nearest neighbour (pknn) classification has been introduced to improve the performance of original k-nearest neighbour (knn) classification algorithm by explicitly modelling uncertainty in the classification of each feature vector. however, an issue common to both knn and pknn is to select the optimal number of neighbours, $k$. the contribution of this paper is to incorporate the uncertainty in $k$ into the decision making, and in so doing use bayesian model averaging to provide improved classification. indeed the problem of assessing the uncertainty in $k$ can be viewed as one of statistical model selection which is one of the most important technical issues in the statistics and machine learning domain. in this paper, a new functional approximation algorithm is proposed to reconstruct the density of the model (order) without relying on time consuming monte carlo simulations. in addition, this algorithm avoids cross validation by adopting bayesian framework. the performance of this algorithm yielded very good performance on several real experimental datasets.","","2013-05-05","","['ji won yoon', 'nial friel']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1726",1305.1184,"probabilistic wind speed forecasting using bayesian model averaging with   truncated normal components","stat.me","bayesian model averaging (bma) is a statistical method for post-processing forecast ensembles of atmospheric variables, obtained from multiple runs of numerical weather prediction models, in order to create calibrated predictive probability density functions (pdfs). the bma predictive pdf of the future weather quantity is the mixture of the individual pdfs corresponding to the ensemble members and the weights and model parameters are estimated using ensemble members and validating observation from a given training period.   in the present paper we introduce a bma model for calibrating wind speed forecasts, where the components pdfs follow truncated normal distribution with cut-off at zero, and apply it to the aladin-huneps ensemble of the hungarian meteorological service. three parameter estimation methods are proposed and each of the corresponding models outperforms the traditional gamma bma model both in calibration and in accuracy of predictions. moreover, since here the maximum likelihood estimation of the parameters does not require numerical optimization, modelling can be performed much faster than in case of gamma mixtures.","10.1016/j.csda.2014.02.013","2013-05-06","2013-05-07","['sándor baran']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1727",1305.2073,"fourier analysis of stationary time series in function space","math.st stat.th","we develop the basic building blocks of a frequency domain framework for drawing statistical inferences on the second-order structure of a stationary sequence of functional data. the key element in such a context is the spectral density operator, which generalises the notion of a spectral density matrix to the functional setting, and characterises the second-order dynamics of the process. our main tool is the functional discrete fourier transform (fdft). we derive an asymptotic gaussian representation of the fdft, thus allowing the transformation of the original collection of dependent random functions into a collection of approximately independent complex-valued gaussian random functions. our results are then employed in order to construct estimators of the spectral density operator based on smoothed versions of the periodogram kernel, the functional generalisation of the periodogram matrix. the consistency and asymptotic law of these estimators are studied in detail. as immediate consequences, we obtain central limit theorems for the mean and the long-run covariance operator of a stationary functional time series. our results do not depend on structural modelling assumptions, but only functional versions of classical cumulant mixing conditions, and are shown to be stable under discrete observation of the individual curves.","10.1214/13-aos1086","2013-05-09","","['victor m. panaretos', 'shahin tavakoli']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1728",1305.2815,"modelling time and vintage variability in retail credit portfolios: the   decomposition approach","stat.ap","in this paper, we consider the problem of modelling historical data on retail credit portfolio performance, with a view to forecasting future performance, and facilitating strategic decision making. we consider a situation, common in practice, where accounts with common origination date (typically month) are aggregated into a single vintage for analysis, and the data for analysis consists of a time series of a univariate portfolio performance variable (for example, the proportion of defaulting accounts) for each vintage over successive time periods since origination. an invaluable management tool for understanding portfolio behaviour can be obtained by decomposing the data series nonparametrically into components of exogenous variability (e), maturity (time since origination; m) and vintage (v), referred to as an emv model. for example, identification of a good macroeconomic model is the key to effective forecasting, particularly in applications such as stress testing, and identification of this can be facilitated by investigation of the macroeconomic component of an emv decomposition. we show that care needs to be taken with such a decomposition, drawing parallels with the age-period-cohort approach, common in demography, epidemiology and sociology. we develop a practical decomposition strategy, and illustrate our approach using data extracted from a credit card portfolio.","","2013-05-13","","['jonathan j. forster', 'agus sudjianto']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1729",1305.3671,"sparse adaptive dirichlet-multinomial-like processes","cs.it math.it math.st stat.th","online estimation and modelling of i.i.d. data for short sequences over large or complex ""alphabets"" is a ubiquitous (sub)problem in machine learning, information theory, data compression, statistical language processing, and document analysis. the dirichlet-multinomial distribution (also called polya urn scheme) and extensions thereof are widely applied for online i.i.d. estimation. good a-priori choices for the parameters in this regime are difficult to obtain though. i derive an optimal adaptive choice for the main parameter via tight, data-dependent redundancy bounds for a related model. the 1-line recommendation is to set the 'total mass' = 'precision' = 'concentration' parameter to m/2ln[(n+1)/m], where n is the (past) sample size and m the number of different symbols observed (so far). the resulting estimator (i) is simple, (ii) online, (iii) fast, (iv) performs well for all m, small, middle and large, (v) is independent of the base alphabet size, (vi) non-occurring symbols induce no redundancy, (vii) the constant sequence has constant redundancy, (viii) symbols that appear only finitely often have bounded/constant contribution to the redundancy, (ix) is competitive with (slow) bayesian mixing over all sub-alphabets.","","2013-05-15","","['marcus hutter']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1730",1305.4152,"sparse approximate inference for spatio-temporal point process models","stat.ml","spatio-temporal point process models play a central role in the analysis of spatially distributed systems in several disciplines. yet, scalable inference remains computa- tionally challenging both due to the high resolution modelling generally required and the analytically intractable likelihood function. here, we exploit the sparsity structure typical of (spatially) discretised log-gaussian cox process models by using approximate message-passing algorithms. the proposed algorithms scale well with the state dimension and the length of the temporal horizon with moderate loss in distributional accuracy. they hence provide a flexible and faster alternative to both non-linear filtering-smoothing type algorithms and to approaches that implement the laplace method or expectation propagation on (block) sparse latent gaussian models. we infer the parameters of the latent gaussian model using a structured variational bayes approach. we demonstrate the proposed framework on simulation studies with both gaussian and point-process observations and use it to reconstruct the conflict intensity and dynamics in afghanistan from the wikileaks afghan war diary.","","2013-05-17","2015-07-06","['botond cseke', 'andrew zammit mangion', 'tom heskes', 'guido sanguinetti']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1731",1305.4153,"factored expectation propagation for input-output fhmm models in systems   biology","stat.ml","we consider the problem of joint modelling of metabolic signals and gene expression in systems biology applications. we propose an approach based on input-output factorial hidden markov models and propose a structured variational inference approach to infer the structure and states of the model. we start from the classical free form structured variational mean field approach and use a expectation propagation to approximate the expectations needed in the variational loop. we show that this corresponds to a factored expectation constrained approximate inference. we validate our model through extensive simulations and demonstrate its applicability on a real world bacterial data set.","","2013-05-17","","['botond cseke', 'guido sanguinetti']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1732",1305.4283,"statistical modelling of summary values leads to accurate approximate   bayesian computations","stat.me","approximate bayesian computation (abc) methods rely on asymptotic arguments, implying that parameter inference can be systematically biased even when sufficient statistics are available. we propose to construct the abc accept/reject step from decision theoretic arguments on a suitable auxiliary space. this framework, referred to as abc*, fully specifies which test statistics to use, how to combine them, how to set the tolerances and how long to simulate in order to obtain accuracy properties on the auxiliary space. akin to maximum-likelihood indirect inference, regularity conditions establish when the abc* approximation to the posterior density is accurate on the original parameter space in terms of the kullback-leibler divergence and the maximum a posteriori point estimate. fundamentally, escaping asymptotic arguments requires knowledge of the distribution of test statistics, which we obtain through modelling the distribution of summary values, data points on a summary level. synthetic examples and an application to time series data of influenza a (h3n2) infections in the netherlands illustrate abc* in action.","","2013-05-18","2014-01-22","['oliver ratmann', 'anton camacho', 'adam meijer', 'gé donker']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1733",1305.5445,"a bayesian localised conditional auto-regressive model for estimating   the health effects of air pollution","stat.me","estimation of the long-term health effects of air pollution is a challenging task, especially when modelling small-area disease incidence data in an ecological study design. the challenge comes from the unobserved underlying spatial correlation structure in these data, which is accounted for using random effects modelled by a globally smooth conditional autoregressive model. these smooth random effects confound the effects of air pollution, which are also globally smooth. to avoid this collinearity a bayesian localised conditional autoregressive model is developed for the random effects. this localised model is flexible spatially, in the sense that it is not only able to model step changes in the random effects surface, but also is able to capture areas of spatial smoothness in the study region. this methodological development allows us to improve the estimation performance of the covariate effects, compared to using traditional conditional auto-regressive models. these results are established using a simulation study, and are then illustrated with our motivating study on air pollution and respiratory ill health in greater glasgow, scotland in 2010. the model shows substantial health effects of particulate matter air pollution and income deprivation, whose effects have been consistently attenuated by the currently available globally smooth models.","","2013-05-23","","['duncan lee', 'alastair rushworth', 'sujit k. sahu']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1734",1305.5812,"a proportional hazard model for the estimation of ionosphere storm   occurrence risk","math.st stat.th","severe ionosphere magnetic storms are feared events for integrity and continuity of navigation systems such as egnos, the european sbas (satellite-based augmentation system) complementing gps and an accurate modelling of this event probability is necessary. our aim for the work presented in this paper is to give an estimation of the frequency of such extreme magnetic storms per time unit (year) throughout a solar cycle. thus, we develop an innovative approach based on a proportional hazard model, inspired by the cox model, with time dependent covariates. the number of storms during a cycle is supposed to be a non-homogeneous poisson process. the intensity of this process could be expressed as the product of a baseline risk and a risk factor. contrary to what is done in the cox model, the baseline risk is one parameter of interest (and not a nuisance one), it is the intensity to estimate. as in extreme value theory, all the high level events will be used to make estimation and the results will be extrapolated to the extreme level ones. after a precise description of the model, we present the estimation results and a model extension. a prediction for the current solar cycle (24th) is also proposed.","","2013-05-24","2015-05-21","['malika chassan', 'jean-marc azaïs', 'guillaume buscarlet', 'norbert suard']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1735",1306.2365,"bayesian solution uncertainty quantification for differential equations","stat.me","we explore probability modelling of discretization uncertainty for system states defined implicitly by ordinary or partial differential equations. accounting for this uncertainty can avoid posterior under-coverage when likelihoods are constructed from a coarsely discretized approximation to system equations. a formalism is proposed for inferring a fixed but a priori unknown model trajectory through bayesian updating of a prior process conditional on model information. a one-step-ahead sampling scheme for interrogating the model is described, its consistency and first order convergence properties are proved, and its computational complexity is shown to be proportional to that of numerical explicit one-step solvers. examples illustrate the flexibility of this framework to deal with a wide variety of complex and large-scale systems. within the calibration problem, discretization uncertainty defines a layer in the bayesian hierarchy, and a markov chain monte carlo algorithm that targets this posterior distribution is presented. this formalism is used for inference on the jak-stat delay differential equation model of protein dynamics from indirectly observed measurements. the discussion outlines implications for the new field of probabilistic numerics.","","2013-06-10","2016-10-23","['oksana a. chkrebtii', 'david a. campbell', 'ben calderhead', 'mark a. girolami']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1736",1306.3277,"bayesian state-space modelling on high-performance hardware using libbi","stat.co","libbi is a software package for state-space modelling and bayesian inference on modern computer hardware, including multi-core central processing units (cpus), many-core graphics processing units (gpus) and distributed-memory clusters of such devices. the software parses a domain-specific language for model specification, then optimises, generates, compiles and runs code for the given model, inference method and hardware platform. in presenting the software, this work serves as an introduction to state-space models and the specialised methods developed for bayesian inference with them. the focus is on sequential monte carlo (smc) methods such as the particle filter for state estimation, and the particle markov chain monte carlo (pmcmc) and smc^2 methods for parameter estimation. all are well-suited to current computer hardware. two examples are given and developed throughout, one a linear three-element windkessel model of the human arterial system, the other a nonlinear lorenz '96 model. these are specified in the prescribed modelling language, and libbi demonstrated by performing inference with them. empirical results are presented, including a performance comparison of the software with different hardware configurations.","","2013-06-13","","['lawrence m. murray']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1737",1306.3301,"aggregation and long memory: recent developments","math.st stat.th","it is well-known that the aggregated time series might have very different properties from those of the individual series, in particular, long memory. at the present time, aggregation has become one of the main tools for modelling of long memory processes. we review recent work on contemporaneous aggregation of random-coefficient ar(1) and related models, with particular focus on various long memory properties of the aggregated process.","","2013-06-14","","['remigijus leipus', 'anne philippe', 'donata puplinskaite', 'donatas surgailis']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1738",1306.4152,"bioclimating modelling: a machine learning perspective","cs.lg stat.ml","many machine learning (ml) approaches are widely used to generate bioclimatic models for prediction of geographic range of organism as a function of climate. applications such as prediction of range shift in organism, range of invasive species influenced by climate change are important parameters in understanding the impact of climate change. however, success of machine learning-based approaches depends on a number of factors. while it can be safely said that no particular ml technique can be effective in all applications and success of a technique is predominantly dependent on the application or the type of the problem, it is useful to understand their behaviour to ensure informed choice of techniques. this paper presents a comprehensive review of machine learning-based bioclimatic model generation and analyses the factors influencing success of such models. considering the wide use of statistical techniques, in our discussion we also include conventional statistical techniques used in bioclimatic modelling.","","2013-06-18","","['maumita bhattacharya']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1739",1306.4438,"joint modelling of chip-seq data via a markov random field model","stat.me","chromatin immunoprecipitation-sequencing (chip-seq) experiments have now become routine in biology for the detection of protein binding sites. in this paper, we present a markov random field model for the joint analysis of multiple chip-seq experiments. the proposed model naturally accounts for spatial dependencies in the data, by assuming first order markov properties, and for the large proportion of zero counts, by using zero-inflated mixture distributions. in contrast to all other available implementations, the model allows for the joint modelling of multiple experiments, by incorporating key aspects of the experimental design. in particular, the model uses the information about replicates and about the different antibodies used in the experiments. an extensive simulation study shows a lower false non-discovery rate for the proposed method, compared to existing methods, at the same false discovery rate. finally, we present an analysis on real data for the detection of histone modifications of two chromatin modifiers from eight chip-seq experiments, including technical replicates with different ip efficiencies.","10.1093/biostatistics/kxt047","2013-06-19","","['yanchun bao', 'veronica vinciotti', 'ernst wit', ""peter 't hoen""]",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1740",1307.1067,"the partial linear model in high dimensions","math.st stat.ap stat.th","partial linear models have been widely used as flexible method for modelling linear components in conjunction with non-parametric ones. despite the presence of the non-parametric part, the linear, parametric part can under certain conditions be estimated with parametric rate. in this paper, we consider a high-dimensional linear part. we show that it can be estimated with oracle rates, using the lasso penalty for the linear part and a smoothness penalty for the nonparametric part.","","2013-07-03","","['patric müller', 'sara van de geer']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE
"1741",1307.5243,"bayesian models for cost-effectiveness analysis in the presence of   structural zero costs","math.st stat.th","bayesian modelling for cost-effectiveness data has received much attention in both the health economics and the statistical literature in recent years. cost-effectiveness data are characterised by a relatively complex structure of relationships linking the suitable measure of clinical benefit (\eg qalys) and the associated costs. simplifying assumptions, such as (bivariate) normality of the underlying distributions are usually not granted, particularly for the cost variable, which is characterised by markedly skewed distributions. in addition, individual-level datasets are often characterised by the presence of structural zeros in the cost variable.   hurdle models can be used to account for the presence of excess zeros in a distribution and have been applied in the context of cost data. we extend their application to cost-effectiveness data, defining a full bayesian model which consists of a selection model for the subjects with null costs, a marginal model for the costs and a conditional model for the measure of effectiveness (conditionally on the observed costs). the model is presented using a working example to describe its main features.","","2013-07-19","","['gianluca baio']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1742",1307.6332,"modelling energy spot prices by volatility modulated l\'{e}vy-driven   volterra processes","q-fin.pr math.st stat.th","this paper introduces the class of volatility modulated l\'{e}vy-driven volterra (vmlv) processes and their important subclass of l\'{e}vy semistationary (lss) processes as a new framework for modelling energy spot prices. the main modelling idea consists of four principles: first, deseasonalised spot prices can be modelled directly in stationarity. second, stochastic volatility is regarded as a key factor for modelling energy spot prices. third, the model allows for the possibility of jumps and extreme spikes and, lastly, it features great flexibility in terms of modelling the autocorrelation structure and the samuelson effect. we provide a detailed analysis of the probabilistic properties of vmlv processes and show how they can capture many stylised facts of energy markets. further, we derive forward prices based on our new spot price models and discuss option pricing. an empirical example based on electricity spot prices from the european energy exchange confirms the practical relevance of our new modelling framework.","10.3150/12-bej476","2013-07-24","","['ole e. barndorff-nielsen', 'fred espen benth', 'almut e. d. veraart']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1743",1308.09,"trading usdchf filtered by gold dynamics via hmm coupling","stat.ml cs.lg","we devise a usdchf trading strategy using the dynamics of gold as a filter. our strategy involves modelling both usdchf and gold using a coupled hidden markov model (chmm). the observations will be indicators, rsi and cci, which will be used as triggers for our trading signals. upon decoding the model in each iteration, we can get the next most probable state and the next most probable observation. hopefully by taking advantage of intermarket analysis and the markov property implicit in the model, trading with these most probable values will produce profitable results.","","2013-08-05","2013-10-29","['donny lee']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1744",1308.1136,"bayesian inference for mat\'ern repulsive processes","stat.me","in many applications involving point pattern data, the poisson process assumption is unrealistic, with the data exhibiting a more regular spread. such a repulsion between events is exhibited by trees for example, because of competition for light and nutrients. other examples include the locations of biological cells and cities, and the times of neuronal spikes. given the many applications of repulsive point processes, there is a surprisingly limited literature developing flexible, realistic and interpretable models, as well as efficient inferential methods. we address this gap by developing a modelling framework around the mat\'ern type-iii repulsive process. we consider a number of extensions of the original mat\'ern type-iii process for both the homogeneous and inhomogeneous cases. we also derive the probability density of this generalized mat\'ern process. this allows us to characterize the posterior distribution of the various latent variables, and leads to a novel and efficient markov chain monte carlo algorithm. we apply our ideas to datasets involving the spatial locations of trees, nerve fiber cells and greyhound bus stations.","","2013-08-05","2015-04-03","['vinayak rao', 'ryan p. adams', 'david b. dunson']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1745",1308.1211,"identification of finite dimensional linear systems driven by levy   processes","math.st stat.th","levy processes are widely used in financial mathematics, telecommunication, economics, queueing theory and natural sciences for modelling. a typical model is obtained by considering finite dimensional linear stochastic siso systems driven by a levy process. in this paper we consider a discrete-time version of this model driven by the increments of a levy process, such a system will be called levy system. we focus on the problem of identifying the dynamics and the noise characteristics of such a levy system. the special feature of this problem is that the statistical description of the noise is given by the characteristic function (c.f.) of the driving noise not by its density function. as an alternative to the maximum likelihood (ml) method we develop and analyze a novel identification method by adapting the so-called empirical characteristic function method (ecf) originally devised for estimating parameters of c.f.-s from i.i.d. samples. precise characterization of the errors of these estimators will be given, and their asymptotic covariance matrices will be obtained. we also demonstrate that the arguments implying asymptotic efficiency for the i.i.d. case can be adapted for the present case.","","2013-08-06","2014-01-06","['laszlo gerencser', 'mate manfay']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1746",1308.5836,"semiparametric stochastic volatility modelling using penalized splines","stat.me q-fin.st","stochastic volatility (sv) models mimic many of the stylized facts attributed to time series of asset returns, while maintaining conceptual simplicity. the commonly made assumption of conditionally normally distributed or student-t-distributed returns, given the volatility, has however been questioned. in this manuscript, we introduce a novel maximum penalized likelihood approach for estimating the conditional distribution in an sv model in a nonparametric way, thus avoiding any potentially critical assumptions on the shape. the considered framework exploits the strengths both of the powerful hidden markov model machinery and of penalized b-splines, and constitutes a powerful and flexible alternative to recently developed bayesian approaches to semiparametric sv modelling. we demonstrate the feasibility of the approach in a simulation study before outlining its potential in applications to three series of returns on stocks and one series of stock index returns.","","2013-08-27","2014-06-17","['roland langrock', 'théo michelot', 'alexander sohn', 'thomas kneib']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1747",1308.585,"modelling group dynamic animal movement","q-bio.qm stat.ap","group dynamic movement is a fundamental aspect of many species' movements. the need to adequately model individuals' interactions with other group members has been recognised, particularly in order to differentiate the role of social forces in individual movement from environmental factors. however, to date, practical statistical methods which can include group dynamics in animal movement models have been lacking. we consider a flexible modelling framework that distinguishes a group-level model, describing the movement of the group's centre, and an individual-level model, such that each individual makes its movement decisions relative to the group centroid. the basic idea is framed within the flexible class of hidden markov models, extending previous work on modelling animal movement by means of multi-state random walks. while in simulation experiments parameter estimators exhibit some bias in non-ideal scenarios, we show that generally the estimation of models of this type is both feasible and ecologically informative. we illustrate the approach using real movement data from 11 reindeer (rangifer tarandus). results indicate a directional bias towards a group centroid for reindeer in an encamped state. though the attraction to the group centroid is relatively weak, our model successfully captures group-influenced movement dynamics. specifically, as compared to a regular mixture of correlated random walks, the group dynamic model more accurately predicts the non-diffusive behaviour of a cohesive mobile group.","10.1111/2041-210x.12155","2013-08-27","","['roland langrock', 'j. grant c. hopcraft', 'paul g. blackwell', 'victoria goodall', 'ruth king', 'mu niu', 'toby a. patterson', 'martin w. pedersen', 'anna skarin', 'robert s. schick']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1748",1309.2435,"bayesian wavelet shrinkage of the haar-fisz transformed wavelet   periodogram","stat.me","it is increasingly being realised that many real world time series are not stationary and exhibit evolving second-order autocovariance or spectral structure. this article introduces a bayesian approach for modelling the evolving wavelet spectrum of a locally stationary wavelet time series. our new method works by combining the advantages of a haar-fisz transformed spectrum with a simple, but powerful, bayesian wavelet shrinkage method. our new method produces excellent and stable spectral estimates and this is demonstrated via simulated data and on differenced infant ecg data. a major additional benefit of the bayesian paradigm is that we obtain rigorous and useful credible intervals of the evolving spectral structure. we show how the bayesian credible intervals provide extra insight into the infant ecg data.","","2013-09-10","","['guy p. nason', 'kara n. stevens']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1749",1309.2737,"on the evidence for cosmic variation of the fine structure constant   (ii): a semi-parametric bayesian model selection analysis of the quasar   dataset","astro-ph.im stat.ap","in the second paper of this series we extend our bayesian reanalysis of the evidence for a cosmic variation of the fine structure constant to the semi-parametric modelling regime. by adopting a mixture of dirichlet processes prior for the unexplained errors in each instrumental subgroup of the benchmark quasar dataset we go some way towards freeing our model selection procedure from the apparent subjectivity of a fixed distributional form. despite the infinite-dimensional domain of the error hierarchy so constructed we are able to demonstrate a recursive scheme for marginal likelihood estimation with prior-sensitivity analysis directly analogous to that presented in paper i, thereby allowing the robustness of our posterior bayes factors to hyper-parameter choice and model specification to be readily verified. in the course of this work we elucidate various similarities between unexplained error problems in the seemingly disparate fields of astronomy and clinical meta-analysis, and we highlight a number of sophisticated techniques for handling such problems made available by past research in the latter. it is our hope that the novel approach to semi-parametric model selection demonstrated herein may serve as a useful reference for others exploring this potentially difficult class of error model.","","2013-09-11","","['ewan cameron', 'tony pettitt']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1750",1309.3802,"monotone function estimation for computer experiments","stat.me","in statistical modeling of computer experiments sometimes prior information is available about the underlying function. for example, the physical system simulated by the computer code may be known to be monotone with respect to some or all inputs. we develop a bayesian approach to gaussian process modelling capable of incorporating monotonicity information for computer model emulation. markov chain monte carlo methods are used to sample from the posterior distribution of the process given the simulator output and monotonicity information. the performance of the proposed approach in terms of predictive accuracy and uncertainty quantification is demonstrated in a number of simulated examples as well as a real queueing system application.","","2013-09-15","2014-06-14","['shirin golchi', 'derek r. bingham', 'hugh chipman', 'david a. campbell']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1751",1309.5122,"variational bayes inference and dirichlet process priors","stat.co","this paper shows how the variational bayes method provides a computational efficient technique in the context of hierarchical modelling using dirichlet process priors, in particular without requiring conjugate prior assumption. it shows, using the so called parameter separation parameterization, a simple criterion under which the variational method works well. based on this framework, its provides a full variational solution for the dirichlet process. the numerical results show that the method is very computationally efficient when compared to mcmc. finally, we propose an empirical method to estimate the truncation level for the truncated dirichlet process.","","2013-09-19","","['hui zhao', 'paul marriott']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1752",1309.5264,"sequential change detection in the presence of unknown parameters","stat.me","it is commonly required to detect change points in sequences of random variables. in the most difficult setting of this problem, change detection must be performed sequentially with new observations being constantly received over time. further, the parameters of both the pre- and post- change distributions may be unknown. in a recent paper by hawkins and zamba (2005), the sequential generalised likelihood ratio test was introduced for detecting changes in this context, under the assumption that the observations follow a gaussian distribution. however, we show that the asymptotic approximation used in their test statistic leads to it being conservative even when a large numbers of observations is available. we propose an improved procedure which is more efficient, in the sense of detecting changes faster, in all situations. we also show that similar issues arise in other parametric change detection contexts, which we illustrate by introducing a novel monitoring procedure for sequences of exponentially distributed random variable, which is an important topic in time-to-failure modelling.","10.1007/s11222-013-9417-1","2013-09-20","","['gordon j ross']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1753",1309.6287,"a stochastic model for speculative bubbles","math.pr math.st q-fin.gn stat.th","this paper aims to provide a simple modelling of speculative bubbles and derive some quantitative properties of its dynamical evolution. starting from a description of individual speculative behaviours, we build and study a second order markov process, which after simple transformations can be viewed as a turning two-dimensional gaussian process. then, our main problem is to ob- tain some bounds for the persistence rate relative to the return time to a given price. in our main results, we prove with both spectral and probabilistic methods that this rate is almost proportional to the turning frequency {\omega} of the model and provide some explicit bounds. in the continuity of this result, we build some estimators of {\omega} and of the pseudo-period of the prices. at last, we end the paper by a proof of the quasi-stationary distribution of the process, as well as the existence of its persistence rate.","","2013-09-22","","['sébastien gadat', 'laurent miclo', 'fabien panloup']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1754",1309.6786,"one-class collaborative filtering with random graphs: annotated version","stat.ml cs.lg","the bane of one-class collaborative filtering is interpreting and modelling the latent signal from the missing class. in this paper we present a novel bayesian generative model for implicit collaborative filtering. it forms a core component of the xbox live architecture, and unlike previous approaches, delineates the odds of a user disliking an item from simply not considering it. the latent signal is treated as an unobserved random graph connecting users with items they might have encountered. we demonstrate how large-scale distributed learning can be achieved through a combination of stochastic gradient descent and mean field variational inference over random graph samples. a fine-grained comparison is done against a state of the art baseline on real world data.","","2013-09-26","2014-09-24","['ulrich paquet', 'noam koenigstein']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1755",1309.6811,"generative multiple-instance learning models for quantitative   electromyography","cs.lg stat.ml","we present a comprehensive study of the use of generative modeling approaches for multiple-instance learning (mil) problems. in mil a learner receives training instances grouped together into bags with labels for the bags only (which might not be correct for the comprised instances). our work was motivated by the task of facilitating the diagnosis of neuromuscular disorders using sets of motor unit potential trains (mupts) detected within a muscle which can be cast as a mil problem. our approach leads to a state-of-the-art solution to the problem of muscle classification. by introducing and analyzing generative models for mil in a general framework and examining a variety of model structures and components, our work also serves as a methodological guide to modelling mil tasks. we evaluate our proposed methods both on mupt datasets and on the musk1 dataset, one of the most widely used benchmarks for mil.","","2013-09-26","","['tameem adel', 'benn smith', 'ruth urner', 'daniel stashuk', 'daniel j. lizotte']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1756",1309.7311,"bayesian inference in sparse gaussian graphical models","stat.ml cs.lg","one of the fundamental tasks of science is to find explainable relationships between observed phenomena. one approach to this task that has received attention in recent years is based on probabilistic graphical modelling with sparsity constraints on model structures. in this paper, we describe two new approaches to bayesian inference of sparse structures of gaussian graphical models (ggms). one is based on a simple modification of the cutting-edge block gibbs sampler for sparse ggms, which results in significant computational gains in high dimensions. the other method is based on a specific construction of the hamiltonian monte carlo sampler, which results in further significant improvements. we compare our fully bayesian approaches with the popular regularisation-based graphical lasso, and demonstrate significant advantages of the bayesian treatment under the same computing costs. we apply the methods to a broad range of simulated data sets, and a real-life financial data set.","10.1017/s0956796814000057","2013-09-27","","['peter orchard', 'felix agakov', 'amos storkey']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1757",1310.0973,"accelerating inference for diffusions observed with measurement error   and large sample sizes using approximate bayesian computation","stat.co stat.ap","in recent years dynamical modelling has been provided with a range of breakthrough methods to perform exact bayesian inference. however it is often computationally unfeasible to apply exact statistical methodologies in the context of large datasets and complex models. this paper considers a nonlinear stochastic differential equation model observed with correlated measurement errors and an application to protein folding modelling. an approximate bayesian computation (abc) mcmc algorithm is suggested to allow inference for model parameters within reasonable time constraints. the abc algorithm uses simulations of ""subsamples"" from the assumed data generating model as well as a so-called ""early rejection"" strategy to speed up computations in the abc-mcmc sampler. using a considerate amount of subsamples does not seem to degrade the quality of the inferential results for the considered applications. a simulation study is conducted to compare our strategy with exact bayesian inference, the latter resulting two orders of magnitude slower than abc-mcmc for the considered setup. finally the abc algorithm is applied to a large size protein data. the suggested methodology is fairly general and not limited to the exemplified model and data.","10.1080/00949655.2014.1002101","2013-10-03","2014-12-22","['umberto picchini', 'julie lyng forman']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE
"1758",1310.1545,"learning hidden structures with relational models by adequately   involving rich information in a network","cs.lg cs.si stat.ml","effectively modelling hidden structures in a network is very practical but theoretically challenging. existing relational models only involve very limited information, namely the binary directional link data, embedded in a network to learn hidden networking structures. there is other rich and meaningful information (e.g., various attributes of entities and more granular information than binary elements such as ""like"" or ""dislike"") missed, which play a critical role in forming and understanding relations in a network. in this work, we propose an informative relational model (infrm) framework to adequately involve rich information and its granularity in a network, including metadata information about each entity and various forms of link data. firstly, an effective metadata information incorporation method is employed on the prior information from relational models mmsb and lfrm. this is to encourage the entities with similar metadata information to have similar hidden structures. secondly, we propose various solutions to cater for alternative forms of link data. substantial efforts have been made towards modelling appropriateness and efficiency, for example, using conjugate priors. we evaluate our framework and its inference algorithms in different datasets, which shows the generality and effectiveness of our models in capturing implicit structures in networks.","","2013-10-06","","['xuhui fan', 'richard yi da xu', 'longbing cao', 'yin song']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1759",1310.2125,"retrieval of experiments with sequential dirichlet process mixtures in   model space","stat.ml cs.ir stat.ap","we address the problem of retrieving relevant experiments given a query experiment, motivated by the public databases of datasets in molecular biology and other experimental sciences, and the need of scientists to relate to earlier work on the level of actual measurement data. since experiments are inherently noisy and databases ever accumulating, we argue that a retrieval engine should possess two particular characteristics. first, it should compare models learnt from the experiments rather than the raw measurements themselves: this allows incorporating experiment-specific prior knowledge to suppress noise effects and focus on what is important. second, it should be updated sequentially from newly published experiments, without explicitly storing either the measurements or the models, which is critical for saving storage space and protecting data privacy: this promotes life long learning. we formulate the retrieval as a ``supermodelling'' problem, of sequentially learning a model of the set of posterior distributions, represented as sets of mcmc samples, and suggest the use of particle-learning-based sequential dirichlet process mixture (dpm) for this purpose. the relevance measure for retrieval is derived from the supermodel through the mixture representation. we demonstrate the performance of the proposed retrieval method on simulated data and molecular biological experiments.","","2013-10-08","2014-03-06","['ritabrata dutta', 'sohan seth', 'samuel kaski']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"1760",1310.3403,"disease mapping via negative binomial regression m-quantiles","stat.me","we introduce a semi-parametric approach to ecological regression for disease mapping, based on modelling the regression m-quantiles of a negative binomial variable. the proposed method is robust to outliers in the model covariates, including those due to measurement error, and can account for both spatial heterogeneity and spatial clustering. a simulation experiment based on the well-known scottish lip cancer data set is used to compare the m-quantile modelling approach and a random effects modelling approach for disease mapping. this suggests that the m-quantile approach leads to predicted relative risks with smaller root mean square error than standard disease mapping methods. the paper concludes with an illustrative application of the m-quantile approach, mapping low birth weight incidence data for english local authority districts for the years 2005-2010.","10.1002/sim.6256","2013-10-12","","['ray chambers', 'emanuela dreassi', 'nicola salvati']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1761",1310.4456,"inference, sampling, and learning in copula cumulative distribution   networks","stat.ml cs.lg","the cumulative distribution network (cdn) is a recently developed class of probabilistic graphical models (pgms) permitting a copula factorization, in which the cdf, rather than the density, is factored. despite there being much recent interest within the machine learning community about copula representations, there has been scarce research into the cdn, its amalgamation with copula theory, and no evaluation of its performance. algorithms for inference, sampling, and learning in these models are underdeveloped compared those of other pgms, hindering widerspread use.   one advantage of the cdn is that it allows the factors to be parameterized as copulae, combining the benefits of graphical models with those of copula theory. in brief, the use of a copula parameterization enables greater modelling flexibility by separating representation of the marginals from the dependence structure, permitting more efficient and robust learning. another advantage is that the cdn permits the representation of implicit latent variables, whose parameterization and connectivity are not required to be specified. unfortunately, that the model can encode only latent relationships between variables severely limits its utility.   in this thesis, we present inference, learning, and sampling for cdns, and further the state-of-the-art. first, we explain the basics of copula theory and the representation of copula cdns. then, we discuss inference in the models, and develop the first sampling algorithm. we explain standard learning methods, propose an algorithm for learning from data missing completely at random (mcar), and develop a novel algorithm for learning models of arbitrary treewidth and size. properties of the models and algorithms are investigated through monte carlo simulations. we conclude with further discussion of the advantages and limitations of cdns, and suggest future work.","","2013-10-16","","['stefan douglas webb']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1762",1310.5177,"the swells survey. vi. hierarchical inference of the initial mass   functions of bulges and discs","astro-ph.im astro-ph.ga physics.data-an stat.ap","the long-standing assumption that the stellar initial mass function (imf) is universal has recently been challenged by a number of observations. several studies have shown that a ""heavy"" imf (e.g., with a salpeter-like abundance of low mass stars and thus normalisation) is preferred for massive early-type galaxies, while this imf is inconsistent with the properties of less massive, later-type galaxies. these discoveries motivate the hypothesis that the imf may vary (possibly very slightly) across galaxies and across components of individual galaxies (e.g. bulges vs discs). in this paper we use a sample of 19 late-type strong gravitational lenses from the swells survey to investigate the imfs of the bulges and discs in late-type galaxies. we perform a joint analysis of the galaxies' total masses (constrained by strong gravitational lensing) and stellar masses (constrained by optical and near-infrared colours in the context of a stellar population synthesis [sps] model, up to an imf normalisation parameter). using minimal assumptions apart from the physical constraint that the total stellar mass within any aperture must be less than the total mass within the aperture, we find that the bulges of the galaxies cannot have imfs heavier (i.e. implying high mass per unit luminosity) than salpeter, while the disc imfs are not well constrained by this data set. we also discuss the necessity for hierarchical modelling when combining incomplete information about multiple astronomical objects. this modelling approach allows us to place upper limits on the size of any departures from universality. more data, including spatially resolved kinematics (as in paper v) and stellar population diagnostics over a range of bulge and disc masses, are needed to robustly quantify how the imf varies within galaxies.","10.1093/mnras/stt2026","2013-10-18","","['brendon j. brewer', 'philip j. marshall', 'matthew w. auger', 'tommaso treu', 'aaron a. dutton', 'matteo barnabè']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1763",1310.5288,"gpatt: fast multidimensional pattern extrapolation with gaussian   processes","stat.ml cs.ai cs.lg stat.me","gaussian processes are typically used for smoothing and interpolation on small datasets. we introduce a new bayesian nonparametric framework -- gpatt -- enabling automatic pattern extrapolation with gaussian processes on large multidimensional datasets. gpatt unifies and extends highly expressive kernels and fast exact inference techniques. without human intervention -- no hand crafting of kernel features, and no sophisticated initialisation procedures -- we show that gpatt can solve large scale pattern extrapolation, inpainting, and kernel discovery problems, including a problem with 383400 training points. we find that gpatt significantly outperforms popular alternative scalable gaussian process methods in speed and accuracy. moreover, we discover profound differences between each of these methods, suggesting expressive kernels, nonparametric representations, and exact inference are useful for modelling large scale multidimensional patterns.","","2013-10-19","2013-12-31","['andrew gordon wilson', 'elad gilboa', 'arye nehorai', 'john p. cunningham']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1764",1310.7714,"an improved bayesian semiparametric model for palaeoclimate   reconstruction: cross-validation based model assessment","stat.ap","fossil-based palaeoclimate reconstruction is an important area of ecological science that has gained momentum in the backdrop of the global climate change debate. the hierarchical bayesian paradigm provides an interesting platform for studying such important scientific issue. however, our cross-validation based assessment of the existing bayesian hierarchical models with respect to two modern proxy data sets based on chironomid and pollen, respectively, revealed that the models are inadequate for the data sets.   in this paper, we model the species assemblages (compositional data) by the zero-inflated multinomial distribution, while modelling the species response functions using dirichlet process based gaussian mixtures. this modelling strategy yielded significantly improved performances, and a formal bayesian test of model adequacy, developed recently, showed that our new model is adequate for both the modern data sets. furthermore, combining together the zero-inflated assumption, importance resampling markov chain monte carlo (irmcmc) and the recently developed transformation-based markov chain monte carlo (tmcmc), we develop a powerful and efficient computational methodology.","","2013-10-29","2013-12-12","['sabyasachi mukhopadhyay', 'sourabh bhattacharya']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1765",1310.7815,"efficient and automatic methods for flexible regression on   spatiotemporal data, with applications to groundwater monitoring","stat.ap","fitting statistical models to spatiotemporal data requires finding the right balance between imposing smoothness and following the data. in the context of p-splines, we propose a bayesian framework for choosing the smoothing parameter which allows the construction of fully-automatic data-driven methods for fitting flexible models to spatiotemporal data. a computationally efficient implementation, exploiting the sparsity of the arising design and penalty matrices, is proposed. the findings are illustrated using a simulation and two examples, all concerned with the modelling of contaminants in groundwater, which suggest that the proposed strategy is more stable that competing strategies based on the use of criteria such as gcv and aic.","","2013-10-29","","['a. w. bowman', 'l. evers', 'd. molinari', 'w. r. jones', 'm. j. spence']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1766",1310.8158,"gwsdat - groundwater spatiotemporal data analysis tool","stat.ap","periodic monitoring of groundwater quality at industrial and commercial sites generates large volumes of spatiotemporal concentration data. data modelling is typically restricted to either the analysis of monotonic trends in individual wells, or independent fitting of spatial concentration distributions (e.g. kriging) to separate monitoring time periods. neither of these techniques satisfactorily elucidate the interaction between spatial and temporal components of the data. potential negative consequences include an incomplete understanding of groundwater plume dynamics, which can lead to the selection of inappropriate remedial strategies. the groundwater spatiotemporal data analysis tool (gwsdat) is a user friendly, open source, decision support tool that has been developed to address these issues. uniquely, gwsdat applies a spatiotemporal model for a more coherent and smooth interpretation of the interaction in spatial and time-series components of groundwater solute concentrations. gwsdat has been designed to work with standard groundwater monitoring data sets, and has no special data requirements. data entry is via a standardised microsoft excel input template. the underlying statistical modelling and graphical output are generated using r. this paper describes in detail the various plotting options available and how the graphical user interface can be used for rapid, rigorous and interactive trend analysis with facilitated report generation. gwsdat has been used extensively in the assessment of soil and groundwater conditions at shell's downstream assets and the discussion section describes the benefits of its applied use. these include rapid interpretation of complex data sets, early identification of new spills, detection of off-site plume migration and simplified preparation of groundwater monitoring reports - all of which facilitate expedited risk assessment and remediation.","","2013-10-30","","['wayne r. jones', 'michael j. spence', 'adrian w. bowman', 'ludger evers', 'daniel a. molinari']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1767",1310.8176,"bayesian regression analysis of data with random effects covariates from   nonlinear longitudinal measurements","stat.me","joint models for a wide class of response variables and longitudinal measurements consist on a mixed-effects model to fit longitudinal trajectories whose random effects enter as covariates in a generalized linear model for the primary response. they provide a useful way to asses association between these two kinds of data, which in clinical studies are often collected jointly on a series of individuals and may help understanding, for instance, the mechanisms of recovery of a certain disease or the efficacy of a given therapy. the most common joint model in this framework is based on a linear mixed model for the longitudinal data. however, for complex datasets the linearity assumption may be too restrictive. some works have considered generalizing this setting with the use of a nonlinear mixed-effects model for the longitudinal trajectories but the proposed estimation procedures based on likelihood approximations have been shown de la cruz et al. (2011) to exhibit some computational efficiency problems. in this article we propose an mcmc-based estimation procedure in the joint model with a nonlinear mixed-effects model for the longitudinal data and a generalized linear model for the primary response. moreover, we consider that the errors in the longitudinal model may be correlated. we apply our method to the analysis of hormone levels measured at the early stages of pregnancy that can be used to predict normal versus abnormal pregnancy outcomes. we also conduct a simulation study to asses the importance of modelling correlated errors and quantify the consequences of model misspecification.","","2013-10-30","2014-07-02","['rolando de la cruz', 'cristian meza', 'ana arribas-gil', 'raymond j. carroll']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1768",1310.8527,"a water level relationship between consecutive gauge stations along   solim\~oes/amazonas main channel: a wavelet approach","stat.ap physics.ao-ph","gauge stations are distributed along the solim\~oes/amazonas main channel to monitor water level changes over time. those measurements help quantify both the water movement and its variability from one gauge station to the next downstream. the objective of this study is to detect changes in the water level relationship between consecutive gauge stations along the solim\~oes/amazonas main channel, since 1980. to carry out the analyses, data spanning from 1980 to 2010 from three consecutive gauges (tefe, manaus and obidos) were used to compute standardized daily anomalies. in particular for infra-annual periods it was possible to detect changes for the water level variability along the solim\~oes/amazonas main channel, by applying the morlet wavelet transformation (wt) and wavelet cross coherence (wcc) methods. it was possible to quantify the waves amplitude for the wt infra-annual scaled-period and were quite similar to the three gauge stations denoting that the water level variability are related to the same hydrological forcing functions. changes in the wcc was detected for the manaus-obidos river stretch and this characteristic might be associated with land cover changes in the floodplains. the next steps of this research, will be to test this hypotheses by integrating land cover changes into the floodplain with hydrological modelling simulations throughout the time-series.","10.2495/ws130051","2013-10-31","","['r. d. somoza', 'e. s. pereira', 'e. m. l. novo', 'c. d. rennó']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1769",1311.0701,"on fast dropout and its applicability to recurrent networks","stat.ml cs.lg cs.ne","recurrent neural networks (rnns) are rich models for the processing of sequential data. recent work on advancing the state of the art has been focused on the optimization or modelling of rnns, mostly motivated by adressing the problems of the vanishing and exploding gradients. the control of overfitting has seen considerably less attention. this paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. we show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them for overconfident predictions and vanishes at minima of an unregularized training loss. the derivatives of that regularizer are exclusively based on the training error signal. one consequence of this is the absense of a global weight attractor, which is particularly appealing for rnns, since the dynamics are not biased towards a certain regime. we positively test the hypothesis that this improves the performance of rnns on four musical data sets.","","2013-11-04","2014-03-05","['justin bayer', 'christian osendorfer', 'daniela korhammer', 'nutan chen', 'sebastian urban', 'patrick van der smagt']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1770",1311.1138,"analysis of the gibbs sampler for hierarchical inverse problems","math.st stat.th","many inverse problems arising in applications come from continuum models where the unknown parameter is a field. in practice the unknown field is discretized resulting in a problem in $\mathbb{r}^n$, with an understanding that refining the discretization, that is increasing $n$, will often be desirable. in the context of bayesian inversion this situation suggests the importance of two issues: (i) defining hyper-parameters in such a way that they are interpretable in the continuum limit $n \to \infty$ and so that their values may be compared between different discretization levels; (ii) understanding the efficiency of algorithms for probing the posterior distribution, as a function of large $n.$ here we address these two issues in the context of linear inverse problems subject to additive gaussian noise within a hierarchical modelling framework based on a gaussian prior for the unknown field and an inverse-gamma prior for a hyper-parameter, namely the amplitude of the prior variance. the structure of the model is such that the gibbs sampler can be easily implemented for probing the posterior distribution. subscribing to the dogma that one should think infinite-dimensionally before implementing in finite dimensions, we present function space intuition and provide rigorous theory showing that as $n$ increases, the component of the gibbs sampler for sampling the amplitude of the prior variance becomes increasingly slower. we discuss a reparametrization of the prior variance that is robust with respect to the increase in dimension; we give numerical experiments which exhibit that our reparametrization prevents the slowing down. our intuition on the behaviour of the prior hyper-parameter, with and without reparametrization, is sufficiently general to include a broad class of nonlinear inverse problems as well as other families of hyper-priors.","","2013-11-05","2014-07-15","['sergios agapiou', 'johnathan m. bardsley', 'omiros papaspiliopoulos', 'andrew m. stuart']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1771",1311.252,"the infinite degree corrected stochastic block model","stat.ml","in stochastic blockmodels, which are among the most prominent statistical models for cluster analysis of complex networks, clusters are defined as groups of nodes with statistically similar link probabilities within and between groups. a recent extension by karrer and newman incorporates a node degree correction to model degree heterogeneity within each group. although this demonstrably leads to better performance on several networks it is not obvious whether modelling node degree is always appropriate or necessary. we formulate the degree corrected stochastic blockmodel as a non-parametric bayesian model, incorporating a parameter to control the amount of degree correction which can then be inferred from data. additionally, our formulation yields principled ways of inferring the number of groups as well as predicting missing links in the network which can be used to quantify the model's predictive performance. on synthetic data we demonstrate that including the degree correction yields better performance both on recovering the true group structure and predicting missing links when degree heterogeneity is present, whereas performance is on par for data with no degree heterogeneity within clusters. on seven real networks (with no ground truth group structure available) we show that predictive performance is about equal whether or not degree correction is included; however, for some networks significantly fewer clusters are discovered when correcting for degree indicating that the data can be more compactly explained by clusters of heterogenous degree nodes.","10.1103/physreve.90.032819","2013-11-11","2014-05-30","['tue herlau', 'mikkel n. schmidt', 'morten mørup']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1772",1311.2994,"bayesian threshold selection for extremal models using measures of   surprise","stat.me stat.co","statistical extreme value theory is concerned with the use of asymptotically motivated models to describe the extreme values of a process. a number of commonly used models are valid for observed data that exceed some high threshold. however, in practice a suitable threshold is unknown and must be determined for each analysis. while there are many threshold selection methods for univariate extremes, there are relatively few that can be applied in the multivariate setting. in addition, there are only a few bayesian-based methods, which are naturally attractive in the modelling of extremes due to data scarcity. the use of bayesian measures of surprise to determine suitable thresholds for extreme value models is proposed. such measures quantify the level of support for the proposed extremal model and threshold, without the need to specify any model alternatives. this approach is easily implemented for both univariate and multivariate extremes.","","2013-11-12","2014-12-09","['j. lee', 'y. fan', 's. a. sisson']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"1773",1311.3888,"spline approximations to conditional archimedean copula","stat.me","we propose a flexible copula model to describe changes with a covariate in the dependence structure of (conditionally exchangeable) random variables. the starting point is a spline approximation to the generator of an archimedean copula. changes in the dependence structure with a covariate $x$ are modelled by flexible regression of the spline coefficients on $x$. the performances and properties of the spline estimate of the reference generator and the abilities of these conditional models to approximate conditional copulas are studied through simulations. inference is made using bayesian arguments with posterior distributions explored using importance sampling or adaptive mcmc algorithms. the modelling strategy is illustrated with two examples.","10.1002/sta4.55","2013-11-15","","['philippe lambert']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1774",1311.5066,"some context-specific graphical models for discrete longitudinal data","math.st stat.th","ron et al (1998) introduced a rich family of models for discrete longitudinal data, called acyclic probabilistic finite automata. these may be described as context-specific graphical models, since they are represented as directed multigraphs that embody context-specific conditional independence relations. here we develop the methodology from a statistical modelling perspective. we show how likelihood ratio tests may be constructed using standard contingency table methods, and indicate how these may be used in model selection. we also show that the models generalize certain subclasses of conventional undirected and directed graphical models.","","2013-11-20","2014-08-13","['david edwards', 'smitha ankinakatte']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1775",1311.5699,"computational inference beyond kingman's coalescent","math.pr q-bio.pe stat.co","full likelihood inference under kingman's coalescent is a computationally challenging problem to which importance sampling (is) and the product of approximate conditionals (pac) method have been applied successfully. both methods can be expressed in terms of families of intractable conditional sampling distributions (csds), and rely on principled approximations for accurate inference. recently, more general $\lambda$- and $\xi$-coalescents have been observed to provide better modelling fits to some genetic data sets. we derive families of approximate csds for finite sites $\lambda$- and $\xi$-coalescents, and use them to obtain ""approximately optimal"" is and pac algorithms for $\lambda$-coalescents, yielding substantial gains in efficiency over existing methods.","10.1239/jap/1437658613","2013-11-22","2015-12-16","['jere koskela', 'paul a. jenkins', 'dario spano']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1776",1311.5769,"an exact, time-independent approach to clone size distributions in   normal and mutated cells","q-bio.qm stat.me","biological tools such as genetic lineage tracing, three dimensional confocal microscopy and next generation dna sequencing are providing new ways to quantify the distribution of clones of normal and mutated cells. population-wide clone size distributions in vivo are complicated by multiple cell types, and overlapping birth and death processes. this has led to the increased need for mathematically informed models to understand their biological significance. standard approaches usually require knowledge of clonal age. we show that modelling on clone size independent of time is an alternative method that offers certain analytical advantages; it can help parameterize these models, and obtain distributions for counts of mutated or proliferating cells, for example. when applied to a general birth-death process common in epithelial progenitors this takes the form of a gamblers ruin problem, the solution of which relates to counting motzkin lattice paths. applying this approach to mutational processes, an alternative, exact, formulation of the classic luria delbruck problem emerges. this approach can be extended beyond neutral models of mutant clonal evolution, and also describe some distributions relating to sub-clones within a tumour. the approaches above are generally applicable to any markovian branching process where the dynamics of different ""coloured"" daughter branches are of interest.","","2013-11-22","","['roshan a', 'jones ph', 'greenman cd']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1777",1311.6334,"learning reputation in an authorship network","cs.si cs.ir cs.lg stat.ml","the problem of searching for experts in a given academic field is hugely important in both industry and academia. we study exactly this issue with respect to a database of authors and their publications. the idea is to use latent semantic indexing (lsi) and latent dirichlet allocation (lda) to perform topic modelling in order to find authors who have worked in a query field. we then construct a coauthorship graph and motivate the use of influence maximisation and a variety of graph centrality measures to obtain a ranked list of experts. the ranked lists are further improved using a markov chain-based rank aggregation approach. the complete method is readily scalable to large datasets. to demonstrate the efficacy of the approach we report on an extensive set of computational simulations using the arnetminer dataset. an improvement in mean average precision is demonstrated over the baseline case of simply using the order of authors found by the topic models.","","2013-11-25","","['charanpal dhanjal', 'stéphan clémençon']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1778",1311.653,"a mixture of generalized hyperbolic factor analyzers","stat.me stat.ml","model-based clustering imposes a finite mixture modelling structure on data for clustering. finite mixture models assume that the population is a convex combination of a finite number of densities, the distribution within each population is a basic assumption of each particular model. among all distributions that have been tried, the generalized hyperbolic distribution has the advantage that is a generalization of several other methods, such as the gaussian distribution, the skew t-distribution, etc. with specific parameters, it can represent either a symmetric or a skewed distribution. while its inherent flexibility is an advantage in many ways, it means the estimation of more parameters than its special and limiting cases. the aim of this work is to propose a mixture of generalized hyperbolic factor analyzers to introduce parsimony and extend the method to high dimensional data. this work can be seen as an extension of the mixture of factor analyzers model to generalized hyperbolic mixtures. the performance of our generalized hyperbolic factor analyzers is illustrated on real data, where it performs favourably compared to its gaussian analogue.","10.1007/s11634-015-0204-z","2013-11-25","2015-05-22","['cristina tortora', 'paul d. mcnicholas', 'ryan p. browne']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1779",1311.7286,"approximate bayesian computation with composite score functions","stat.co","both approximate bayesian computation (abc) and composite likelihood methods are useful for bayesian and frequentist inference, respectively, when the likelihood function is intractable. we propose to use composite likelihood score functions as summary statistics in abc in order to obtain accurate approximations to the posterior distribution. this is motivated by the use of the score function of the full likelihood, and extended to general unbiased estimating functions in complex models. moreover, we show that if the composite score is suitably standardised, the resulting abc procedure is invariant to reparameterisations and automatically adjusts the curvature of the composite likelihood, and of the corresponding posterior distribution. the method is illustrated through examples with simulated data, and an application to modelling of spatial extreme rainfall data is discussed.","10.1007/s11222-015-9551-z","2013-11-28","2015-02-24","['erlis ruli', 'nicola sartori', 'laura ventura']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1780",1311.7582,"bayesian nonparametric location-scale-shape mixtures","stat.me","discrete mixture models are one of the most successful approaches for density estimation. under a bayesian nonparametric framework, dirichlet process location-scale mixture of gaussian kernels is the golden standard, both having nice theoretical properties and computational tractability. in this paper we explore the use of the skew-normal kernel, which can naturally accommodate several degrees of skewness by the use of a third parameter. the choice of this kernel function allows us to formulate nonparametric location-scale-shape mixture prior with large support and good performance in different applications. asymptotically, we show that this modelling framework is consistent in frequentist sense. efficient gibbs sampling algorithms are also discussed and the performance of the methods are tested through simulations and applications to galaxy velocity and fertility data. extensions to accommodate discrete data are also discussed.","","2013-11-29","","['antonio canale', 'bruno scarpa']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1781",1312.002,"sloshing in the lng shipping industry: risk modelling through   multivariate heavy-tail analysis","math.st stat.th","in the liquefied natural gas (lng) shipping industry, the phenomenon of sloshing can lead to the occurrence of very high pressures in the tanks of the vessel. the issue of modelling or estimating the probability of the simultaneous occurrence of such extremal pressures is now crucial from the risk assessment point of view. in this paper, heavy-tail modelling, widely used as a conservative approach to risk assessment and corresponding to a worst-case risk analysis, is applied to the study of sloshing. multivariate heavy-tailed distributions are considered, with sloshing pressures investigated by means of small-scale replica tanks instrumented with d >1 sensors. when attempting to fit such nonparametric statistical models, one naturally faces computational issues inherent in the phenomenon of dimensionality. the primary purpose of this article is to overcome this barrier by introducing a novel methodology. for d-dimensional heavy-tailed distributions, the structure of extremal dependence is entirely characterised by the angular measure, a positive measure on the intersection of a sphere with the positive orthant in rd. as d increases, the mutual extremal dependence between variables becomes difficult to assess. based on a spectral clustering approach, we show here how a low dimensional approximation to the angular measure may be found. the nonparametric method proposed for model sloshing has been successfully applied to pressure data. the parsimonious representation thus obtained proves to be very convenient for the simulation of multivariate heavy-tailed distributions, allowing for the implementation of monte-carlo simulation schemes in estimating the probability of failure. besides confirming its performance on artificial data, the methodology has been implemented on a real data set specifically collected for risk assessment of sloshing in the lng shipping industry.","","2013-11-29","","['antoine dematteo', 'stéphan clemencon', 'nicolas vayatis', 'mathilde mougeot']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1782",1312.0286,"efficient learning and planning with compressed predictive states","cs.lg stat.ml","predictive state representations (psrs) offer an expressive framework for modelling partially observable systems. by compactly representing systems as functions of observable quantities, the psr learning approach avoids using local-minima prone expectation-maximization and instead employs a globally optimal moment-based algorithm. moreover, since psrs do not require a predetermined latent state structure as an input, they offer an attractive framework for model-based reinforcement learning when agents must plan without a priori access to a system model. unfortunately, the expressiveness of psrs comes with significant computational cost, and this cost is a major factor inhibiting the use of psrs in applications. in order to alleviate this shortcoming, we introduce the notion of compressed psrs (cpsrs). the cpsr learning approach combines recent advancements in dimensionality reduction, incremental matrix decomposition, and compressed sensing. we show how this approach provides a principled avenue for learning accurate approximations of psrs, drastically reducing the computational costs associated with learning while also providing effective regularization. going further, we propose a planning framework which exploits these learned models. and we show that this approach facilitates model-learning and planning in large complex partially observable domains, a task that is infeasible without the principled use of compression.","","2013-12-01","2014-07-20","['william l. hamilton', 'mahdi milani fard', 'joelle pineau']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1783",1312.0859,"accelerated failure time models for competing risks in a cluster   weighted modelling framework","stat.me","a novel approach for dealing with censored competing risks regression data is proposed. this is implemented by a mixture of accelerated failure time (aft) models for a competing risks scenario within a cluster-weighted modelling (cwm) framework. specifically, we make use of the log-normal aft model here but any commonly used aft model can be utilized. the alternating expectation conditional maximization algorithm (aecm) is used for parameter estimation and bootstrapping for standard error estimation. finally, we present our results on some simulated and real competing risks data.","","2013-12-03","","['utkarsh j. dang', 'paul d. mcnicholas']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1784",1312.1794,"statistical modelling of citation exchange between statistics journals","stat.ap cs.dl","rankings of scholarly journals based on citation data are often met with skepticism by the scientific community. part of the skepticism is due to disparity between the common perception of journals' prestige and their ranking based on citation counts. a more serious concern is the inappropriate use of journal rankings to evaluate the scientific influence of authors. this paper focuses on analysis of the table of cross-citations among a selection of statistics journals. data are collected from the web of science database published by thomson reuters. our results suggest that modelling the exchange of citations between journals is useful to highlight the most prestigious journals, but also that journal citation data are characterized by considerable heterogeneity, which needs to be properly summarized. inferential conclusions require care in order to avoid potential over-interpretation of insignificant differences between journal ratings. comparison with published ratings of institutions from the uk's research assessment exercise shows strong correlation at aggregate level between assessed research quality and journal citation `export scores' within the discipline of statistics.","10.1111/rssa.12124","2013-12-06","2015-04-03","['cristiano varin', 'manuela cattelan', 'david firth']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1785",1312.224,"testing for residual correlation of any order in the autoregressive   process","math.st stat.th","we are interested in the implications of a linearly autocorrelated driven noise on the asymptotic behavior of the usual least squares estimator in a stable autoregressive process. we show that the least squares estimator is not consistent and we suggest a sharp analysis of its almost sure limiting value as well as its asymptotic normality. we also establish the almost sure convergence and the asymptotic normality of the estimated serial correlation parameter of the driven noise. then, we derive a statistical procedure enabling to test for correlation of any order in the residuals of an autoregressive modelling, giving clearly better results than the commonly used portmanteau tests of ljung-box and box-pierce, and appearing to outperform the breusch-godfrey procedure on small-sized samples.","","2013-12-08","2017-03-13","['frédéric proïa']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1786",1312.2393,"a dynamic probabilistic principal components model for the analysis of   longitudinal metabolomic data","stat.ap stat.me","in a longitudinal metabolomics study, multiple metabolites are measured from several observations at many time points. interest lies in reducing the dimensionality of such data and in highlighting influential metabolites which change over time. a dynamic probabilistic principal components analysis (dppca) model is proposed to achieve dimension reduction while appropriately modelling the correlation due to repeated measurements. this is achieved by assuming an autoregressive model for some of the model parameters. linear mixed models are subsequently used to identify influential metabolites which change over time. the proposed model is used to analyse data from a longitudinal metabolomics animal study.","","2013-12-09","","['gift nyamundanda', 'isobel claire gormley', 'lorraine brennan']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1787",1312.2423,"the gamma-count distribution in the analysis of experimental   underdispersed data","stat.ap","event counts are response variables with non-negative integer values representing the number of times that an event occurs within a fixed domain such as a time interval, a geographical area or a cell of a contingency table. analysis of counts by gaussian regression models ignores the discreteness, asymmetry and heterocedasticity and is inefficient, providing unrealistic standard errors or possibily negative predictions of the expected number of events. the poisson regression is the standard model for count data with underlying assumptions on the generating process which may be implausible in many applications. statisticians have long recognized the limitation of imposing equidispersion under the poisson regression model. a typical situation is when the conditional variance exceeds the conditional mean, in which case models allowing for overdispersion are routinely used. less reported is the case of underdispersion with fewer modelling alternatives and assessments available in the literature. one of such alternatives, the gamma-count model, is adopted here in the analysis of an agronomic experiment designed to investigate the effect of levels of defoliation on different phenological states upon the number of cotton bolls. results show improvements over the poisson model and the semiparametric quasi-poisson model in capturing the observed variability in the data. estimating rather than assuming the underlying variance process lead to important insights into the process.","10.1080/02664763.2014.922168","2013-12-09","","['walmes marques zeviani', 'paulo justiniano ribeiro', 'wagner hugo bonat', 'silvia emiko shimakura', 'joel augusti muniz']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1788",1312.2923,"lagrangian time series models for ocean surface drifter trajectories","stat.ap physics.ao-ph physics.flu-dyn stat.me","this paper proposes stochastic models for the analysis of ocean surface trajectories obtained from freely-drifting satellite-tracked instruments. the proposed time series models are used to summarise large multivariate datasets and infer important physical parameters of inertial oscillations and other ocean processes. nonstationary time series methods are employed to account for the spatiotemporal variability of each trajectory. because the datasets are large, we construct computationally efficient methods through the use of frequency-domain modelling and estimation, with the data expressed as complex-valued time series. we detail how practical issues related to sampling and model misspecification may be addressed using semi-parametric techniques for time series, and we demonstrate the effectiveness of our stochastic models through application to both real-world data and to numerical model output.","10.1111/rssc.12112","2013-12-10","2015-04-21","['adam m. sykulski', 'sofia c. olhede', 'jonathan m. lilly', 'eric danioux']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1789",1312.4479,"parametric modelling of multivariate count data using probabilistic   graphical models","stat.ml cs.lg stat.me","multivariate count data are defined as the number of items of different categories issued from sampling within a population, which individuals are grouped into categories. the analysis of multivariate count data is a recurrent and crucial issue in numerous modelling problems, particularly in the fields of biology and ecology (where the data can represent, for example, children counts associated with multitype branching processes), sociology and econometrics. we focus on i) identifying categories that appear simultaneously, or on the contrary that are mutually exclusive. this is achieved by identifying conditional independence relationships between the variables; ii)building parsimonious parametric models consistent with these relationships; iii) characterising and testing the effects of covariates on the joint distribution of the counts. to achieve these goals, we propose an approach based on graphical probabilistic models, and more specifically partially directed acyclic graphs.","","2013-12-16","","['pierre fernique', 'jean-baptiste durand', 'yann guédon']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1790",1312.5911,"estimating time-changes in noisy l\'evy models","math.st q-fin.st stat.th","in quantitative finance, we often model asset prices as a noisy ito semimartingale. as this model is not identifiable, approximating by a time-changed levy process can be useful for generative modelling. we give a new estimate of the normalised volatility or time change in this model, which obtains minimax convergence rates, and is unaffected by infinite-variation jumps. in the semimartingale model, our estimate remains accurate for the normalised volatility, obtaining convergence rates as good as any previously implied in the literature.","10.1214/14-aos1250","2013-12-20","2014-11-14","['adam d. bull']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1791",1312.622,"dynamic modelling of hepatitis c virus transmission among people who   inject drugs: a methodological review","stat.ap math.ds q-bio.qm","equipment sharing among people who inject drugs (pwid) is a key risk factor in infection by hepatitis c virus (hcv). both the effectiveness and cost-effectiveness of interventions aimed at reducing hcv transmission in this population (such as opioid substitution therapy, needle exchange programs or improved treatment) are difficult to evaluate using field surveys. ethical issues and complicated access to the pwid population make it difficult to gather epidemiological data. in this context, mathematical modelling of hcv transmission is a useful alternative for comparing the cost and effectiveness of various interventions. several models have been developed in the past few years. they are often based on strong hypotheses concerning the population structure. this review presents compartmental and individual-based models in order to underline their strengths and limits in the context of hcv infection among pwid. the final section discusses the main results of the papers.","10.1111/jvh.12337","2013-12-21","2015-02-09","['anthony cousien', 'viet chi tran', 'sylvie deuffic-burban', 'marie jauffret-roustide', 'jean-stephane dhersin', 'yazdan yazdanpanah']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1792",1312.6407,"multivariate markov-switching models and tail risk interdependence","stat.me","markov switching models are often used to analyze financial returns because of their ability to capture frequently observed stylized facts. in this paper we consider a multivariate student-t version of the model as a viable alternative to the usual multivariate gaussian distribution, providing a natural robust extension that accounts for heavy-tails and time varying non-linear correlations. moreover, these modelling assumptions allow us to capture extreme tail co-movements which are of fundamental importance to assess the underlying dependence structure of asset returns during extreme events such as financial crisis. for the considered model we provide new risk interdependence measures which generalize the existing ones, like the conditional value-at-risk (covar). the proposed measures aim to capture interconnections among multiple connecting market participants which is particularly relevant during period of crisis when several institutions may contemporaneously experience distress instances. those measures are analytically evaluated on the predictive distribution of the modes in order to provide a forward-looking risk quantification. application on a set of u.s. banks is considered to show that the right specification of the model conditional distribution along with a multiple risk interdependence measure may help to better understand how the overall risk is shared among institutions.","","2013-12-22","2014-03-02","['mauro bernardi', 'antonello maruotti', 'lea petrella']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1793",1401.0168,"efficient inference and simulation for elliptical pareto processes","stat.me","recent advances in extreme value theory have established $\ell$-pareto processes as the natural limits for extreme events defined in terms of exceedances of a risk functional. here we provide methods for the practical modelling of data based on a tractable yet flexible dependence model. we introduce the class of elliptical $\ell$-pareto processes, which arise as the limit of threshold exceedances of certain elliptical processes characterized by a correlation function and a shape parameter. an efficient inference method based on maximizing a full likelihood with partial censoring is developed. novel procedures for exact conditional and unconditional simulation are proposed. these ideas are illustrated using precipitation extremes in switzerland.","10.1093/biomet/asv045","2013-12-31","2015-11-11","['emeric thibaud', 'thomas opitz']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1794",1401.1052,"inverse bayesian estimation of gravitational mass density in galaxies   from missing kinematic data","stat.ap astro-ph.ga stat.me","in this paper we focus on a type of inverse problem in which the data is expressed as an unknown function of the sought and unknown model function (or its discretised representation as a model parameter vector). in particular, we deal with situations in which training data is not available. then we cannot model the unknown functional relationship between data and the unknown model function (or parameter vector) with a gaussian process of appropriate dimensionality. a bayesian method based on state space modelling is advanced instead. within this framework, the likelihood is expressed in terms of the probability density function ($pdf$) of the state space variable and the sought model parameter vector is embedded within the domain of this $pdf$. as the measurable vector lives only inside an identified sub-volume of the system state space, the $pdf$ of the state space variable is projected onto the space of the measurables, and it is in terms of the projected state space density that the likelihood is written; the final form of the likelihood is achieved after convolution with the distribution of measurement errors. application motivated vague priors are invoked and the posterior probability density of the model parameter vectors, given the data is computed. inference is performed by taking posterior samples with adaptive mcmc. the method is illustrated on synthetic as well as real galactic data.","","2014-01-06","","['dalia chakrabarty', 'prasenjit saha']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1795",1401.2649,"personalised medicine: taking a new look at the patient","stat.ap","personalised medicine strives to identify the right treatment for the right patient at the right time, integrating different types of biological and environmental information. such information come from a variety of sources: omics data (genomic, proteomic, metabolomic, etc.), live molecular diagnostics, and other established diagnostics routinely used by medical doctors. integrating these different kinds of data, which are all high-dimensional, presents significant challenges in knowledge representation and subsequent reasoning. the ultimate goal of such a modelling effort is to elucidate the flow of information that links genes, protein signalling and other physiological responses to external stimuli such as environmental conditions or the progress of a disease.","","2014-01-12","","['marco scutari']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1796",1401.4342,"modelling a response as a function of high frequency count data: the   association between physical activity and fat mass","stat.ap","we present a new statistical modelling approach where the response is a function of high frequency count data. our application is about investigating the relationship between the health outcome fat mass and physical activity (pa) measured by accelerometer. the accelerometer quantifies the intensity of physical activity as counts per epoch over a given period of time. we use data from the avon longitudinal study of parents and children (alspac) where accelerometer data is available as a time series of accelerometer counts per minute over seven days for a subset of children. in order to compare accelerometer profiles between individuals and to reduce the high dimension a functional summary of the profiles is used. we use the histogram as a functional summary due to its simplicity, suitability and ease of interpretation. our model is an extension of generalised regression of scalars on functions or signal regression. it allows also multi-dimensional functional predictors and additive non-linear predictors for metric covariates. the additive multidimensional functional predictors allow investigating specific questions about whether the effect of pa varies over its intensity, by gender, by time of day or by day of the week. the key feature of the model is that it utilises the full profile of measured pa without requiring cut-points defining intensity levels for light, moderate and vigorous activity. we show that the (not necessarily causal) effect of pa is not linear and not constant over the activity intensity. also, there is little evidence to suggest that the effect of pa intensity varies by gender or whether it happens on weekdays or on weekends.","10.1177/0962280215595832","2014-01-17","","['nicole h. augustin', 'calum mattocks', 'julian j. faraway', 'sonja greven', 'andy r. ness']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1797",1401.5613,"a precision of the sequential change point detection","math.st math.pr stat.ap stat.th","a random sequence having two segments being the homogeneous markov processes is registered. each segment has his own transition probability law and the length of the segment is unknown and random. the transition probabilities of each process are known and a priori distribution of the disorder moment is given. the decision maker aim is to detect the moment of the transition probabilities change. the detection of the disorder rarely is precise. the decision maker accepts some deviation in estimation of the disorder moment. in the considered model the aim is to indicate the change point with fixed, bounded error with maximal probability. the case with various precision for over and under estimation of this point is analysed. the case when the disorder does not appears with positive probability is also included. the results insignificantly extends range of application, explain the structure of optimal detector in various circumstances and shows new details of the solution construction. the motivation for this investigation is the modelling of the attacks in the node of networks. the objectives is to detect one of the attack immediately or in very short time before or after it appearance with highest probability. the problem is reformulated to optimal stopping of the observed sequences. the detailed analysis of the problem is presented to show the form of optimal decision function.","","2014-01-22","","['a. ochman-gozdek', 'w. sarnowski', 'k. j. szajowski']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE
"1798",1401.5747,"multiple imputation for continuous variables using a bayesian principal   component analysis","stat.me","we propose a multiple imputation method based on principal component analysis (pca) to deal with incomplete continuous data. to reflect the uncertainty of the parameters from one imputation to the next, we use a bayesian treatment of the pca model. using a simulation study and real data sets, the method is compared to two classical approaches: multiple imputation based on joint modelling and on fully conditional modelling. contrary to the others, the proposed method can be easily used on data sets where the number of individuals is less than the number of variables and when the variables are highly correlated. in addition, it provides unbiased point estimates of quantities of interest, such as an expectation, a regression coefficient or a correlation coefficient, with a smaller mean squared error. furthermore, the widths of the confidence intervals built for the quantities of interest are often smaller whilst ensuring a valid coverage.","","2014-01-22","2015-08-19","['vincent audigier', 'françois husson', 'julie josse']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1799",1401.7214,"exploring dependence between categorical variables: benefits and   limitations of using variable selection within bayesian clustering in   relation to log-linear modelling with interaction terms","stat.me","this manuscript is concerned with relating two approaches that can be used to explore complex dependence structures between categorical variables, namely bayesian partitioning of the covariate space incorporating a variable selection procedure that highlights the covariates that drive the clustering, and log-linear modelling with interaction terms. we derive theoretical results on this relation and discuss if they can be employed to assist log-linear model determination, demonstrating advantages and limitations with simulated and real data sets. the main advantage concerns sparse contingency tables. inferences from clustering can potentially reduce the number of covariates considered and, subsequently, the number of competing log-linear models, making the exploration of the model space feasible. variable selection within clustering can inform on marginal independence in general, thus allowing for a more efficient exploration of the log-linear model space. however, we show that the clustering structure is not informative on the existence of interactions in a consistent manner. this work is of interest to those who utilize log-linear models, as well as practitioners such as epidemiologists that use clustering models to reduce the dimensionality in the data and to reveal interesting patterns on how covariates combine.","","2014-01-28","2016-01-05","['michail papathomas', 'sylvia richardson']",0,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1800",1402.0859,"the informed sampler: a discriminative approach to bayesian inference in   generative computer vision models","cs.cv cs.lg stat.ml","computer vision is hard because of a large variability in lighting, shape, and texture; in addition the image signal is non-additive due to occlusion. generative models promised to account for this variability by accurately modelling the image formation process as a function of latent variables with prior beliefs. bayesian posterior inference could then, in principle, explain the observation. while intuitively appealing, generative models for computer vision have largely failed to deliver on that promise due to the difficulty of posterior inference. as a result the community has favoured efficient discriminative approaches. we still believe in the usefulness of generative models in computer vision, but argue that we need to leverage existing discriminative or even heuristic computer vision methods. we implement this idea in a principled way with an ""informed sampler"" and in careful experiments demonstrate it on challenging generative models which contain renderer programs as their components. we concentrate on the problem of inverting an existing graphics rendering engine, an approach that can be understood as ""inverse graphics"". the informed sampler, using simple discriminative proposals based on existing computer vision technology, achieves significant improvements of inference.","10.1016/j.cviu.2015.03.002","2014-02-04","2015-03-07","['varun jampani', 'sebastian nowozin', 'matthew loper', 'peter v. gehler']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1801",1402.1213,"a statistical modelling approach to detecting community in networks","cs.si stat.ap","there has been considerable recent interest in algorithms for finding communities in networks - groups of vertex within which connections are dense (frequent), but between which connections are sparser (rare). most of the current literature advocates an heuristic approach to the removal of the edges (i.e., removing the links that are less significant using a well-designed function). in this article, we will investigate a technique for uncovering latent communities using a new modelling approach, based on how information spread within a network. it will prove to be easy to use, robust and scalable. it makes supplementary information related to the network/community structure (different communications, consecutive observations) easier to integrate. we will demonstrate the efficiency of our approach by providing some illustrating real-world applications, like the famous zachary karate club, or the amazon political books buyers network.","","2014-02-05","","['adrien ickowicz']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1802",1402.1389,"distributed variational inference in sparse gaussian process regression   and latent variable models","stat.ml cs.lg","gaussian processes (gps) are a powerful tool for probabilistic inference over functions. they have been applied to both regression and non-linear dimensionality reduction, and offer desirable properties such as uncertainty estimates, robustness to over-fitting, and principled ways for tuning hyper-parameters. however the scalability of these models to big datasets remains an active topic of research. we introduce a novel re-parametrisation of variational inference for sparse gp regression and latent variable models that allows for an efficient distributed algorithm. this is done by exploiting the decoupling of the data given the inducing points to re-formulate the evidence lower bound in a map-reduce setting. we show that the inference scales well with data and computational resources, while preserving a balanced distribution of the load among the nodes. we further demonstrate the utility in scaling gaussian processes to big data. we show that gp performance improves with increasing amounts of data in regression (on flight data with 2 million records) and latent variable modelling (on mnist). the results show that gps perform better than many common models often used for big data.","","2014-02-06","2014-09-29","['yarin gal', 'mark van der wilk', 'carl e. rasmussen']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1803",1402.1876,"information theory and image understanding: an application to   polarimetric sar imagery","stat.me math.st stat.th","this work presents a comprehensive examination of the use of information theory for understanding polarimetric synthetic aperture radar (polsar) images by means of contrast measures that can be used as test statistics. due to the phenomenon called `speckle', common to all images obtained with coherent illumination such as polsar imagery, accurate modelling is required in their processing and analysis. the scaled multilook complex wishart distribution has proven to be a successful approach for modelling radar backscatter from forest and pasture areas. classification, segmentation, and image analysis techniques which depend on this model have been devised, and many of them employ some kind of dissimilarity measure. specifically, we introduce statistical tests for analyzing contrast in such images. these tests are based on the chi-square, kullback-leibler, r\'enyi, bhattacharyya, and hellinger distances. results obtained by monte carlo experiments reveal the kullback-leibler distance as the best one with respect to the empirical test sizes under several situations which include pure and contaminated data. the proposed methodology was applied to actual data, obtained by an e-sar sensor over surroundings of we$\beta$ssling, bavaria, germany.","","2014-02-08","","['a. c. frery', 'a. d. c. nascimento', 'r. j. cintra']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1804",1402.2492,"risk margin quantile function via parametric and non-parametric bayesian   quantile regression","q-fin.rm stat.ap stat.co","we develop quantile regression models in order to derive risk margin and to evaluate capital in non-life insurance applications. by utilizing the entire range of conditional quantile functions, especially higher quantile levels, we detail how quantile regression is capable of providing an accurate estimation of risk margin and an overview of implied capital based on the historical volatility of a general insurers loss portfolio. two modelling frameworks are considered based around parametric and nonparametric quantile regression models which we develop specifically in this insurance setting.   in the parametric quantile regression framework, several models including the flexible generalized beta distribution family, asymmetric laplace (al) distribution and power pareto distribution are considered under a bayesian regression framework. the bayesian posterior quantile regression models in each case are studied via markov chain monte carlo (mcmc) sampling strategies.   in the nonparametric quantile regression framework, that we contrast to the parametric bayesian models, we adopted an al distribution as a proxy and together with the parametric al model, we expressed the solution as a scale mixture of uniform distributions to facilitate implementation. the models are extended to adopt dynamic mean, variance and skewness and applied to analyze two real loss reserve data sets to perform inference and discuss interesting features of quantile regression for risk margin calculations.","","2014-02-11","","['alice x. d. dong', 'jennifer s. k. chan', 'gareth w. peters']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE
"1805",1402.3014,"joint inference of misaligned irregular time series with application to   greenland ice core data","stat.ap","ice cores provide insight into the past climate over many millennia. due to ice compaction, the raw data for any single core are irregular in time. multiple cores have different irregularities; jointly these series are misaligned. after processing, such data are made available to researchers as regular time series: a data product. typically, these cores are independently processed. in this paper, we consider a fast bayesian method for the joint processing of multiple irregular series. this is shown to be more efficient. further, our approach permits a realistic modelling of the impact of the multiple sources of uncertainty. the methodology is illustrated with the analysis of a pair of ice cores (gisp2 and grip). our data products, in the form of marginal posterior distributions on an arbitrary temporal grid, are finite gaussian mixtures. we can also produce sample paths from the joint posterior distribution to study non-linear functionals of interest. more generally, the concept of joint analysis via hierarchical gaussian process model can be widely extended as the models used can be viewed within the larger context of continuous space-time processes.","","2014-02-12","2014-09-22","['thinh k. doan', 'andrew c. parnell', 'john haslett']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1806",1402.307,"squeezing bottlenecks: exploring the limits of autoencoder semantic   representation capabilities","cs.ir cs.lg stat.ml","we present a comprehensive study on the use of autoencoders for modelling text data, in which (differently from previous studies) we focus our attention on the following issues: i) we explore the suitability of two different models bda and rsda for constructing deep autoencoders for text data at the sentence level; ii) we propose and evaluate two novel metrics for better assessing the text-reconstruction capabilities of autoencoders; and iii) we propose an automatic method to find the critical bottleneck dimensionality for text language representations (below which structural information is lost).","","2014-02-13","","['parth gupta', 'rafael e. banchs', 'paolo rosso']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1807",1402.3433,"thresholds in choice behaviour and the size of travel time savings","stat.me","travel time savings are usually the most substantial economic benefit of transport infrastructure projects. however, questions surround whether small time savings are as valuable per unit as larger savings. thresholds in individual choice behaviour are one reason cited for a discounted unit value for small time savings. we demonstrate different approaches for modelling these thresholds using synthetic and stated choice data. we show that the consideration of thresholds is important, even if the discounted unit value for small travel time savings is rejected for transport project appraisal. if an existing threshold is ignored in model estimation, the value of travel time savings will be biased. the presented procedure might also be useful to model thresholds in other contexts of choice behaviour.","","2014-02-14","2015-09-02","['andy obermeyer', 'martin treiber', 'christos evangelinos']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1808",1402.3783,"map-aware models for indoor wireless localization systems: an   experimental study","cs.it math.it stat.ap","the accuracy of indoor wireless localization systems can be substantially enhanced by map-awareness, i.e., by the knowledge of the map of the environment in which localization signals are acquired. in fact, this knowledge can be exploited to cancel out, at least to some extent, the signal degradation due to propagation through physical obstructions, i.e., to the so called non-line-of-sight bias. this result can be achieved by developing novel localization techniques that rely on proper map-aware statistical modelling of the measurements they process. in this manuscript a unified statistical model for the measurements acquired in map-aware localization systems based on time-of-arrival and received signal strength techniques is developed and its experimental validation is illustrated. finally, the accuracy of the proposed map-aware model is assessed and compared with that offered by its map-unaware counterparts. our numerical results show that, when the quality of acquired measurements is poor, map-aware modelling can enhance localization accuracy by up to 110% in certain scenarios.","","2014-02-16","","['francesco montorsi', 'fabrizio pancaldi', 'giorgio m. vitetta']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1809",1402.5161,"statistical constraints","cs.ai stat.me","we introduce statistical constraints, a declarative modelling tool that links statistics and constraint programming. we discuss two statistical constraints and some associated filtering algorithms. finally, we illustrate applications to standard problems encountered in statistics and to a novel inspection scheduling problem in which the aim is to find inspection plans with desirable statistical properties.","10.3233/978-1-61499-419-0-777","2014-02-20","2014-08-14","['roberto rossi', 'steven prestwich', 's. armagan tarim']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1810",1403.051,"bayesian density estimation via multiple sequential inversions of 2-d   images with application in electron microscopy","stat.ap cond-mat.mtrl-sci","we present a new bayesian methodology to learn the unknown material density of a given sample by inverting its two-dimensional images that are taken with a scanning electron microscope. an image results from a sequence of projections of the convolution of the density function with the unknown microscopy correction function that we also learn from the data. we invoke a novel design of experiment, involving imaging at multiple values of the parameter that controls the sub-surface depth from which information about the density structure is carried, to result in the image. real-life material density functions are characterised by high density contrasts and typically are highly discontinuous, implying that they exhibit correlation structures that do not vary smoothly. in the absence of training data, modelling such correlation structures of real material density functions is not possible. so we discretise the material sample and treat values of the density function at chosen locations inside it as independent and distribution-free parameters. resolution of the available image dictates the discretisation length of the model; three models pertaining to distinct resolution classes are developed. we develop priors on the material density, such that these priors adapt to the sparsity inherent in the density function. the likelihood is defined in terms of the distance between the convolution of the unknown functions and the image data. the posterior probability density of the unknowns given the data is expressed using the developed priors on the density and priors on the microscopy correction function as elicitated from the microscopy literature. we achieve posterior samples using an adaptive metropolis-within-gibbs inference scheme. the method is applied to learn the material density of a 3-d sample of a real nano-structure and of simulated alloy samples.","","2014-03-03","","['dalia chakrabarty', 'fabio rigat', 'nare gabrielyan', 'richard beanland', 'shashi paul']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1811",1403.0623,"global solar irradiation prediction using a multi-gene genetic   programming approach","cs.ne cs.ce stat.ap","in this paper, a nonlinear symbolic regression technique using an evolutionary algorithm known as multi-gene genetic programming (mggp) is applied for a data-driven modelling between the dependent and the independent variables. the technique is applied for modelling the measured global solar irradiation and validated through numerical simulations. the proposed modelling technique shows improved results over the fuzzy logic and artificial neural network (ann) based approaches as attempted by contemporary researchers. the method proposed here results in nonlinear analytical expressions, unlike those with neural networks which is essentially a black box modelling approach. this additional flexibility is an advantage from the modelling perspective and helps to discern the important variables which affect the prediction. due to the evolutionary nature of the algorithm, it is able to get out of local minima and converge to a global optimum unlike the back-propagation (bp) algorithm used for training neural networks. this results in a better percentage fit than the ones obtained using neural networks by contemporary researchers. also a hold-out cross validation is done on the obtained genetic programming (gp) results which show that the results generalize well to new data and do not over-fit the training samples. the multi-gene gp results are compared with those, obtained using its single-gene version and also the same with four classical regression models in order to show the effectiveness of the adopted approach.","10.1063/1.4850495","2014-03-03","","['indranil pan', 'daya shankar pandey', 'saptarshi das']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1812",1403.0648,"multi-period trading prediction markets with connections to machine   learning","cs.gt cs.lg q-fin.tr stat.ml","we present a new model for prediction markets, in which we use risk measures to model agents and introduce a market maker to describe the trading process. this specific choice on modelling tools brings us mathematical convenience. the analysis shows that the whole market effectively approaches a global objective, despite that the market is designed such that each agent only cares about its own goal. additionally, the market dynamics provides a sensible algorithm for optimising the global objective. an intimate connection between machine learning and our markets is thus established, such that we could 1) analyse a market by applying machine learning methods to the global objective, and 2) solve machine learning problems by setting up and running certain markets.","","2014-03-03","","['jinli hu', 'amos storkey']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1813",1403.0848,"netconomics: novel forecasting techniques from the combination of big   data, network science and economics","q-fin.gn physics.soc-ph stat.me","the combination of the network theoretic approach with recently available abundant economic data leads to the development of novel analytic and computational tools for modelling and forecasting key economic indicators. the main idea is to introduce a topological component into the analysis, taking into account consistently all higher-order interactions. we present three basic methodologies to demonstrate different approaches to harness the resulting network gain. first, a multiple linear regression optimisation algorithm is used to generate a relational network between individual components of national balance of payment accounts. this model describes annual statistics with a high accuracy and delivers good forecasts for the majority of indicators. second, an early-warning mechanism for global financial crises is presented, which combines network measures with standard economic indicators. from the analysis of the cross-border portfolio investment network of long-term debt securities, the proliferation of a wide range of over-the-counter-traded financial derivative products, such as credit default swaps, can be described in terms of gross-market values and notional outstanding amounts, which are associated with increased levels of market interdependence and systemic risk. third, considering the flow-network of goods traded between g-20 economies, network statistics provide better proxies for key economic measures than conventional indicators. for example, it is shown that a country's gate-keeping potential, as a measure for local power, projects its annual change of gdp generally far better than the volume of its imports or exports.","","2014-03-04","","['andreas joseph', 'irena vodenska', 'eugene stanley', 'guanrong chen']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1814",1403.1452,"the evolution of boosting algorithms - from machine learning to   statistical modelling","stat.me","the concept of boosting emerged from the field of machine learning. the basic idea is to boost the accuracy of a weak classifying tool by combining various instances into a more accurate prediction. this general concept was later adapted to the field of statistical modelling. this review article attempts to highlight this evolution of boosting algorithms from machine learning to statistical modelling. we describe the adaboost algorithm for classification as well as the two most prominent statistical boosting approaches, gradient boosting and likelihood-based boosting. although both appraoches are typically treated separately in the literature, they share the same methodological roots and follow the same fundamental concepts. compared to the initial machine learning algorithms, which must be seen as black-box prediction schemes, statistical boosting result in statistical models which offer a straight-forward interpretation. we highlight the methodological background and present the most common software implementations. worked out examples and corresponding r code can be found in the appendix.","10.3414/me13-01-0122","2014-03-06","2014-11-18","['andreas mayr', 'harald binder', 'olaf gefeller', 'matthias schmid']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1815",1403.1692,"extending statistical boosting - an overview of recent methodological   developments","stat.me","boosting algorithms to simultaneously estimate and select predictor effects in statistical models have gained substantial interest during the last decade. this review article aims to highlight recent methodological developments regarding boosting algorithms for statistical modelling especially focusing on topics relevant for biomedical research. we suggest a unified framework for gradient boosting and likelihood-based boosting (statistical boosting) which have been addressed strictly separated in the literature up to now. statistical boosting algorithms have been adapted to carry out unbiased variable selection and automated model choice during the fitting process and can nowadays be applied in almost any possible type of regression setting in combination with a large amount of different types of predictor effects. the methodological developments on statistical boosting during the last ten years can be grouped into three different lines of research: (i) efforts to ensure variable selection leading to sparser models, (ii) developments regarding different types of predictor effects and their selection (model choice), (iii) approaches to extend the statistical boosting framework to new regression settings.","10.3414/me13-01-0123","2014-03-07","2014-11-18","['andreas mayr', 'harald binder', 'olaf gefeller', 'matthias schmid']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1816",1403.1783,"bayesian spatio-temporal epidemic models with applications to sheep pox","stat.ap","epidemic data often possess certain characteristics, such as the presence of many zeros, the spatial nature of the disease spread mechanism or environmental noise. this paper addresses these issues via suitable bayesian modelling. in doing so we utilise stochastic regression models appropriate for spatio-temporal count data with an excess number of zeros. the developed regression framework can incorporate serial correlation and time varying covariates through an ornstein uhlenbeck process formulation. in addition, we explore the effect of different priors, including default options and techniques based upon variations of mixtures of $g$-priors. the effect of different distance kernels for the epidemic model component is investigated. we proceed by developing branching process-based methods for testing scenarios for disease control, thus linking traditional spatio-temporal models with epidemic processes, useful in policy-focused decision making. the approach is illustrated with an application to a sheep pox dataset from the evros region, greece.","","2014-03-07","","['c. malesios', 'n. demiris', 'k. kalogeropoulos', 'i. ntzoufras']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1817",1403.464,"communication communities in moocs","cs.cy cs.si stat.ml","massive open online courses (moocs) bring together thousands of people from different geographies and demographic backgrounds -- but to date, little is known about how they learn or communicate. we introduce a new content-analysed mooc dataset and use bayesian non-negative matrix factorization (bnmf) to extract communities of learners based on the nature of their online forum posts. we see that bnmf yields a superior probabilistic generative model for online discussions when compared to other models, and that the communities it learns are differentiated by their composite students' demographic and course performance indicators. these findings suggest that computationally efficient probabilistic generative modelling of moocs can reveal important insights for educational researchers and practitioners and help to develop more intelligent and responsive online learning environments.","","2014-03-18","2014-04-16","['nabeel gillani', 'rebecca eynon', 'michael osborne', 'isis hjorth', 'stephen roberts']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1818",1404.1733,"comment on ""comparing two formulations of skew distributions with   special reference to model-based clustering"" by a. azzalini, r. browne, m.   genton, and p. mcnicholas","stat.me","in this paper, we comment on the recent comparison in azzalini et al. (2014) of two different distributions proposed in the literature for the modelling of data that have asymmetric and possibly long-tailed clusters. they are referred to as the restricted and unrestricted skew t-distributions by lee and mclachlan (2013a). firstly, we wish to point out that in lee and mclachlan (2014b), which preceded this comparison, it is shown how a distribution belonging to the broader class, the canonical fundamental skew t (cfust) class, can be fitted with essentially no additional computational effort than for the unrestricted distribution. the cfust class includes the restricted and unrestricted distributions as special cases. thus the user now has the option of letting the data decide as to which model is appropriate for their particular dataset. secondly, we wish to identify several statements in the comparison by azzalini et al.(2014) that demonstrate a serious misunderstanding of the reporting of results in lee and mclachlan (2014a) on the relative performance of these two skew t-distributions. in particular, there is an apparent misunderstanding of the nomenclature that has been adopted to distinguish between these two models. thirdly, we take the opportunity to report here that we have obtained improved fits, in some cases a marked improvement, for the unrestricted model for various cases corresponding to different combinations of the variables in the two real datasets that were used in azzalini et al. (2014) to mount their claims on the relative superiority of the restricted and unrestricted models. for one case the misclassification rate of our fit under the unrestricted model is less than one third of their reported error rate. our results thus reverse their claims on the ranking of the restricted and unrestricted models in such cases.","","2014-04-07","","['geoffrey j. mclachlan', 'sharon x. lee']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1819",1404.2189,"data mining for censored time-to-event data: a bayesian network model   for predicting cardiovascular risk from electronic health record data","stat.ml stat.ap","models for predicting the risk of cardiovascular events based on individual patient characteristics are important tools for managing patient care. most current and commonly used risk prediction models have been built from carefully selected epidemiological cohorts. however, the homogeneity and limited size of such cohorts restricts the predictive power and generalizability of these risk models to other populations. electronic health data (ehd) from large health care systems provide access to data on large, heterogeneous, and contemporaneous patient populations. the unique features and challenges of ehd, including missing risk factor information, non-linear relationships between risk factors and cardiovascular event outcomes, and differing effects from different patient subgroups, demand novel machine learning approaches to risk model development. in this paper, we present a machine learning approach based on bayesian networks trained on ehd to predict the probability of having a cardiovascular event within five years. in such data, event status may be unknown for some individuals as the event time is right-censored due to disenrollment and incomplete follow-up. since many traditional data mining methods are not well-suited for such data, we describe how to modify both modelling and assessment techniques to account for censored observation times. we show that our approach can lead to better predictive performance than the cox proportional hazards model (i.e., a regression-based approach commonly used for censored, time-to-event data) or a bayesian network with {\em{ad hoc}} approaches to right-censoring. our techniques are motivated by and illustrated on data from a large u.s. midwestern health care system.","","2014-04-08","","['sunayan bandyopadhyay', 'julian wolfson', 'david m. vock', 'gabriela vazquez-benitez', 'gediminas adomavicius', 'mohamed elidrisi', 'paul e. johnson', ""patrick j. o'connor""]",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"1820",1404.3046,"ecf identification of garch systems driven by l\'evy processes","math.st stat.th","l\'evy processes are widely used in financial mathematics, telecommunication, economics, queueing theory and natural sciences for modelling. we propose an essentially asymptotically efficient estimation method for the system parameters of general autoregressive conditional heteroscedasticity (garch) processes. as an alternative to the maximum likelihood (ml) method we develop and analyze a novel identification method by adapting the so-called empirical characteristic function method (ecf) originally devised for estimating parameters of c.f.-s from i.i.d. samples. precise characterization of the errors of these estimators will be given, and their asymptotic covariance matrices will be obtained.","","2014-04-11","","['máté mánfay', 'lászló gerencsér', 'zsanett orlovits']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1821",1404.4077,"model-based clustering using copulas with applications","stat.me","the majority of model-based clustering techniques is based on multivariate normal models and their variants. in this paper copulas are used for the construction of flexible families of models for clustering applications. the use of copulas in model-based clustering offers two direct advantages over current methods: i) the appropriate choice of copulas provides the ability to obtain a range of exotic shapes for the clusters, and ii) the explicit choice of marginal distributions for the clusters allows the modelling of multivariate data of various modes (either discrete or continuous) in a natural way. this paper introduces and studies the framework of copula-based finite mixture models for clustering applications. estimation in the general case can be performed using standard em, and, depending on the mode of the data, more efficient procedures are provided that can fully exploit the copula structure. the closure properties of the mixture models under marginalization are discussed, and for continuous, real-valued data parametric rotations in the sample space are introduced, with a parallel discussion on parameter identifiability depending on the choice of copulas for the components. the exposition of the methodology is accompanied and motivated by the analysis of real and artificial data.","10.1007/s11222-015-9590-5","2014-04-15","2015-07-02","['ioannis kosmidis', 'dimitris karlis']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1822",1404.4175,"meg decoding across subjects","stat.ml cs.lg q-bio.nc","brain decoding is a data analysis paradigm for neuroimaging experiments that is based on predicting the stimulus presented to the subject from the concurrent brain activity. in order to make inference at the group level, a straightforward but sometimes unsuccessful approach is to train a classifier on the trials of a group of subjects and then to test it on unseen trials from new subjects. the extreme difficulty is related to the structural and functional variability across the subjects. we call this approach ""decoding across subjects"". in this work, we address the problem of decoding across subjects for magnetoencephalographic (meg) experiments and we provide the following contributions: first, we formally describe the problem and show that it belongs to a machine learning sub-field called transductive transfer learning (ttl). second, we propose to use a simple ttl technique that accounts for the differences between train data and test data. third, we propose the use of ensemble learning, and specifically of stacked generalization, to address the variability across subjects within train data, with the aim of producing more stable classifiers. on a face vs. scramble task meg dataset of 16 subjects, we compare the standard approach of not modelling the differences across subjects, to the proposed one of combining ttl and ensemble learning. we show that the proposed approach is consistently more accurate than the standard one.","","2014-04-16","","['emanuele olivetti', 'seyed mostafa kia', 'paolo avesani']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1823",1404.4414,"probit transformation for nonparametric kernel estimation of the copula   density","stat.me math.st stat.th","copula modelling has become ubiquitous in modern statistics. here, the problem of nonparametrically estimating a copula density is addressed. arguably the most popular nonparametric density estimator, the kernel estimator is not suitable for the unit-square-supported copula densities, mainly because it is heavily affected by boundary bias issues. in addition, most common copulas admit unbounded densities, and kernel methods are not consistent in that case. in this paper, a kernel-type copula density estimator is proposed. it is based on the idea of transforming the uniform marginals of the copula density into normal distributions via the probit function, estimating the density in the transformed domain, which can be accomplished without boundary problems, and obtaining an estimate of the copula density through back-transformation. although natural, a raw application of this procedure was, however, seen not to perform very well in the earlier literature. here, it is shown that, if combined with local likelihood density estimation methods, the idea yields very good and easy to implement estimators, fixing boundary issues in a natural way and able to cope with unbounded copula densities. the asymptotic properties of the suggested estimators are derived, and a practical way of selecting the crucially important smoothing parameters is devised. finally, extensive simulation studies and a real data analysis evidence their excellent performance compared to their main competitors.","","2014-04-16","","['gery geenens', 'arthur charpentier', 'davy paindaveine']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1824",1405.1299,"model-based clustering of gaussian copulas for mixed data","stat.me","clustering task of mixed data is a challenging problem. in a probabilistic framework, the main difficulty is due to a shortage of conventional distributions for such data. in this paper, we propose to achieve the mixed data clustering with a gaussian copula mixture model, since copulas, and in particular the gaussian ones, are powerful tools for easily modelling the distribution of multivariate variables. indeed, considering a mixing of continuous, integer and ordinal variables (thus all having a cumulative distribution function), this copula mixture model defines intra-component dependencies similar to a gaussian mixture, so with classical correlation meaning. simultaneously, it preserves standard margins associated to continuous, integer and ordered features, namely the gaussian, the poisson and the ordered multinomial distributions. as an interesting by-product, the proposed mixture model generalizes many well-known ones and also provides tools of visualization based on the parameters. at a practical level, the bayesian inference is retained and it is achieved with a metropolis-within-gibbs sampler. experiments on simulated and real data sets finally illustrate the expected advantages of the proposed model for mixed data: flexible and meaningful parametrization combined with visualization features.","","2014-05-06","2015-09-29","['matthieu marbac', 'christophe biernacki', 'vincent vandewalle']",0,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1825",1405.2105,"hybrid copula estimators","stat.me","an extension of the empirical copula is considered by combining an estimator of a multivariate cumulative distribution function with estimators of the marginal cumulative distribution functions for marginal estimators that are not necessarily equal to the margins of the joint estimator. such a hybrid estimator may be reasonable when there is additional information available for some margins in the form of additional data or stronger modelling assumptions. a functional central limit theorem is established and some examples are developed.","","2014-05-08","2014-11-28","['johan segers']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1826",1406.0182,"discriminant functions arising from selection distributions: theory and   simulation","stat.co stat.me","the assumption of normality in data has been considered in the field of statistical analysis for a long time. however, in many practical situations, this assumption is clearly unrealistic. it has recently been suggested that the use of distributions indexed by skewness/shape parameters produce more exibility in the modelling of different applications. consequently, the results show a more realistic interpretation for these problems. for these reasons, the aim of this paper is to investigate the effects of the generalisation of a discrimination function method through the class of multivariate extended skew-elliptical distributions, study in detail the multivariate extended skew-normal case and develop a quadratic approximation function for this family of distributions. a simulation study is reported to evaluate the adequacy of the proposed classification rule as well as the performance of the em algorithm to estimate the model parameters.","","2014-06-01","","['reinaldo b. arellano-valle', 'javier e. contreras-reyes']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"1827",1406.0808,"robust improper maximum likelihood: tuning, computation, and a   comparison with other methods for robust gaussian clustering","stat.me stat.co","the two main topics of this paper are the introduction of the ""optimally tuned improper maximum likelihood estimator"" (otrimle) for robust clustering based on the multivariate gaussian model for clusters, and a comprehensive simulation study comparing the otrimle to maximum likelihood in gaussian mixtures with and without noise component, mixtures of t-distributions, and the tclust approach for trimmed clustering. the otrimle uses an improper constant density for modelling outliers and noise. this can be chosen optimally so that the non-noise part of the data looks as close to a gaussian mixture as possible. some deviation from gaussianity can be traded in for lowering the estimated noise proportion. covariance matrix constraints and computation of the otrimle are also treated. in the simulation study, all methods are confronted with setups in which their model assumptions are not exactly fulfilled, and in order to evaluate the experiments in a standardized way by misclassification rates, a new model-based definition of ""true clusters"" is introduced that deviates from the usual identification of mixture components with clusters. in the study, every method turns out to be superior for one or more setups, but the otrimle achieves the most satisfactory overall performance. the methods are also applied to two real datasets, one without and one with known ""true"" clusters.","10.1080/01621459.2015.1100996","2014-06-02","2017-01-28","['pietro coretto', 'christian hennig']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"1828",1406.1245,"modelling receiver operating characteristic curves using gaussian   mixtures","stat.me stat.ap stat.co","the receiver operating characteristic curve is widely applied in measuring the performance of diagnostic tests. many direct and indirect approaches have been proposed for modelling the roc curve, and because of its tractability, the gaussian distribution has typically been used to model both populations. we propose using a gaussian mixture model, leading to a more flexible approach that better accounts for atypical data. monte carlo simulation is used to circumvent the issue of absence of a closed-form. we show that our method performs favourably when compared to the crude binormal curve and to the semi-parametric frequentist binormal roc using the famous labroc procedure.","10.1016/j.csda.2015.04.010","2014-06-04","","['amay cheam', 'paul d. mcnicholas']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"1829",1406.1655,"variational inference of latent state sequences using recurrent networks","stat.ml cs.lg","recent advances in the estimation of deep directed graphical models and recurrent networks let us contribute to the removal of a blind spot in the area of probabilistc modelling of time series. the proposed methods i) can infer distributed latent state-space trajectories with nonlinear transitions, ii) scale to large data sets thanks to the use of a stochastic objective and fast, approximate inference, iii) enable the design of rich emission models which iv) will naturally lead to structured outputs. two different paths of introducing latent state sequences are pursued, leading to the variational recurrent auto encoder (vrae) and the variational one step predictor (vosp). the use of independent wiener processes as priors on the latent state sequence is a viable compromise between efficient computation of the kullback-leibler divergence from the variational approximation of the posterior and maintaining a reasonable belief in the dynamics. we verify our methods empirically, obtaining results close or superior to the state of the art. we also show qualitative results for denoising and missing value imputation.","","2014-06-06","2014-09-30","['justin bayer', 'christian osendorfer']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1830",1406.2933,"optimal designs for copula models","stat.me","copula modelling has in the past decade become a standard tool in many areas of applied statistics. however, a largely neglected aspect concerns the design of related experiments. particularly the issue of whether the estimation of copula parameters can be enhanced by optimizing experimental conditions and how robust all the parameter estimates for the model are with respect to the type of copula employed. in this paper an equivalence theorem for (bivariate) copula models is provided that allows formulation of efficient design algorithms and quick checks of whether designs are optimal or at least efficient. some examples illustrate that in practical situations considerable gains in design efficiency can be achieved. a natural comparison between different copula models with respect to design efficiency is provided as well.","","2014-06-11","","['elisa perrone', 'werner g. müller']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1831",1406.3106,"on lindley-exponential distribution: properties and application","stat.ap stat.me","in this paper, we introduce a new distribution generated by lindley random variable which offers a more flexible model for modelling lifetime data. various statistical properties like distribution function, survival function, moments, entropy, and limiting distribution of extreme order statistics are established. inference for a random sample from the proposed distribution is investigated and maximum likelihood estimation method is used for estimating parameters of this distribution. the applicability of the proposed distribution is shown through real data sets.","","2014-06-11","","['deepesh bhati', 'mohd. aamir malik']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1832",1406.3774,"markov-switching generalized additive models","stat.me stat.co","we consider markov-switching regression models, i.e. models for time series regression analyses where the functional relationship between covariates and response is subject to regime switching controlled by an unobservable markov chain. building on the powerful hidden markov model machinery and the methods for penalized b-splines routinely used in regression analyses, we develop a framework for nonparametrically estimating the functional form of the effect of the covariates in such a regression model, assuming an additive structure of the predictor. the resulting class of markov-switching generalized additive models is immensely flexible, and contains as special cases the common parametric markov-switching regression models and also generalized additive and generalized linear models. the feasibility of the suggested maximum penalized likelihood approach is demonstrated by simulation and further illustrated by modelling how energy price in spain depends on the euro/dollar exchange rate.","","2014-06-14","2015-05-10","['roland langrock', 'thomas kneib', 'richard glennie', 'théo michelot']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"1833",1406.383,"modelling, visualising and summarising documents with a single   convolutional neural network","cs.cl cs.lg stat.ml","capturing the compositional process which maps the meaning of words to that of documents is a central challenge for researchers in natural language processing and information retrieval. we introduce a model that is able to represent the meaning of documents by embedding them in a low dimensional vector space, while preserving distinctions of word and sentence order crucial for capturing nuanced semantics. our model is based on an extended dynamic convolution neural network, which learns convolution filters at both the sentence and document level, hierarchically learning to capture and compose low level lexical features into high level semantic concepts. we demonstrate the effectiveness of this model on a range of document modelling tasks, achieving strong results with no feature engineering and with a more compact model. inspired by recent advances in visualising deep convolution networks for computer vision, we present a novel visualisation technique for our document networks which not only provides insight into their learning process, but also can be interpreted to produce a compelling automatic summarisation system for texts.","","2014-06-15","","['misha denil', 'alban demiraj', 'nal kalchbrenner', 'phil blunsom', 'nando de freitas']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1834",1406.4643,"vector quantile regression: an optimal transport approach","stat.me","we propose a notion of conditional vector quantile function and a vector quantile regression. a \emph{conditional vector quantile function} (cvqf) of a random vector $y$, taking values in $\mathbb{r}^d$ given covariates $z=z$, taking values in $\mathbb{r}% ^k$, is a map $u \longmapsto q_{y\mid z}(u,z)$, which is monotone, in the sense of being a gradient of a convex function, and such that given that vector $u$ follows a reference non-atomic distribution $f_u$, for instance uniform distribution on a unit cube in $\mathbb{r}^d$, the random vector $q_{y\mid z}(u,z)$ has the distribution of $y$ conditional on $z=z$. moreover, we have a strong representation, $y = q_{y\mid z}(u,z)$ almost surely, for some version of $u$. the \emph{vector quantile regression} (vqr) is a linear model for cvqf of $y$ given $z$. under correct specification, the notion produces strong representation, $y=\beta \left(u\right) ^\top f(z)$, for $f(z)$ denoting a known set of transformations of $z$, where $u \longmapsto \beta(u)^\top f(z)$ is a monotone map, the gradient of a convex function, and the quantile regression coefficients $u \longmapsto \beta(u)$ have the interpretations analogous to that of the standard scalar quantile regression. as $f(z)$ becomes a richer class of transformations of $z$, the model becomes nonparametric, as in series modelling. a key property of vqr is the embedding of the classical monge-kantorovich's optimal transportation problem at its core as a special case. in the classical case, where $y$ is scalar, vqr reduces to a version of the classical qr, and cvqf reduces to the scalar conditional quantile function. an application to multiple engel curve estimation is considered.","","2014-06-18","2015-09-27","['guillaume carlier', 'victor chernozhukov', 'alfred galichon']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1835",1406.5005,"computation and visualisation for large-scale gaussian updates","stat.co","in geostatistics, and also in other applications in science and engineering, we are now performing updates on gaussian process models with many thousands or even millions of components. these large-scale inferences involve computational challenges, because the updating equations cannot be solved as written, owing to the size and cost of the matrix operations. they also involve representational challenges, to account for judgements of heterogeneity concerning the underlying fields, and diverse sources of observations.   diagnostics are particularly valuable in this situation. we present a diagnostic and visualisation tool for large-scale gaussian updates, the `medal plot'. this shows the updated uncertainty for each observation, and also summarises the sharing of information across observations, as a proxy for the sharing of information across the state vector. it allows us to `sanity-check' the code implementing the update, but it can also reveal unexpected features in our modelling. we discuss computational issues for large-scale updates, and we illustrate with an application to assess mass trends in the antarctic ice sheet.","","2014-06-19","","['jonathan rougier', 'andrew zammit mangion', 'nana schoen']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1836",1406.6728,"bayesian survival modelling of university outcomes","stat.ap","the aim of this paper is to model the length of registration at university and its associated academic outcome for undergraduate students at the pontificia universidad cat\'olica de chile. survival time is defined as the time until the end of the enrollment period, which can relate to different reasons - graduation or two types of dropout - that are driven by different processes. hence, a competing risks model is employed for the analysis. the issue of separation of the outcomes (which precludes maximum likelihood estimation) is handled through the use of bayesian inference with an appropriately chosen prior. we are interested in identifying important determinants of university outcomes and the associ- ated model uncertainty is formally addressed through bayesian model averaging. the methodology introduced for modelling university outcomes is applied to three selected degree programmes, which are particularly affected by dropout and late graduation.","","2014-06-25","","['catalina a. vallejos', 'mark f. j. steel']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1837",1406.7665,"interleaved factorial non-homogeneous hidden markov models for energy   disaggregation","stat.ap","to reduce energy demand in households it is useful to know which electrical appliances are in use at what times. monitoring individual appliances is costly and intrusive, whereas data on overall household electricity use is more easily obtained. in this paper, we consider the energy disaggregation problem where a household's electricity consumption is disaggregated into the component appliances. the factorial hidden markov model (fhmm) is a natural model to fit this data. we enhance this generic model by introducing two constraints on the state sequence of the fhmm. the first is to use a non-homogeneous markov chain, modelling how appliance usage varies over the day, and the other is to enforce that at most one chain changes state at each time step. this yields a new model which we call the interleaved factorial non-homogeneous hidden markov model (ifnhmm). we evaluated the ability of this model to perform disaggregation in an ultra-low frequency setting, over a data set of 251 english households. in this new setting, the ifnhmm outperforms the fhmm in terms of recovering the energy used by the component appliances, due to that stronger constraints have been imposed on the states of the hidden markov chains. interestingly, we find that the variability in model performance across households is significant, underscoring the importance of using larger scale data in the disaggregation problem.","","2014-06-30","","['mingjun zhong', 'nigel goddard', 'charles sutton']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1838",1407.0064,"the zero & $n$-inflated binomial distribution with applications","stat.me","in this article we consider the distribution arising when two zero-inflated poisson count processes are constrained by their sum total, resulting in a novel zero & $n$-inflated binomial distribution. this result motivates a general class of model for applications in which a sum-constrained count response is subject to multiple sources of heterogeneity, principally an excess of zeroes and $n$'s in the underlying count generating process. two examples from the ecological regression literature are used to illustrate the wide applicability of the proposed model, and serve to detail its substantial superiority in modelling performance as compared to competing models. we also present an extension to the modelling framework for more complex cases, considering a gender study dataset which is overdispersed relative to the new likelihood, and conclude the article with the description of a general framework for a zero & $n$-inflated multinomial distribution.","","2014-06-30","2016-02-17","['james sweeney', 'john haslett', 'andrew c. parnell']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1839",1407.1576,"a statistical modelling and analysis of phevs' power demand in smart   grids","cs.ce stat.ap","electric vehicles (evs) and particularly plug-in hybrid electric vehicles (phevs) are foreseen to become popular in the near future. not only are they much more environmentally friendly than conventional internal combustion engine (ice) vehicles, their fuel can also be catered from diverse energy sources and resources. however, they add significant load on the power grid as they become widespread. the characteristics of this extra load follow the patterns of people's driving behaviours. in particular, random parameters such as arrival time and driven distance of the vehicles determine their expected demand profile from the power grid. in this paper, we first present a model for uncoordinated charging power demand of phevs based on a stochastic process and accordingly we characterize the ev's expected daily power demand profile. next, we adopt different distributions for the ev's charging time following some available empirical research data in the literature. simulation results show that the ev's expected daily power demand profiles obtained under the uniform, gaussian with positive support and rician distributions for charging time are identical when the first and second order statistics of these distributions are the same. this gives us useful insights into the long-term planning for upgrading power systems' infrastructure to accommodate phevs. in addition, the results from this modelling can be incorporated into designing demand response (dr) algorithms and evaluating the available dr techniques more accurately.","","2014-07-07","","['farshad rassaei', 'wee-seng soh', 'kee-chaing chua']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1840",1407.4139,"subjectivity, bayesianism, and causality","cs.ai stat.me stat.ml","bayesian probability theory is one of the most successful frameworks to model reasoning under uncertainty. its defining property is the interpretation of probabilities as degrees of belief in propositions about the state of the world relative to an inquiring subject. this essay examines the notion of subjectivity by drawing parallels between lacanian theory and bayesian probability theory, and concludes that the latter must be enriched with causal interventions to model agency. the central contribution of this work is an abstract model of the subject that accommodates causal interventions in a measure-theoretic formalisation. this formalisation is obtained through a game-theoretic ansatz based on modelling the inside and outside of the subject as an extensive-form game with imperfect information between two players. finally, i illustrate the expressiveness of this model with an example of causal induction.","","2014-07-15","2015-04-24","['pedro a. ortega']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1841",1407.4184,"inference for biased models: a quasi-instrumental variable approach","stat.me","for linear regression models who are not exactly sparse in the sense that the coefficients of the insignificant variables are not exactly zero, the working models obtained by a variable selection are often biased. even in sparse cases, after a variable selection, when some significant variables are missing, the working models are biased as well. thus, under such situations, root-n consistent estimation and accurate prediction could not be expected. in this paper, a novel remodelling method is proposed to produce an unbiased model when quasi-instrumental variables are introduced. the root-n estimation consistency and the asymptotic normality can be achieved, and the prediction accuracy can be promoted as well. the performance of the new method is examined through simulation studies.","","2014-07-15","","['lu lin', 'lixing zhu', 'yujie gai']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1842",1407.5155,"sparse and spurious: dictionary learning with noise and outliers","cs.lg stat.ml","a popular approach within the signal processing and machine learning communities consists in modelling signals as sparse linear combinations of atoms selected from a learned dictionary. while this paradigm has led to numerous empirical successes in various fields ranging from image to audio processing, there have only been a few theoretical arguments supporting these evidences. in particular, sparse coding, or sparse dictionary learning, relies on a non-convex procedure whose local minima have not been fully analyzed yet. in this paper, we consider a probabilistic model of sparse signals, and show that, with high probability, sparse coding admits a local minimum around the reference dictionary generating the signals. our study takes into account the case of over-complete dictionaries, noisy signals, and possible outliers, thus extending previous work limited to noiseless settings and/or under-complete dictionaries. the analysis we conduct is non-asymptotic and makes it possible to understand how the key quantities of the problem, such as the coherence or the level of noise, can scale with respect to the dimension of the signals, the number of atoms, the sparsity and the number of observations.","","2014-07-19","2015-08-22","['rémi gribonval', 'rodolphe jenatton', 'francis bach']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1843",1407.529,"event-controlled constructions of random fields of maxima with   non-max-stable dependence","stat.me physics.geo-ph","max-stable random fields can be constructed according to schlather (2002) with a random function or a stationary process and a kind of random event magnitude. these are applied for the modelling of natural hazards. we simply extend these event-controlled constructions to random fields of maxima with non-max-stable dependence structure (copula). the theory for the variant with a stationary process is obvious; the parameter(s) of its correlation function is/are determined by the event magnitude. the introduced variant with random functions can only be researched numerically. the scaling of the random function is exponentially determined by the event magnitude. the location parameter of the gumbel margins depends only on this exponential function in the researched examples; the scale parameter of the margins is normalized. in addition, we propose a method for the parameter estimation for such constructions by using kendall's tau. the spatial dependence in relation to the block size is considered therein. finally, we briefly discuss some issues like the sampling.","","2014-07-20","","['mathias raschke']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1844",1407.6128,"permutation models for collaborative ranking","cs.ir cs.lg stat.ml","we study the problem of collaborative filtering where ranking information is available. focusing on the core of the collaborative ranking process, the user and their community, we propose new models for representation of the underlying permutations and prediction of ranks. the first approach is based on the assumption that the user makes successive choice of items in a stage-wise manner. in particular, we extend the plackett-luce model in two ways - introducing parameter factoring to account for user-specific contribution, and modelling the latent community in a generative setting. the second approach relies on log-linear parameterisation, which relaxes the discrete-choice assumption, but makes learning and inference much more involved. we propose mcmc-based learning and inference methods and derive linear-time prediction algorithms.","","2014-07-23","","['truyen tran', 'svetha venkatesh']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1845",1407.6895,"bayesian exponential random graph models with nodal random effects","stat.ap stat.me","we extend the well-known and widely used exponential random graph model (ergm) by including nodal random effects to compensate for heterogeneity in the nodes of a network. the bayesian framework for ergms proposed by caimo and friel (2011) yields the basis of our modelling algorithm. a central question in network models is the question of model selection and following the bayesian paradigm we focus on estimating bayes factors. to do so we develop an approximate but feasible calculation of the bayes factor which allows one to pursue model selection. two data examples and a small simulation study illustrate our mixed model approach and the corresponding model selection.","","2014-07-25","2015-01-12","['stephanie thiemichen', 'nial friel', 'alberto caimo', 'göran kauermann']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1846",1408.0047,"cumulative restricted boltzmann machines for ordinal matrix data   analysis","stat.ml cs.ir cs.lg stat.ap stat.me","ordinal data is omnipresent in almost all multiuser-generated feedback - questionnaires, preferences etc. this paper investigates modelling of ordinal data with gaussian restricted boltzmann machines (rbms). in particular, we present the model architecture, learning and inference procedures for both vector-variate and matrix-variate ordinal data. we show that our model is able to capture latent opinion profile of citizens around the world, and is competitive against state-of-art collaborative filtering techniques on large-scale public datasets. the model thus has the potential to extend application of rbms to diverse domains such as recommendation systems, product reviews and expert assessments.","","2014-07-31","","['truyen tran', 'dinh phung', 'svetha venkatesh']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE
"1847",1408.0711,"clustering using skewed multivariate heavy tailed distributions with   flexible tail behaviour","stat.me stat.ap","the family of location and scale mixtures of gaussians has the ability to generate a number of flexible distributional forms. it nests as particular cases several important asymmetric distributions like the generalised hyperbolic distribution. the generalised hyperbolic distribution in turn nests many other well known distributions such as the normal inverse gaussian (nig) whose practical relevance has been widely documented in the literature. in a multivariate setting, we propose to extend the standard location and scale mixture concept into a so called multiple scaled framework which has the advantage of allowing different tail and skewness behaviours in each dimension of the variable space with arbitrary correlation between dimensions. estimation of the parameters is provided via an em algorithm with a particular focus on nig distributions. inference is then extended to cover the case of mixtures of such multiple scaled distributions for application to clustering. assessments on simulated and real data confirm the gain in degrees of freedom and flexibility in modelling data of varying tail behaviour and directional shape.","","2014-08-01","","['darren wraith', 'florence forbes']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1848",1408.116,"mixed-variate restricted boltzmann machines","stat.ml cs.lg stat.me","modern datasets are becoming heterogeneous. to this end, we present in this paper mixed-variate restricted boltzmann machines for simultaneously modelling variables of multiple types and modalities, including binary and continuous responses, categorical options, multicategorical choices, ordinal assessment and category-ranked preferences. dependency among variables is modeled using latent binary variables, each of which can be interpreted as a particular hidden aspect of the data. the proposed model, similar to the standard rbms, allows fast evaluation of the posterior for the latent variables. hence, it is naturally suitable for many common tasks including, but not limited to, (a) as a pre-processing step to convert complex input data into a more convenient vectorial representation through the latent posteriors, thereby offering a dimensionality reduction capacity, (b) as a classifier supporting binary, multiclass, multilabel, and label-ranking outputs, or a regression tool for continuous outputs and (c) as a data completion tool for multimodal and heterogeneous data. we evaluate the proposed model on a large-scale dataset using the world opinion survey results on three tasks: feature extraction and visualization, data completion and prediction.","","2014-08-05","","['truyen tran', 'dinh phung', 'svetha venkatesh']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE
"1849",1408.1191,"cluster detection and risk estimation for spatio-temporal health data","stat.me","in epidemiological disease mapping one aims to estimate the spatio-temporal pattern in disease risk and identify high-risk clusters, allowing health interventions to be appropriately targeted. bayesian spatio-temporal models are used to estimate smoothed risk surfaces, but this is contrary to the aim of identifying groups of areal units that exhibit elevated risks compared with their neighbours. therefore, in this paper we propose a new bayesian hierarchical modelling approach for simultaneously estimating disease risk and identifying high-risk clusters in space and time. inference for this model is based on markov chain monte carlo simulation, using the freely available r package carbayesst that has been developed in conjunction with this paper. our methodology is motivated by two case studies, the first of which assesses if there is a relationship between public health districts and colon cancer clusters in georgia, while the second looks at the impact of the smoking ban in public places in england on cardiovascular disease clusters.","","2014-08-06","2014-11-10","['duncan lee', 'andrew lawson']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1850",1408.1554,"a complete data frame work for fitting power law distributions","stat.co physics.soc-ph","over the last few decades power law distributions have been suggested as forming generative mechanisms in a variety of disparate fields, such as, astrophysics, criminology and database curation. however, fitting these heavy tailed distributions requires care, especially since the power law behaviour may only be present in the distributional tail. current state of the art methods for fitting these models rely on estimating the cut-off parameter $x_{\min}$. this results in the majority of collected data being discarded. this paper provides an alternative, principled approached for fitting heavy tailed distributions. by directly modelling the deviation from the power law distribution, we can fit and compare a variety of competing models in a single unified framework.","","2014-08-07","2014-08-24","['colin s. gillespie']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1851",1408.506,"modelling across extremal dependence classes","stat.me","different dependence scenarios can arise in multivariate extremes, entailing careful selection of an appropriate class of models. in bivariate extremes, the variables are either asymptotically dependent or are asymptotically independent. most available statistical models suit one or other of these cases, but not both, resulting in a stage in the inference that is unaccounted for, but can substantially impact subsequent extrapolation. existing modelling solutions to this problem are either applicable only on sub-domains, or appeal to multiple limit theories. we introduce a unified representation for bivariate extremes that encompasses a wide variety of dependence scenarios, and applies when at least one variable is large. our representation motivates a parametric model that encompasses both dependence classes. we implement a simple version of this model, and show that it performs well in a range of settings.","","2014-08-21","2015-10-29","['jennifer wadsworth', 'jonathan tawn', 'anthony davison', 'daniel elton']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1852",1408.7025,"synthesising evidence to estimate pandemic (2009) a/h1n1 influenza   severity in 2009-2011","stat.ap","knowledge of the severity of an influenza outbreak is crucial for informing and monitoring appropriate public health responses, both during and after an epidemic. however, case-fatality, case-intensive care admission and case-hospitalisation risks are difficult to measure directly. bayesian evidence synthesis methods have previously been employed to combine fragmented, under-ascertained and biased surveillance data coherently and consistently, to estimate case-severity risks in the first two waves of the 2009 a/h1n1 influenza pandemic experienced in england. we present in detail the complex probabilistic model underlying this evidence synthesis, and extend the analysis to also estimate severity in the third wave of the pandemic strain during the 2010/2011 influenza season. we adapt the model to account for changes in the surveillance data available over the three waves. we consider two approaches: (a) a two-stage approach using posterior distributions from the model for the first two waves to inform priors for the third wave model; and (b) a one-stage approach modelling all three waves simultaneously. both approaches result in the same key conclusions: (1) that the age-distribution of the case-severity risks is ""u""-shaped, with children and older adults having the highest severity; (2) that the age-distribution of the infection attack rate changes over waves, school-age children being most affected in the first two waves and the attack rate in adults over 25 increasing from the second to third waves; and (3) that when averaged over all age groups, case-severity appears to increase over the three waves. the extent to which the final conclusion is driven by the change in age-distribution of those infected over time is subject to discussion.","10.1214/14-aoas775","2014-08-29","2015-02-03","['anne m. presanis', 'richard g. pebody', 'paul j. birrell', 'brian d. m. tom', 'helen k. green', 'hayley durnall', 'douglas fleming', 'daniela de angelis']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1853",1409.0733,"integral approximation by kernel smoothing","math.st stat.th","let $(x_1,\ldots,x_n)$ be an i.i.d. sequence of random variables in $\mathbb{r}^d$, $d\geq 1$. we show that, for any function $\varphi :\mathbb{r}^d\rightarrow\mathbb{r}$, under regularity conditions, \[n^ {1/2}\biggl(n^{-1}\sum_{i=1}^n\frac{\varphi(x_i)}{\widehat{f}^(x_i)}- \int \varphi(x)\,dx\biggr)\stackrel{\mathbb{p}}{\longrightarrow}0,\] where $\widehat{f}$ is the classical kernel estimator of the density of $x_1$. this result is striking because it speeds up traditional rates, in root $n$, derived from the central limit theorem when $\widehat{f}=f$. although this paper highlights some applications, we mainly address theoretical issues related to the later result. we derive upper bounds for the rate of convergence in probability. these bounds depend on the regularity of the functions $\varphi$ and $f$, the dimension $d$ and the bandwidth of the kernel estimator $\widehat{f}$. moreover, they are shown to be accurate since they are used as renormalizing sequences in two central limit theorems each reflecting different degrees of smoothness of $\varphi$. as an application to regression modelling with random design, we provide the asymptotic normality of the estimation of the linear functionals of a regression function. as a consequence of the above result, the asymptotic variance does not depend on the regression function. finally, we debate the choice of the bandwidth for integral approximation and we highlight the good behavior of our procedure through simulations.","10.3150/15-bej725","2014-09-02","2016-06-06","['bernard delyon', 'françois portier']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1854",1409.0743,"does non-stationary spatial data always require non-stationary random   fields?","stat.me stat.ap","a stationary spatial model is an idealization and we expect that the true dependence structures of physical phenomena are spatially varying, but how should we handle this non-stationarity in practice? we study the challenges involved in applying a flexible non-stationary model to a dataset of annual precipitation in the conterminous us, where exploratory data analysis shows strong evidence of a non-stationary covariance structure.   the aim of this paper is to investigate the modelling pipeline once non-stationarity has been detected in spatial data. we show that there is a real danger of over-fitting the model and that careful modelling is necessary in order to properly account for varying second-order structure. in fact, the example shows that sometimes non-stationary gaussian random fields are not necessary to model non-stationary spatial data.","","2014-09-02","2015-09-14","['geir-arne fuglstad', 'daniel simpson', 'finn lindgren', 'håvard rue']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1855",1409.094,"high-performance kernel machines with implicit distributed optimization   and randomization","stat.ml cs.dc cs.lg","in order to fully utilize ""big data"", it is often required to use ""big models"". such models tend to grow with the complexity and size of the training data, and do not make strong parametric assumptions upfront on the nature of the underlying statistical dependencies. kernel methods fit this need well, as they constitute a versatile and principled statistical methodology for solving a wide range of non-parametric modelling problems. however, their high computational costs (in storage and time) pose a significant barrier to their widespread adoption in big data applications.   we propose an algorithmic framework and high-performance implementation for massive-scale training of kernel-based statistical models, based on combining two key technical ingredients: (i) distributed general purpose convex optimization, and (ii) the use of randomization to improve the scalability of kernel methods. our approach is based on a block-splitting variant of the alternating directions method of multipliers, carefully reconfigured to handle very large random feature matrices, while exploiting hybrid parallelism typically found in modern clusters of multicore machines. our implementation supports a variety of statistical learning tasks by enabling several loss functions, regularization schemes, kernels, and layers of randomized approximations for both dense and sparse datasets, in a highly extensible framework. we evaluate the ability of our framework to learn models on data from applications, and provide a comparison against existing sequential and parallel libraries.","","2014-09-02","2015-04-16","['vikas sindhwani', 'haim avron']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1856",1409.2642,"exploiting timss and pirls combined data: multivariate multilevel   modelling of student achievement","stat.ap","we exploit a multivariate multilevel model for the analysis of the italian sample of the timss\&pirls 2011 combined international database on fourth grade students. the multivariate approach jointly considers educational achievement on reading, mathematics and science, thus allowing us to test for differential associations of the covariates with the three outcomes, and to estimate the residual correlations between pairs of outcomes at student and class levels. multilevel modelling allows us to disentangle student and contextual factors affecting achievement. we also account for territorial differences in wealth by means of an index from an external source. the model residuals point out classes with high or low performance. as educational achievement is measured by plausible values, the estimates are obtained through multiple imputation formulas. the results, while confirming the role of traditional student and contextual factors, reveal interesting patterns of achievement in italian primary schools.","","2014-09-09","2015-08-16","['leonardo grilli', 'fulvia pennoni', 'carla rampichini', 'isabella romeo']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1857",1409.2824,"scalable bayesian modelling of paired symbols","stat.ml","we present a novel, scalable and bayesian approach to modelling the occurrence of pairs of symbols (i,j) drawn from a large vocabulary. observed pairs are assumed to be generated by a simple popularity based selection process followed by censoring using a preference function. by basing inference on the well-founded principle of variational bounding, and using new site-independent bounds, we show how a scalable inference procedure can be obtained for large data sets. state of the art results are presented on real-world movie viewing data.","","2014-09-09","2014-09-10","['ulrich paquet', 'noam koenigstein', 'ole winther']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1858",1409.4512,"estimation and testing for covariance-spectral spatial-temporal models","stat.me stat.ap","in this paper we explore a covariance spectral modelling strategy for spatial-temporal processes which involves a spectral approach for time but a covariance approach for space.it facilitates the analysis of coherence between the temporal frequency components at different spatial sites. stein(2005) developed a semi-parametric model within this framework.the purpose of this paper is to give a deeper insight into the properties of his model and to develop simple and more intuitive methods of estimation and testing. an example is given using the irish wind speed data.","","2014-09-16","","['a. m. mosammam', 'j. t. kent']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1859",1409.6219,"flexible modelling in statistics: past, present and future","stat.me math.st stat.th","in times where more and more data become available and where the data exhibit rather complex structures (significant departure from symmetry, heavy or light tails), flexible modelling has become an essential task for statisticians as well as researchers and practitioners from domains such as economics, finance or environmental sciences. this is reflected by the wealth of existing proposals for flexible distributions; well-known examples are azzalini's skew-normal, tukey's $g$-and-$h$, mixture and two-piece distributions, to cite but these. my aim in the present paper is to provide an introduction to this research field, intended to be useful both for novices and professionals of the domain. after a description of the research stream itself, i will narrate the gripping history of flexible modelling, starring emblematic heroes from the past such as edgeworth and pearson, then depict three of the most used flexible families of distributions, and finally provide an outlook on future flexible modelling research by posing challenging open questions.","","2014-09-22","","['christophe ley']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE
"1860",1409.7454,"a bayesian spatial temporal mixtures approach to kinetic parametric   images in dynamic positron emission tomography","stat.ap","we present a fully bayesian statistical approach to the problem of compartmental modelling in the context of positron emission tomography. we cluster homogeneous region of interest and perform kinetic parameter estimation simultaneously. a mixture modelling approach is adopted, incorporating both spatial and temporal information based on reconstructed dynamic pet image. our modelling approach is flexible, and provides uncertainty estimates for the estimated kinetic parameters. crucially, the proposed method allows us to determine the unknown number of clusters, which has a great impact on resulting estimated kinetic parameters. we demonstrate our method on simulated dynamic myocardial pet data, and show that our method is superior to standard curve-fitting approach.","","2014-09-25","2016-02-07","['wanchuang zhu', 'jinsong ouyang', 'yothin rakvongthai', 'n. j. guehl', 'd. w. wooten', 'g. el fakhri', 'm. d. normandin', 'yanan fan']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1861",1409.8083,"variational inference for probabilistic latent tensor factorization with   kl divergence","stat.co cs.na","probabilistic latent tensor factorization (pltf) is a recently proposed probabilistic framework for modelling multi-way data. not only the common tensor factorization models but also any arbitrary tensor factorization structure can be realized by the pltf framework. this paper presents full bayesian inference via variational bayes that facilitates more powerful modelling and allows more sophisticated inference on the pltf framework. we illustrate our approach on model order selection and link prediction.","","2014-09-29","","['beyza ermis', 'y. kenan yılmaz', 'a. taylan cemgil', 'evrim acar']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1862",1409.8202,"short-term predictability of photovoltaic production over italy","cs.lg stat.ap","photovoltaic (pv) power production increased drastically in europe throughout the last years. about the 6% of electricity in italy comes from pv and for an efficient management of the power grid an accurate and reliable forecasting of production would be needed. starting from a dataset of electricity production of 65 italian solar plants for the years 2011-2012 we investigate the possibility to forecast daily production from one to ten days of lead time without using on site measurements. our study is divided in two parts: an assessment of the predictability of meteorological variables using weather forecasts and an analysis on the application of data-driven modelling in predicting solar power production. we calibrate a svm model using available observations and then we force the same model with the predicted variables from weather forecasts with a lead time from one to ten days. as expected, solar power production is strongly influenced by cloudiness and clear sky, in fact we observe that while during summer we obtain a general error under the 10% (slightly lower in south italy), during winter the error is abundantly above the 20%.","","2014-09-29","","['matteo de felice', 'marcello petitta', 'paolo m. ruti']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1863",1410.0347,"bootstrap confidence sets under model misspecification","math.st stat.th","a multiplier bootstrap procedure for construction of likelihood-based confidence sets is considered for finite samples and a possible model misspecification. theoretical results justify the bootstrap validity for a small or moderate sample size and allow to control the impact of the parameter dimension $p$: the bootstrap approximation works if $p^3/n$ is small. the main result about bootstrap validity continues to apply even if the underlying parametric model is misspecified under the so-called small modelling bias condition. in the case when the true model deviates significantly from the considered parametric family, the bootstrap procedure is still applicable but it becomes a bit conservative: the size of the constructed confidence sets is increased by the modelling bias. we illustrate the results with numerical examples for misspecified linear and logistic regressions.","10.1214/15-aos1355","2014-10-01","2015-11-17","['vladimir spokoiny', 'mayya zhilova']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1864",1410.0908,"probit normal correlated topic models","stat.ml cs.ir cs.lg","the logistic normal distribution has recently been adapted via the transformation of multivariate gaus- sian variables to model the topical distribution of documents in the presence of correlations among topics. in this paper, we propose a probit normal alternative approach to modelling correlated topical structures. our use of the probit model in the context of topic discovery is novel, as many authors have so far con- centrated solely of the logistic model partly due to the formidable inefficiency of the multinomial probit model even in the case of very small topical spaces. we herein circumvent the inefficiency of multinomial probit estimation by using an adaptation of the diagonal orthant multinomial probit in the topic models context, resulting in the ability of our topic modelling scheme to handle corpuses with a large number of latent topics. an additional and very important benefit of our method lies in the fact that unlike with the logistic normal model whose non-conjugacy leads to the need for sophisticated sampling schemes, our ap- proach exploits the natural conjugacy inherent in the auxiliary formulation of the probit model to achieve greater simplicity. the application of our proposed scheme to a well known associated press corpus not only helps discover a large number of meaningful topics but also reveals the capturing of compellingly intuitive correlations among certain topics. besides, our proposed approach lends itself to even further scalability thanks to various existing high performance algorithms and architectures capable of handling millions of documents.","","2014-10-03","","['xingchen yu', 'ernest fokoue']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1865",1410.2323,"principal component analysis for second-order stationary vector time   series","stat.me","we extend the principal component analysis (pca) to second-order stationary vector time series in the sense that we seek for a contemporaneous linear transformation for a $p$-variate time series such that the transformed series is segmented into several lower-dimensional subseries, and those subseries are uncorrelated with each other both contemporaneously and serially. therefore those lower-dimensional series can be analysed separately as far as the linear dynamic structure is concerned. technically it boils down to an eigenanalysis for a positive definite matrix. when $p$ is large, an additional step is required to perform a permutation in terms of either maximum cross-correlations or fdr based on multiple tests. the asymptotic theory is established for both fixed $p$ and diverging $p$ when the sample size $n$ tends to infinity. numerical experiments with both simulated and real data sets indicate that the proposed method is an effective initial step in analysing multiple time series data, which leads to substantial dimension reduction in modelling and forecasting high-dimensional linear dynamical structures. unlike pca for independent data, there is no guarantee that the required linear transformation exists. when it does not, the proposed method provides an approximate segmentation which leads to the advantages in, for example, forecasting for future values. the method can also be adapted to segment multiple volatility processes.","10.1214/17-aos1613","2014-10-08","2017-04-12","['jinyuan chang', 'bin guo', 'qiwei yao']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1866",1410.5362,"prediction of synchrostate transitions in eeg signals using markov chain   models","q-bio.nc physics.med-ph stat.ap stat.ml","this paper proposes a stochastic model using the concept of markov chains for the inter-state transitions of the millisecond order quasi-stable phase synchronized patterns or synchrostates, found in multi-channel electroencephalogram (eeg) signals. first and second order transition probability matrices are estimated for markov chain modelling from 100 trials of 128-channel eeg signals during two different face perception tasks. prediction accuracies with such finite markov chain models for synchrostate transition are also compared, under a data-partitioning based cross-validation scheme.","10.1109/lsp.2014.2352251","2014-10-20","","['wasifa jamal', 'saptarshi das', 'ioana-anastasia oprescu', 'koushik maharatna']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE
"1867",1410.7365,"multiple output regression with latent noise","stat.ml","in high-dimensional data, structured noise caused by observed and unobserved factors affecting multiple target variables simultaneously, imposes a serious challenge for modeling, by masking the often weak signal. therefore, (1) explaining away the structured noise in multiple-output regression is of paramount importance. additionally, (2) assumptions about the correlation structure of the regression weights are needed. we note that both can be formulated in a natural way in a latent variable model, in which both the interesting signal and the noise are mediated through the same latent factors. under this assumption, the signal model then borrows strength from the noise model by encouraging similar effects on correlated targets. we introduce a hyperparameter for the \emph{latent signal-to-noise ratio} which turns out to be important for modelling weak signals, and an ordered infinite-dimensional shrinkage prior that resolves the rotational unidentifiability in reduced-rank regression models. simulations and prediction experiments with metabolite, gene expression, fmri measurement, and macroeconomic time series data show that our model equals or exceeds the state-of-the-art performance and, in particular, outperforms the standard approach of assuming independent noise and signal models.","","2014-10-27","2016-02-03","['jussi gillberg', 'pekka marttinen', 'matti pirinen', 'antti j. kangas', 'pasi soininen', 'mehreen ali', 'aki s. havulinna', 'marjo-riitta marjo-riitta järvelin', 'mika ala-korpela', 'samuel kaski']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1868",1410.8276,"functional regression approximate bayesian computation for gaussian   process density estimation","stat.co","we propose a novel bayesian nonparametric method for hierarchical modelling on a set of related density functions, where grouped data in the form of samples from each density function are available. borrowing strength across the groups is a major challenge in this context. to address this problem, we introduce a hierarchically structured prior, defined over a set of univariate density functions, using convenient transformations of gaussian processes. inference is performed through approximate bayesian computation (abc), via a novel functional regression adjustment. the performance of the proposed method is illustrated via a simulation study and an analysis of rural high school exam performance in brazil.","","2014-10-30","","['g. s. rodrigues', 'david j. nott', 's. a. sisson']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1869",1410.857,"a partially linear framework for massive heterogeneous data","math.st stat.th","we consider a partially linear framework for modelling massive heterogeneous data. the major goal is to extract common features across all sub-populations while exploring heterogeneity of each sub-population. in particular, we propose an aggregation type estimator for the commonality parameter that possesses the (non-asymptotic) minimax optimal bound and asymptotic distribution as if there were no heterogeneity. this oracular result holds when the number of sub-populations does not grow too fast. a plug-in estimator for the heterogeneity parameter is further constructed, and shown to possess the asymptotic distribution as if the commonality information were available. we also test the heterogeneity among a large number of sub-populations. all the above results require to regularize each sub-estimation as though it had the entire sample size. our general theory applies to the divide-and-conquer approach that is often used to deal with massive homogeneous data. a technical by-product of this paper is the statistical inferences for the general kernel ridge regression. thorough numerical results are also provided to back up our theory.","","2014-10-30","2016-01-24","['tianqi zhao', 'guang cheng', 'han liu']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE
"1870",1411.0414,"nonparametric estimation of extremal dependence","stat.me","there is an increasing interest to understand the dependence structure of a random vector not only in the center of its distribution but also in the tails. extreme-value theory tackles the problem of modelling the joint tail of a multivariate distribution by modelling the marginal distributions and the dependence structure separately. for estimating dependence at high levels, the stable tail dependence function and the spectral measure are particularly convenient. these objects also lie at the basis of nonparametric techniques for modelling the dependence among extremes in the max-domain of attraction setting. in case of asymptotic independence, this setting is inadequate, and more refined tail dependence coefficients exist, serving, among others, to discriminate between asymptotic dependence and independence. throughout, the methods are illustrated on financial data.","","2014-11-03","","['anna kiriliouk', 'johan segers', 'michal warchol']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1871",1411.056,"multivariate response and parsimony for gaussian cluster-weighted models","stat.co stat.me stat.ml","a family of parsimonious gaussian cluster-weighted models is presented. this family concerns a multivariate extension to cluster-weighted modelling that can account for correlations between multivariate responses. parsimony is attained by constraining parts of an eigen-decomposition imposed on the component covariance matrices. a sufficient condition for identifiability is provided and an expectation-maximization algorithm is presented for parameter estimation. model performance is investigated on both synthetic and classical real data sets and compared with some popular approaches. finally, accounting for linear dependencies in the presence of a linear regression structure is shown to offer better performance, vis-\`{a}-vis clustering, over existing methodologies.","","2014-11-03","2016-02-26","['utkarsh j. dang', 'antonio punzo', 'paul d. mcnicholas', 'salvatore ingrassia', 'ryan p. browne']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,FALSE
"1872",1411.0606,"clustvarsel: a package implementing variable selection for model-based   clustering in r","stat.co","finite mixture modelling provides a framework for cluster analysis based on parsimonious gaussian mixture models. variable or feature selection is of particular importance in situations where only a subset of the available variables provide clustering information. this enables the selection of a more parsimonious model, yielding more efficient estimates, a clearer interpretation and, often, improved clustering partitions. this paper describes the r package clustvarsel which performs subset selection for model-based clustering. an improved version of the methodology of raftery and dean (2006) is implemented in the new version 2 of the package to find the (locally) optimal subset of variables with group/cluster information in a dataset. search over the solution space is performed using either a stepwise greedy search or a headlong algorithm. adjustments for speeding up these algorithms are discussed, as well as a parallel implementation of the stepwise search. usage of the package is presented through the discussion of several data examples.","","2014-11-03","","['luca scrucca', 'adrian e. raftery']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1873",1411.1243,"using twitter to predict football outcomes","stat.ml cs.cl cs.si","twitter has been proven to be a notable source for predictive modelling on various domains such as the stock market, the dissemination of diseases or sports outcomes. however, such a study has not been conducted in football (soccer) so far. the purpose of this research was to study whether data mined from twitter can be used for this purpose. we built a set of predictive models for the outcome of football games of the english premier league for a 3 month period based on tweets and we studied whether these models can overcome predictive models which use only historical data and simple football statistics. moreover, combined models are constructed using both twitter and historical data. the final results indicate that data mined from twitter can indeed be a useful source for predicting games in the premier league. the final twitter-based model performs significantly better than chance when measured by cohen's kappa and is comparable to the model that uses simple statistics and historical data. combining both models raises the performance higher than it was achieved by each individual model. thereby, this study provides evidence that twitter derived features can indeed provide useful information for the prediction of football (soccer) outcomes.","","2014-11-05","","['stylianos kampakis', 'andreas adamides']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1874",1411.1292,"monitoring count time series in r: aberration detection in public health   surveillance","stat.co","public health surveillance aims at lessening disease burden, e.g., in case of infectious diseases by timely recognizing emerging outbreaks. seen from a statistical perspective, this implies the use of appropriate methods for monitoring time series of aggregated case reports. this paper presents the tools for such automatic aberration detection offered by the r package surveillance. we introduce the functionality for the visualization, modelling and monitoring of surveillance time series. with respect to modelling we focus on univariate time series modelling based on generalized linear models (glms), multivariate glms, generalized additive models and generalized additive models for location, shape and scale. this ranges from illustrating implementational improvements and extensions of the well-known farrington algorithm, e.g, by spline-modelling or by treating it in a bayesian context. furthermore, we look at categorical time series and address overdispersion using beta-binomial or dirichlet-multinomial modelling. with respect to monitoring we consider detectors based on either a shewhart-like single timepoint comparison between the observed count and the predictive distribution or by likelihood-ratio based cumulative sum methods. finally, we illustrate how surveillance can support aberration detection in practice by integrating it into the monitoring workflow of a public health institution. altogether, the present article shows how well surveillance can support automatic aberration detection in a public health surveillance context.","10.18637/jss.v070.i10","2014-11-05","","['salmon maëlle', 'schumacher dirk', 'höhle michael']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1875",1411.2182,"a censored bayesian hierarchical model for precipitation","stat.ap","modelling of precipitation, including extremes, is important for hydrological and agricultural applications. traditionally, because of large sample properties for data over a large threshold value, generalised pareto (gp) distributions are often used for modelling extreme rainfall. it can be shown that under certain conditions the generalised hyperbolic (gh) distributions can approximate the power law decay of the gp distribution in the tails. given their flexible form, this raises the possibility that distributions from the gh family serve as a model for the entire rainfall distribution thus avoiding the need to select a threshold. in this paper, we use a flexible censored hierarchical model that leverages the gh distribution to accommodate data subject to heavy tails and an excessive number of zeros. the fitted model allows estimation of probabilities and return periods of the rainfall extremes, and it produces narrower credible intervals in the tails than the traditional gp method. the model not only fits the tails of the rainfall distribution, but fits the whole distribution very well. it also efficiently represents short-term dependencies in the data so it is suitable for evaluating duration over and below thresholds as well as duration of zero rainfall.","","2014-11-08","","['yang liu', 'philip kokic', 'k. shuvo bakar']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1876",1411.2624,"bayesian non-parametric inference for infectious disease data","stat.me stat.ap stat.co","we propose a framework for bayesian non-parametric estimation of the rate at which new infections occur assuming that the epidemic is partially observed. the developed methodology relies on modelling the rate at which new infections occur as a function which only depends on time. two different types of prior distributions are proposed namely using step-functions and b-splines. the methodology is illustrated using both simulated and real datasets and we show that certain aspects of the epidemic such as seasonality and super-spreading events are picked up without having to explicitly incorporate them into a parametric model.","","2014-11-10","2014-12-15","['edward s. knock', 'theodore kypraios']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE
"1877",1411.3921,"inference for trans-dimensional bayesian models with diffusive nested   sampling","stat.co astro-ph.im physics.data-an","many inference problems involve inferring the number $n$ of components in some region, along with their properties $\{\mathbf{x}_i\}_{i=1}^n$, from a dataset $\mathcal{d}$. a common statistical example is finite mixture modelling. in the bayesian framework, these problems are typically solved using one of the following two methods: i) by executing a monte carlo algorithm (such as nested sampling) once for each possible value of $n$, and calculating the marginal likelihood or evidence as a function of $n$; or ii) by doing a single run that allows the model dimension $n$ to change (such as markov chain monte carlo with birth/death moves), and obtaining the posterior for $n$ directly. in this paper we present a general approach to this problem that uses trans-dimensional mcmc embedded within a nested sampling algorithm, allowing us to explore the posterior distribution and calculate the marginal likelihood (summed over $n$) even if the problem contains a phase transition or other difficult features such as multimodality. we present two example problems, finding sinusoidal signals in noisy data, and finding and measuring galaxies in a noisy astronomical image. both of the examples demonstrate phase transitions in the relationship between the likelihood and the cumulative prior mass, highlighting the need for nested sampling.","","2014-11-14","2015-01-14","['brendon j. brewer']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE
"1878",1411.4715,"predictive inference for spatio-temporal precipitation data and its   extremes","stat.ap stat.me","modelling of precipitation and its extremes is important for urban and agriculture planning purposes. we present a method for producing spatial predictions and measures of uncertainty for spatio-temporal data that is heavy-tailed and subject to substaintial skewness which often arise in measurements of many environmental processes, and we apply the method to precipitation data in south-west western australia. a generalised hyperbolic bayesian hierarchical model is constructed for the intensity, frequency and duration of daily precipitation, including the extremes. unlike models based on extreme value theory, which only model maxima of finite-sized blocks or exceedances above a large threshold, the proposed model uses all the data available efficiently, and hence not only fits the extremes but also models the entire rainfall distribution. it captures spatial and temporal clustering, as well as spatially and temporally varying volatility and skewness. the model assumes that the regional precipitation is driven by a latent process characterised by geographical and climatological covariates. effects not fully described by the covariates are captured by spatial and temporal structure in the hierarchies. inference is provided by mcmc using a metropolis-hastings algorithm and spatial interpolation method, which provide a natural approach for estimating uncertainty. similarly both spatial and temporal predictions with uncertainty can be produced with the model.","","2014-11-17","","['yang liu', 'philip kokic']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1879",1411.4846,"modelling and analysis of time in-homogeneous recurrent event processes   in a heterogeneous population: a case study of hrts","stat.ap","in this work we present a method for the statistical analysis of continually monitored data arising in a recurrent diseases problem. the model enables individual level inference in the presence of time transience and population heterogeneity. this is achieved by applying bayesian hierarchical modelling, where marked point processes are used as descriptions of the individual data, with latent variables providing a means of modelling long range dependence and transience over time. in addition to providing a sound probabilistic formulation of a rather complex data set, the proposed method is also successful in prediction of future outcomes. computational difficulties arising from the analytic intractability of this bayesian model were solved by implementing the method into the bugs software and using standard computational facilities.   we illustrate this approach by an analysis of a data set on hormone replacement therapies (hrts). the data contain, in the form of diaries on bleeding patterns maintained by individual patients, detailed information on how they responded to different hrts. the proposed model is able to capture the essential features of these treatments as well as provide realistic individual level predictions on the future bleeding patterns.","","2014-11-18","","['madhuchhanda bhattacharjee', 'elja arjas']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1880",1411.5988,"clustering evolving data using kernel-based methods","cs.si cs.lg stat.ml","in this thesis, we propose several modelling strategies to tackle evolving data in different contexts. in the framework of static clustering, we start by introducing a soft kernel spectral clustering (sksc) algorithm, which can better deal with overlapping clusters with respect to kernel spectral clustering (ksc) and provides more interpretable outcomes. afterwards, a whole strategy based upon ksc for community detection of static networks is proposed, where the extraction of a high quality training sub-graph, the choice of the kernel function, the model selection and the applicability to large-scale data are key aspects. this paves the way for the development of a novel clustering algorithm for the analysis of evolving networks called kernel spectral clustering with memory effect (mksc), where the temporal smoothness between clustering results in successive time steps is incorporated at the level of the primal optimization problem, by properly modifying the ksc formulation. later on, an application of ksc to fault detection of an industrial machine is presented. here, a smart pre-processing of the data by means of a proper windowing operation is necessary to catch the ongoing degradation process affecting the machine. in this way, in a genuinely unsupervised manner, it is possible to raise an early warning when necessary, in an online fashion. finally, we propose a new algorithm called incremental kernel spectral clustering (iksc) for online learning of non-stationary data. this ambitious challenge is faced by taking advantage of the out-of-sample property of kernel spectral clustering (ksc) to adapt the initial model, in order to tackle merging, splitting or drifting of clusters across time. real-world applications considered in this thesis include image segmentation, time-series clustering, community detection of static and evolving networks.","","2014-11-20","","['rocco langone']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1881",1412.1344,"estimation of space deformation model for non-stationary random   functions","stat.me stat.ap","stationary random functions have been successfully applied in geostatistical applications for decades. in some instances, the assumption of a homogeneous spatial dependence structure across the entire domain of interest is unrealistic. a practical approach for modelling and estimating non-stationary spatial dependence structure is considered. this consists in transforming a non-stationary random function into a stationary and isotropic one via a bijective continuous deformation of the index space. so far, this approach has been successfully applied in the context of data from several independent realizations of a random function. in this work, we propose an approach for non-stationary geostatistical modelling using space deformation in the context of a single realization with possibly irregularly spaced data. the estimation method is based on a non-stationary variogram kernel estimator which serves as a dissimilarity measure between two locations in the geographical space. the proposed procedure combines aspects of kernel smoothing, weighted non-metric multi-dimensional scaling and thin-plate spline radial basis functions. on a simulated data, the method is able to retrieve the true deformation. performances are assessed on both synthetic and real datasets. it is shown in particular that our approach outperforms the stationary approach. beyond the prediction, the proposed method can also serve as a tool for exploratory analysis of the non-stationarity.","","2014-12-03","","['francky fouedjio', 'nicolas desassis', 'thomas romary']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE
"1882",1412.137,"nested variational compression in deep gaussian processes","stat.ml","deep gaussian processes provide a flexible approach to probabilistic modelling of data using either supervised or unsupervised learning. for tractable inference approximations to the marginal likelihood of the model must be made. the original approach to approximate inference in these models used variational compression to allow for approximate variational marginalization of the hidden variables leading to a lower bound on the marginal likelihood of the model [damianou and lawrence, 2013]. in this paper we extend this idea with a nested variational compression. the resulting lower bound on the likelihood can be easily parallelized or adapted for stochastic variational inference.","","2014-12-03","","['james hensman', 'neil d. lawrence']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1883",1412.323,"max-factor individual risk models with application to credit portfolios","stat.me q-fin.rm q-fin.st","individual risk models need to capture possible correlations as failing to do so typically results in an underestimation of extreme quantiles of the aggregate loss. such dependence modelling is particularly important for managing credit risk, for instance, where joint defaults are a major cause of concern. often, the dependence between the individual loss occurrence indicators is driven by a small number of unobservable factors. conditional loss probabilities are then expressed as monotone functions of linear combinations of these hidden factors. however, combining the factors in a linear way allows for some compensation between them. such diversification effects are not always desirable and this is why the present work proposes a new model replacing linear combinations with maxima. these max-factor models give more insight into which of the factors is dominant.","","2014-12-10","","['michel denuit', 'anna kiriliouk', 'johan segers']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1884",1412.4355,"designs for generalized linear models with random block effects via   information matrix approximations","stat.me","the selection of optimal designs for generalized linear mixed models is complicated by the fact that the fisher information matrix, on which most optimality criteria depend, is computationally expensive to evaluate. our focus is on the design of experiments for likelihood estimation of parameters in the conditional model. we provide two novel approximations that substantially reduce the computational cost of evaluating the information matrix by complete enumeration of response outcomes, or monte carlo approximations thereof: (i) an asymptotic approximation which is accurate when there is strong dependence between observations in the same block; (ii) an approximation via kriging interpolators. for logistic random intercept models, we show how interpolation can be especially effective for finding pseudo-bayesian designs that incorporate uncertainty in the values of the model parameters. the new results are used to provide the first evaluation of the efficiency, for estimating conditional models, of optimal designs from closed-form approximations to the information matrix derived from marginal models. it is found that correcting for the marginal attenuation of parameters in binary-response models yields much improved designs, typically with very high efficiencies. however, in some experiments exhibiting strong dependence, designs for marginal models may still be inefficient for conditional modelling. our asymptotic results provide some theoretical insights into why such inefficiencies occur.","10.1093/biomet/asv005","2014-12-14","","['timothy w. waite', 'david c. woods']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1885",1412.5236,"the supervised hierarchical dirichlet process","stat.ml cs.lg","we propose the supervised hierarchical dirichlet process (shdp), a nonparametric generative model for the joint distribution of a group of observations and a response variable directly associated with that whole group. we compare the shdp with another leading method for regression on grouped data, the supervised latent dirichlet allocation (slda) model. we evaluate our method on two real-world classification problems and two real-world regression problems. bayesian nonparametric regression models based on the dirichlet process, such as the dirichlet process-generalised linear models (dp-glm) have previously been explored; these models allow flexibility in modelling nonlinear relationships. however, until now, hierarchical dirichlet process (hdp) mixtures have not seen significant use in supervised problems with grouped data since a straightforward application of the hdp on the grouped data results in learnt clusters that are not predictive of the responses. the shdp solves this problem by allowing for clusters to be learnt jointly from the group structure and from the label assigned to each group.","10.1109/tpami.2014.2315802","2014-12-16","","['andrew m. dai', 'amos j. storkey']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE
"1886",1412.5351,"a comparative analysis of the uk and italian small businesses using   generalised extreme value models","stat.ap q-fin.gn","this paper presents a cross-country comparison of significant predictors of small business failure between italy and the uk. financial measures of profitability, leverage, coverage, liquidity, scale and non-financial information are explored, some commonalities and differences are highlighted. several models are considered, starting with the logis- tic regression which is a standard approach in credit risk modelling. some important improvements are investigated. generalised extreme value (gev) regression is applied to correct for the symmetric link function of the logistic regression. the assumption of non-linearity is relaxed through application of bgeva, non-parametric additive model based on the gev link function. two methods of handling missing values are compared: multiple imputation and weights of evidence (woe) transformation. the results suggest that the best predictive performance is obtained by bgeva, thus implying the necessity of taking into account the relative volume of defaults and non-linear patterns when modelling sme performance. woe for the majority of models considered show better prediction as compared to multiple imputation, suggesting that missing values could be informative and should not be assumed to be missing at random.","","2014-12-17","","['galina andreeva', 'raffaella calabrese', 'silvia angela osmetti']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1887",1412.7334,"a hidden markov approach to disability insurance","stat.ap","point and interval estimation of future disability inception and recovery rates are predominantly carried out by combining generalized linear models (glm) with time series forecasting techniques into a two-step method involving parameter estimation from historical data and subsequent calibration of a time series model. this approach may in fact lead to both conceptual and numerical problems since any time trend components of the model are incoherently treated as both model parameters and realizations of a stochastic process. we suggest that this general two-step approach can be improved in the following way: first, we assume a stochastic process form for the time trend component. the corresponding transition densities are then incorporated into the likelihood, and the model parameters are estimated using the expectation-maximization algorithm. we illustrate the modelling procedure by fitting the model to swedish disability claims data.","","2014-12-23","","['boualem djehiche', 'björn löfdahl']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE
"1888",1501.02157,"quantile regression for longitudinal data: unobserved heterogeneity and   informative missingness","stat.me","linear quantile regression models aim at providing a detailed and robust picture of the (conditional) response distribution as function of a set of observed covariates. longitudinal data represent an interesting field of application of such models; due to their peculiar features, they represent a substantial challenge, in that the standard, cross-sectional, model representation needs to be extended for dealing with such kind of data. in fact, repeated observations from the same statistical unit poses a problem of dependence; in a conditional perspective, this dependence could be ascribed to sources of unobserved, individual-specific, heterogeneity. along these lines, quantile regression models have recently been extended to the analysis of longitudinal, continuous, responses, by modelling dependence via time-constant or time-varying random effects. in this manuscript, we introduce a general quantile regression model for longitudinal, continuous, responses where time-varying and time-constant random parameters are jointly taken into account. a further feature of longitudinal designs is the presence of partially incomplete sequences, due to some individuals leaving the study before its designed end. the missing data process may produce a selection of units which can be informative with respect to the parameters of the longitudinal data model. to deal with the case of irretrievable drop-out, we introduce a pattern mixture version of the linear quantile hidden markov model, where we account for time-varying heterogeneity and for changes in the fixed effect vector due to differential propensities to stay in the study. the proposed models are illustrated using a well known benchmark dataset on longitudinal dynamics of cd4 cells and by means of a large scale simulation study, entailing different quantiles and both complete and partially complete (ie subject to drop-out) individual sequences.","","2015-01-09","2015-07-29","['maria francesca marino', 'nikos tzavidis', ""marco alfo'""]",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
"1889",1501.02248,"a particle multi-target tracker for superpositional measurements using   labeled random finite sets","stat.me stat.co","in this paper we present a general solution for multi-target tracking with superpositional measurements. measurements that are functions of the sum of the contributions of the targets present in the surveillance area are called superpositional measurements. we base our modelling on labeled random finite set (rfs) in order to jointly estimate the number of targets and their trajectories. this modelling leads to a labeled version of mahler's multi-target bayes filter. however, a straightforward implementation of this tracker using sequential monte carlo (smc) methods is not feasible due to the difficulties of sampling in high dimensional spaces. we propose an efficient multi-target sampling strategy based on superpositional approximate cphd (sa-cphd) filter and the recently introduced labeled multi-bernoulli (lmb) and vo-vo densities. the applicability of the proposed approach is verified through simulation in a challenging radar application with closely spaced targets and low signal-to-noise ratio.","","2014-12-19","2015-06-02","['francesco papi', 'du yong kim']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE
"1890",1501.03731,"robust linear spectral unmixing using anomaly detection","stat.me","this paper presents a bayesian algorithm for linear spectral unmixing of hyperspectral images that accounts for anomalies present in the data. the model proposed assumes that the pixel reflectances are linear mixtures of unknown endmembers, corrupted by an additional nonlinear term modelling anomalies and additive gaussian noise. a markov random field is used for anomaly detection based on the spatial and spectral structures of the anomalies. this allows outliers to be identified in particular regions and wavelengths of the data cube. a bayesian algorithm is proposed to estimate the parameters involved in the model yielding a joint linear unmixing and anomaly detection algorithm. simulations conducted with synthetic and real hyperspectral images demonstrate the accuracy of the proposed unmixing and outlier detection strategy for the analysis of hyperspectral images.","","2015-01-15","2015-10-03","['yoann altmann', 'steve mclaughlin', 'alfred hero']",0,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE
