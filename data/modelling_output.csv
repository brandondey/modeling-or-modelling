id,title,categories,abstract,doi,created,updated,authors
0705.0304,mod\'elisations prospectives de l'occupation du sol. le cas d'une   montagne m\'editerran\'eenne,stat.ap stat.me,"the authors apply three methods of prospective modelling to high resolution georeferenced land cover data in a mediterranean mountain area: gis approach, non linear parametric model and neuronal network. land cover prediction to the latest known date is used to validate the models. in the frame of spatial-temporal dynamics in open systems results are encouraging and comparable. correct prediction scores are about 73 %. the results analysis focuses on geographic location, land cover categories and parametric distance to reality of the residues. crossing the three models show the high degree of convergence and a relative similitude of the results obtained by the two statistic approaches compared to the gis supervised model. steps under work are the application of the models to other test areas and the identification of respective advantages to develop an integrated model.",,2007-05-02,2007-05-09,"['martin paegelow', 'nathalie villa', 'laurence cornez', 'frédéric ferraty', 'louis ferré', 'pascal sarda']"
0705.2605,sample eigenvalue based detection of high dimensional signals in white   noise using relatively few samples,math.st stat.th,"we present a mathematically justifiable, computationally simple, sample eigenvalue based procedure for estimating the number of high-dimensional signals in white noise using relatively few samples. the main motivation for considering a sample eigenvalue based scheme is the computational simplicity and the robustness to eigenvector modelling errors which are can adversely impact the performance of estimators that exploit information in the sample eigenvectors.   there is, however, a price we pay by discarding the information in the sample eigenvectors; we highlight a fundamental asymptotic limit of sample eigenvalue based detection of weak/closely spaced high-dimensional signals from a limited sample size. this motivates our heuristic definition of the effective number of identifiable signals which is equal to the number of ""signal"" eigenvalues of the population covariance matrix which exceed the noise variance by a factor strictly greater than 1+sqrt(dimensionality of the system/sample size). the fundamental asymptotic limit brings into sharp focus why, when there are too few samples available so that the effective number of signals is less than the actual number of signals, underestimation of the model order is unavoidable (in an asymptotic sense) when using any sample eigenvalue based detection scheme, including the one proposed herein. the analysis reveals why adding more sensors can only exacerbate the situation. numerical simulations are used to demonstrate that the proposed estimator consistently estimates the true number of signals in the dimension fixed, large sample size limit and the effective number of identifiable signals in the large dimension, large sample size limit.",,2007-05-17,,"['n. raj rao', 'alan edelman']"
0706.0787,construction of bayesian deformable models via stochastic approximation   algorithm: a convergence study,stat.co,"the problem of the definition and the estimation of generative models based on deformable templates from raw data is of particular importance for modelling non aligned data affected by various types of geometrical variability. this is especially true in shape modelling in the computer vision community or in probabilistic atlas building for computational anatomy (ca). a first coherent statistical framework modelling the geometrical variability as hidden variables has been given by allassonni\`ere, amit and trouv\'e (jrss 2006). setting the problem in a bayesian context they proved the consistency of the map estimator and provided a simple iterative deterministic algorithm with an em flavour leading to some reasonable approximations of the map estimator under low noise conditions. in this paper we present a stochastic algorithm for approximating the map estimator in the spirit of the saem algorithm. we prove its convergence to a critical point of the observed likelihood with an illustration on images of handwritten digits.",,2007-06-06,2009-01-16,"['stéphanie allassonnière', 'estelle kuhn', 'alain trouvé']"
0707.3794,binary models for marginal independence,math.st stat.th,"log-linear models are a classical tool for the analysis of contingency tables. in particular, the subclass of graphical log-linear models provides a general framework for modelling conditional independences. however, with the exception of special structures, marginal independence hypotheses cannot be accommodated by these traditional models. focusing on binary variables, we present a model class that provides a framework for modelling marginal independences in contingency tables. the approach taken is graphical and draws on analogies to multivariate gaussian models for marginal independence. for the graphical model representation we use bi-directed graphs, which are in the tradition of path diagrams. we show how the models can be parameterized in a simple fashion, and how maximum likelihood estimation can be performed using a version of the iterated conditional fitting algorithm. finally we consider combining these models with symmetry restrictions.",10.1111/j.1467-9868.2007.00636.x,2007-07-25,,"['mathias drton', 'thomas s. richardson']"
0707.4242,importance tempering,stat.co stat.ap,"simulated tempering (st) is an established markov chain monte carlo (mcmc) method for sampling from a multimodal density $\pi(\theta)$. typically, st involves introducing an auxiliary variable $k$ taking values in a finite subset of $[0,1]$ and indexing a set of tempered distributions, say $\pi_k(\theta) \propto \pi(\theta)^k$. in this case, small values of $k$ encourage better mixing, but samples from $\pi$ are only obtained when the joint chain for $(\theta,k)$ reaches $k=1$. however, the entire chain can be used to estimate expectations under $\pi$ of functions of interest, provided that importance sampling (is) weights are calculated. unfortunately this method, which we call importance tempering (it), can disappoint. this is partly because the most immediately obvious implementation is na\""ive and can lead to high variance estimators. we derive a new optimal method for combining multiple is estimators and prove that the resulting estimator has a highly desirable property related to the notion of effective sample size. we briefly report on the success of the optimal combination in two modelling scenarios requiring reversible-jump mcmc, where the na\""ive approach fails.",,2007-07-28,2008-11-03,"['robert b. gramacy', 'richard j. samworth', 'ruth king']"
0708.1071,statistical thinking: from tukey to vardi and beyond,math.st stat.th,"data miners (minors?) and neural networkers tend to eschew modelling, misled perhaps by misinterpretation of strongly expressed views of john tukey. i discuss vardi's views of these issues as well as other aspects of vardi's work in emision tomography and in sampling bias.",10.1214/074921707000000210,2007-08-08,,['larry shepp']
0708.1321,graphical methods for efficient likelihood inference in gaussian   covariance models,math.st stat.th,"in graphical modelling, a bi-directed graph encodes marginal independences among random variables that are identified with the vertices of the graph. we show how to transform a bi-directed graph into a maximal ancestral graph that (i) represents the same independence structure as the original bi-directed graph, and (ii) minimizes the number of arrowheads among all ancestral graphs satisfying (i). here the number of arrowheads of an ancestral graph is the number of directed edges plus twice the number of bi-directed edges. in gaussian models, this construction can be used for more efficient iterative maximization of the likelihood function and to determine when maximum likelihood estimates are equal to empirical counterparts.",,2007-08-09,2008-03-25,"['mathias drton', 'thomas s. richardson']"
0708.1566,markov chain modelling for reliability estimation of engineering systems   at different scales - some considerations,stat.ap stat.me,"the concepts of probability, statistics and stochastic theory are being successfully used in structural engineering. markov chain modelling is a simple stochastic process model that has found its application in both describing stochastic evolution of system and in system reliability estimation. the recent developments in markov chain monte carlo and the possible integration of bayesian theory within markov chain theory have enhanced its application possibilities. however, the application possibility can be furthered to range over wider scales of application (perhaps from nano- to macro-) by considering the developments in physics (in particular quantum physics). this paper tries to present the results of quantum physics that would help in interpretation of transition probability matrix. however, care has to be taken in the choice of densities in computing the transition probability matrix. the paper is based on available literature, and the aim is only to make an attempt to show how markov chain can be used to model systems at various scales.",,2007-08-11,,['k. balaji rao']
0708.1965,tail asymptotics and estimation for elliptical distributions,math.pr math.st stat.th,"let (x,y) be a bivariate elliptical random vector with associated random radius in the gumbel max-domain of attraction. in this paper we obtain a second order asymptotic expansion of the joint survival probability p(x > x, y> y) for x,y large. further, based on the asymptotic bounds we discuss some aspects of the statistical modelling of joint survival probabilities and the survival conditional excess probability.",,2007-08-14,2008-05-15,['enkelejd hashorva']
0709.0447,local mixture models of exponential families,math.st stat.th,"exponential families are the workhorses of parametric modelling theory. one reason for their popularity is their associated inference theory, which is very clean, both from a theoretical and a computational point of view. one way in which this set of tools can be enriched in a natural and interpretable way is through mixing. this paper develops and applies the idea of local mixture modelling to exponential families. it shows that the highly interpretable and flexible models which result have enough structure to retain the attractive inferential properties of exponential families. in particular, results on identification, parameter orthogonality and log-concavity of the likelihood are proved.",10.3150/07-bej6170,2007-09-04,,"['karim anaya-izquierdo', 'paul marriott']"
0709.2936,bayesian classification and regression with high dimensional features,stat.ml,"this thesis responds to the challenges of using a large number, such as thousands, of features in regression and classification problems.   there are two situations where such high dimensional features arise. one is when high dimensional measurements are available, for example, gene expression data produced by microarray techniques. for computational or other reasons, people may select only a small subset of features when modelling such data, by looking at how relevant the features are to predicting the response, based on some measure such as correlation with the response in the training data. although it is used very commonly, this procedure will make the response appear more predictable than it actually is. in chapter 2, we propose a bayesian method to avoid this selection bias, with application to naive bayes models and mixture models.   high dimensional features also arise when we consider high-order interactions. the number of parameters will increase exponentially with the order considered. in chapter 3, we propose a method for compressing a group of parameters into a single one, by exploiting the fact that many predictor variables derived from high-order interactions have the same values for all the training cases. the number of compressed parameters may have converged before considering the highest possible order. we apply this compression method to logistic sequence prediction models and logistic classification models.   we use both simulated data and real data to test our methods in both chapters.",,2007-09-18,,['longhai li']
0709.3427,mutual information for the selection of relevant variables in   spectrometric nonlinear modelling,cs.lg cs.ne stat.ap,"data from spectrophotometers form vectors of a large number of exploitable variables. building quantitative models using these variables most often requires using a smaller set of variables than the initial one. indeed, a too large number of input variables to a model results in a too large number of parameters, leading to overfitting and poor generalization abilities. in this paper, we suggest the use of the mutual information measure to select variables from the initial set. the mutual information measures the information content in input variables with respect to the model output, without making any assumption on the model that will be used; it is thus suitable for nonlinear modelling. in addition, it leads to the selection of variables among the initial set, and not to linear or nonlinear combinations of them. without decreasing the model performances compared to other variable projection methods, it allows therefore a greater interpretability of the results.",10.1016/j.chemolab.2005.06.010,2007-09-21,,"['fabrice rossi', 'amaury lendasse', 'damien françois', 'vincent wertz', 'michel verleysen']"
0710.3742,bayesian online changepoint detection,stat.ml,"changepoints are abrupt variations in the generative parameters of a data sequence. online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. while frequentist methods have yielded online filtering and prediction techniques, most bayesian papers have focused on the retrospective segmentation problem. here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. we compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. our implementation is highly modular so that the algorithm may be applied to a variety of types of data. we illustrate this modularity by demonstrating the algorithm on three different real-world data sets.",,2007-10-19,,"['ryan prescott adams', 'david j. c. mackay']"
0801.1966,symmetry of models versus models of symmetry,math.st stat.th,"a model for a subject's beliefs about a phenomenon may exhibit symmetry, in the sense that it is invariant under certain transformations. on the other hand, such a belief model may be intended to represent that the subject believes or knows that the phenomenon under study exhibits symmetry. we defend the view that these are fundamentally different things, even though the difference cannot be captured by bayesian belief models. in fact, the failure to distinguish between both situations leads to laplace's so-called principle of insufficient reason, which has been criticised extensively in the literature.   we show that there are belief models (imprecise probability models, coherent lower previsions) that generalise and include the bayesian belief models, but where this fundamental difference can be captured. this leads to two notions of symmetry for such belief models: weak invariance (representing symmetry of beliefs) and strong invariance (modelling beliefs of symmetry). we discuss various mathematical as well as more philosophical aspects of these notions. we also discuss a few examples to show the relevance of our findings both to probabilistic modelling and to statistical inference, and to the notion of exchangeability in particular.",,2008-01-13,,"['gert de cooman', 'enrique miranda']"
0801.2231,spatial modelling for mixed-state observations,math.st stat.th,"in several application fields like daily pluviometry data modelling, or motion analysis from image sequences, observations contain two components of different nature. a first part is made with discrete values accounting for some symbolic information and a second part records a continuous (real-valued) measurement. we call such type of observations ""mixed-state observations"". this paper introduces spatial models suited for the analysis of these kinds of data. we consider multi-parameter auto-models whose local conditional distributions belong to a mixed state exponential family. specific examples with exponential distributions are detailed, and we present some experimental results for modelling motion measurements from video sequences.",10.1214/08-ejs173,2008-01-15,2008-03-27,"['cécile hardouin', 'jian-feng yao']"
0802.0214,multivariate stochastic volatility with bayesian dynamic linear models,q-fin.st stat.ap stat.me,"this paper develops a bayesian procedure for estimation and forecasting of the volatility of multivariate time series. the foundation of this work is the matrix-variate dynamic linear model, for the volatility of which we adopt a multiplicative stochastic evolution, using wishart and singular multivariate beta distributions. a diagonal matrix of discount factors is employed in order to discount the variances element by element and therefore allowing a flexible and pragmatic variance modelling approach. diagnostic tests and sequential model monitoring are discussed in some detail. the proposed estimation theory is applied to a four-dimensional time series, comprising spot prices of aluminium, copper, lead and zinc of the london metal exchange. the empirical findings suggest that the proposed bayesian procedure can be effectively applied to financial data, overcoming many of the disadvantages of existing volatility models.",10.1016/j.jspi.2007.03.057,2008-02-01,,['k. triantafyllopoulos']
0802.1521,stochastic algorithm for parameter estimation for dense deformable   template mixture model,stat.co math.st stat.th,"estimating probabilistic deformable template models is a new approach in the fields of computer vision and probabilistic atlases in computational anatomy. a first coherent statistical framework modelling the variability as a hidden random variable has been given by allassonni\`ere, amit and trouv\'e in [1] in simple and mixture of deformable template models. a consistent stochastic algorithm has been introduced in [2] to face the problem encountered in [1] for the convergence of the estimation algorithm for the one component model in the presence of noise. we propose here to go on in this direction of using some ""saem-like"" algorithm to approximate the map estimator in the general bayesian setting of mixture of deformable template model. we also prove the convergence of this algorithm toward a critical point of the penalised likelihood of the observations and illustrate this with handwritten digit images.",,2008-02-11,2009-01-16,"['stéphanie allassonnière', 'estelle kuhn']"
0803.0860,l\'{e}vy-based growth models,math.st stat.th,"in the present paper, we give a condensed review, for the nonspecialist reader, of a new modelling framework for spatio-temporal processes, based on l\'{e}vy theory. we show the potential of the approach in stochastic geometry and spatial statistics by studying l\'{e}vy-based growth modelling of planar objects. the growth models considered are spatio-temporal stochastic processes on the circle. as a by product, flexible new models for space--time covariance functions on the circle are provided. an application of the l\'{e}vy-based growth models to tumour growth is discussed.",10.3150/07-bej6130,2008-03-06,,"['kristjana ýr jónsdóttir', 'jürgen schmiegel', 'eva b. vedel jensen']"
0804.4738,structural shrinkage of nonparametric spectral estimators for   multivariate time series,math.st stat.th,"in this paper we investigate the performance of periodogram based estimators of the spectral density matrix of possibly high-dimensional time series. we suggest and study shrinkage as a remedy against numerical instabilities due to deteriorating condition numbers of (kernel) smoothed periodogram matrices. moreover, shrinking the empirical eigenvalues in the frequency domain towards one another also improves at the same time the mean squared error (mse) of these widely used nonparametric spectral estimators. compared to some existing time domain approaches, restricted to i.i.d. data, in the frequency domain it is necessary to take the size of the smoothing span as ""effective or local sample size"" into account. while b\""{o}hm and von sachs (2007) proposes a multiple of the identity matrix as optimal shrinkage target in the absence of knowledge about the multidimensional structure of the data, here we consider ""structural"" shrinkage. we assume that the spectral structure of the data is induced by underlying factors. however, in contrast to actual factor modelling suffering from the need to choose the number of factors, we suggest a model-free approach. our final estimator is the asymptotically mse-optimal linear combination of the smoothed periodogram and the parametric estimator based on an underfitting (and hence deliberately misspecified) factor model. we complete our theoretical considerations by some extensive simulation studies. in the situation of data generated from a higher-order factor model, we compare all four types of involved estimators (including the one of b\""{o}hm and von sachs (2007)).",10.1214/08-ejs236,2008-04-30,2008-08-13,"['hilmar böhm', 'rainer von sachs']"
0805.2096,garch modelling in continuous time for irregularly spaced time series   data,q-fin.st math.st stat.th,"the discrete-time garch methodology which has had such a profound influence on the modelling of heteroscedasticity in time series is intuitively well motivated in capturing many `stylized facts' concerning financial series, and is now almost routinely used in a wide range of situations, often including some where the data are not observed at equally spaced intervals of time. however, such data is more appropriately analyzed with a continuous-time model which preserves the essential features of the successful garch paradigm. one possible such extension is the diffusion limit of nelson, but this is problematic in that the discrete-time garch model and its continuous-time diffusion limit are not statistically equivalent. as an alternative, kl\""{u}ppelberg et al. recently introduced a continuous-time version of the garch (the `cogarch' process) which is constructed directly from a background driving l\'{e}vy process. the present paper shows how to fit this model to irregularly spaced time series data using discrete-time garch methodology, by approximating the cogarch with an embedded sequence of discrete-time garch series which converges to the continuous-time model in a strong sense (in probability, in the skorokhod metric), as the discrete approximating grid grows finer. this property is also especially useful in certain other applications, such as options pricing. the way is then open to using, for the cogarch, similar statistical techniques to those already worked out for garch models and to illustrate this, an empirical investigation using stock index data is carried out.",10.3150/07-bej6189,2008-05-14,,"['ross a. maller', 'gernot müller', 'alex szimayer']"
0808.3466,"on sequential monte carlo, partial rejection control and approximate   bayesian computation",stat.co math.st stat.th,"we present a sequential monte carlo sampler variant of the partial rejection control algorithm, and show that this variant can be considered as a sequential monte carlo sampler with a modified mutation kernel. we prove that the new sampler can reduce the variance of the incremental importance weights when compared with standard sequential monte carlo samplers. we provide a study of theoretical properties of the new algorithm, and make connections with some existing algorithms. finally, the sampler is adapted for application under the challenging ""likelihood free,"" approximate bayesian computation modelling framework, where we demonstrate superior performance over existing likelihood-free samplers.",,2008-08-26,2009-11-11,"['g. w. peters', 'y. fan', 's. a. sisson']"
0808.4031,hybrid data regression modelling in measurement,stat.ap stat.me,"measurement involves the determination of quantitative estimates of physical quantities from experiment, along with estimates of their associated uncertainties. herewith an experimental system model is the key to extracting information from the experimental data. the measurement information obtained depends directly on the quality of the model. with this concern novel regression modelling techniques have been fashioned by data integration from computer-simulation and physical designed experiments. these techniques have allowed attaining the advanced level of model completeness, parsimony, and precision via approximation of the exact unknown model by mathematical product of available theoretical and appropriate empirical functions. the purpose of this approximation is to represent adequately the true model on the considered region of factor space with all advantages of theoretical modelling. this allows a further focus on the measurement science of issue. pneumatic gauge hybrid data candidate model building, solving and validation reviled that such adequate models permit to attain minimum discrepancy from empirical evidence.",,2008-08-29,,['vladimir b. bokov']
0808.4111,relative entropy and statistics,cs.it math.it math.st stat.th,"formalising the confrontation of opinions (models) to observations (data) is the task of inferential statistics. information theory provides us with a basic functional, the relative entropy (or kullback-leibler divergence), an asymmetrical measure of dissimilarity between the empirical and the theoretical distributions. the formal properties of the relative entropy turn out to be able to capture every aspect of inferential statistics, as illustrated here, for simplicity, on dices (= i.i.d. process with finitely many outcomes): refutability (strict or probabilistic): the asymmetry data / models; small deviations: rejecting a single hypothesis; competition between hypotheses and model selection; maximum likelihood: model inference and its limits; maximum entropy: reconstructing partially observed data; em-algorithm; flow data and gravity modelling; determining the order of a markov chain.",,2008-08-29,2010-04-03,['françois bavaud']
0809.2932,stability selection,stat.me,"estimation of structure, such as in variable selection, graphical modelling or cluster analysis is notoriously difficult, especially for high-dimensional data. we introduce stability selection. it is based on subsampling in combination with (high-dimensional) selection algorithms. as such, the method is extremely general and has a very wide range of applicability. stability selection provides finite sample control for some error rates of false discoveries and hence a transparent principle to choose a proper amount of regularisation for structure estimation. variable selection and structure estimation improve markedly for a range of selection methods if stability selection is applied. we prove for randomised lasso that stability selection will be variable selection consistent even if the necessary conditions needed for consistency of the original lasso method are violated. we demonstrate stability selection for variable selection and gaussian graphical modelling, using real and simulated data.",,2008-09-17,2009-05-16,"['nicolai meinshausen', 'peter buehlmann']"
0812.2548,copulas for markovian dependence,math.pr math.st stat.th,"copulas have been popular to model dependence for multivariate distributions, but have not been used much in modelling temporal dependence of univariate time series. this paper demonstrates some difficulties with using copulas even for markov processes: some tractable copulas such as mixtures between copulas of complete co- and countermonotonicity and independence (fr\'{e}chet copulas) are shown to imply quite a restricted type of markov process and archimedean copulas are shown to be incompatible with markov chains. we also investigate markov chains that are spreadable or, equivalently, conditionally i.i.d.",10.3150/09-bej214,2008-12-13,2010-10-08,['andreas n. lagerås']
0901.1518,second-order refined peaks-over-threshold modelling for heavy-tailed   distributions,math.st stat.th,"modelling excesses over a high threshold using the pareto or generalized pareto distribution (pd/gpd) is the most popular approach in extreme value statistics. this method typically requires high thresholds in order for the (g)pd to fit well and in such a case applies only to a small upper fraction of the data. the extension of the (g)pd proposed in this paper is able to describe the excess distribution for lower thresholds in case of heavy tailed distributions. this yields a statistical model that can be fitted to a larger portion of the data. moreover, estimates of tail parameters display stability for a larger range of thresholds. our findings are supported by asymptotic results, simulations and a case study.",,2009-01-12,,"['jan beirlant', 'elisabeth joossens', 'johan segers']"
0902.1598,p-order rounded integer-valued autoregressive (rinar(p)) process,stat.me math.st stat.th,"an extension of the rinar(1) process for modelling discrete-time dependent counting processes is considered. the model rinar(p) investigated here is a direct and natural extension of the real ar(p) model. compared to classical inar(p) models based on the thinning operator, the new models have several advantages: simple innovation structure ; autoregressive coefficients with arbitrary signs ; possible negative values for time series ; possible negative values for the autocorrelation function. the conditions for the stationarity and ergodicity, of the rinar(p) model, are given. for parameter estimation, we consider the least squares estimator and we prove its consistency under suitable identifiability condition. simulation experiments as well as analysis of real data sets are carried out to assess the performance of the model.",,2009-02-10,,['m. kachour']
0903.2651,perfect simulation of spatial point processes using dominated coupling   from the past with application to a multiscale area-interaction point process,stat.me stat.ap stat.co,"we consider perfect simulation algorithms for locally stable point processes based on dominated coupling from the past. a version of the algorithm is developed which is feasible for processes which are neither purely attractive nor purely repulsive. such processes include multiscale area-interaction processes, which are capable of modelling point patterns whose clustering structure varies across scales. we prove correctness of the algorithm and existence of these processes. an application to the redwood seedlings data is discussed.",,2009-03-15,,"['graeme k. ambler', 'bernard w. silverman']"
0904.0317,"integrating remote sensing, gis and prediction models to monitor the   deforestation and erosion in peten reserve, guatemala",stat.ap,"this contribution provides a strategy for studying and modelling the deforestation and soil deterioration in the natural forest reserve of peten, guatemala, using a poor spatial database. a multispectral image processing of spot and tm landsat data permits to understand the behaviour of the past land cover dynamics; a multi-temporal analysis of normalized difference vegetation and hydric stress index, most informative rgb (according to statistical criteria) and principal components, points out the importance and the direction of environmental impacts. we gain from the remote sensing images new environmental criteria (distance from roads, oil pipe-line, dem, etc.) which influence the spatial allocation of predicted land cover probabilities. we are comparing the results of different prospective approaches (markov chains, multi criteria evaluation and cellular automata; neural networks) analysing the residues for improving the final model of future deforestation risk.",,2009-04-02,,"['roberto bruno', 'marco follador', 'martin paegelow', 'fernanda renno', 'nathalie villa']"
0905.2441,on the utility of graphics cards to perform massively parallel   simulation of advanced monte carlo methods,stat.co,"we present a case-study on the utility of graphics cards to perform massively parallel simulation of advanced monte carlo methods. graphics cards, containing multiple graphics processing units (gpus), are self-contained parallel computational devices that can be housed in conventional desktop and laptop computers. for certain classes of monte carlo algorithms they offer massively parallel simulation, with the added advantage over conventional distributed multi-core processors that they are cheap, easily accessible, easy to maintain, easy to code, dedicated local devices with low power consumption. on a canonical set of stochastic simulation examples including population-based markov chain monte carlo methods and sequential monte carlo methods, we find speedups from 35 to 500 fold over conventional single-threaded computer code. our findings suggest that gpus have the potential to facilitate the growth of statistical modelling into complex data rich domains through the availability of cheap and accessible many-core computation. we believe the speedup we observe should motivate wider use of parallelizable simulation methods and greater methodological attention to their design.",10.1198/jcgs.2010.10039,2009-05-14,2009-07-15,"['anthony lee', 'christopher yau', 'michael b. giles', 'arnaud doucet', 'christopher c. holmes']"
0906.2530,"observed universality of phase transitions in high-dimensional geometry,   with implications for modern data analysis and signal processing",math.st cs.it math.it physics.data-an stat.co stat.th,"we review connections between phase transitions in high-dimensional combinatorial geometry and phase transitions occurring in modern high-dimensional data analysis and signal processing. in data analysis, such transitions arise as abrupt breakdown of linear model selection, robust data fitting or compressed sensing reconstructions, when the complexity of the model or the number of outliers increases beyond a threshold. in combinatorial geometry these transitions appear as abrupt changes in the properties of face counts of convex polytopes when the dimensions are varied. the thresholds in these very different problems appear in the same critical locations after appropriate calibration of variables.   these thresholds are important in each subject area: for linear modelling, they place hard limits on the degree to which the now-ubiquitous high-throughput data analysis can be successful; for robustness, they place hard limits on the degree to which standard robust fitting methods can tolerate outliers before breaking down; for compressed sensing, they define the sharp boundary of the undersampling/sparsity tradeoff in undersampling theorems.   existing derivations of phase transitions in combinatorial geometry assume the underlying matrices have independent and identically distributed (iid) gaussian elements. in applications, however, it often seems that gaussianity is not required. we conducted an extensive computational experiment and formal inferential analysis to test the hypothesis that these phase transitions are {\it universal} across a range of underlying matrix ensembles. the experimental results are consistent with an asymptotic large-$n$ universality across matrix ensembles; finite-sample universality can be rejected.",10.1098/rsta.2009.0152,2009-06-14,,"['david l. donoho', 'jared tanner']"
0907.3220,inter genre similarity modelling for automatic music genre   classification,cs.sd cs.ai stat.ml,"music genre classification is an essential tool for music information retrieval systems and it has been finding critical applications in various media platforms. two important problems of the automatic music genre classification are feature extraction and classifier design. this paper investigates inter-genre similarity modelling (igs) to improve the performance of automatic music genre classification. inter-genre similarity information is extracted over the mis-classified feature population. once the inter-genre similarity is modelled, elimination of the inter-genre similarity reduces the inter-genre confusion and improves the identification rates. inter-genre similarity modelling is further improved with iterative igs modelling(iigs) and score modelling for igs elimination(smigs). experimental results with promising classification improvements are provided.",,2009-07-18,,"['ulas bagci', 'engin erzin']"
0907.5151,locally stationary long memory estimation,math.st stat.th,"there exists a wide literature on modelling strongly dependent time series using a longmemory parameter d, including more recent work on semiparametric wavelet estimation. as a generalization of these latter approaches, in this work we allow the long-memory parameter d to be varying over time. we embed our approach into the framework of locally stationary processes. we show weak consistency and a central limit theorem for our log-regression wavelet estimator of the time-dependent d in a gaussian context. both simulations and a real data example complete our work on providing a fairly general approach.",,2009-07-29,2010-05-31,"['françois roueff', 'rainer von sachs']"
0908.0050,online learning for matrix factorization and sparse coding,stat.ml cs.lg math.oc,"sparse coding--that is, modelling data vectors as sparse linear combinations of basis elements--is widely used in machine learning, neuroscience, signal processing, and statistics. this paper focuses on the large-scale matrix factorization problem that consists of learning the basis set, adapting it to specific data. variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. in this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large datasets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. a proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large datasets.",,2009-08-01,2010-02-11,"['julien mairal', 'francis bach', 'jean ponce', 'guillermo sapiro']"
0908.2359,online em algorithm for hidden markov models,stat.co stat.ml,"online (also called ""recursive"" or ""adaptive"") estimation of fixed model parameters in hidden markov models is a topic of much interest in times series modelling. in this work, we propose an online parameter estimation algorithm that combines two key ideas. the first one, which is deeply rooted in the expectation-maximization (em) methodology consists in reparameterizing the problem using complete-data sufficient statistics. the second ingredient consists in exploiting a purely recursive form of smoothing in hmms based on an auxiliary recursion. although the proposed online em algorithm resembles a classical stochastic approximation (or robbins-monro) algorithm, it is sufficiently different to resist conventional analysis of convergence. we thus provide limited results which identify the potential limiting points of the recursion as well as the large-sample behavior of the quantities involved in the algorithm. the performance of the proposed algorithm is numerically evaluated through simulations in the case of a noisily observed markov chain. in this case, the algorithm reaches estimation results that are comparable to that of the maximum likelihood estimator for large sample sizes.",,2009-08-17,2011-02-15,['olivier cappé']
0909.5299,parameter estimation for multivariate diffusion systems,stat.me,"diffusion processes are widely used for modelling real-world phenomena. except for select cases however, analytical expressions do not exist for a diffusion process' transitional probabilities. it is proposed that the cumulant truncation procedure can be applied to predict the evolution of the cumulants of the system. these predictions may be subsequently used within the saddlepoint procedure to approximate the transitional probabilities. an approximation to the likelihood of the diffusion system is then easily derived. the method is applicable for a wide-range of diffusion systems - including multivariate, irreducible diffusion systems that existing estimation schemes struggle with. not only is the accuracy of the saddlepoint comparable with the hermite expansion - a popular approximation to a diffusion system's transitional density - it also appears to be less susceptible to increasing lags between successive samplings of the diffusion process. furthermore, the saddlepoint is more stable in regions of the parameter space that are far from the maximum likelihood estimates. hence, the saddlepoint method can be naturally incorporated within a markov chain monte carlo (mcmc) routine in order to provide reliable estimates and credibility intervals of the diffusion model's parameters. the method is applied to fit the heston model to daily observations of the s&p 500 and vix indices from december 2009 to november 2010.",,2009-09-29,2012-09-13,['melvin m. varughese']
0910.4443,stochastic epidemic models: a survey,math.pr math.st stat.ap stat.me stat.th,"this paper is a survey paper on stochastic epidemic models. a simple stochastic epidemic model is defined and exact and asymptotic model properties (relying on a large community) are presented. the purpose of modelling is illustrated by studying effects of vaccination and also in terms of inference procedures for important parameters, such as the basic reproduction number and the critical vaccination coverage. several generalizations towards realism, e.g. multitype and household epidemic models, are also presented, as is a model for endemic diseases.",,2009-10-23,,['tom britton']
0910.5185,nonparametric methods for volatility density estimation,stat.me math.st q-fin.st stat.th,"stochastic volatility modelling of financial processes has become increasingly popular. the proposed models usually contain a stationary volatility process. we will motivate and review several nonparametric methods for estimation of the density of the volatility process. both models based on discretely sampled continuous time processes and discrete time models will be discussed.   the key insight for the analysis is a transformation of the volatility density estimation problem to a deconvolution model for which standard methods exist. three type of nonparametric density estimators are reviewed: the fourier-type deconvolution kernel density estimator, a wavelet deconvolution density estimator and a penalized projection estimator. the performance of these estimators will be compared. key words: stochastic volatility models, deconvolution, density estimation, kernel estimator, wavelets, minimum contrast estimation, mixing",,2009-10-27,,"['bert van es', 'peter spreij', 'harry van zanten']"
0911.1472,variable second-order inclusion probabilities as a tool to predict the   sampling variance,stat.ap stat.me,"a generalization of gy's theory for the variance of the fundamental sampling error is reviewed. practical situations where the generalized model potentially leads to more accurate variance estimates are identified as: clustering of particles, differences in densities or sizes of the particles or repulsive inter-particle forces. two general approaches for estimating an input parameter for the generalized model are discussed. the first approach consists of modelling based on physical properties of particles such as size, density and electrostatic forces between particles. the second approach uses image analysis of actual samples. further research into both methods is proposed and a suggestion is made to use line-intercept sampling combined with markov chain modelling in the second approach.   it is concluded that although, at the moment, it is too early for a routine application of the generalized theory, the generalization has the potential of providing more accurate variance estimates than are possible in the theory of gy. therefore, further research into the development and expansion of the generalized theory is worthwhile.",,2009-11-07,,['bastiaan geelhoed']
0911.3270,nonparametric bayesian inference on bivariate extremes,math.st stat.th,"the tail of a bivariate distribution function in the domain of attraction of a bivariate extreme-value distribution may be approximated by the one of its extreme-value attractor. the extreme-value attractor has margins that belong to a three-parameter family and a dependence structure which is characterised by a spectral measure, that is a probability measure on the unit interval with mean equal to one half. as an alternative to parametric modelling of the spectral measure, we propose an infinite-dimensional model which is at the same time manageable and still dense within the class of spectral measures. inference is done in a bayesian framework, using the censored-likelihood approach. in particular, we construct a prior distribution on the class of spectral measures and develop a trans-dimensional markov chain monte carlo algorithm for numerical computations. the method provides a bivariate predictive density which can be used for predicting the extreme outcomes of the bivariate distribution. in a practical perspective, this is useful for computing rare event probabilities and extreme conditional quantiles. the methodology is validated by simulations and applied to a data-set of danish fire insurance claims.",,2009-11-17,2012-05-10,"['simon guillotte', 'francois perron', 'johan segers']"
0911.4046,super-linear convergence of dual augmented-lagrangian algorithm for   sparsity regularized estimation,stat.ml cs.lg stat.me,"we analyze the convergence behaviour of a recently proposed algorithm for regularized estimation called dual augmented lagrangian (dal). our analysis is based on a new interpretation of dal as a proximal minimization algorithm. we theoretically show under some conditions that dal converges super-linearly in a non-asymptotic and global sense. due to a special modelling of sparse estimation problems in the context of machine learning, the assumptions we make are milder and more natural than those made in conventional analysis of augmented lagrangian algorithms. in addition, the new interpretation enables us to generalize dal to wide varieties of sparse estimation problems. we experimentally confirm our analysis in a large scale $\ell_1$-regularized logistic regression problem and extensively compare the efficiency of dal algorithm to previously proposed algorithms on both synthetic and benchmark datasets.",,2009-11-20,2011-01-02,"['ryota tomioka', 'taiji suzuki', 'masashi sugiyama']"
0911.5242,the ilium forward modelling algorithm for multivariate parameter   estimation and its application to derive stellar parameters from gaia   spectrophotometry,astro-ph.im astro-ph.ga astro-ph.sr cs.ne stat.ml,"i introduce an algorithm for estimating parameters from multidimensional data based on forward modelling. in contrast to many machine learning approaches it avoids fitting an inverse model and the problems associated with this. the algorithm makes explicit use of the sensitivities of the data to the parameters, with the goal of better treating parameters which only have a weak impact on the data. the forward modelling approach provides uncertainty (full covariance) estimates in the predicted parameters as well as a goodness-of-fit for observations. i demonstrate the algorithm, ilium, with the estimation of stellar astrophysical parameters (aps) from simulations of the low resolution spectrophotometry to be obtained by gaia. the ap accuracy is competitive with that obtained by a support vector machine. for example, for zero extinction stars covering a wide range of metallicity, surface gravity and temperature, ilium can estimate teff to an accuracy of 0.3% at g=15 and to 4% for (lower signal-to-noise ratio) spectra at g=20. [fe/h] and logg can be estimated to accuracies of 0.1-0.4dex for stars with g<=18.5. if extinction varies a priori over a wide range (av=0-10mag), then teff and av can be estimated quite accurately (3-4% and 0.1-0.2mag respectively at g=15), but there is a strong and ubiquitous degeneracy in these parameters which limits our ability to estimate either accurately at faint magnitudes. using the forward model we can map these degeneracies (in advance), and thus provide a complete probability distribution over solutions. (abridged)",10.1111/j.1365-2966.2009.16125.x,2009-11-27,2010-01-04,['c. a. l. bailer-jones']
0912.4729,likelihood-free bayesian inference for alpha-stable models,stat.co,"$\alpha$-stable distributions are utilised as models for heavy-tailed noise in many areas of statistics, finance and signal processing engineering.   however, in general, neither univariate nor multivariate $\alpha$-stable models admit closed form densities which can be evaluated pointwise. this complicates the inferential procedure.   as a result, $\alpha$-stable models are practically limited to the univariate setting under the bayesian paradigm, and to bivariate models under the classical framework.   in this article we develop a novel bayesian approach to modelling univariate and multivariate $\alpha$-stable distributions based on recent advances in ""likelihood-free"" inference.   we present an evaluation of the performance of this procedure in 1, 2 and 3 dimensions, and provide an analysis of real daily currency exchange rate data. the proposed approach provides a feasible inferential methodology at a moderate computational cost.",,2009-12-23,,"['g. w. peters', 's. a. sisson', 'y. fan']"
1001.4656,on bayesian data analysis,stat.me,this introduction to bayesian statistics presents the main concepts as well as the principal reasons advocated in favour of a bayesian modelling. we cover the various approaches to prior determination as well as the basis asymptotic arguments in favour of using bayes estimators. the testing aspects of bayesian inference are also examined in details.,,2010-01-26,2010-02-09,"['christian p. robert', 'judith rousseau']"
1003.0243,perfect simulation using dominated coupling from the past with   application to area-interaction point processes and wavelet thresholding,stat.me stat.co,"we consider perfect simulation algorithms for locally stable point processes based on dominated coupling from the past, and apply these methods in two different contexts. a new version of the algorithm is developed which is feasible for processes which are neither purely attractive nor purely repulsive. such processes include multiscale area-interaction processes, which are capable of modelling point patterns whose clustering structure varies across scales. the other topic considered is nonparametric regression using wavelets, where we use a suitable area-interaction process on the discrete space of indices of wavelet coefficients to model the notion that if one wavelet coefficient is non-zero then it is more likely that neighbouring coefficients will be also. a method based on perfect simulation within this model shows promising results compared to the standard methods which threshold coefficients independently.",,2010-02-28,,"['graeme k. ambler', 'bernard w. silverman']"
1003.3988,colouring and breaking sticks: random distributions and heterogeneous   clustering,stat.me stat.co,"we begin by reviewing some probabilistic results about the dirichlet process and its close relatives, focussing on their implications for statistical modelling and analysis. we then introduce a class of simple mixture models in which clusters are of different `colours', with statistical characteristics that are constant within colours, but different between colours. thus cluster identities are exchangeable only within colours. the basic form of our model is a variant on the familiar dirichlet process, and we find that much of the standard modelling and computational machinery associated with the dirichlet process may be readily adapted to our generalisation. the methodology is illustrated with an application to the partially-parametric clustering of gene expression profiles.",,2010-03-21,,['peter j. green']
1004.1341,algebraic comparison of partial lists in bioinformatics,stat.ml q-bio.qm,"the outcome of a functional genomics pipeline is usually a partial list of genomic features, ranked by their relevance in modelling biological phenotype in terms of a classification or regression model. due to resampling protocols or just within a meta-analysis comparison, instead of one list it is often the case that sets of alternative feature lists (possibly of different lengths) are obtained. here we introduce a method, based on the algebraic theory of symmetric groups, for studying the variability between lists (""list stability"") in the case of lists of unequal length. we provide algorithms evaluating stability for lists embedded in the full feature set or just limited to the features occurring in the partial lists. the method is demonstrated first on synthetic data in a gene filtering task and then for finding gene profiles on a recent prostate cancer dataset.",10.1371/journal.pone.0036540,2010-04-08,,"['giuseppe jurman', 'samantha riccadonna', 'roberto visintainer', 'cesare furlanello']"
1004.3871,practical estimation of high dimensional stochastic differential   mixed-effects models,stat.co math.ds stat.me,"stochastic differential equations (sdes) are established tools to model physical phenomena whose dynamics are affected by random noise. by estimating parameters of an sde intrinsic randomness of a system around its drift can be identified and separated from the drift itself. when it is of interest to model dynamics within a given population, i.e. to model simultaneously the performance of several experiments or subjects, mixed-effects modelling allows for the distinction of between and within experiment variability. a framework to model dynamics within a population using sdes is proposed, representing simultaneously several sources of variation: variability between experiments using a mixed-effects approach and stochasticity in the individual dynamics using sdes. these ""stochastic differential mixed-effects models"" have applications in e.g. pharmacokinetics/pharmacodynamics and biomedical modelling. a parameter estimation method is proposed and computational guidelines for an efficient implementation are given. finally the method is evaluated using simulations from standard models like the two-dimensional ornstein-uhlenbeck (ou) and the square root models.",10.1016/j.csda.2010.10.003,2010-04-22,2010-10-03,"['umberto picchini', 'susanne ditlevsen']"
1004.4116,assessing molecular variability in cancer genomes,q-bio.pe stat.co,"the dynamics of tumour evolution are not well understood. in this paper we provide a statistical framework for evaluating the molecular variation observed in different parts of a colorectal tumour. a multi-sample version of the ewens sampling formula forms the basis for our modelling of the data, and we provide a simulation procedure for use in obtaining reference distributions for the statistics of interest. we also describe the large-sample asymptotics of the joint distributions of the variation observed in different parts of the tumour. while actual data should be evaluated with reference to the simulation procedure, the asymptotics serve to provide theoretical guidelines, for instance with reference to the choice of possible statistics.",,2010-04-13,,"['a. d. barbour', 'simon tavaré']"
1006.2940,lasso isotone for high dimensional additive isotonic regression,stat.me stat.co stat.ml,"additive isotonic regression attempts to determine the relationship between a multi-dimensional observation variable and a response, under the constraint that the estimate is the additive sum of univariate component effects that are monotonically increasing. in this article, we present a new method for such regression called lasso isotone (liso). liso adapts ideas from sparse linear modelling to additive isotonic regression. thus, it is viable in many situations with high dimensional predictor variables, where selection of significant versus insignificant variables are required. we suggest an algorithm involving a modification of the backfitting algorithm cpav. we give a numerical convergence result, and finally examine some of its properties through simulations. we also suggest some possible extensions that improve performance, and allow calculation to be carried out when the direction of the monotonicity is unknown.",,2010-06-15,,"['zhou fang', 'nicolai meinshausen']"
1007.0296,a bayesian view of the poisson-dirichlet process,math.st cs.lg math.pr stat.th,"the two parameter poisson-dirichlet process (pdp), a generalisation of the dirichlet process, is increasingly being used for probabilistic modelling in discrete areas such as language technology, bioinformatics, and image analysis. there is a rich literature about the pdp and its derivative distributions such as the chinese restaurant process (crp). this article reviews some of the basic theory and then the major results needed for bayesian modelling of discrete problems including details of priors, posteriors and computation.   the pdp allows one to build distributions over countable partitions. the pdp has two other remarkable properties: first it is partially conjugate to itself, which allows one to build hierarchies of pdps, and second using a marginalised relative the crp, one gets fragmentation and clustering properties that lets one layer partitions to build trees. this article presents the basic theory for understanding the notion of partitions and distributions over them, the pdp and the crp, and the important properties of conjugacy, fragmentation and clustering, as well as some key related properties such as consistency and convergence. this article also presents a bayesian interpretation of the poisson-dirichlet process based on an improper and infinite dimensional dirichlet distribution. this means we can understand the process as just another dirichlet and thus all its sampling properties emerge naturally.   the theory of pdps is usually presented for continuous distributions (more generally referred to as non-atomic distributions), however, when applied to discrete distributions its remarkable conjugacy property emerges. this context and basic results are also presented, as well as techniques for computing the second order stirling numbers that occur in the posteriors for discrete distributions.",,2010-07-02,2012-02-15,"['wray buntine', 'marcus hutter']"
1007.0394,non-uniform state space reconstruction and coupling detection,nlin.cd cs.it math.it physics.data-an q-bio.nc stat.me,"we investigate the state space reconstruction from multiple time series derived from continuous and discrete systems and propose a method for building embedding vectors progressively using information measure criteria regarding past, current and future states. the embedding scheme can be adapted for different purposes, such as mixed modelling, cross-prediction and granger causality. in particular we apply this method in order to detect and evaluate information transfer in coupled systems. as a practical application, we investigate in records of scalp epileptic eeg the information flow across brain areas.",10.1103/physreve.82.016207,2010-07-02,,"['ioannis vlachos', 'dimitris kugiumtzis']"
1007.4622,adaptive wavelet estimation of the diffusion coefficient under additive   error measurements,math.st stat.th,"we study nonparametric estimation of the diffusion coefficient from discrete data, when the observations are blurred by additional noise. such issues have been developed over the last 10 years in several application fields and in particular in high frequency financial data modelling, however mainly from a parametric and semiparametric point of view. this paper addresses the nonparametric estimation of the path of the (possibly stochastic) diffusion coefficient in a relatively general setting. by developing pre-averaging techniques combined with wavelet thresholding, we construct adaptive estimators that achieve a nearly optimal rate within a large scale of smoothness constraints of besov type. since the diffusion coefficient is usually genuinely random, we propose a new criterion to assess the quality of estimation; we retrieve the usual minimax theory when this approach is restricted to a deterministic diffusion coefficient. in particular, we take advantage of recent results of reiss [33] of asymptotic equivalence between a gaussian diffusion with additive noise and gaussian white noise model, in order to prove a sharp lower bound.",,2010-07-27,2011-12-29,"['marc hoffmann', 'axel munk', 'johannes schmidt-hieber']"
1007.5388,reference priors of nuisance parameters in bayesian sequential   population analysis,math.st stat.th,"prior distributions elicited for modelling the natural fluctuations or the uncertainty on parameters of bayesian fishery population models, can be chosen among a vast range of statistical laws. since the statistical framework is defined by observational processes, observational parameters enter into the estimation and must be considered random, similarly to parameters or states of interest like population levels or real catches. the former are thus perceived as nuisance parameters whose values are intrinsically linked to the considered experiment, which also require noninformative priors. in fishery research jeffreys methodology has been presented by millar (2002) as a practical way to elicit such priors. however they can present wrong properties in multiparameter contexts. therefore we suggest to use the elicitation method proposed by berger and bernardo to avoid paradoxical results raised by jeffreys priors. these benchmark priors are derived here in the framework of sequential population analysis.",,2010-07-30,2010-10-11,['nicolas bousquet']
1008.1636,censoring outdegree compromises inferences of social network peer   effects and autocorrelation,stat.me,"i examine the consequences of modelling contagious influence in a social network with incomplete edge information, namely in the situation where each individual may name a limited number of friends, so that extra outbound ties are censored. in particular, i consider a prototypical time series configuration where a property of the ""ego"" is affected in a causal fashion by the properties of their ""alters"" at a previous time point, both in the total number of alters as well as the deviation from a central value. this is considered with three potential methods for naming one's friends: a strict upper limit on the number of declarations, a flexible limit, and an instruction where a person names a prespecified fraction of their friends. i find that one of two effects is present in the estimation of these effects: either that the size of the effect is inflated in magnitude, or that the estimators instead are centered about zero rather than related to the true effect. the degree of heterogeneity in friend count is one of the major factors into whether such an analysis can be salvaged by post-hoc adjustments.",,2010-08-10,2011-01-06,['andrew c. thomas']
1008.2870,efficient and robust estimation for a class of generalized linear   longitudinal mixed models,stat.me stat.ap,"we propose a versatile and computationally efficient estimating equation method for a class of hierarchical multiplicative generalized linear mixed models with additive dispersion components, based on explicit modelling of the covariance structure. the class combines longitudinal and random effects models and retains a marginal as well as a conditional interpretation. the estimation procedure combines that of generalized estimating equations for the regression with residual maximum likelihood estimation for the association parameters. this avoids the multidimensional integral of the conventional generalized linear mixed models likelihood and allows an extension of the robust empirical sandwich estimator for use with both association and regression parameters. the method is applied to a set of otolith data, used for age determination of fish.",,2010-08-17,,"['rené holst', 'bent jørgensen']"
1009.1690,probabilistic models over ordered partitions with application in   learning to rank,cs.ir stat.ml,"this paper addresses the general problem of modelling and learning rank data with ties. we propose a probabilistic generative model, that models the process as permutations over partitions. this results in super-exponential combinatorial state space with unknown numbers of partitions and unknown ordering among them. we approach the problem from the discrete choice theory, where subsets are chosen in a stagewise manner, reducing the state space per each stage significantly. further, we show that with suitable parameterisation, we can still learn the models in linear time. we evaluate the proposed models on the problem of learning to rank with the data from the recently held yahoo! challenge, and demonstrate that the models are competitive against well-known rivals.",,2010-09-09,2010-10-04,"['tran the truyen', 'dinh q. phung', 'svetha venkatesh']"
1010.1069,cooperative distributed sequential spectrum sensing,cs.it math.it stat.ap,we consider cooperative spectrum sensing for cognitive radios. we develop an energy efficient detector with low detection delay using sequential hypothesis testing. sequential probability ratio test (sprt) is used at both the local nodes and the fusion center. we also analyse the performance of this algorithm and compare with the simulations. modelling uncertainties in the distribution parameters are considered. slow fading with and without perfect channel state information at the cognitive radios is taken into account.,,2010-10-06,2012-11-23,"['jithin k s', 'vinod sharma', 'raghav gopalarathnam']"
1010.4406,impact of insurance for operational risk: is it worthwhile to insure or   be insured for severe losses?,q-fin.rm q-fin.st stat.ap stat.co stat.me,"under the basel ii standards, the operational risk (oprisk) advanced measurement approach allows a provision for reduction of capital as a result of insurance mitigation of up to 20%. this paper studies the behaviour of different insurance policies in the context of capital reduction for a range of possible extreme loss models and insurance policy scenarios in a multi-period, multiple risk settings. a loss distributional approach (lda) for modelling of the annual loss process, involving homogeneous compound poisson processes for the annual losses, with heavy tailed severity models comprised of alpha-stable severities is considered. there has been little analysis of such models to date and it is believed, insurance models will play more of a role in oprisk mitigation and capital reduction in future. the first question of interest is when would it be equitable for a bank or financial institution to purchase insurance for heavy tailed oprisk losses under different insurance policy scenarios? the second question then pertains to solvency ii and addresses what the insurers capital would be for such operational risk scenarios under different policy offerings. in addition we consider the insurers perspective with respect to fair premium as a percentage above the expected annual claim for each insurance policy. the intention being to address questions related to var reduction under basel ii, scr under solvency ii and fair insurance premiums in oprisk for different extreme loss scenarios. in the process we provide closed form solutions for the distribution of loss process and claims process in an lda structure as well as closed form analytic solutions for the expected shortfall, scr and mcr under basel ii and solvency ii. we also provide closed form analytic solutions for the annual loss distribution of multiple risks including insurance mitigation.",,2010-10-21,2010-11-02,"['gareth w. peters', 'aaron d. byrnes', 'pavel v. shevchenko']"
1011.1379,model selection by loss rank for classification and unsupervised   learning,stat.me stat.ml,"hutter (2007) recently introduced the loss rank principle (lorp) as a generalpurpose principle for model selection. the lorp enjoys many attractive properties and deserves further investigations. the lorp has been well-studied for regression framework in hutter and tran (2010). in this paper, we study the lorp for classification framework, and develop it further for model selection problems in unsupervised learning where the main interest is to describe the associations between input measurements, like cluster analysis or graphical modelling. theoretical properties and simulation studies are presented.",,2010-11-05,,"['minh-ngoc tran', 'marcus hutter']"
1012.3407,translating biomarkers between multi-way time-series experiments,stat.ml,"translating potential disease biomarkers between multi-species 'omics' experiments is a new direction in biomedical research. the existing methods are limited to simple experimental setups such as basic healthy-diseased comparisons. most of these methods also require an a priori matching of the variables (e.g., genes or metabolites) between the species. however, many experiments have a complicated multi-way experimental design often involving irregularly-sampled time-series measurements, and for instance metabolites do not always have known matchings between organisms. we introduce a bayesian modelling framework for translating between multiple species the results from 'omics' experiments having a complex multi-way, time-series experimental design. the underlying assumption is that the unknown matching can be inferred from the response of the variables to multiple covariates including time.",,2010-12-15,,"['ilkka huopaniemi', 'tommi suvitaival', 'matej orešič', 'samuel kaski']"
1102.5228,some covariance models based on normal scale mixtures,math.st stat.th,"modelling spatio-temporal processes has become an important issue in current research. since gaussian processes are essentially determined by their second order structure, broad classes of covariance functions are of interest. here, a new class is described that merges and generalizes various models presented in the literature, in particular models in gneiting (j. amer. statist. assoc. 97 (2002) 590--600) and stein (nonstationary spatial covariance functions (2005) univ. chicago). furthermore, new models and a multivariate extension are introduced.",10.3150/09-bej226,2011-02-25,,['martin schlather']
1102.5606,ornstein-uhlenbeck type processes with heavy distribution tails,math.pr math.st stat.th,"we consider a transformed ornstein-uhlenbeck process model that can be a good candidate for modelling real-life processes characterized by a combination of time-reverting behaviour with heavy distribution tails. we begin with presenting the results of an exploratory statistical analysis of the log prices of a major australian public company, demonstrating several key features typical of such time series. motivated by these findings, we suggest a simple transformed ornstein-uhlenbeck process model and analyze its properties showing that the model is capable of replicating our empirical findings. we also discuss three different estimators for the drift coefficient in the underlying (unobservable) ornstein-uhlenbeck process which is the key descriptor of dependence in the process.",,2011-02-28,,"['k. borovkov', 'g. decrouez']"
1104.0896,on identifying significant edges in graphical models of molecular   networks,stat.ml stat.me,"objective: modelling the associations from high-throughput experimental molecular data has provided unprecedented insights into biological pathways and signalling mechanisms. graphical models and networks have especially proven to be useful abstractions in this regard. ad-hoc thresholds are often used in conjunction with structure learning algorithms to determine significant associations. the present study overcomes this limitation by proposing a statistically-motivated approach for identifying significant associations in a network.   methods and materials: a new method that identifies significant associations in graphical models by estimating the threshold minimising the $l_{\mathrm{1}}$ norm between the cumulative distribution function (cdf) of the observed edge confidences and those of its asymptotic counterpart is proposed. the effectiveness of the proposed method is demonstrated on popular synthetic data sets as well as publicly available experimental molecular data corresponding to gene and protein expression profiles.   results: the improved performance of the proposed approach is demonstrated across the synthetic data sets using sensitivity, specificity and accuracy as performance metrics. the results are also demonstrated across varying sample sizes and three different structure learning algorithms with widely varying assumptions. in all cases, the proposed approach has specificity and accuracy close to 1, while sensitivity increases linearly in the logarithm of the sample size. the estimated threshold systematically outperforms common ad-hoc ones in terms of sensitivity while maintaining comparable levels of specificity and accuracy. networks from experimental data sets are reconstructed accurately with respect to the results from the original papers.",,2011-04-05,2013-04-23,"['marco scutari', 'radhakrishnan nagarajan']"
1104.1608,lattices of graphical gaussian models with symmetries,math.st stat.th,"in order to make graphical gaussian models a viable modelling tool when the number of variables outgrows the number of observations, model classes which place equality restrictions on concentrations or partial correlations have previously been introduced in the literature. the models can be represented by vertex and edge coloured graphs. the need for model selection methods makes it imperative to understand the structure of model classes. we identify four model classes that form complete lattices of models with respect to model inclusion, which qualifies them for an edwards-havr\'anek model selection procedure. two classes turn out most suitable for a corresponding model search. we obtain an explicit search algorithm for one of them and provide a model search example for the other.",10.3390/sym3030653,2011-04-08,2011-09-19,['helene gehrmann']
1104.3503,resid: a practical stochastic model for software reliability,stat.ap cs.se,"a new approach called resid is proposed in this paper for estimating reliability of a software allowing for imperfect debugging. unlike earlier approaches based on counting number of bugs or modelling inter-failure time gaps, resid focuses on the probability of ""bugginess"" of different parts of a program buggy. this perspective allows an easy way to incorporate the structure of the software under test, as well as imperfect debugging. one main design objective behind resid is ease of implementation in practical scenarios.",,2011-04-18,,['arnab chakraborty']
1104.4422,the multivariate watson distribution: maximum-likelihood estimation and   other aspects,stat.co math.ca,"this paper studies fundamental aspects of modelling data using multivariate watson distributions. although these distributions are natural for modelling axially symmetric data (i.e., unit vectors where $\pm \x$ are equivalent), for high-dimensions using them can be difficult. why so? largely because for watson distributions even basic tasks such as maximum-likelihood are numerically challenging. to tackle the numerical difficulties some approximations have been derived---but these are either grossly inaccurate in high-dimensions (\emph{directional statistics}, mardia & jupp. 2000) or when reasonably accurate (\emph{j. machine learning research, w. & c.p., v2}, bijral \emph{et al.}, 2007, pp. 35--42), they lack theoretical justification. we derive new approximations to the maximum-likelihood estimates; our approximations are theoretically well-defined, numerically accurate, and easy to compute. we build on our parameter estimation and discuss mixture-modelling with watson distributions; here we uncover a hitherto unknown connection to the ""diametrical clustering"" algorithm of dhillon \emph{et al.} (\emph{bioinformatics}, 19(13), 2003, pp. 1612--1619).",,2011-04-22,2012-05-25,"['suvrit sra', 'dmitrii karp']"
1105.6075,marginal log-linear parameters for graphical markov models,stat.me,"marginal log-linear (mll) models provide a flexible approach to multivariate discrete data. mll parametrizations under linear constraints induce a wide variety of models, including models defined by conditional independences. we introduce a sub-class of mll models which correspond to acyclic directed mixed graphs (admgs) under the usual global markov property. we characterize for precisely which graphs the resulting parametrization is variation independent. the mll approach provides the first description of admg models in terms of a minimal list of constraints. the parametrization is also easily adapted to sparse modelling techniques, which we illustrate using several examples of real data.",10.1111/rssb.12020,2011-05-30,2012-10-31,"['robin j. evans', 'thomas s. richardson']"
1106.4509,machine learning markets,cs.ai cs.ma cs.ne q-fin.tr stat.ml,"prediction markets show considerable promise for developing flexible mechanisms for machine learning. here, machine learning markets for multivariate systems are defined, and a utility-based framework is established for their analysis. this differs from the usual approach of defining static betting functions. it is shown that such markets can implement model combination methods used in machine learning, such as product of expert and mixture of expert approaches as equilibrium pricing models, by varying agent utility functions. they can also implement models composed of local potentials, and message passing methods. prediction markets also allow for more flexible combinations, by combining multiple different utility functions. conversely, the market mechanisms implement inference in the relevant probabilistic models. this means that market mechanism can be utilized for implementing parallelized model building and inference for probabilistic modelling.",,2011-06-22,,['amos storkey']
1107.1811,modelling outliers and structural breaks in dynamic linear models with a   novel use of a heavy tailed prior for the variances: an alternative to the   inverted gamma,stat.me,modelling outliers and structural breaks in dynamic linear models with a novel use of a heavy tailed prior for the variances: an alternative to the inverted gamma,,2011-07-09,2013-01-24,"['jairo fuquene', 'maria perez', 'luis pericchi']"
1107.2205,sequential monte carlo em for multivariate probit models,stat.me stat.co,"multivariate probit models (mpm) have the appealing feature of capturing some of the dependence structure between the components of multidimensional binary responses. the key for the dependence modelling is the covariance matrix of an underlying latent multivariate gaussian. most approaches to mle in multivariate probit regression rely on mcem algorithms to avoid computationally intensive evaluations of multivariate normal orthant probabilities. as an alternative to the much used gibbs sampler a new smc sampler for truncated multivariate normals is proposed. the algorithm proceeds in two stages where samples are first drawn from truncated multivariate student $t$ distributions and then further evolved towards a gaussian. the sampler is then embedded in a mcem algorithm. the sequential nature of smc methods can be exploited to design a fully sequential version of the em, where the samples are simply updated from one iteration to the next rather than resampled from scratch. recycling the samples in this manner significantly reduces the computational cost. an alternative view of the standard conditional maximisation step provides the basis for an iterative procedure to fully perform the maximisation needed in the em algorithm. the identifiability of mpm is also thoroughly discussed. in particular, the likelihood invariance can be embedded in the em algorithm to ensure that constrained and unconstrained maximisation are equivalent. a simple iterative procedure is then derived for either maximisation which takes effectively no computational time. the method is validated by applying it to the widely analysed six cities dataset and on a higher dimensional simulated example. previous approaches to the six cities overly restrict the parameter space but, by considering the correct invariance, the maximum likelihood is quite naturally improved when treating the full unrestricted model.",10.1016/j.csda.2013.10.019,2011-07-12,2013-11-14,"['giusi moffa', 'jack kuipers']"
1107.2699,linear latent force models using gaussian processes,stat.ml cs.ai,"purely data driven approaches for machine learning present difficulties when data is scarce relative to the complexity of the model or when the model is forced to extrapolate. on the other hand, purely mechanistic approaches need to identify and specify all the interactions in the problem at hand (which may not be feasible) and still leave the issue of how to parameterize the system. in this paper, we present a hybrid approach using gaussian processes and differential equations to combine data driven modelling with a physical model of the system. we show how different, physically-inspired, kernel functions can be developed through sensible, simple, mechanistic assumptions about the underlying system. the versatility of our approach is illustrated with three case studies from motion capture, computational biology and geostatistics.",,2011-07-13,2020-03-13,"['mauricio a. álvarez', 'david luengo', 'neil d. lawrence']"
1107.3904,asymptotics of the discrete log-concave maximum likelihood estimator and   related applications,stat.me,"the assumption of log-concavity is a flexible and appealing nonparametric shape constraint in distribution modelling. in this work, we study the log-concave maximum likelihood estimator (mle) of a probability mass function (pmf). we show that the mle is strongly consistent and derive its pointwise asymptotic theory under both the well- and misspecified setting. our asymptotic results are used to calculate confidence intervals for the true log-concave pmf. both the mle and the associated confidence intervals may be easily computed using the r package logcondiscr. we illustrate our theoretical results using recent data from the h1n1 pandemic in ontario, canada.",,2011-07-20,2012-10-14,"['fadoua balabdaoui', 'hanna jankowski', 'kaspar rufibach', 'marios pavlides']"
1107.4464,max-stable processes for modelling extremes observed in space and time,stat.me,"max-stable processes have proved to be useful for the statistical modelling of spatial extremes. several representations of max-stable random fields have been proposed in the literature. for statistical inference it is often assumed that there is no temporal dependence, i.e., the observations at spatial locations are independent in time. we use two representations of stationary max-stable spatial random fields and extend the concepts to the space-time domain. in a first approach, we extend the idea of constructing max-stable random fields as limits of normalized and rescaled pointwise maxima of independent gaussian random fields, which was introduced by kabluchko, schlather and de haan [2009], who construct max-stable random fields associated to a class of variograms. we use a similar approach based on a well-known result by h\""usler and reiss and apply specific spatio-temporal covariance models for the underlying gaussian random field, which satisfy weak regularity assumptions. furthermore, we extend smith's storm profile model to a space-time setting and provide explicit expressions for the bivariate distribution functions.   the tail dependence coefficient is an important measure of extremal dependence. we show how the spatio-temporal covariance function underlying the gaussian random field can be interpreted in terms of the tail dependence coefficient. within this context, we examine different concepts for constructing spatio-temporal covariance models and analyse several specific examples, including gneiting's class of nonseparable stationary covariance functions.",,2011-07-22,,"['richard a. davis', 'claudia klüppelberg', 'christina steinkohl']"
1108.0184,modelling chaotic data,math.st stat.th,"this paper extends the subjects dicussed in the data analysis and dynamical systems courses by looking at the subject of modelling data. this task is nontrivial as the underlying process could be non-linear. in the paper some common methods, including global and local polynomial fitting, are discussed in terms of their applicability, level of computation and accuracy. one example method, measure based reconstruction, has been investigated in greater detail and experimentation is carried out to evaluate the method. in this project we shall be looking at the different ways one can model chaotic time series. the reason we are going to look at a range of methods is that different methods are ""good"" for different applications. as the ""goodness"" of a model is subjective to the task one wishes to do, we will investigate a selected models and compare the prediction to see how one goes about testing a model.",,2011-07-31,,['vincent mellor']
1108.1884,estimation of parameters in dna mixture analysis,stat.me,"in cowell et al. (2007), a bayesian network for analysis of mixed traces of dna was presented using gamma distributions for modelling peak sizes in the electropherogram. it was demonstrated that the analysis was sensitive to the choice of a variance factor and hence this should be adapted to any new trace analysed. in the present paper we discuss how the variance parameter can be estimated by maximum likelihood to achieve this. the unknown proportions of dna from each contributor can similarly be estimated by maximum likelihood jointly with the variance parameter. furthermore we discuss how to incorporate prior knowledge about the parameters in a bayesian analysis. the proposed estimation methods are illustrated through a few examples of applications for calculating evidential value in casework and for mixture deconvolution.",10.1080/02664763.2013.817549,2011-08-09,2013-06-20,"['therese graversen', 'steffen lauritzen']"
1108.2840,generalised elastic nets,q-bio.nc cs.lg stat.ml,"the elastic net was introduced as a heuristic algorithm for combinatorial optimisation and has been applied, among other problems, to biological modelling. it has an energy function which trades off a fitness term against a tension term. in the original formulation of the algorithm the tension term was implicitly based on a first-order derivative. in this paper we generalise the elastic net model to an arbitrary quadratic tension term, e.g. derived from a discretised differential operator, and give an efficient learning algorithm. we refer to these as generalised elastic nets (gens). we give a theoretical analysis of the tension term for 1d nets with periodic boundary conditions, and show that the model is sensitive to the choice of finite difference scheme that represents the discretised derivative. we illustrate some of these issues in the context of cortical map models, by relating the choice of tension term to a cortical interaction function. in particular, we prove that this interaction takes the form of a mexican hat for the original elastic net, and of progressively more oscillatory mexican hats for higher-order derivatives. the results apply not only to generalised elastic nets but also to other methods using discrete differential penalties, and are expected to be useful in other areas, such as data analysis, computer graphics and optimisation problems.",,2011-08-13,,"['miguel á. carreira-perpiñán', 'geoffrey j. goodhill']"
1108.4912,the importance of prior choice in model selection: a density dependence   example,stat.me,"we perform a bayesian analysis on abundance data for ten species of north american duck, using the results to investigate the evidence in favour of biologically motivated hypotheses about the causes and mechanisms of density dependence in these species. we explore the capabilities of our methods to detect density dependent effects, both by simulation and through analyzes of real data. the effect of the prior choice on predictive accuracy is also examined. we conclude that our priors, which are motivated by considering the dynamics of the system of interest, offer clear advances over the priors used by previous authors for the duck data sets. we use this analysis as a motivating example to demonstrate the importance of careful parameter prior selection if we are to perform a balanced model selection procedure. we also present some simple guidelines that can be followed in a wide variety of modelling frameworks where vague parameter prior choice is not a viable option. these will produce parameter priors that not only greatly reduce bias in selecting certain models, but improve the predictive ability of the resulting model-averaged predictor.",,2011-08-24,,"['james d. lawrence', 'dr. robert b. gramacy', 'dr. len thomas', 'prof. stephen t. buckland']"
1109.0863,identifying differentially expressed transcripts from rna-seq data with   biological variation,q-bio.gn stat.ap,"motivation: high-throughput sequencing enables expression analysis at the level of individual transcripts. the analysis of transcriptome expression levels and differential expression estimation requires a probabilistic approach to properly account for ambiguity caused by shared exons and finite read sampling as well as the intrinsic biological variance of transcript expression.   results: we present bitseq (bayesian inference of transcripts from sequencing data), a bayesian approach for estimation of transcript expression level from rna-seq experiments. inferred relative expression is represented by markov chain monte carlo (mcmc) samples from the posterior probability distribution of a generative model of the read data. we propose a novel method for differential expression analysis across replicates which propagates uncertainty from the sample-level model while modelling biological variance using an expression-level-dependent prior. we demonstrate the advantages of our method using simulated data as well as an rna-seq dataset with technical and biological replication for both studied conditions.   availability: the implementation of the transcriptome expression estimation and differential expression analysis, bitseq, has been written in c++.",10.1093/bioinformatics/bts260,2011-09-05,2012-03-05,"['peter glaus', 'antti honkela', 'magnus rattray']"
1109.4706,on the fitting of mixtures of multivariate skew t-distributions via the   em algorithm,stat.me,"we show how the expectation-maximization (em) algorithm can be applied exactly for the fitting of mixtures of general multivariate skew t (mst) distributions, eliminating the need for computationally expensive monte carlo estimation. finite mixtures of mst distributions have proven to be useful in modelling heterogeneous data with asymmetric and heavy tail behaviour. recently, they have been exploited as an effective tool for modelling flow cytometric data. however, without restrictions on the the characterizations of the component skew t-distributions, monte carlo methods have been used to fit these models. in this paper, we show how the em algorithm can be implemented for the iterative computation of the maximum likelihood estimates of the model parameters without resorting to monte carlo methods for mixtures with unrestricted mst components. the fast calculation of semi-infinite integrals on the e-step of the em algorithm is effected by noting that they can be put in the form of moments of the truncated multivariate t-distribution, which subsequently can be expressed in terms of the non-truncated form of the t-distribution function for which fast algorithms are available. we demonstrate the usefulness of the proposed methodology by some applications to three real data sets.",,2011-09-22,2012-09-05,"['s. x. lee', 'g. j. mclachlan']"
1109.5894,learning item trees for probabilistic modelling of implicit feedback,cs.lg stat.ml,"user preferences for items can be inferred from either explicit feedback, such as item ratings, or implicit feedback, such as rental histories. research in collaborative filtering has concentrated on explicit feedback, resulting in the development of accurate and scalable models. however, since explicit feedback is often difficult to collect it is important to develop effective models that take advantage of the more widely available implicit feedback. we introduce a probabilistic approach to collaborative filtering with implicit feedback based on modelling the user's item selection process. in the interests of scalability, we restrict our attention to tree-structured distributions over items and develop a principled and efficient algorithm for learning item trees from data. we also identify a problem with a widely used protocol for evaluating implicit feedback models and propose a way of addressing it using a small quantity of explicit feedback data.",,2011-09-27,,"['andriy mnih', 'yee whye teh']"
1109.6042,some fundamental properties of a multivariate von mises distribution,math.st stat.th,"in application areas like bioinformatics multivariate distributions on angles are encountered which show significant clustering. one approach to statistical modelling of such situations is to use mixtures of unimodal distributions. in the literature (mardia et al., 2011), the multivariate von mises distribution, also known as the multivariate sine distribution, has been suggested for components of such models, but work in the area has been hampered by the fact that no good criteria for the von mises distribution to be unimodal were available. in this article we study the question about when a multivariate von mises distribution is unimodal. we give sufficient criteria for this to be the case and show examples of distributions with multiple modes when these criteria are violated. in addition, we propose a method to generate samples from the von mises distribution in the case of high concentration.",10.1080/03610926.2012.670353,2011-09-27,2013-07-05,"['kanti v. mardia', 'jochen voss']"
1109.6804,comparing probabilistic models for melodic sequences,stat.ml,"modelling the real world complexity of music is a challenge for machine learning. we address the task of modeling melodic sequences from the same music genre. we perform a comparative analysis of two probabilistic models; a dirichlet variable length markov model (dirichlet-vmm) and a time convolutional restricted boltzmann machine (tc-rbm). we show that the tc-rbm learns descriptive music features, such as underlying chords and typical melody transitions and dynamics. we assess the models for future prediction and compare their performance to a vmm, which is the current state of the art in melody generation. we show that both models perform significantly better than the vmm, with the dirichlet-vmm marginally outperforming the tc-rbm. finally, we evaluate the short order statistics of the models, using the kullback-leibler divergence between test sequences and model samples, and show that our proposed methods match the statistics of the music genre significantly better than the vmm.",,2011-09-30,,"['athina spiliopoulou', 'amos storkey']"
1110.2894,two algorithms for fitting constrained marginal models,stat.co stat.me,"we study in detail the two main algorithms which have been considered for fitting constrained marginal models to discrete data, one based on lagrange multipliers and the other on a regression model. we show that the updates produced by the two methods are identical, but that the lagrangian method is more efficient in the case of identically distributed observations. we provide a generalization of the regression algorithm for modelling the effect of exogenous individual-level covariates, a context in which the use of the lagrangian algorithm would be infeasible for even moderate sample sizes. an extension of the method to likelihood-based estimation under $l_1$-penalties is also considered.",10.1016/j.csda.2013.02.001,2011-10-13,2012-12-24,"['robin j. evans', 'antonio forcina']"
1110.4400,functional uniform priors for nonlinear modelling,stat.co stat.me,"this paper considers the topic of finding prior distributions when a major component of the statistical model depends on a nonlinear function. using results on how to construct uniform distributions in general metric spaces, we propose a prior distribution that is uniform in the space of functional shapes of the underlying nonlinear function and then back-transform to obtain a prior distribution for the original model parameters. the primary application considered in this article is nonlinear regression, but the idea might be of interest beyond this case. for nonlinear regression the so constructed priors have the advantage that they are parametrization invariant and do not violate the likelihood principle, as opposed to uniform distributions on the parameters or the jeffrey's prior, respectively. the utility of the proposed priors is demonstrated in the context of nonlinear regression modelling in clinical dose-finding trials, through a real data example and simulation. in addition the proposed priors are used for calculation of an optimal bayesian design.",10.1111/j.1541-0420.2012.01747.x,2011-10-19,,['björn bornkamp']
1110.4713,kernel topic models,cs.lg stat.ml,"latent dirichlet allocation models discrete data as a mixture of discrete distributions, using dirichlet beliefs over the mixture weights. we study a variation of this concept, in which the documents' mixture weight beliefs are replaced with squashed gaussian distributions. this allows documents to be associated with elements of a hilbert space, admitting kernel topic models (ktm), modelling temporal, spatial, hierarchical, social and other structure between documents. the main challenge is efficient approximate inference on the latent gaussian. we present an approximate algorithm cast around a laplace approximation in a transformed basis. the ktm can also be interpreted as a type of gaussian process latent variable model, or as a topic model conditional on document features, uncovering links between earlier work in these areas.",,2011-10-21,,"['philipp hennig', 'david stern', 'ralf herbrich', 'thore graepel']"
1110.6054,lgcp an r package for inference with spatio-temporal log-gaussian cox   processes,stat.co,"this paper introduces an r package for spatio-temporal prediction and forecasting for log-gaussian cox processes. the main computational tool for these models is markov chain monte carlo and the new package, lgcp, therefore also provides an extensible suite of functions for implementing mcmc algorithms for processes of this type. the modelling framework and details of inferential procedures are first presented before a tour of lgcp functionality is given via a walk-through data-analysis. topics covered include reading in and converting data, estimation of the key components and parameters of the model, specifying output and simulation quantities, computation of monte carlo expectations, post-processing and simulation of data sets.",,2011-10-27,,"['benjamin m. taylor', 'tilman m. davies', 'barry s. rowlingson', 'peter j. diggle']"
1111.2411,an individual-based model of infection spread in an urban environment,q-bio.pe stat.ap,"an individual-based model of the infectious disease spread among the urban population is considered. a system of stochastic equations, which describes changes in quantities of four population groups, susceptible, exposed, infected individuals and individuals in the state of remission, is built. the system of equations of the model is supplemented with correlations, which consider disease heaviness and duration for every infected individual. an algorithm and a modelling program based on monte-carlo methods which allows to investigate the group number dynamics is developed.",,2011-11-10,,['vasiliy leonenko']
1111.6899,extended generalised pareto models for tail estimation,stat.me stat.ap,"the most popular approach in extreme value statistics is the modelling of threshold exceedances using the asymptotically motivated generalised pareto distribution. this approach involves the selection of a high threshold above which the model fits the data well. sometimes, few observations of a measurement process might be recorded in applications and so selecting a high quantile of the sample as the threshold leads to almost no exceedances. in this paper we propose extensions of the generalised pareto distribution that incorporate an additional shape parameter while keeping the tail behaviour unaffected. the inclusion of this parameter offers additional structure for the main body of the distribution, improves the stability of the modified scale, tail index and return level estimates to threshold choice and allows a lower threshold to be selected. we illustrate the benefits of the proposed models with a simulation study and two case studies.",10.1016/j.jspi.2012.07.001,2011-11-29,,"['ioannis papastathopoulos', 'jonathan a. tawn']"
1112.0152,a skew-t-normal multi-level reduced-rank functional pca model with   applications to replicated `omics time series data sets,stat.me,"a powerful study design in the fields of genomics and metabolomics is the 'replicated time course experiment' where individual time series are observed for a sample of biological units, such as human patients, termed replicates. standard practice for analysing these data sets is to fit each variable (e.g. gene transcript) independently with a functional mixed-effects model to account for between-replicate variance. however, such an independence assumption is biologically implausible given that the variables are known to be highly correlated.   in this article we present a skew-t-normal multi-level reduced-rank functional principal components analysis (fpca) model for simultaneously modelling the between-variable and between-replicate variance. the reduced-rank fpca model is computationally efficient and, analogously with a standard pca for vectorial data, provides a low dimensional representation that can be used to identify the major patterns of temporal variation. using an example case study exploring the genetic response to bcg infection we demonstrate that these low dimensional representations are eminently biologically interpretable. we also show using a simulation study that modelling all variables simultaneously greatly reduces the estimation error compared to the independence assumption.",,2011-12-01,,"['maurice berk', 'giovanni montana']"
1112.4204,bayesian approaches to copula modelling,stat.me,"copula models have become one of the most widely used tools in the applied modelling of multivariate data. similarly, bayesian methods are increasingly used to obtain efficient likelihood-based inference. however, to date, there has been only limited use of bayesian approaches in the formulation and estimation of copula models. this article aims to address this shortcoming in two ways. first, to introduce copula models and aspects of copula theory that are especially relevant for a bayesian analysis. second, to outline bayesian approaches to formulating and estimating copula models, and their advantages over alternative methods. copulas covered include archimedean, copulas constructed by inversion, and vine copulas; along with their interpretation as transformations. a number of parameterisations of a correlation matrix of a gaussian copula are considered, along with hierarchical priors that allow for bayesian selection and model averaging for each parameterisation. markov chain monte carlo sampling schemes for fitting gaussian and d-vine copulas, with and without selection, are given in detail. the relationship between the prior for the parameters of a d-vine, and the prior for a correlation matrix of a gaussian copula, is discussed. last, it is shown how to compute bayesian inference when the data are discrete-valued using data augmentation. this approach generalises popular bayesian methods for the estimation of models for multivariate binary and other ordinal data to more general copula models. bayesian data augmentation has substantial advantages over other methods of estimation for this class of models.",10.1093/acprof:oso/9780199695607.001.0001,2011-12-18,,['michael stanley smith']
1112.5389,bayesian analysis of hierarchical multi-fidelity codes,math.st stat.th,"this paper deals with the gaussian process based approximation of a code which can be run at different levels of accuracy. this method, which is a particular case of co-kriging, allows us to improve a surrogate model of a complex computer code using fast approximations of it. in particular, we focus on the case of a large number of code levels on the one hand and on a bayesian approach when we have two levels on the other hand. the main results of this paper are a new approach to estimate the model parameters which provides a closed form expression for an important parameter of the model (the scale factor), a reduction of the numerical complexity by simplifying the covariance matrix inversion, and a new bayesian modelling that gives an explicit representation of the joint distribution of the parameters and that is not computationally expensive. a thermodynamic example is used to illustrate the comparison between 2-level and 3-level co-kriging.",,2011-12-22,2012-09-24,['loic le gratiet']
1201.0155,carma processes driven by non-gaussian noise,math.pr math.st stat.th,"we present an outline of the theory of certain l\'evy-driven, multivariate stochastic processes, where the processes are represented by rational transfer functions (continuous-time autoregressive moving average or carma models) and their applications in non-gaussian time series modelling. we discuss in detail their definition, their spectral representation, the equivalence to linear state space models and further properties like the second order structure and the tail behaviour under a heavy-tailed input. furthermore, we study the estimation of the parameters using quasi-maximum likelihood estimates for the auto-regressive and moving average parameters, as well as how to estimate the driving l\'evy process.",,2011-12-30,,['robert stelzer']
1201.0633,general bound of overfitting for mlp regression models,math.st stat.th,"multilayer perceptrons (mlp) with one hidden layer have been used for a long time to deal with non-linear regression. however, in some task, mlp's are too powerful models and a small mean square error (mse) may be more due to overfitting than to actual modelling. if the noise of the regression model is gaussian, the overfitting of the model is totally determined by the behavior of the likelihood ratio test statistic (lrts), however in numerous cases the assumption of normality of the noise is arbitrary if not false. in this paper, we present an universal bound for the overfitting of such model under weak assumptions, this bound is valid without gaussian or identifiability assumptions. the main application of this bound is to give a hint about determining the true architecture of the mlp model when the number of data goes to infinite. as an illustration, we use this theoretical result to propose and compare effective criteria to find the true architecture of an mlp.",10.1016/j.neucom.2011.11.028,2012-01-03,,['joseph rynkiewicz']
1201.1461,a class of infinitely divisible multivariate and matrix gamma   distributions and cone-valued generalised gamma convolutions,math.pr math.st stat.th,"classes of multivariate and cone valued infinitely divisible gamma distributions are introduced. particular emphasis is put on the cone-valued case, due to the relevance of infinitely divisible distributions on the positive semi-definite matrices in applications. the cone-valued class of generalised gamma convolutions is studied. in particular, a characterisation in terms of an it\^o-wiener integral with respect to an infinitely divisible random measure associated to the jumps of a l\'evy process is established.   a new example of an infinitely divisible positive definite gamma random matrix is introduced. it has properties which make it appealing for modelling under an infinite divisibility framework. an interesting relation of the moments of the l\'evy measure and the wishart distribution is highlighted which we suppose to be important when considering the limiting distribution of the eigenvalues .",,2012-01-06,,"['victor pérez-abreu', 'robert stelzer']"
1201.2899,parameter estimation using empirical likelihood combined with market   information,stat.me q-fin.pr,"during the last decade levy processes with jumps have received increasing popularity for modelling market behaviour for both derviative pricing and risk management purposes. chan et al. (2009) introduced the use of empirical likelihood methods to estimate the parameters of various diffusion processes via their characteristic functions which are readily avaiable in most cases. return series from the market are used for estimation. in addition to the return series, there are many derivatives actively traded in the market whose prices also contain information about parameters of the underlying process. this observation motivates us, in this paper, to combine the return series and the associated derivative prices observed at the market so as to provide a more reflective estimation with respect to the market movement and achieve a gain of effciency. the usual asymptotic properties, including consistency and asymptotic normality, are established under suitable regularity conditions. simulation and case studies are performed to demonstrate the feasibility and effectiveness of the proposed method.",,2012-01-13,,"['steven kou', 'tony sit', 'zhiliang ying']"
1201.3245,space-time modelling of extreme events,stat.me,"max-stable processes are the natural analogues of the generalized extreme-value distribution for the modelling of extreme events in space and time. under suitable conditions, these processes are asymptotically justified models for maxima of independent replications of random fields, and they are also suitable for the modelling of joint individual extreme measurements over high thresholds. this paper extends a model of schlather (2001) to the space-time framework, and shows how a pairwise censored likelihood can be used for consistent estimation under mild mixing conditions. estimator efficiency is also assessed and the choice of pairs to be included in the pairwise likelihood is discussed based on computations for simple time series models. the ideas are illustrated by an application to hourly precipitation data over switzerland.",10.1111/rssb.12035,2012-01-16,,"['raphaël huser', 'a. c. davison']"
1201.3380,on the relationship between odes and dbns,stat.ap q-bio.qm,"recently, li et al. (bioinformatics 27(19), 2686-91, 2011) proposed a method, called differential equation-based local dynamic bayesian network (deldbn), for reverse engineering gene regulatory networks from time-course data. we commend the authors for an interesting paper that draws attention to the close relationship between dynamic bayesian networks (dbns) and differential equations (des). their central claim is that modifying a dbn to model euler approximations to the gradient rather than expression levels themselves is beneficial for network inference. the empirical evidence provided is based on time-course data with equally-spaced observations. however, as we discuss below, in the particular case of equally-spaced observations, euler approximations and conventional dbns lead to equivalent statistical models that, absent artefacts due to the estimation procedure, yield networks with identical inter-gene edge sets. here, we discuss further the relationship between des and conventional dbns and present new empirical results on unequally spaced data which demonstrate that modelling euler approximations in a dbn can lead to improved network reconstruction.",,2012-01-16,2012-03-02,"['chris. j. oates', 'steven. m. hill', 'sach mukherjee']"
1201.3935,reliability-based design optimization of imperfect shells using adaptive   kriging meta-models,stat.ap,"the optimal and robust design of structures has gained much attention in the past ten years due to the ever increasing need for manufacturers to build robust systems at the lowest cost. reliability-based design optimization (rbdo) allows the analyst to minimize some cost function while ensuring some minimal performances cast as admissible probabilities of failure for a set of performance functions. in order to address real-world problems in which the performance is assessed through computational models (e.g. large scale finite element models) meta-modelling techniques have been developed in the past decade. this paper introduces adaptive kriging surrogate models to solve the rbdo problem. the latter is cast in an augmented space that ""sums up"" the range of the design space and the aleatory uncertainty in the design parameters and the environmental conditions. thus the surrogate model is used (i) for evaluating robust estimates of the probabilities of failure (and for enhancing the computational experimental design by adaptive sampling) in order to achieve the requested accuracy and (ii) for applying the gradient-based optimization algorithm. the approach is applied to the optimal design of imperfect stiffened cylinder shells used in submarine engineering. for this application the performance of the structure is related to buckling which is addressed here by means of the asymptotic numerical method.",,2012-01-18,2012-04-24,"['vincent dubourg', 'jean-marc bourinet', 'bruno sudret']"
1201.5568,dynamic trees for streaming and massive data contexts,stat.me stat.ml,"data collection at a massive scale is becoming ubiquitous in a wide variety of settings, from vast offline databases to streaming real-time information. learning algorithms deployed in such contexts must rely on single-pass inference, where the data history is never revisited. in streaming contexts, learning must also be temporally adaptive to remain up-to-date against unforeseen changes in the data generating mechanism. although rapidly growing, the online bayesian inference literature remains challenged by massive data and transient, evolving data streams. non-parametric modelling techniques can prove particularly ill-suited, as the complexity of the model is allowed to increase with the sample size. in this work, we take steps to overcome these challenges by porting standard streaming techniques, like data discarding and downweighting, into a fully bayesian framework via the use of informative priors and active learning heuristics. we showcase our methods by augmenting a modern non-parametric modelling framework, dynamic trees, and illustrate its performance on a number of practical examples. the end product is a powerful streaming regression and classification tool, whose performance compares favourably to the state-of-the-art.",,2012-01-26,,"['christoforos anagnostopoulos', 'robert b. gramacy']"
1202.0709,mcmc methods for functions: modifying old algorithms to make them faster,stat.co stat.me,"many problems arising in applications result in the need to probe a probability distribution for functions. examples include bayesian nonparametric statistics and conditioned diffusion processes. standard mcmc algorithms typically become arbitrarily slow under the mesh refinement dictated by nonparametric description of the unknown function. we describe an approach to modifying a whole range of mcmc methods, applicable whenever the target measure has density with respect to a gaussian process or gaussian random field reference measure, which ensures that their speed of convergence is robust under mesh refinement. gaussian processes or random fields are fields whose marginal distributions, when evaluated at any finite set of $n$ points, are $\mathbb{r}^n$-valued gaussians. the algorithmic approach that we describe is applicable not only when the desired probability measure has density with respect to a gaussian process or gaussian random field reference measure, but also to some useful non-gaussian reference measures constructed through random truncation. in the applications of interest the data is often sparse and the prior specification is an essential part of the overall modelling strategy. these gaussian-based reference measures are a very flexible modelling tool, finding wide-ranging application. examples are shown in density estimation, data assimilation in fluid mechanics, subsurface geophysics and image registration. the key design principle is to formulate the mcmc method so that it is, in principle, applicable for functions; this may be achieved by use of proposals based on carefully chosen time-discretizations of stochastic dynamical systems which exactly preserve the gaussian reference measure. taking this approach leads to many new algorithms which can be implemented via minor modification of existing algorithms, yet which show enormous speed-up on a wide range of applied problems.",10.1214/13-sts421,2012-02-03,2013-10-10,"['s. l. cotter', 'g. o. roberts', 'a. m. stuart', 'd. white']"
1203.0106,sparsity-promoting bayesian dynamic linear models,stat.me stat.co stat.ml,"sparsity-promoting priors have become increasingly popular over recent years due to an increased number of regression and classification applications involving a large number of predictors. in time series applications where observations are collected over time, it is often unrealistic to assume that the underlying sparsity pattern is fixed. we propose here an original class of flexible bayesian linear models for dynamic sparsity modelling. the proposed class of models expands upon the existing bayesian literature on sparse regression using generalized multivariate hyperbolic distributions. the properties of the models are explored through both analytic results and simulation studies. we demonstrate the model on a financial application where it is shown that it accurately represents the patterns seen in the analysis of stock and derivative data, and is able to detect major events by filtering an artificial portfolio of assets.",,2012-03-01,,"['françois caron', 'luke bornn', 'arnaud doucet']"
1203.1515,multiple change point estimation in stationary ergodic time series,stat.ml cs.it math.it math.st stat.th,"given a heterogeneous time-series sample, the objective is to find points in time (called change points) where the probability distribution generating the data has changed. the data are assumed to have been generated by arbitrary unknown stationary ergodic distributions. no modelling, independence or mixing assumptions are made. a novel, computationally efficient, nonparametric method is proposed, and is shown to be asymptotically consistent in this general framework. the theoretical results are complemented with experimental evaluations.",,2012-03-07,2015-05-11,"['azadeh khaleghi', 'daniil ryabko']"
1203.3083,clustering in networks with the collapsed stochastic block model,stat.co,"an efficient mcmc algorithm is presented to cluster the nodes of a network such that nodes with similar role in the network are clustered together. this is known as block-modelling or block-clustering. the model is the stochastic blockmodel (sbm) with block parameters integrated out. the resulting marginal distribution defines a posterior over the number of clusters and cluster memberships. sampling from this posterior is simpler than from the original sbm as transdimensional mcmc can be avoided. the algorithm is based on the allocation sampler. it requires a prior to be placed on the number of clusters, thereby allowing the number of clusters to be directly estimated by the algorithm, rather than being given as an input parameter. synthetic and real data are used to test the speed and accuracy of the model and algorithm, including the ability to estimate the number of clusters. the algorithm can scale to networks with up to ten thousand nodes and tens of millions of edges.",,2012-03-14,2012-11-08,"['aaron f. mcdaid', 'thomas brendan murphy', 'nial friel', 'neil j hurley']"
1203.3366,enhancing bayesian risk prediction for epidemics using contact tracing,stat.me q-bio.pe,"contact tracing data collected from disease outbreaks has received relatively little attention in the epidemic modelling literature because it is thought to be unreliable: infection sources might be wrongly attributed, or data might be missing due to resource contraints in the questionnaire exercise. nevertheless, these data might provide a rich source of information on disease transmission rate. this paper presents novel methodology for combining contact tracing data with rate-based contact network data to improve posterior precision, and therefore predictive accuracy. we present an advancement in bayesian inference for epidemics that assimilates these data, and is robust to partial contact tracing. using a simulation study based on the british poultry industry, we show how the presence of contact tracing data improves posterior predictive accuracy, and can directly inform a more effective control strategy.",,2012-03-15,,"['chris jewell', 'gareth roberts']"
1203.5446,a bayesian model committee approach to forecasting global solar   radiation,stat.ap cs.lg,"this paper proposes to use a rather new modelling approach in the realm of solar radiation forecasting. in this work, two forecasting models: autoregressive moving average (arma) and neural network (nn) models are combined to form a model committee. the bayesian inference is used to affect a probability to each model in the committee. hence, each model's predictions are weighted by their respective probability. the models are fitted to one year of hourly global horizontal irradiance (ghi) measurements. another year (the test set) is used for making genuine one hour ahead (h+1) out-of-sample forecast comparisons. the proposed approach is benchmarked against the persistence model. the very first results show an improvement brought by this approach.",,2012-03-24,,"['philippe lauret', 'auline rodler', 'marc muselli', 'mathieu david', 'hadja diagne', 'cyril voyant']"
1204.4611,applications of the likelihood theory in finance: modelling and pricing,math.st stat.th,"this paper discusses the connection between mathematical finance and statistical modelling which turns out to be more than a formal mathematical correspondence. we like to figure out how common results and notions in statistics and their meaning can be translated to the world of mathematical finance and vice versa. a lot of similarities can be expressed in terms of lecam's theory for statistical experiments which is the theory of the behaviour of likelihood processes. for positive prices the arbitrage free financial assets fit into filtered experiments. it is shown that they are given by filtered likelihood ratio processes. from the statistical point of view, martingale measures, completeness and pricing formulas are revisited. the pricing formulas for various options are connected with the power functions of tests. for instance the black-scholes price of a european option has an interpretation as bayes risk of a neyman pearson test. under contiguity the convergence of financial experiments and option prices are obtained. in particular, the approximation of ito type price processes by discrete models and the convergence of associated option prices is studied. the result relies on the central limit theorem for statistical experiments, which is well known in statistics in connection with local asymptotic normal (lan) families. as application certain continuous time option prices can be approximated by related discrete time pricing formulas.",,2012-04-20,,"['arnold janssen', 'martin tietje']"
1204.5581,statistical inference for max-stable processes in space and time,stat.me,"max-stable processes have proved to be useful for the statistical modelling of spatial extremes. several representations of max-stable random fields have been proposed in the literature. one such representation is based on a limit of normalized and scaled pointwise maxima of stationary gaussian processes that was first introduced by kabluchko, schlather and de haan (2009).   this paper deals with statistical inference for max-stable space-time processes that are defined in an analogous fashion. we describe pairwise likelihood estimation, where the pairwise density of the process is used to estimate the model parameters and prove strong consistency and asymptotic normality of the parameter estimates for an increasing space-time dimension, i.e., as the joint number of spatial locations and time points tends to infinity. a simulation study shows that the proposed method works well for these models.",,2012-04-25,,"['richard a. davis', 'claudia klüppelberg', 'christina steinkohl']"
1204.5963,on a reliable peer-review process,cs.gt stat.ap stat.ot,"we propose an enhanced peer-review process where the reviewers are encouraged to truthfully disclose their reviews. we start by modelling that process using a bayesian model where the uncertainty regarding the quality of the manuscript is taken into account. after that, we introduce a scoring function to evaluate the reported reviews. under mild assumptions, we show that reviewers strictly maximize their expected scores by telling the truth. we also show how those scores can be used in order to reach consensus.",,2012-04-26,2013-06-26,"['arthur carvalho', 'kate larson']"
1205.1997,model-based clustering in networks with stochastic community finding,stat.co cs.si physics.soc-ph,"in the model-based clustering of networks, blockmodelling may be used to identify roles in the network. we identify a special case of the stochastic block model (sbm) where we constrain the cluster-cluster interactions such that the density inside the clusters of nodes is expected to be greater than the density between clusters. this corresponds to the intuition behind community-finding methods, where nodes tend to clustered together if they link to each other. we call this model stochastic community finding (scf) and present an efficient mcmc algorithm which can cluster the nodes, given the network. the algorithm is evaluated on synthetic data and is applied to a social network of interactions at a karate club and at a monastery, demonstrating how the scf finds the 'ground truth' clustering where sometimes the sbm does not. the scf is only one possible form of constraint or specialization that may be applied to the sbm. in a more supervised context, it may be appropriate to use other specializations to guide the sbm.",,2012-05-09,2012-10-28,"['aaron f. mcdaid', 'brendan thomas murphy', 'nial friel', 'neil j. hurley']"
1205.4159,theory of dependent hierarchical normalized random measures,cs.lg math.st stat.ml stat.th,"this paper presents theory for normalized random measures (nrms), normalized generalized gammas (nggs), a particular kind of nrm, and dependent hierarchical nrms which allow networks of dependent nrms to be analysed. these have been used, for instance, for time-dependent topic modelling. in this paper, we first introduce some mathematical background of completely random measures (crms) and their construction from poisson processes, and then introduce nrms and nggs. slice sampling is also introduced for posterior inference. the dependency operators in poisson processes and for the corresponding crms and nrms is then introduced and posterior inference for the ngg presented. finally, we give dependency and composition results when applying these operators to nrms so they can be used in a network with hierarchical and dependent relations.",,2012-05-18,2012-05-25,"['changyou chen', 'wray buntine', 'nan ding']"
1206.1268,parameter estimation through ignorance,physics.data-an math-ph math.mp stat.ap,"dynamical modelling lies at the heart of our understanding of physical systems. its role in science is deeper than mere operational forecasting, in that it allows us to evaluate the adequacy of the mathematical structure of our models. despite the importance of model parameters, there is no general method of parameter estimation outside linear systems. a new relatively simple method of parameter estimation for nonlinear systems is presented, based on variations in the accuracy of probability forecasts. it is illustrated on the logistic map, the henon map and the 12-d lorenz96 flow, and its ability to outperform linear least squares in these systems is explored at various noise levels and sampling rates. as expected, it is more effective when the forecast error distributions are non-gaussian. the new method selects parameter values by minimizing a proper, local skill score for continuous probability forecasts as a function of the parameter values. this new approach is easier to implement in practice than alternative nonlinear methods based on the geometry of attractors or the ability of the model to shadow the observations. new direct measures of inadequacy in the model, the ""implied ignorance"" and the information deficit are introduced.",10.1103/physreve.86.016213,2012-06-06,,"['hailiang du', 'leonard a. smith']"
1206.2054,maximum a posteriori covariance estimation using a power inverse wishart   prior,stat.me,"the estimation of the covariance matrix is an initial step in many multivariate statistical methods such as principal components analysis and factor analysis, but in many practical applications the dimensionality of the sample space is large compared to the number of samples, and the usual maximum likelihood estimate is poor. typically, improvements are obtained by modelling or regularization. from a practical point of view, these methods are often computationally heavy and rely on approximations. as a fast substitute, we propose an easily calculable maximum a posteriori (map) estimator based on a new class of prior distributions generalizing the inverse wishart prior, discuss its properties, and demonstrate the estimator on simulated and real data.",,2012-06-10,,"['søren feodor nielsen', 'jon sporring']"
1206.3833,"a bayesian spatio-temporal model of panel design data: airborne particle   number concentration in brisbane, australia",stat.ap physics.ao-ph physics.data-an,"this paper outlines a methodology for semi-parametric spatio-temporal modelling of data which is dense in time but sparse in space, obtained from a split panel design, the most feasible approach to covering space and time with limited equipment. the data are hourly averaged particle number concentration (pnc) and were collected, as part of the ultrafine particles from transport emissions and child health (uptech) project. two weeks of continuous measurements were taken at each of a number of government primary schools in the brisbane metropolitan area. the monitoring equipment was taken to each school sequentially. the school data are augmented by data from long term monitoring stations at three locations in brisbane, australia.   fitting the model helps describe the spatial and temporal variability at a subset of the uptech schools and the long-term monitoring sites. the temporal variation is modelled hierarchically with penalised random walk terms, one common to all sites and a term accounting for the remaining temporal trend at each site. parameter estimates and their uncertainty are computed in a computationally efficient approximate bayesian inference environment, r-inla.   the temporal part of the model explains daily and weekly cycles in pnc at the schools, which can be used to estimate the exposure of school children to ultrafine particles (ufps) emitted by vehicles. at each school and long-term monitoring site, peaks in pnc can be attributed to the morning and afternoon rush hour traffic and new particle formation events. the spatial component of the model describes the school to school variation in mean pnc at each school and within each school ground. it is shown how the spatial model can be expanded to identify spatial patterns at the city scale with the inclusion of more spatial locations.",10.1002/env.2597,2012-06-18,2013-02-06,"['sam clifford', 'sama low choy', 'mandana mazaheri', 'farhad salimi', 'lidia morawska', 'kerrie mengsersen']"
1206.4610,manifold relevance determination,cs.lg cs.cv stat.ml,"in this paper we present a fully bayesian latent variable model which exploits conditional nonlinear(in)-dependence structures to learn an efficient latent representation. the latent space is factorized to represent shared and private information from multiple views of the data. in contrast to previous approaches, we introduce a relaxation to the discrete segmentation and allow for a ""softly"" shared latent space. further, bayesian techniques allow us to automatically estimate the dimensionality of the latent spaces. the model is capable of capturing structure underlying extremely high dimensional spaces. this is illustrated by modelling unprocessed images with tenths of thousands of pixels. this also allows us to directly generate novel images from the trained model by sampling from the discovered latent spaces. we also demonstrate the model by prediction of human pose in an ambiguous setting. our bayesian framework allows us to perform disambiguation in a principled manner by including latent space priors which incorporate the dynamic nature of the data.",,2012-06-18,,"['andreas damianou', 'carl ek', 'michalis titsias', 'neil lawrence']"
1206.6839,fitting graphical interaction models to multivariate time series,stat.me,"graphical interaction models have become an important tool for analysing multivariate time series. in these models, the interrelationships among the components of a time series are described by undirected graphs in which the vertices depict the components while the edges indictate possible dependencies between the components. current methods for the identification of the graphical structure are based on nonparametric spectral stimation, which prevents application of common model selection strategies. in this paper, we present a parametric approach for graphical interaction modelling of multivariate stationary time series. the proposed models generalize covariance selection models to the time series setting and are formulated in terms of inverse covariances. we show that these models correspond to vector autoregressive models under conditional independence constraints encoded by undirected graphs. furthermore, we discuss maximum likelihood estimation based on whittle's approximation to the log-likelihood function and propose an iterative method for solving the resulting likelihood equations. the concepts are illustrated by an example.",,2012-06-27,,['michael eichler']
1206.6868,bayesian random fields: the bethe-laplace approximation,cs.lg stat.ml,"while learning the maximum likelihood value of parameters of an undirected graphical model is hard, modelling the posterior distribution over parameters given data is harder. yet, undirected models are ubiquitous in computer vision and text modelling (e.g. conditional random fields). but where bayesian approaches for directed models have been very successful, a proper bayesian treatment of undirected models in still in its infant stages. we propose a new method for approximating the posterior of the parameters given data based on the laplace approximation. this approximation requires the computation of the covariance matrix over features which we compute using the linear response approximation based in turn on loopy belief propagation. we develop the theory for conditional and 'unconditional' random fields with or without hidden variables. in the conditional setting we introduce a new variant of bagging suitable for structured domains. here we run the loopy max-product algorithm on a 'super-graph' composed of graphs for individual models sampled from the posterior and connected by constraints. experiments on real world data validate the proposed methods.",,2012-06-27,,"['max welling', 'sridevi parise']"
1207.0360,a sarimax coupled modelling applied to individual load curves intraday   forecasting,stat.ap,"a dynamic coupled modelling is investigated to take temperature into account in the individual energy consumption forecasting. the objective is both to avoid the inherent complexity of exhaustive sarimax models and to take advantage of the usual linear relation between energy consumption and temperature for thermosensitive customers. we first recall some issues related to individual load curves forecasting. then, we propose and study the properties of a dynamic coupled modelling taking temperature into account as an exogenous contribution and its application to the intraday prediction of energy consumption. finally, these theoretical results are illustrated on a real individual load curve. the authors discuss the relevance of such an approach and anticipate that it could form a substantial alternative to the commonly used methods for energy consumption forecasting of individual customers.",,2012-07-02,,"['sophie bercu', 'frédéric proïa']"
1207.0558,bayesian semi-parametric forecasting of ultrafine particle number   concentration with penalised splines and autoregressive errors,stat.ap physics.ao-ph physics.data-an stat.me,"observational time series data often exhibit both cyclic temporal trends and autocorrelation and may also depend on covariates. as such, there is a need for flexible regression models that are able to capture these trends and model any residual autocorrelation simultaneously. modelling the autocorrelation in the residuals leads to more realistic forecasts than an assumption of independence. in this paper we propose a method which combines spline-based semi-parametric regression modelling with the modelling of auto-regressive errors.   the method is applied to a simulated data set in order to show its efficacy and to ultrafine particle number concentration in helsinki, finland, to show its use in real world problems.",,2012-07-02,2012-09-25,"['sam clifford', 'bjarke mølgaard', 'sama low choy', 'jukka corander', 'kaarle hämeri', 'kerrie mengersen', 'tareq hussein']"
1207.1727,mixtures of shifted asymmetric laplace distributions,stat.me stat.co stat.ml,"a mixture of shifted asymmetric laplace distributions is introduced and used for clustering and classification. a variant of the em algorithm is developed for parameter estimation by exploiting the relationship with the general inverse gaussian distribution. this approach is mathematically elegant and relatively computationally straightforward. our novel mixture modelling approach is demonstrated on both simulated and real data to illustrate clustering and classification applications. in these analyses, our mixture of shifted asymmetric laplace distributions performs favourably when compared to the popular gaussian approach. this work, which marks an important step in the non-gaussian model-based clustering and classification direction, concludes with discussion as well as suggestions for future work.",10.1109/tpami.2013.216,2012-07-06,2012-12-21,"['brian c. franczak', 'ryan p. browne', 'paul d. mcnicholas']"
1207.1916,"how good are matlab, octave and scilab for computational modelling?",cs.ms stat.co,"in this article we test the accuracy of three platforms used in computational modelling: matlab, octave and scilab, running on i386 architecture and three operating systems (windows, ubuntu and mac os). we submitted them to numerical tests using standard data sets and using the functions provided by each platform. a monte carlo study was conducted in some of the datasets in order to verify the stability of the results with respect to small departures from the original input. we propose a set of operations which include the computation of matrix determinants and eigenvalues, whose results are known. we also used data provided by nist (national institute of standards and technology), a protocol which includes the computation of basic univariate statistics (mean, standard deviation and first-lag correlation), linear regression and extremes of probability distributions. the assessment was made comparing the results computed by the platforms with certified values, that is, known results, computing the number of correct significant digits.",,2012-07-08,,"['eliana s. de almeida', 'antonio c. medeiros', 'alejandro c. frery']"
1207.4110,the minimum information principle for discriminative learning,cs.lg stat.ml,"exponential models of distributions are widely used in machine learning for classiffication and modelling. it is well known that they can be interpreted as maximum entropy models under empirical expectation constraints. in this work, we argue that for classiffication tasks, mutual information is a more suitable information theoretic measure to be optimized. we show how the principle of minimum mutual information generalizes that of maximum entropy, and provides a comprehensive framework for building discriminative classiffiers. a game theoretic interpretation of our approach is then given, and several generalization bounds provided. we present iterative algorithms for solving the minimum information problem and its convex dual, and demonstrate their performance on various classiffication tasks. the results show that minimum information classiffiers outperform the corresponding maximum entropy models.",,2012-07-11,,"['amir globerson', 'naftali tishby']"
1207.4125,applying discrete pca in data analysis,cs.lg stat.ml,"methods for analysis of principal components in discrete data have existed for some time under various names such as grade of membership modelling, probabilistic latent semantic analysis, and genotype inference with admixture. in this paper we explore a number of extensions to the common theory, and present some application of these methods to some common statistical tasks. we show that these methods can be interpreted as a discrete version of ica. we develop a hierarchical version yielding components at different levels of detail, and additional techniques for gibbs sampling. we compare the algorithms on a text prediction task using support vector machines, and to information retrieval.",,2012-07-11,,"['wray l. buntine', 'aleks jakulin']"
1207.4145,joint discovery of haplotype blocks and complex trait associations from   snp sequences,q-bio.gn cs.ce stat.me,"haplotypes, the global patterns of dna sequence variation, have important implications for identifying complex traits. recently, blocks of limited haplotype diversity have been discovered in human chromosomes, intensifying the research on modelling the block structure as well as the transitions or co-occurrence of the alleles in these blocks as a way to compress the variability and infer the associations more robustly. the haplotype block structure analysis is typically complicated by the fact that the phase information for each snp is missing, i.e., the observed allele pairs are not given in a consistent order across the sequence. the techniques for circumventing this require additional information, such as family data, or a more complex sequencing procedure. in this paper we present a hierarchical statistical model and the associated learning and inference algorithms that simultaneously deal with the allele ambiguity per locus, missing data, block estimation, and the complex trait association. while the blo structure may differ from the structures inferred by other methods, which use the pedigree information or previously known alleles, the parameters we estimate, including the learned block structure and the estimated block transitions per locus, define a good model of variability in the set. the method is completely datadriven and can detect chron's disease from the snp data taken from the human chromosome 5q31 with the detection rate of 80% and a small error variance.",,2012-07-11,,"['nebojsa jojic', 'vladimir jojic', 'david heckerman']"
1208.3659,thermal effects for shaft-pre-stress on rotor dynamic system,stat.ap,"this paper outlines study behaviour of rotating shaft with high speed under thermal effects. the method of obtaining the frequency response functions of a rotor system with study whirl effect in this revision the raw data obtained from the experimental results (using smart office program) are curve-fitted by theoretical data regenerated from some of the experimental data and simulating it using finite element (ansys 12). (fe) models using the eigen analysis capability were used to simulate the vibration. the results were compared with experimental data show analysis data with acceptable accuracy and performance. the rotating effect causes un-symmetry in the system matrices, resulting in complexity in decoupling the mathematical models of the system for the purpose of modal analysis. different method is therefore required, which can handle general system matrices rather than symmetrical matrices, which is normal for passive structures. mathematical model of the system from the test data can be assembled. the frequency response functions are extracted, campbell diagram are draw and simulated. (fe) is used to carry out such as simulation since it has good capability for eigen analysis and also good graphical facility.   keywords: thermal effects, modelling, campbell diagram, whirl, rotor dynamics.",,2012-08-16,,"['hisham a. h. al-khazali', 'mohamad r. askari']"
1208.4462,accept & reject statement-based uncertainty models,math.pr math.st stat.me stat.th,"we develop a framework for modelling and reasoning with uncertainty based on accept and reject statements about gambles. it generalises the frameworks found in the literature based on statements of acceptability, desirability, or favourability and clarifies their relative position. next to the statement-based formulation, we also provide a translation in terms of preference relations, discuss---as a bridge to existing frameworks---a number of simplified variants, and show the relationship with prevision-based uncertainty models. we furthermore provide an application to modelling symmetry judgements.",10.1016/j.ijar.2014.12.003,2012-08-22,2015-01-23,"['erik quaeghebeur', 'gert de cooman', 'filip hermans']"
1208.5311,assessing the health of richibucto estuary with the latent health factor   index,stat.ap q-bio.qm,"the ability to quantitatively assess the health of an ecosystem is often of great interest to those tasked with monitoring and conserving ecosystems. for decades, research in this area has relied upon multimetric indices of various forms. although indices may be numbers, many are constructed based on procedures that are highly qualitative in nature, thus limiting the quantitative rigour of the practical interpretations made from these indices. the statistical modelling approach to construct the latent health factor index (lhfi) was recently developed to express ecological data, collected to construct conventional multimetric health indices, in a rigorous quantitative model that integrates qualitative features of ecosystem health and preconceived ecological relationships among such features. this hierarchical modelling approach allows (a) statistical inference of health for observed sites and (b) prediction of health for unobserved sites, all accompanied by formal uncertainty statements. thus far, the lhfi approach has been demonstrated and validated on freshwater ecosystems. the goal of this paper is to adapt this approach to modelling estuarine ecosystem health, particularly that of the previously unassessed system in richibucto in new brunswick, canada. field data correspond to biotic health metrics that constitute the azti marine biotic index (ambi) and abiotic predictors preconceived to influence biota. we also briefly discuss related lhfi research involving additional metrics that form the infaunal trophic index (iti). our paper is the first to construct a scientifically sensible model to rigorously identify the collective explanatory capacity of salinity, distance downstream, channel depth, and silt-clay content --- all regarded a priori as qualitatively important abiotic drivers --- towards site health in the richibucto ecosystem.",10.1371/journal.pone.0065697,2012-08-27,2013-06-24,"['margaret wu', 'grace s. chiu', 'lin lu']"
1208.5376,conditional simulation of max-stable processes,stat.me stat.ap,"since many environmental processes such as heat waves or precipitation are spatial in extent, it is likely that a single extreme event affects several locations and the areal modelling of extremes is therefore essential if the spatial dependence of extremes has to be appropriately taken into account. this paper proposes a framework for conditional simulations of max-stable processes and give closed forms for brown-resnick and schlather processes. we test the method on simulated data and give an application to extreme rainfall around zurich and extreme temperature in switzerland. results show that the proposed framework provides accurate conditional simulations and can handle real-sized problems.",,2012-08-27,,"['clément dombry', 'frédéric éyi-minko', 'mathieu ribatet']"
1209.1988,computational information geometry: theory and practice,math.st stat.co stat.th,"this paper lays the foundations for a unified framework for numerically and computationally applying methods drawn from a range of currently distinct geometrical approaches to statistical modelling. in so doing, it extends information geometry from a manifold based approach to one where the simplex is the fundamental geometrical object, thereby allowing applications to models which do not have a fixed dimension or support. finally, it starts to build a computational framework which will act as a proxy for the 'space of all distributions' that can be used, in particular, to investigate model selection and model uncertainty. a varied set of substantive running examples is used to illustrate theoretical and practical aspects of the discussion. further developments are briefly indicated.",,2012-09-10,,"['karim anaya-izquierdo', 'frank critchley', 'paul marriott', 'paul w. vos']"
1209.3730,a bayesian method for the analysis of deterministic and stochastic time   series,astro-ph.im astro-ph.sr physics.data-an stat.ml,"i introduce a general, bayesian method for modelling univariate time series data assumed to be drawn from a continuous, stochastic process. the method accommodates arbitrary temporal sampling, and takes into account measurement uncertainties for arbitrary error models (not just gaussian) on both the time and signal variables. any model for the deterministic component of the variation of the signal with time is supported, as is any model of the stochastic component on the signal and time variables. models illustrated here are constant and sinusoidal models for the signal mean combined with a gaussian stochastic component, as well as a purely stochastic model, the ornstein-uhlenbeck process. the posterior probability distribution over model parameters is determined via monte carlo sampling. models are compared using the ""cross-validation likelihood"", in which the posterior-averaged likelihood for different partitions of the data are combined. in principle this is more robust to changes in the prior than is the evidence (the prior-averaged likelihood). the method is demonstrated by applying it to the light curves of 11 ultra cool dwarf stars, claimed by a previous study to show statistically significant variability. this is reassessed here by calculating the cross-validation likelihood for various time series models, including a null hypothesis of no variability beyond the error bars. 10 of 11 light curves are confirmed as being significantly variable, and one of these seems to be periodic, with two plausible periods identified. another object is best described by the ornstein-uhlenbeck process, a conclusion which is obviously limited to the set of models actually tested.",10.1051/0004-6361/201220109,2012-09-17,2012-10-23,['c. a. l. bailer-jones']
1209.5561,supervised blockmodelling,cs.lg cs.si stat.ml,"collective classification models attempt to improve classification performance by taking into account the class labels of related instances. however, they tend not to learn patterns of interactions between classes and/or make the assumption that instances of the same class link to each other (assortativity assumption). blockmodels provide a solution to these issues, being capable of modelling assortative and disassortative interactions, and learning the pattern of interactions in the form of a summary network. the supervised blockmodel provides good classification performance using link structure alone, whilst simultaneously providing an interpretable summary of network interactions to allow a better understanding of the data. this work explores three variants of supervised blockmodels of varying complexity and tests them on four structurally different real world networks.",,2012-09-25,,['leto peel']
1210.0685,local stability and robustness of sparse dictionary learning in the   presence of noise,stat.ml cs.lg,"a popular approach within the signal processing and machine learning communities consists in modelling signals as sparse linear combinations of atoms selected from a learned dictionary. while this paradigm has led to numerous empirical successes in various fields ranging from image to audio processing, there have only been a few theoretical arguments supporting these evidences. in particular, sparse coding, or sparse dictionary learning, relies on a non-convex procedure whose local minima have not been fully analyzed yet. in this paper, we consider a probabilistic model of sparse signals, and show that, with high probability, sparse coding admits a local minimum around the reference dictionary generating the signals. our study takes into account the case of over-complete dictionaries and noisy signals, thus extending previous work limited to noiseless settings and/or under-complete dictionaries. the analysis we conduct is non-asymptotic and makes it possible to understand how the key quantities of the problem, such as the coherence or the level of noise, can scale with respect to the dimension of the signals, the number of atoms, the sparsity and the number of observations.",,2012-10-02,,"['rodolphe jenatton', 'rémi gribonval', 'francis bach']"
1210.2503,gaussian process modelling of multiple short time series,stat.ml q-bio.qm stat.me,"we present techniques for effective gaussian process (gp) modelling of multiple short time series. these problems are common when applying gp models independently to each gene in a gene expression time series data set. such sets typically contain very few time points. naive application of common gp modelling techniques can lead to severe over-fitting or under-fitting in a significant fraction of the fitted models, depending on the details of the data set. we propose avoiding over-fitting by constraining the gp length-scale to values that focus most of the energy spectrum to frequencies below the nyquist frequency corresponding to the sampling frequency in the data set. under-fitting can be avoided by more informative priors on observation noise. combining these methods allows applying gp methods reliably automatically to large numbers of independent instances of short time series. this is illustrated with experiments with both synthetic data and real gene expression data.",,2012-10-09,,"['hande topa', 'antti honkela']"
1210.3060,markov kernels and the conditional extreme value model,math.st stat.th,"the classical approach to multivariate extreme value modelling assumes that the joint distribution belongs to a multivariate domain of attraction. this requires each marginal distribution be individually attracted to a univariate extreme value distribution. an apparently more flexible extremal model for multivariate data was proposed by heffernan and tawn under which not all the components are required to belong to an extremal domain of attraction but assumes instead the existence of an asymptotic approximation to the conditional distribution of the random vector given one of the components is extreme. combined with the knowledge that the conditioning component belongs to a univariate domain of attraction, this leads to an approximation of the probability of certain risk regions. the original focus on conditional distributions had technical drawbacks but is natural in several contexts. we place this approach in the context of the more general approach using convergence of measures and multivariate regular variation on cones.",,2012-10-10,,"['sidney resnick', 'david zeber']"
1210.3087,therapeutic hypothermia: quantification of the transition of core body   temperature using the flexible mixture bent-cable model for longitudinal data,stat.me q-bio.to,"by reducing core body temperature, t_c, induced hypothermia is a therapeutic tool to prevent brain damage resulting from physical trauma. however, all physiological systems begin to slow down due to hypothermia that in turn can result in increased risk of mortality. therefore, quantification of the transition of t_c to early hypothermia is of great clinical interest. conceptually, t_c may exhibit an either gradual or abrupt transition. bent-cable regression is an appealing statistical tool to model such data due to the model's flexibility and greatly interpretable regression coefficients. it handles more flexibly models that traditionally have been handled by low-order polynomial models (for gradual transition) or piecewise linear changepoint models (for abrupt change). we consider a rat model for humans to quantify the temporal trend of t_c to primarily address the question: what is the critical time point associated with a breakdown in the compensatory mechanisms following the start of hypothermia therapy? to this end, we develop a bayesian modelling framework for bent-cable regression of longitudinal data to simultaneously account for gradual and abrupt transitions. our analysis reveals that: (a) about 39% of rats exhibit a gradual transition in t_c; (b) the critical time point is approximately the same regardless of transition type; (c) both transition types show a significant increase of t_c followed by a significant decrease.",10.1111/anzs.12047,2012-10-10,2013-04-14,"['shahedul a khan', 'grace s chiu', 'joel a dubin']"
1210.3831,graphical modelling in genetics and systems biology,stat.me q-bio.mn stat.ml,"graphical modelling has a long history in statistics as a tool for the analysis of multivariate data, starting from wright's path analysis and gibbs' applications to statistical physics at the beginning of the last century. in its modern form, it was pioneered by lauritzen and wermuth and pearl in the 1980s, and has since found applications in fields as diverse as bioinformatics, customer satisfaction surveys and weather forecasts.   genetics and systems biology are unique among these fields in the dimension of the data sets they study, which often contain several hundreds of variables and only a few tens or hundreds of observations. this raises problems in both computational complexity and the statistical significance of the resulting networks, collectively known as the ""curse of dimensionality"". furthermore, the data themselves are difficult to model correctly due to the limited understanding of the underlying mechanisms. in the following, we will illustrate how such challenges affect practical graphical modelling and some possible solutions.",,2012-10-14,2014-01-12,['marco scutari']
1210.3851,an introduction to particle integration methods: with applications to   risk and insurance,q-fin.cp math.st q-fin.rm stat.th,"interacting particle methods are increasingly used to sample from complex and high-dimensional distributions. these stochastic particle integration techniques can be interpreted as an universal acceptance-rejection sequential particle sampler equipped with adaptive and interacting recycling mechanisms. practically, the particles evolve randomly around the space independently and to each particle is associated a positive potential function. periodically, particles with high potentials duplicate at the expense of low potential particle which die. this natural genetic type selection scheme appears in numerous applications in applied probability, physics, bayesian statistics, signal processing, biology, and information engineering. it is the intention of this paper to introduce them to risk modeling. from a purely mathematical point of view, these stochastic samplers can be interpreted as feynman-kac particle integration methods. these functional models are natural mathematical extensions of the traditional change of probability measures, commonly used to design an importance sampling strategy. in this article, we provide a brief introduction to the stochastic modeling and the theoretical analysis of these particle algorithms. then we conclude with an illustration of a subset of such methods to resolve important risk measure and capital estimation in risk and insurance modelling.",,2012-10-14,2012-10-29,"['p. del moral', 'g. w. peters', 'ch. vergé']"
1210.4844,plackett-luce regression: a new bayesian model for polychotomous data,stat.me stat.ap stat.co,"multinomial logistic regression is one of the most popular models for modelling the effect of explanatory variables on a subject choice between a set of specified options. this model has found numerous applications in machine learning, psychology or economy. bayesian inference in this model is non trivial and requires, either to resort to a metropolishastings algorithm, or rejection sampling within a gibbs sampler. in this paper, we propose an alternative model to multinomial logistic regression. the model builds on the plackett-luce model, a popular model for multiple comparisons. we show that the introduction of a suitable set of auxiliary variables leads to an expectation-maximization algorithm to find maximum a posteriori estimates of the parameters. we further provide a full bayesian treatment by deriving a gibbs sampler, which only requires to sample from highly standard distributions. we also propose a variational approximate inference scheme. all are very simple to implement. one property of our plackett-luce regression model is that it learns a sparse set of feature weights. we compare our method to sparse bayesian multinomial logistic regression and show that it is competitive, especially in presence of polychotomous data.",,2012-10-16,,"['cedric archambeau', 'francois caron']"
1210.7485,approximation multivariate distribution with pair copula using the   orthonormal polynomial and legendre multiwavelets basis functions,stat.co,"in this paper, we concentrate on new methodologies for copulas introduced and developed by joe, cooke, bedford, kurowica, daneshkhah and others on the new class of graphical models called vines as a way of constructing higher dimensional distributions. we develop the approximation method presented by bedford et al (2012) at which they show that any $n$-dimensional copula density can be approximated arbitrarily well pointwise using a finite parameter set of 2-dimensional copulas in a vine or pair-copula construction. our constructive approach involves the use of minimum information copulas that can be specified to any required degree of precision based on the available data or experts' judgements. by using this method, we are able to use a fixed finite dimensional family of copulas to be employed in a vine construction, with the promise of a uniform level of approximation.   the basic idea behind this method is to use a two-dimensional ordinary polynomial series to approximate any log-density of a bivariate copula function by truncating the series at an appropriate point. we present an alternative approximation of the multivariate distribution of interest by considering orthonormal polynomial and legendre multiwavelets as the basis functions. we show the derived approximations are more precise and computationally faster with better properties than the one proposed by bedford et al. (2012). we then apply our method to modelling a dataset of norwegian financial data that was previously analysed in the series of papers, and finally compare our results by them.",,2012-10-28,,"['alireza daneshkhah', 'golamali parham', 'omid chatrabgoun', 'm. jokar']"
1211.0174,laplace approximation for logistic gaussian process density estimation   and regression,stat.co stat.me stat.ml,"logistic gaussian process (lgp) priors provide a flexible alternative for modelling unknown densities. the smoothness properties of the density estimates can be controlled through the prior covariance structure of the lgp, but the challenge is the analytically intractable inference. in this paper, we present approximate bayesian inference for lgp density estimation in a grid using laplace's method to integrate over the non-gaussian posterior distribution of latent function values and to determine the covariance function parameters with type-ii maximum a posteriori (map) estimation. we demonstrate that laplace's method with map is sufficiently fast for practical interactive visualisation of 1d and 2d densities. our experiments with simulated and real 1d data sets show that the estimation accuracy is close to a markov chain monte carlo approximation and state-of-the-art hierarchical infinite gaussian mixture models. we also construct a reduced-rank approximation to speed up the computations for dense 2d grids, and demonstrate density regression with the proposed laplace approach.",,2012-11-01,2013-10-07,"['jaakko riihimäki', 'aki vehtari']"
1211.0358,deep gaussian processes,stat.ml cs.lg math.pr,in this paper we introduce deep gaussian process (gp) models. deep gps are a deep belief network based on gaussian process mappings. the data is modeled as the output of a multivariate gp. the inputs to that gaussian process are then governed by another gp. a single layer model is equivalent to a standard gp or the gp latent variable model (gp-lvm). we perform inference in the model by approximate variational marginalization. this results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. our fully bayesian treatment allows for the application of deep models even when data is scarce. model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.,,2012-11-01,2013-03-22,"['andreas c. damianou', 'neil d. lawrence']"
1211.0906,algorithm runtime prediction: methods & evaluation,cs.ai cs.lg cs.pf stat.ml,"perhaps surprisingly, it is possible to predict how long an algorithm will take to run on a previously unseen input, using machine learning techniques to build a model of the algorithm's runtime as a function of problem-specific instance features. such models have important applications to algorithm analysis, portfolio-based algorithm selection, and the automatic configuration of parameterized algorithms. over the past decade, a wide variety of techniques have been studied for building such models. here, we describe extensions and improvements of existing models, new families of models, and -- perhaps most importantly -- a much more thorough treatment of algorithm parameters as model inputs. we also comprehensively describe new and existing features for predicting algorithm runtime for propositional satisfiability (sat), travelling salesperson (tsp) and mixed integer programming (mip) problems. we evaluate these innovations through the largest empirical analysis of its kind, comparing to a wide range of runtime modelling techniques from the literature. our experiments consider 11 algorithms and 35 instance distributions; they also span a very wide range of sat, mip, and tsp instances, with the least structured having been generated uniformly at random and the most structured having emerged from real industrial applications. overall, we demonstrate that our new models yield substantially better runtime predictions than previous approaches in terms of their generalization to new problem instances, to new algorithms from a parameterized space, and to both simultaneously.",,2012-11-05,2013-10-26,"['frank hutter', 'lin xu', 'holger h. hoos', 'kevin leyton-brown']"
1211.1328,random walk kernels and learning curves for gaussian process regression   on random graphs,stat.ml cond-mat.dis-nn cond-mat.stat-mech cs.lg,"we consider learning on graphs, guided by kernels that encode similarity between vertices. our focus is on random walk kernels, the analogues of squared exponential kernels in euclidean spaces. we show that on large, locally treelike, graphs these have some counter-intuitive properties, specifically in the limit of large kernel lengthscales. we consider using these kernels as covariance matrices of e.g.\ gaussian processes (gps). in this situation one typically scales the prior globally to normalise the average of the prior variance across vertices. we demonstrate that, in contrast to the euclidean case, this generically leads to significant variation in the prior variance across vertices, which is undesirable from the probabilistic modelling point of view. we suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for gaussian process regression. numerical calculations as well as novel theoretical predictions for the learning curves using belief propagation make it clear that one obtains distinctly different probabilistic models depending on the choice of normalisation. our method for predicting the learning curves using belief propagation is significantly more accurate than previous approximations and should become exact in the limit of large random graphs.",,2012-11-06,2013-09-30,"['matthew urry', 'peter sollich']"
1211.1717,bayesian learning and predictability in a stochastic nonlinear dynamical   model,stat.ap,"bayesian inference methods are applied within a bayesian hierarchical modelling framework to the problems of joint state and parameter estimation, and of state forecasting. we explore and demonstrate the ideas in the context of a simple nonlinear marine biogeochemical model. a novel approach is proposed to the formulation of the stochastic process model, in which ecophysiological properties of plankton communities are represented by autoregressive stochastic processes. this approach captures the effects of changes in plankton communities over time, and it allows the incorporation of literature metadata on individual species into prior distributions for process model parameters. the approach is applied to a case study at ocean station papa, using particle markov chain monte carlo computational techniques. the results suggest that, by drawing on objective prior information, it is possible to extract useful information about model state and a subset of parameters, and even to make useful long-term forecasts, based on sparse and noisy observations.",10.1890/12-0312.1,2012-11-07,,"['john parslow', 'noel cressie', 'edward p. campbell', 'emlyn jones', 'lawrence murray']"
1211.3602,on mixtures of skew normal and skew t-distributions,stat.me,"finite mixture of skew distributions have emerged as an effective tool in modelling heterogeneous data with asymmetric features. with various proposals appearing rapidly in the recent years, which are similar but not identical, the connections between them and their relative performance becomes rather unclear. this paper aims to provide a concise overview of these developments by presenting a systematic classification of the existing skew distributions into four types, thereby clarifying their close relationships. this also aids in understanding the link between some of the proposed expectation-maximization (em) based algorithms for the computation of the maximum likelihood estimates of the parameters of the models. the final part of this paper presents an illustration of the performance of these mixture models in clustering a real dataset, relative to other non-elliptically contoured clustering methods and associated algorithms for their implementation.",10.1007/s11634-013-0132-8,2012-11-15,2013-05-28,"['sharon x. lee', 'geoffrey j. mclachlan']"
1211.5290,emmix-uskew: an r package for fitting mixtures of multivariate skew   t-distributions via the em algorithm,stat.co stat.me,"this paper describes an algorithm for fitting finite mixtures of unrestricted multivariate skew t (fm-umst) distributions. the package emmix-uskew implements a closed-form expectation-maximization (em) algorithm for computing the maximum likelihood (ml) estimates of the parameters for the (unrestricted) fm-mst model in r. emmix-uskew also supports visualization of fitted contours in two and three dimensions, and random sample generation from a specified fm-umst distribution.   finite mixtures of skew t-distributions have proven to be useful in modelling heterogeneous data with asymmetric and heavy tail behaviour, for example, datasets from flow cytometry. in recent years, various versions of mixtures with multivariate skew t (mst) distributions have been proposed. however, these models adopted some restricted characterizations of the component mst distributions so that the e-step of the em algorithm can be evaluated in closed form. this paper focuses on mixtures with unrestricted mst components, and describes an iterative algorithm for the computation of the ml estimates of its model parameters.   the usefulness of the proposed algorithm is demonstrated in three applications to real data sets. the first example illustrates the use of the main function fmmst in the package by fitting a mst distribution to a bivariate unimodal flow cytometric sample. the second example fits a mixture of mst distributions to the australian institute of sport (ais) data, and demonstrate that emmix-uskew can provide better clustering results than mixtures with restricted mst components. in the third example, emmix-uskew is applied to classify cells in a trivariate flow cytometric dataset. comparisons with other available methods suggests that the emmix-uskew result achieved a lower misclassification rate with respect to the labels given by benchmark gating analysis.",,2012-11-22,2013-03-27,"['sharon x. lee', 'geoffrey j. mclachlan']"
1211.5620,pair-copula bayesian networks,stat.me,"pair-copula bayesian networks (pcbns) are a novel class of multivariate statistical models, which combine the distributional flexibility of pair-copula constructions (pccs) with the parsimony of conditional independence models associated with directed acyclic graphs (dag). we are first to provide generic algorithms for random sampling and likelihood inference in arbitrary pcbns as well as for selecting orderings of the parents of the vertices in the underlying graphs. model selection of the dag is facilitated using a version of the well-known pc algorithm which is based on a novel test for conditional independence of random variables tailored to the pcc framework. a simulation study shows the pc algorithm's high aptitude for structure estimation in non-gaussian pcbns. the proposed methods are finally applied to modelling financial return data.",,2012-11-23,,"['alexander bauer', 'claudia czado']"
1301.1318,efficient eigen-updating for spectral graph clustering,stat.ml,"partitioning a graph into groups of vertices such that those within each group are more densely connected than vertices assigned to different groups, known as graph clustering, is often used to gain insight into the organisation of large scale networks and for visualisation purposes. whereas a large number of dedicated techniques have been recently proposed for static graphs, the design of on-line graph clustering methods tailored for evolving networks is a challenging problem, and much less documented in the literature. motivated by the broad variety of applications concerned, ranging from the study of biological networks to the analysis of networks of scientific references through the exploration of communications networks such as the world wide web, it is the main purpose of this paper to introduce a novel, computationally efficient, approach to graph clustering in the evolutionary context. namely, the method promoted in this article can be viewed as an incremental eigenvalue solution for the spectral clustering method described by ng. et al. (2001). the incremental eigenvalue solution is a general technique for finding the approximate eigenvectors of a symmetric matrix given a change. as well as outlining the approach in detail, we present a theoretical bound on the quality of the approximate eigenvectors using perturbation theory. we then derive a novel spectral clustering algorithm called incremental approximate spectral clustering (iasc). the iasc algorithm is simple to implement and its efficacy is demonstrated on both synthetic and real datasets modelling the evolution of a hiv epidemic, a citation network and the purchase history graph of an e-commerce website.",,2013-01-07,2014-01-27,"['charanpal dhanjal', 'romaric gaudel', 'stéphan clémençon']"
1301.1817,a toolbox for fitting complex spatial point process models using   integrated nested laplace approximation (inla),stat.ap,"this paper develops methodology that provides a toolbox for routinely fitting complex models to realistic spatial point pattern data. we consider models that are based on log-gaussian cox processes and include local interaction in these by considering constructed covariates. this enables us to use integrated nested laplace approximation and to considerably speed up the inferential task. in addition, methods for model comparison and model assessment facilitate the modelling process. the performance of the approach is assessed in a simulation study. to demonstrate the versatility of the approach, models are fitted to two rather different examples, a large rainforest data set with covariates and a point pattern with multiple marks.",10.1214/11-aoas530,2013-01-09,,"['janine b. illian', 'sigrunn h. sørbye', 'håvard rue']"
1301.2318,statistical modeling in continuous speech recognition (csr)(invited   talk),cs.lg cs.ai stat.ml,"automatic continuous speech recognition (csr) is sufficiently mature that a variety of real world applications are now possible including large vocabulary transcription and interactive spoken dialogues. this paper reviews the evolution of the statistical modelling techniques which underlie current-day systems, specifically hidden markov models (hmms) and n-grams. starting from a description of the speech signal and its parameterisation, the various modelling assumptions and their consequences are discussed. it then describes various techniques by which the effects of these assumptions can be mitigated. despite the progress that has been made, the limitations of current modelling techniques are still evident. the paper therefore concludes with a brief review of some of the more fundamental modelling work now in progress.",,2013-01-10,,['steve young']
1301.3558,model selection for gaussian mixture models,stat.me math.st stat.ml stat.th,"this paper is concerned with an important issue in finite mixture modelling, the selection of the number of mixing components. we propose a new penalized likelihood method for model selection of finite multivariate gaussian mixture models. the proposed method is shown to be statistically consistent in determining of the number of components. a modified em algorithm is developed to simultaneously select the number of components and to estimate the mixing weights, i.e. the mixing probabilities, and unknown parameters of gaussian distributions. simulations and a real data analysis are presented to illustrate the performance of the proposed method.",,2013-01-15,,"['tao huang', 'heng peng', 'kun zhang']"
1301.3581,d-optimal factorial designs under generalized linear models,math.st stat.th,"generalized linear models (glms) have been used widely for modelling the mean response both for discrete and continuous random variables with an emphasis on categorical response. recently yang, mandal and majumdar (2013) considered full factorial and fractional factorial locally d-optimal designs for binary response and two-level experimental factors. in this paper, we extend their results to a general setup with response belonging to a single-parameter exponential family and for multi-level predictors.",,2013-01-15,2013-05-03,"['jie yang', 'abhyuday mandal']"
1301.3863,yggdrasil - a statistical package for learning split models,cs.ai cs.ms stat.me,"there are two main objectives of this paper. the first is to present a statistical framework for models with context specific independence structures, i.e., conditional independences holding only for sepcific values of the conditioning variables. this framework is constituted by the class of split models. split models are extension of graphical models for contigency tables and allow for a more sophisticiated modelling than graphical models. the treatment of split models include estimation, representation and a markov property for reading off those independencies holding in a specific context. the second objective is to present a software package named yggdrasil which is designed for statistical inference in split models, i.e., for learning such models on the basis of data.",,2013-01-16,,['soren hojsgaard']
1301.4144,non-parametric bayesian modelling of digital gene expression data,q-bio.qm q-bio.gn stat.ap stat.ml,"next-generation sequencing technologies provide a revolutionary tool for generating gene expression data. starting with a fixed rna sample, they construct a library of millions of differentially abundant short sequence tags or ""reads"", which constitute a fundamentally discrete measure of the level of gene expression. a common limitation in experiments using these technologies is the low number or even absence of biological replicates, which complicates the statistical analysis of digital gene expression data. analysis of this type of data has often been based on modified tests originally devised for analysing microarrays; both these and even de novo methods for the analysis of rna-seq data are plagued by the common problem of low replication. we propose a novel, non-parametric bayesian approach for the analysis of digital gene expression data. we begin with a hierarchical model for modelling over-dispersed count data and a blocked gibbs sampling algorithm for inferring the posterior distribution of model parameters conditional on these counts. the algorithm compensates for the problem of low numbers of biological replicates by clustering together genes with tag counts that are likely sampled from a common distribution and using this augmented sample for estimating the parameters of this distribution. the number of clusters is not decided a priori, but it is inferred along with the remaining model parameters. we demonstrate the ability of this approach to model biological data with high fidelity by applying the algorithm on a public dataset obtained from cancerous and non-cancerous neural tissues.",10.4172/jcsb.1000131,2013-01-17,,"['dimitrios v. vavoulis', 'julian gough']"
1301.5007,ergodicity and scaling limit of a constrained multivariate hawkes   process,stat.ap q-fin.cp q-fin.tr,"we introduce a multivariate hawkes process with constraints on its conditional density. it is a multivariate point process with conditional intensity similar to that of a multivariate hawkes process but certain events are forbidden with respect to boundary conditions on a multidimensional constraint variable, whose evolution is driven by the point process. we study this process in the special case where the fertility function is exponential so that the process is entirely described by an underlying markov chain, which includes the constraint variable. some conditions on the parameters are established to ensure the ergodicity of the chain. moreover, scaling limits are derived for the integrated point process. this study is primarily motivated by the stochastic modelling of a limit order book for high frequency financial data analysis.",,2013-01-18,2014-02-13,"['ban zheng', 'françois roueff', 'frédéric abergel']"
1302.0893,probabilistic quantitative precipitation forecasting using ensemble   model output statistics,stat.ap physics.ao-ph,"statistical post-processing of dynamical forecast ensembles is an essential component of weather forecasting. in this article, we present a post-processing method that generates full predictive probability distributions for precipitation accumulations based on ensemble model output statistics (emos). we model precipitation amounts by a generalized extreme value distribution that is left-censored at zero. this distribution permits modelling precipitation on the original scale without prior transformation of the data. a closed form expression for its continuous rank probability score can be derived and permits computationally efficient model fitting. we discuss an extension of our approach that incorporates further statistics characterizing the spatial variability of precipitation amounts in the vicinity of the location of interest. the proposed emos method is applied to daily 18-h forecasts of 6-h accumulated precipitation over germany in 2011 using the cosmo-de ensemble prediction system operated by the german meteorological service. it yields calibrated and sharp predictive distributions and compares favourably with extended logistic regression and bayesian model averaging which are state of the art approaches for precipitation post-processing. the incorporation of neighbourhood information further improves predictive performance and turns out to be a useful strategy to account for displacement errors of the dynamical forecasts in a probabilistic forecasting framework.",10.1002/qj.2183,2013-02-04,,['michael scheuerer']
1302.1571,score and information for recursive exponential models with incomplete   data,stat.me cs.ai,"recursive graphical models usually underlie the statistical modelling concerning probabilistic expert systems based on bayesian networks. this paper defines a version of these models, denoted as recursive exponential models, which have evolved by the desire to impose sophisticated domain knowledge onto local fragments of a model. besides the structural knowledge, as specified by a given model, the statistical modelling may also include expert opinion about the values of parameters in the model. it is shown how to translate imprecise expert knowledge into approximately conjugate prior distributions. based on possibly incomplete data, the score and the observed information are derived for these models. this accounts for both the traditional score and observed information, derived as derivatives of the log-likelihood, and the posterior score and observed information, derived as derivatives of the log-posterior distribution. throughout the paper the specialization into recursive graphical models is accounted for by a simple example.",,2013-02-06,,['bo thiesson']
1302.2373,parsimonious skew mixture models for model-based clustering and   classification,stat.me,"in recent work, robust mixture modelling approaches using skewed distributions have been explored to accommodate asymmetric data. we introduce parsimony by developing skew-t and skew-normal analogues of the popular gpcm family that employ an eigenvalue decomposition of a positive-semidefinite matrix. the methods developed in this paper are compared to existing models in both an unsupervised and semi-supervised classification framework. parameter estimation is carried out using the expectation-maximization algorithm and models are selected using the bayesian information criterion. the efficacy of these extensions is illustrated on simulated and benchmark clustering data sets.",10.1016/j.csda.2013.07.008,2013-02-10,,"['irene vrbik', 'paul d. mcnicholas']"
1302.4245,gaussian process kernels for pattern discovery and extrapolation,stat.ml cs.ai stat.me,"gaussian processes are rich distributions over functions, which provide a bayesian nonparametric approach to smoothing and interpolation. we introduce simple closed form kernels that can be used with gaussian processes to discover patterns and enable extrapolation. these kernels are derived by modelling a spectral density -- the fourier transform of a kernel -- with a gaussian mixture. the proposed kernels support a broad class of stationary covariances, but gaussian process inference remains simple and analytic. we demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric co2 trends and airline passenger data. we also show that we can reconstruct standard covariances within our framework.",,2013-02-18,2013-12-31,"['andrew gordon wilson', 'ryan prescott adams']"
1302.4404,analysis of forensic dna mixtures with artefacts,stat.me stat.ap,"dna is now routinely used in criminal investigations and court cases, although dna samples taken at crime scenes are of varying quality and therefore present challenging problems for their interpretation. we present a statistical model for the quantitative peak information obtained from an electropherogram (epg) of a forensic dna sample and illustrate its potential use for the analysis of criminal cases. in contrast to most previously used methods, we directly model the peak height information and incorporates important artefacts associated with the production of the epg. our model has a number of unknown parameters, and we show that these can be estimated by the method of maximum likelihood in the presence of multiple unknown contributors, and their approximate standard errors calculated; the computations exploit a bayesian network representation of the model. a case example from a uk trial, as reported in the literature, is used to illustrate the efficacy and use of the model, both in finding likelihood ratios to quantify the strength of evidence, and in the deconvolution of mixtures for the purpose of finding likely profiles of one or more unknown contributors to a dna sample. our model is readily extended to simultaneous analysis of more than one mixture as illustrated in a case example. we show that combination of evidence from several samples may give an evidential strength close to that of a single source trace and thus modelling of peak height information provides for a potentially very efficient mixture analysis.",,2013-02-18,2013-12-10,"['r. g. cowell', 't. graversen', 's. lauritzen', 'j. mortera']"
1302.5714,bayes linear variance structure learning for inspection of large scale   physical systems,stat.me,"modelling of inspection data for large scale physical systems is critical to assessment of their integrity. we present a general method for inference about system state and associated model variance structure from spatially distributed time series which are typically short, irregular, incomplete and not directly observable. bayes linear analysis simplifies parameter estimation and avoids often-unrealistic distributional assumptions. second-order exchangeability judgements facilitate variance learning for sparse inspection time-series. the model is applied to inspection data for minimum wall thickness from corroding pipe-work networks on a full-scale offshore platform, and shown to give materially different forecasts of remnant life compared to an equivalent model neglecting variance learning.",,2013-02-22,,"['david randell', 'michael goldstein', 'philip jonathan']"
1302.5849,pathways-driven sparse regression identifies pathways and genes   associated with high-density lipoprotein cholesterol in two asian cohorts,stat.me stat.ap,"standard approaches to analysing data in genome-wide association studies (gwas) ignore any potential functional relationships between genetic markers. in contrast gene pathways analysis uses prior information on functional structure within the genome to identify pathways associated with a trait of interest. in a second step, important single nucleotide polymorphisms (snps) or genes may be identified within associated pathways. most pathways methods begin by testing snps one at a time, and so fail to capitalise on the potential advantages inherent in a multi-snp, joint modelling approach. here we describe a dual-level, sparse regression model for the simultaneous identification of pathways, genes and snps associated with a quantitative trait. our method takes account of various factors specific to the joint modelling of pathways with genome-wide data, including widespread correlation between genetic predictors, and the fact that variants may overlap multiple pathways. we use a resampling strategy that exploits finite sample variability to provide robust rankings for pathways, snps and genes. we test our method through simulation, and use it to perform pathways-driven snp selection in a search for pathways, genes and snps associated with variation in serum high-density lipoprotein cholesterol (hdlc) levels in two separate gwas cohorts of asian adults. by comparing results from both cohorts we identify a number of candidate pathways including those associated with cardiomyopathy, and t cell receptor and ppar signalling. highlighted genes include those associated with the l-type calcium channel, adenylate cyclase, integrin, laminin, mapk signalling and immune function.",,2013-02-23,,"['m. silver', 'p. chen', 'l. ruoying', 'c. y. cheng', 't. y. wong', 'e. tai', 'y. y. teo', 'g. montana']"
1302.5856,a press statistic for two-block partial least squares regression,stat.me,"predictive modelling of multivariate data where both the covariates and responses are high-dimensional is becoming an increasingly popular task in many data mining applications. partial least squares (pls) regression often turns out to be a useful model in these situations since it performs dimensionality reduction by assuming the existence of a small number of latent factors that may explain the linear dependence between input and output. in practice, the number of latent factors to be retained, which controls the complexity of the model and its predictive ability, has to be carefully selected. typically this is done by cross validating a performance measure, such as the predictive error. although cross validation works well in many practical settings, it can be computationally expensive. various extensions to pls have also been proposed for regularising the pls solution and performing simultaneous dimensionality reduction and variable selection, but these come at the expense of additional complexity parameters that also need to be tuned by cross-validation. in this paper we derive a computationally efficient alternative to leave-one-out cross validation (loocv), a predicted sum of squares (press) statistic for two-block pls. we show that the press is nearly identical to loocv but has the computational expense of only a single pls model fit. examples of the press for selecting the number of latent factors and regularisation parameters are provided.",10.1109/ukci.2010.5625583,2013-02-23,,"['brian mcwilliams', 'giovanni montana']"
1303.1219,analysis of partially observed networks via exponential-family random   network models,stat.me,"exponential-family random network (ern) models specify a joint representation of both the dyads of a network and nodal characteristics. this class of models allow the nodal characteristics to be modelled as stochastic processes, expanding the range and realism of exponential-family approaches to network modelling. in this paper we develop a theory of inference for ern models when only part of the network is observed, as well as specific methodology for missing data, including non-ignorable mechanisms for network-based sampling designs and for latent class models. in particular, we consider data collected via contact tracing, of considerable importance to infectious disease epidemiology and public health.",,2013-03-05,,"['ian e. fellows', 'mark s. handcock']"
1303.2316,capturing patterns via parsimonious t mixture models,stat.me stat.ap,this paper exploits a simplified version of the mixture of multivariate t-factor analyzers (mtfa) for robust mixture modelling and clustering of high-dimensional data that frequently contain a number of outliers. two classes of eight parsimonious t mixture models are introduced and computation of maximum likelihood estimates of parameters is achieved using the alternating expectation conditional maximization (aecm) algorithm. the usefulness of the methodology is illustrated through applications of image compression and compact facial representation.,,2013-03-10,,"['tsung-i lin', 'paul d. mcnicholas', 'hsiu j. ho']"
1303.2739,machine learning for bioclimatic modelling,cs.lg stat.ap,"many machine learning (ml) approaches are widely used to generate bioclimatic models for prediction of geographic range of organism as a function of climate. applications such as prediction of range shift in organism, range of invasive species influenced by climate change are important parameters in understanding the impact of climate change. however, success of machine learning-based approaches depends on a number of factors. while it can be safely said that no particular ml technique can be effective in all applications and success of a technique is predominantly dependent on the application or the type of the problem, it is useful to understand their behavior to ensure informed choice of techniques. this paper presents a comprehensive review of machine learning-based bioclimatic model generation and analyses the factors influencing success of such models. considering the wide use of statistical techniques, in our discussion we also include conventional statistical techniques used in bioclimatic modelling.",,2013-03-11,,['maumita bhattacharya']
1303.2910,understanding operational risk capital approximations: first and second   orders,q-fin.rm math.st stat.th,"we set the context for capital approximation within the framework of the basel ii / iii regulatory capital accords. this is particularly topical as the basel iii accord is shortly due to take effect. in this regard, we provide a summary of the role of capital adequacy in the new accord, highlighting along the way the significant loss events that have been attributed to the operational risk class that was introduced in the basel ii and iii accords. then we provide a semi-tutorial discussion on the modelling aspects of capital estimation under a loss distributional approach (lda). our emphasis is to focus on the important loss processes with regard to those that contribute most to capital, the so called high consequence, low frequency loss processes. this leads us to provide a tutorial overview of heavy tailed loss process modelling in oprisk under basel iii, with discussion on the implications of such tail assumptions for the severity model in an lda structure. this provides practitioners with a clear understanding of the features that they may wish to consider when developing oprisk severity models in practice. from this discussion on heavy tailed severity models, we then develop an understanding of the impact such models have on the right tail asymptotics of the compound loss process and we provide detailed presentation of what are known as first and second order tail approximations for the resulting heavy tailed loss process. from this we develop a tutorial on three key families of risk measures and their equivalent second order asymptotic approximations: value-at-risk (basel iii industry standard); expected shortfall (es) and the spectral risk measure. these then form the capital approximations.",,2013-03-12,,"['gareth w. peters', 'rodrigo s. targino', 'pavel v. shevchenko']"
1303.6223,random intersection trees,stat.ml stat.co stat.me,"finding interactions between variables in large and high-dimensional datasets is often a serious computational challenge. most approaches build up interaction sets incrementally, adding variables in a greedy fashion. the drawback is that potentially informative high-order interactions may be overlooked. here, we propose at an alternative approach for classification problems with binary predictor variables, called random intersection trees. it works by starting with a maximal interaction that includes all variables, and then gradually removing variables if they fail to appear in randomly chosen observations of a class of interest. we show that informative interactions are retained with high probability, and the computational complexity of our procedure is of order $p^\kappa$ for a value of $\kappa$ that can reach values as low as 1 for very sparse data; in many more general settings, it will still beat the exponent $s$ obtained when using a brute force search constrained to order $s$ interactions. in addition, by using some new ideas based on min-wise hash schemes, we are able to further reduce the computational cost. interactions found by our algorithm can be used for predictive modelling in various forms, but they are also often of interest in their own right as useful characterisations of what distinguishes a certain class from others.",,2013-03-25,,"['rajen dinesh shah', 'nicolai meinshausen']"
1303.6746,exploiting correlation and budget constraints in bayesian multi-armed   bandit optimization,stat.ml cs.lg,"we address the problem of finding the maximizer of a nonlinear smooth function, that can only be evaluated point-wise, subject to constraints on the number of permitted function evaluations. this problem is also known as fixed-budget best arm identification in the multi-armed bandit literature. we introduce a bayesian approach for this problem and show that it empirically outperforms both the existing frequentist counterpart and other bayesian optimization methods. the bayesian approach places emphasis on detailed modelling, including the modelling of correlations among the arms. as a result, it can perform well in situations where the number of arms is much larger than the number of allowed function evaluation, whereas the frequentist counterpart is inapplicable. this feature enables us to develop and deploy practical applications, such as automatic machine learning toolboxes. the paper presents comprehensive comparisons of the proposed approach, thompson sampling, classical bayesian optimization techniques, more recent bayesian bandit approaches, and state-of-the-art best arm identification methods. this is the first comparison of many of these methods in the literature and allows us to examine the relative merits of their different features.",,2013-03-27,2013-11-11,"['matthew w. hoffman', 'bobak shahriari', 'nando de freitas']"
1303.7090,gaussian process models for periodicity detection,math.st stat.th,"we consider the problem of detecting and quantifying the periodic component of a function given noise-corrupted observations of a limited number of input/output tuples. our approach is based on gaussian process regression which provides a flexible non-parametric framework for modelling periodic data. we introduce a novel decomposition of the covariance function as the sum of periodic and aperiodic kernels. this decomposition allows for the creation of sub-models which capture the periodic nature of the signal and its complement. to quantify the periodicity of the signal, we derive a periodicity ratio which reflects the uncertainty in the fitted sub-models. although the method can be applied to many kernels, we give a special emphasis to the mat\'ern family, from the expression of the reproducing kernel hilbert space inner product to the implementation of the associated periodic kernels in a gaussian process toolkit. the proposed method is illustrated by considering the detection of periodically expressed genes in the arabidopsis genome.",,2013-03-28,2016-08-19,"['nicolas durrande', 'james hensman', 'magnus rattray', 'neil d. lawrence']"
1304.2069,robustification of elliott's on-line em algorithm for hmms,stat.me q-fin.st stat.co,"in this paper, we establish a robustification of an on-line algorithm for modelling asset prices within a hidden markov model (hmm). in this hmm framework, parameters of the model are guided by a markov chain in discrete time, parameters of the asset returns are therefore able to switch between different regimes. the parameters are estimated through an on-line algorithm, which utilizes incoming information from the market and leads to adaptive optimal estimates. we robustify this algorithm step by step against additive outliers appearing in the observed asset prices with the rationale to better handle possible peaks or missings in asset returns.",,2013-04-07,,"['christina erlwein', 'peter ruckdeschel']"
1304.4302,a framework for modelling probabilistic uncertainty in rainfall scenario   analysis,stat.me math.pr,"predicting future probable values of model parameters, is an essential pre-requisite for assessing model decision reliability in an uncertain environment. scenario analysis is a methodology for modelling uncertainty in water resources management modelling. uncertainty if not considered appropriately in decision making will decrease reliability of decisions, especially in long-term planning. one of the challenges in scenario analysis is how scenarios are made. one of the most approved methods is statistical modelling based on auto-regressive models. stream flow future scenarios in developed basins that human has made changes to the natural flow process could not be generated directly by arma modelling. in this case, making scenarios for monthly rainfall and using it in a water resources system model makes more sense. rainfall is an ephemeral process which has zero values in some months which introduces some limitations in making use of monthly arma model. therefore, a two stage modelling approach is adopted here which in the first stage yearly modelling is done. within this yearly model three ranges are identified: dry, normal and wet. in the normal range yearly arma modelling is used. dry and wet range are considered as random processes and are modeled by frequency analysis. monthly distribution of rainfall, which is extracted from available data from a moving average are considered to be deterministic and fixed in time. each rainfall scenario is composed of a yearly arma process super-imposed by dry and wet events according to the frequency analysis. this modelling framework is applied to available data from three rain-gauge stations in iran. results show this modelling approach has better consistency with observed data in comparison with making use of arma modelling alone.",,2013-04-15,,"['seyed hamed alemohammad', 'reza ardakanian', 'akbar karimi']"
1304.6949,exploring a new class of non-stationary spatial gaussian random fields   with varying local anisotropy,stat.me,"gaussian random fields (grfs) constitute an important part of spatial modelling, but can be computationally infeasible for general covariance structures. an efficient approach is to specify grfs via stochastic partial differential equations (spdes) and derive gaussian markov random field (gmrf) approximations of the solutions. we consider the construction of a class of non-stationary grfs with varying local anisotropy, where the local anisotropy is introduced by allowing the coefficients in the spde to vary with position. this is done by using a form of diffusion equation driven by gaussian white noise with a spatially varying diffusion matrix. this allows for the introduction of parameters that control the grf by parametrizing the diffusion matrix. these parameters and the grf may be considered to be part of a hierarchical model and the parameters estimated in a bayesian framework. the results show that the use of an spde with non-constant coefficients is a promising way of creating non-stationary spatial gmrfs that allow for physical interpretability of the parameters, although there are several remaining challenges that would need to be solved before these models can be put to general practical use.",,2013-04-25,2014-04-25,"['geir-arne fuglstad', 'finn lindgren', 'daniel simpson', 'håvard rue']"
1305.1002,efficient estimation of the number of neighbours in probabilistic k   nearest neighbour classification,cs.lg stat.ml,"probabilistic k-nearest neighbour (pknn) classification has been introduced to improve the performance of original k-nearest neighbour (knn) classification algorithm by explicitly modelling uncertainty in the classification of each feature vector. however, an issue common to both knn and pknn is to select the optimal number of neighbours, $k$. the contribution of this paper is to incorporate the uncertainty in $k$ into the decision making, and in so doing use bayesian model averaging to provide improved classification. indeed the problem of assessing the uncertainty in $k$ can be viewed as one of statistical model selection which is one of the most important technical issues in the statistics and machine learning domain. in this paper, a new functional approximation algorithm is proposed to reconstruct the density of the model (order) without relying on time consuming monte carlo simulations. in addition, this algorithm avoids cross validation by adopting bayesian framework. the performance of this algorithm yielded very good performance on several real experimental datasets.",,2013-05-05,,"['ji won yoon', 'nial friel']"
1305.1184,probabilistic wind speed forecasting using bayesian model averaging with   truncated normal components,stat.me,"bayesian model averaging (bma) is a statistical method for post-processing forecast ensembles of atmospheric variables, obtained from multiple runs of numerical weather prediction models, in order to create calibrated predictive probability density functions (pdfs). the bma predictive pdf of the future weather quantity is the mixture of the individual pdfs corresponding to the ensemble members and the weights and model parameters are estimated using ensemble members and validating observation from a given training period.   in the present paper we introduce a bma model for calibrating wind speed forecasts, where the components pdfs follow truncated normal distribution with cut-off at zero, and apply it to the aladin-huneps ensemble of the hungarian meteorological service. three parameter estimation methods are proposed and each of the corresponding models outperforms the traditional gamma bma model both in calibration and in accuracy of predictions. moreover, since here the maximum likelihood estimation of the parameters does not require numerical optimization, modelling can be performed much faster than in case of gamma mixtures.",10.1016/j.csda.2014.02.013,2013-05-06,2013-05-07,['sándor baran']
1305.2073,fourier analysis of stationary time series in function space,math.st stat.th,"we develop the basic building blocks of a frequency domain framework for drawing statistical inferences on the second-order structure of a stationary sequence of functional data. the key element in such a context is the spectral density operator, which generalises the notion of a spectral density matrix to the functional setting, and characterises the second-order dynamics of the process. our main tool is the functional discrete fourier transform (fdft). we derive an asymptotic gaussian representation of the fdft, thus allowing the transformation of the original collection of dependent random functions into a collection of approximately independent complex-valued gaussian random functions. our results are then employed in order to construct estimators of the spectral density operator based on smoothed versions of the periodogram kernel, the functional generalisation of the periodogram matrix. the consistency and asymptotic law of these estimators are studied in detail. as immediate consequences, we obtain central limit theorems for the mean and the long-run covariance operator of a stationary functional time series. our results do not depend on structural modelling assumptions, but only functional versions of classical cumulant mixing conditions, and are shown to be stable under discrete observation of the individual curves.",10.1214/13-aos1086,2013-05-09,,"['victor m. panaretos', 'shahin tavakoli']"
1305.2815,modelling time and vintage variability in retail credit portfolios: the   decomposition approach,stat.ap,"in this paper, we consider the problem of modelling historical data on retail credit portfolio performance, with a view to forecasting future performance, and facilitating strategic decision making. we consider a situation, common in practice, where accounts with common origination date (typically month) are aggregated into a single vintage for analysis, and the data for analysis consists of a time series of a univariate portfolio performance variable (for example, the proportion of defaulting accounts) for each vintage over successive time periods since origination. an invaluable management tool for understanding portfolio behaviour can be obtained by decomposing the data series nonparametrically into components of exogenous variability (e), maturity (time since origination; m) and vintage (v), referred to as an emv model. for example, identification of a good macroeconomic model is the key to effective forecasting, particularly in applications such as stress testing, and identification of this can be facilitated by investigation of the macroeconomic component of an emv decomposition. we show that care needs to be taken with such a decomposition, drawing parallels with the age-period-cohort approach, common in demography, epidemiology and sociology. we develop a practical decomposition strategy, and illustrate our approach using data extracted from a credit card portfolio.",,2013-05-13,,"['jonathan j. forster', 'agus sudjianto']"
1305.3671,sparse adaptive dirichlet-multinomial-like processes,cs.it math.it math.st stat.th,"online estimation and modelling of i.i.d. data for short sequences over large or complex ""alphabets"" is a ubiquitous (sub)problem in machine learning, information theory, data compression, statistical language processing, and document analysis. the dirichlet-multinomial distribution (also called polya urn scheme) and extensions thereof are widely applied for online i.i.d. estimation. good a-priori choices for the parameters in this regime are difficult to obtain though. i derive an optimal adaptive choice for the main parameter via tight, data-dependent redundancy bounds for a related model. the 1-line recommendation is to set the 'total mass' = 'precision' = 'concentration' parameter to m/2ln[(n+1)/m], where n is the (past) sample size and m the number of different symbols observed (so far). the resulting estimator (i) is simple, (ii) online, (iii) fast, (iv) performs well for all m, small, middle and large, (v) is independent of the base alphabet size, (vi) non-occurring symbols induce no redundancy, (vii) the constant sequence has constant redundancy, (viii) symbols that appear only finitely often have bounded/constant contribution to the redundancy, (ix) is competitive with (slow) bayesian mixing over all sub-alphabets.",,2013-05-15,,['marcus hutter']
1305.4152,sparse approximate inference for spatio-temporal point process models,stat.ml,"spatio-temporal point process models play a central role in the analysis of spatially distributed systems in several disciplines. yet, scalable inference remains computa- tionally challenging both due to the high resolution modelling generally required and the analytically intractable likelihood function. here, we exploit the sparsity structure typical of (spatially) discretised log-gaussian cox process models by using approximate message-passing algorithms. the proposed algorithms scale well with the state dimension and the length of the temporal horizon with moderate loss in distributional accuracy. they hence provide a flexible and faster alternative to both non-linear filtering-smoothing type algorithms and to approaches that implement the laplace method or expectation propagation on (block) sparse latent gaussian models. we infer the parameters of the latent gaussian model using a structured variational bayes approach. we demonstrate the proposed framework on simulation studies with both gaussian and point-process observations and use it to reconstruct the conflict intensity and dynamics in afghanistan from the wikileaks afghan war diary.",,2013-05-17,2015-07-06,"['botond cseke', 'andrew zammit mangion', 'tom heskes', 'guido sanguinetti']"
1305.4153,factored expectation propagation for input-output fhmm models in systems   biology,stat.ml,we consider the problem of joint modelling of metabolic signals and gene expression in systems biology applications. we propose an approach based on input-output factorial hidden markov models and propose a structured variational inference approach to infer the structure and states of the model. we start from the classical free form structured variational mean field approach and use a expectation propagation to approximate the expectations needed in the variational loop. we show that this corresponds to a factored expectation constrained approximate inference. we validate our model through extensive simulations and demonstrate its applicability on a real world bacterial data set.,,2013-05-17,,"['botond cseke', 'guido sanguinetti']"
1305.4283,statistical modelling of summary values leads to accurate approximate   bayesian computations,stat.me,"approximate bayesian computation (abc) methods rely on asymptotic arguments, implying that parameter inference can be systematically biased even when sufficient statistics are available. we propose to construct the abc accept/reject step from decision theoretic arguments on a suitable auxiliary space. this framework, referred to as abc*, fully specifies which test statistics to use, how to combine them, how to set the tolerances and how long to simulate in order to obtain accuracy properties on the auxiliary space. akin to maximum-likelihood indirect inference, regularity conditions establish when the abc* approximation to the posterior density is accurate on the original parameter space in terms of the kullback-leibler divergence and the maximum a posteriori point estimate. fundamentally, escaping asymptotic arguments requires knowledge of the distribution of test statistics, which we obtain through modelling the distribution of summary values, data points on a summary level. synthetic examples and an application to time series data of influenza a (h3n2) infections in the netherlands illustrate abc* in action.",,2013-05-18,2014-01-22,"['oliver ratmann', 'anton camacho', 'adam meijer', 'gé donker']"
1305.5445,a bayesian localised conditional auto-regressive model for estimating   the health effects of air pollution,stat.me,"estimation of the long-term health effects of air pollution is a challenging task, especially when modelling small-area disease incidence data in an ecological study design. the challenge comes from the unobserved underlying spatial correlation structure in these data, which is accounted for using random effects modelled by a globally smooth conditional autoregressive model. these smooth random effects confound the effects of air pollution, which are also globally smooth. to avoid this collinearity a bayesian localised conditional autoregressive model is developed for the random effects. this localised model is flexible spatially, in the sense that it is not only able to model step changes in the random effects surface, but also is able to capture areas of spatial smoothness in the study region. this methodological development allows us to improve the estimation performance of the covariate effects, compared to using traditional conditional auto-regressive models. these results are established using a simulation study, and are then illustrated with our motivating study on air pollution and respiratory ill health in greater glasgow, scotland in 2010. the model shows substantial health effects of particulate matter air pollution and income deprivation, whose effects have been consistently attenuated by the currently available globally smooth models.",,2013-05-23,,"['duncan lee', 'alastair rushworth', 'sujit k. sahu']"
1305.5812,a proportional hazard model for the estimation of ionosphere storm   occurrence risk,math.st stat.th,"severe ionosphere magnetic storms are feared events for integrity and continuity of navigation systems such as egnos, the european sbas (satellite-based augmentation system) complementing gps and an accurate modelling of this event probability is necessary. our aim for the work presented in this paper is to give an estimation of the frequency of such extreme magnetic storms per time unit (year) throughout a solar cycle. thus, we develop an innovative approach based on a proportional hazard model, inspired by the cox model, with time dependent covariates. the number of storms during a cycle is supposed to be a non-homogeneous poisson process. the intensity of this process could be expressed as the product of a baseline risk and a risk factor. contrary to what is done in the cox model, the baseline risk is one parameter of interest (and not a nuisance one), it is the intensity to estimate. as in extreme value theory, all the high level events will be used to make estimation and the results will be extrapolated to the extreme level ones. after a precise description of the model, we present the estimation results and a model extension. a prediction for the current solar cycle (24th) is also proposed.",,2013-05-24,2015-05-21,"['malika chassan', 'jean-marc azaïs', 'guillaume buscarlet', 'norbert suard']"
1306.2365,bayesian solution uncertainty quantification for differential equations,stat.me,"we explore probability modelling of discretization uncertainty for system states defined implicitly by ordinary or partial differential equations. accounting for this uncertainty can avoid posterior under-coverage when likelihoods are constructed from a coarsely discretized approximation to system equations. a formalism is proposed for inferring a fixed but a priori unknown model trajectory through bayesian updating of a prior process conditional on model information. a one-step-ahead sampling scheme for interrogating the model is described, its consistency and first order convergence properties are proved, and its computational complexity is shown to be proportional to that of numerical explicit one-step solvers. examples illustrate the flexibility of this framework to deal with a wide variety of complex and large-scale systems. within the calibration problem, discretization uncertainty defines a layer in the bayesian hierarchy, and a markov chain monte carlo algorithm that targets this posterior distribution is presented. this formalism is used for inference on the jak-stat delay differential equation model of protein dynamics from indirectly observed measurements. the discussion outlines implications for the new field of probabilistic numerics.",,2013-06-10,2016-10-23,"['oksana a. chkrebtii', 'david a. campbell', 'ben calderhead', 'mark a. girolami']"
1306.3277,bayesian state-space modelling on high-performance hardware using libbi,stat.co,"libbi is a software package for state-space modelling and bayesian inference on modern computer hardware, including multi-core central processing units (cpus), many-core graphics processing units (gpus) and distributed-memory clusters of such devices. the software parses a domain-specific language for model specification, then optimises, generates, compiles and runs code for the given model, inference method and hardware platform. in presenting the software, this work serves as an introduction to state-space models and the specialised methods developed for bayesian inference with them. the focus is on sequential monte carlo (smc) methods such as the particle filter for state estimation, and the particle markov chain monte carlo (pmcmc) and smc^2 methods for parameter estimation. all are well-suited to current computer hardware. two examples are given and developed throughout, one a linear three-element windkessel model of the human arterial system, the other a nonlinear lorenz '96 model. these are specified in the prescribed modelling language, and libbi demonstrated by performing inference with them. empirical results are presented, including a performance comparison of the software with different hardware configurations.",,2013-06-13,,['lawrence m. murray']
1306.3301,aggregation and long memory: recent developments,math.st stat.th,"it is well-known that the aggregated time series might have very different properties from those of the individual series, in particular, long memory. at the present time, aggregation has become one of the main tools for modelling of long memory processes. we review recent work on contemporaneous aggregation of random-coefficient ar(1) and related models, with particular focus on various long memory properties of the aggregated process.",,2013-06-14,,"['remigijus leipus', 'anne philippe', 'donata puplinskaite', 'donatas surgailis']"
1306.4152,bioclimating modelling: a machine learning perspective,cs.lg stat.ml,"many machine learning (ml) approaches are widely used to generate bioclimatic models for prediction of geographic range of organism as a function of climate. applications such as prediction of range shift in organism, range of invasive species influenced by climate change are important parameters in understanding the impact of climate change. however, success of machine learning-based approaches depends on a number of factors. while it can be safely said that no particular ml technique can be effective in all applications and success of a technique is predominantly dependent on the application or the type of the problem, it is useful to understand their behaviour to ensure informed choice of techniques. this paper presents a comprehensive review of machine learning-based bioclimatic model generation and analyses the factors influencing success of such models. considering the wide use of statistical techniques, in our discussion we also include conventional statistical techniques used in bioclimatic modelling.",,2013-06-18,,['maumita bhattacharya']
1306.4438,joint modelling of chip-seq data via a markov random field model,stat.me,"chromatin immunoprecipitation-sequencing (chip-seq) experiments have now become routine in biology for the detection of protein binding sites. in this paper, we present a markov random field model for the joint analysis of multiple chip-seq experiments. the proposed model naturally accounts for spatial dependencies in the data, by assuming first order markov properties, and for the large proportion of zero counts, by using zero-inflated mixture distributions. in contrast to all other available implementations, the model allows for the joint modelling of multiple experiments, by incorporating key aspects of the experimental design. in particular, the model uses the information about replicates and about the different antibodies used in the experiments. an extensive simulation study shows a lower false non-discovery rate for the proposed method, compared to existing methods, at the same false discovery rate. finally, we present an analysis on real data for the detection of histone modifications of two chromatin modifiers from eight chip-seq experiments, including technical replicates with different ip efficiencies.",10.1093/biostatistics/kxt047,2013-06-19,,"['yanchun bao', 'veronica vinciotti', 'ernst wit', ""peter 't hoen""]"
1307.1067,the partial linear model in high dimensions,math.st stat.ap stat.th,"partial linear models have been widely used as flexible method for modelling linear components in conjunction with non-parametric ones. despite the presence of the non-parametric part, the linear, parametric part can under certain conditions be estimated with parametric rate. in this paper, we consider a high-dimensional linear part. we show that it can be estimated with oracle rates, using the lasso penalty for the linear part and a smoothness penalty for the nonparametric part.",,2013-07-03,,"['patric müller', 'sara van de geer']"
1307.5243,bayesian models for cost-effectiveness analysis in the presence of   structural zero costs,math.st stat.th,"bayesian modelling for cost-effectiveness data has received much attention in both the health economics and the statistical literature in recent years. cost-effectiveness data are characterised by a relatively complex structure of relationships linking the suitable measure of clinical benefit (\eg qalys) and the associated costs. simplifying assumptions, such as (bivariate) normality of the underlying distributions are usually not granted, particularly for the cost variable, which is characterised by markedly skewed distributions. in addition, individual-level datasets are often characterised by the presence of structural zeros in the cost variable.   hurdle models can be used to account for the presence of excess zeros in a distribution and have been applied in the context of cost data. we extend their application to cost-effectiveness data, defining a full bayesian model which consists of a selection model for the subjects with null costs, a marginal model for the costs and a conditional model for the measure of effectiveness (conditionally on the observed costs). the model is presented using a working example to describe its main features.",,2013-07-19,,['gianluca baio']
1307.6332,modelling energy spot prices by volatility modulated l\'{e}vy-driven   volterra processes,q-fin.pr math.st stat.th,"this paper introduces the class of volatility modulated l\'{e}vy-driven volterra (vmlv) processes and their important subclass of l\'{e}vy semistationary (lss) processes as a new framework for modelling energy spot prices. the main modelling idea consists of four principles: first, deseasonalised spot prices can be modelled directly in stationarity. second, stochastic volatility is regarded as a key factor for modelling energy spot prices. third, the model allows for the possibility of jumps and extreme spikes and, lastly, it features great flexibility in terms of modelling the autocorrelation structure and the samuelson effect. we provide a detailed analysis of the probabilistic properties of vmlv processes and show how they can capture many stylised facts of energy markets. further, we derive forward prices based on our new spot price models and discuss option pricing. an empirical example based on electricity spot prices from the european energy exchange confirms the practical relevance of our new modelling framework.",10.3150/12-bej476,2013-07-24,,"['ole e. barndorff-nielsen', 'fred espen benth', 'almut e. d. veraart']"
1308.0900,trading usdchf filtered by gold dynamics via hmm coupling,stat.ml cs.lg,"we devise a usdchf trading strategy using the dynamics of gold as a filter. our strategy involves modelling both usdchf and gold using a coupled hidden markov model (chmm). the observations will be indicators, rsi and cci, which will be used as triggers for our trading signals. upon decoding the model in each iteration, we can get the next most probable state and the next most probable observation. hopefully by taking advantage of intermarket analysis and the markov property implicit in the model, trading with these most probable values will produce profitable results.",,2013-08-05,2013-10-29,['donny lee']
1308.1136,bayesian inference for mat\'ern repulsive processes,stat.me,"in many applications involving point pattern data, the poisson process assumption is unrealistic, with the data exhibiting a more regular spread. such a repulsion between events is exhibited by trees for example, because of competition for light and nutrients. other examples include the locations of biological cells and cities, and the times of neuronal spikes. given the many applications of repulsive point processes, there is a surprisingly limited literature developing flexible, realistic and interpretable models, as well as efficient inferential methods. we address this gap by developing a modelling framework around the mat\'ern type-iii repulsive process. we consider a number of extensions of the original mat\'ern type-iii process for both the homogeneous and inhomogeneous cases. we also derive the probability density of this generalized mat\'ern process. this allows us to characterize the posterior distribution of the various latent variables, and leads to a novel and efficient markov chain monte carlo algorithm. we apply our ideas to datasets involving the spatial locations of trees, nerve fiber cells and greyhound bus stations.",,2013-08-05,2015-04-03,"['vinayak rao', 'ryan p. adams', 'david b. dunson']"
1308.1211,identification of finite dimensional linear systems driven by levy   processes,math.st stat.th,"levy processes are widely used in financial mathematics, telecommunication, economics, queueing theory and natural sciences for modelling. a typical model is obtained by considering finite dimensional linear stochastic siso systems driven by a levy process. in this paper we consider a discrete-time version of this model driven by the increments of a levy process, such a system will be called levy system. we focus on the problem of identifying the dynamics and the noise characteristics of such a levy system. the special feature of this problem is that the statistical description of the noise is given by the characteristic function (c.f.) of the driving noise not by its density function. as an alternative to the maximum likelihood (ml) method we develop and analyze a novel identification method by adapting the so-called empirical characteristic function method (ecf) originally devised for estimating parameters of c.f.-s from i.i.d. samples. precise characterization of the errors of these estimators will be given, and their asymptotic covariance matrices will be obtained. we also demonstrate that the arguments implying asymptotic efficiency for the i.i.d. case can be adapted for the present case.",,2013-08-06,2014-01-06,"['laszlo gerencser', 'mate manfay']"
1308.5836,semiparametric stochastic volatility modelling using penalized splines,stat.me q-fin.st,"stochastic volatility (sv) models mimic many of the stylized facts attributed to time series of asset returns, while maintaining conceptual simplicity. the commonly made assumption of conditionally normally distributed or student-t-distributed returns, given the volatility, has however been questioned. in this manuscript, we introduce a novel maximum penalized likelihood approach for estimating the conditional distribution in an sv model in a nonparametric way, thus avoiding any potentially critical assumptions on the shape. the considered framework exploits the strengths both of the powerful hidden markov model machinery and of penalized b-splines, and constitutes a powerful and flexible alternative to recently developed bayesian approaches to semiparametric sv modelling. we demonstrate the feasibility of the approach in a simulation study before outlining its potential in applications to three series of returns on stocks and one series of stock index returns.",,2013-08-27,2014-06-17,"['roland langrock', 'théo michelot', 'alexander sohn', 'thomas kneib']"
1308.5850,modelling group dynamic animal movement,q-bio.qm stat.ap,"group dynamic movement is a fundamental aspect of many species' movements. the need to adequately model individuals' interactions with other group members has been recognised, particularly in order to differentiate the role of social forces in individual movement from environmental factors. however, to date, practical statistical methods which can include group dynamics in animal movement models have been lacking. we consider a flexible modelling framework that distinguishes a group-level model, describing the movement of the group's centre, and an individual-level model, such that each individual makes its movement decisions relative to the group centroid. the basic idea is framed within the flexible class of hidden markov models, extending previous work on modelling animal movement by means of multi-state random walks. while in simulation experiments parameter estimators exhibit some bias in non-ideal scenarios, we show that generally the estimation of models of this type is both feasible and ecologically informative. we illustrate the approach using real movement data from 11 reindeer (rangifer tarandus). results indicate a directional bias towards a group centroid for reindeer in an encamped state. though the attraction to the group centroid is relatively weak, our model successfully captures group-influenced movement dynamics. specifically, as compared to a regular mixture of correlated random walks, the group dynamic model more accurately predicts the non-diffusive behaviour of a cohesive mobile group.",10.1111/2041-210x.12155,2013-08-27,,"['roland langrock', 'j. grant c. hopcraft', 'paul g. blackwell', 'victoria goodall', 'ruth king', 'mu niu', 'toby a. patterson', 'martin w. pedersen', 'anna skarin', 'robert s. schick']"
1309.2435,bayesian wavelet shrinkage of the haar-fisz transformed wavelet   periodogram,stat.me,"it is increasingly being realised that many real world time series are not stationary and exhibit evolving second-order autocovariance or spectral structure. this article introduces a bayesian approach for modelling the evolving wavelet spectrum of a locally stationary wavelet time series. our new method works by combining the advantages of a haar-fisz transformed spectrum with a simple, but powerful, bayesian wavelet shrinkage method. our new method produces excellent and stable spectral estimates and this is demonstrated via simulated data and on differenced infant ecg data. a major additional benefit of the bayesian paradigm is that we obtain rigorous and useful credible intervals of the evolving spectral structure. we show how the bayesian credible intervals provide extra insight into the infant ecg data.",,2013-09-10,,"['guy p. nason', 'kara n. stevens']"
1309.2737,on the evidence for cosmic variation of the fine structure constant   (ii): a semi-parametric bayesian model selection analysis of the quasar   dataset,astro-ph.im stat.ap,"in the second paper of this series we extend our bayesian reanalysis of the evidence for a cosmic variation of the fine structure constant to the semi-parametric modelling regime. by adopting a mixture of dirichlet processes prior for the unexplained errors in each instrumental subgroup of the benchmark quasar dataset we go some way towards freeing our model selection procedure from the apparent subjectivity of a fixed distributional form. despite the infinite-dimensional domain of the error hierarchy so constructed we are able to demonstrate a recursive scheme for marginal likelihood estimation with prior-sensitivity analysis directly analogous to that presented in paper i, thereby allowing the robustness of our posterior bayes factors to hyper-parameter choice and model specification to be readily verified. in the course of this work we elucidate various similarities between unexplained error problems in the seemingly disparate fields of astronomy and clinical meta-analysis, and we highlight a number of sophisticated techniques for handling such problems made available by past research in the latter. it is our hope that the novel approach to semi-parametric model selection demonstrated herein may serve as a useful reference for others exploring this potentially difficult class of error model.",,2013-09-11,,"['ewan cameron', 'tony pettitt']"
1309.3802,monotone function estimation for computer experiments,stat.me,"in statistical modeling of computer experiments sometimes prior information is available about the underlying function. for example, the physical system simulated by the computer code may be known to be monotone with respect to some or all inputs. we develop a bayesian approach to gaussian process modelling capable of incorporating monotonicity information for computer model emulation. markov chain monte carlo methods are used to sample from the posterior distribution of the process given the simulator output and monotonicity information. the performance of the proposed approach in terms of predictive accuracy and uncertainty quantification is demonstrated in a number of simulated examples as well as a real queueing system application.",,2013-09-15,2014-06-14,"['shirin golchi', 'derek r. bingham', 'hugh chipman', 'david a. campbell']"
1309.5122,variational bayes inference and dirichlet process priors,stat.co,"this paper shows how the variational bayes method provides a computational efficient technique in the context of hierarchical modelling using dirichlet process priors, in particular without requiring conjugate prior assumption. it shows, using the so called parameter separation parameterization, a simple criterion under which the variational method works well. based on this framework, its provides a full variational solution for the dirichlet process. the numerical results show that the method is very computationally efficient when compared to mcmc. finally, we propose an empirical method to estimate the truncation level for the truncated dirichlet process.",,2013-09-19,,"['hui zhao', 'paul marriott']"
1309.5264,sequential change detection in the presence of unknown parameters,stat.me,"it is commonly required to detect change points in sequences of random variables. in the most difficult setting of this problem, change detection must be performed sequentially with new observations being constantly received over time. further, the parameters of both the pre- and post- change distributions may be unknown. in a recent paper by hawkins and zamba (2005), the sequential generalised likelihood ratio test was introduced for detecting changes in this context, under the assumption that the observations follow a gaussian distribution. however, we show that the asymptotic approximation used in their test statistic leads to it being conservative even when a large numbers of observations is available. we propose an improved procedure which is more efficient, in the sense of detecting changes faster, in all situations. we also show that similar issues arise in other parametric change detection contexts, which we illustrate by introducing a novel monitoring procedure for sequences of exponentially distributed random variable, which is an important topic in time-to-failure modelling.",10.1007/s11222-013-9417-1,2013-09-20,,['gordon j ross']
1309.6287,a stochastic model for speculative bubbles,math.pr math.st q-fin.gn stat.th,"this paper aims to provide a simple modelling of speculative bubbles and derive some quantitative properties of its dynamical evolution. starting from a description of individual speculative behaviours, we build and study a second order markov process, which after simple transformations can be viewed as a turning two-dimensional gaussian process. then, our main problem is to ob- tain some bounds for the persistence rate relative to the return time to a given price. in our main results, we prove with both spectral and probabilistic methods that this rate is almost proportional to the turning frequency {\omega} of the model and provide some explicit bounds. in the continuity of this result, we build some estimators of {\omega} and of the pseudo-period of the prices. at last, we end the paper by a proof of the quasi-stationary distribution of the process, as well as the existence of its persistence rate.",,2013-09-22,,"['sébastien gadat', 'laurent miclo', 'fabien panloup']"
1309.6786,one-class collaborative filtering with random graphs: annotated version,stat.ml cs.lg,"the bane of one-class collaborative filtering is interpreting and modelling the latent signal from the missing class. in this paper we present a novel bayesian generative model for implicit collaborative filtering. it forms a core component of the xbox live architecture, and unlike previous approaches, delineates the odds of a user disliking an item from simply not considering it. the latent signal is treated as an unobserved random graph connecting users with items they might have encountered. we demonstrate how large-scale distributed learning can be achieved through a combination of stochastic gradient descent and mean field variational inference over random graph samples. a fine-grained comparison is done against a state of the art baseline on real world data.",,2013-09-26,2014-09-24,"['ulrich paquet', 'noam koenigstein']"
1309.6811,generative multiple-instance learning models for quantitative   electromyography,cs.lg stat.ml,"we present a comprehensive study of the use of generative modeling approaches for multiple-instance learning (mil) problems. in mil a learner receives training instances grouped together into bags with labels for the bags only (which might not be correct for the comprised instances). our work was motivated by the task of facilitating the diagnosis of neuromuscular disorders using sets of motor unit potential trains (mupts) detected within a muscle which can be cast as a mil problem. our approach leads to a state-of-the-art solution to the problem of muscle classification. by introducing and analyzing generative models for mil in a general framework and examining a variety of model structures and components, our work also serves as a methodological guide to modelling mil tasks. we evaluate our proposed methods both on mupt datasets and on the musk1 dataset, one of the most widely used benchmarks for mil.",,2013-09-26,,"['tameem adel', 'benn smith', 'ruth urner', 'daniel stashuk', 'daniel j. lizotte']"
1309.7311,bayesian inference in sparse gaussian graphical models,stat.ml cs.lg,"one of the fundamental tasks of science is to find explainable relationships between observed phenomena. one approach to this task that has received attention in recent years is based on probabilistic graphical modelling with sparsity constraints on model structures. in this paper, we describe two new approaches to bayesian inference of sparse structures of gaussian graphical models (ggms). one is based on a simple modification of the cutting-edge block gibbs sampler for sparse ggms, which results in significant computational gains in high dimensions. the other method is based on a specific construction of the hamiltonian monte carlo sampler, which results in further significant improvements. we compare our fully bayesian approaches with the popular regularisation-based graphical lasso, and demonstrate significant advantages of the bayesian treatment under the same computing costs. we apply the methods to a broad range of simulated data sets, and a real-life financial data set.",10.1017/s0956796814000057,2013-09-27,,"['peter orchard', 'felix agakov', 'amos storkey']"
1310.0973,accelerating inference for diffusions observed with measurement error   and large sample sizes using approximate bayesian computation,stat.co stat.ap,"in recent years dynamical modelling has been provided with a range of breakthrough methods to perform exact bayesian inference. however it is often computationally unfeasible to apply exact statistical methodologies in the context of large datasets and complex models. this paper considers a nonlinear stochastic differential equation model observed with correlated measurement errors and an application to protein folding modelling. an approximate bayesian computation (abc) mcmc algorithm is suggested to allow inference for model parameters within reasonable time constraints. the abc algorithm uses simulations of ""subsamples"" from the assumed data generating model as well as a so-called ""early rejection"" strategy to speed up computations in the abc-mcmc sampler. using a considerate amount of subsamples does not seem to degrade the quality of the inferential results for the considered applications. a simulation study is conducted to compare our strategy with exact bayesian inference, the latter resulting two orders of magnitude slower than abc-mcmc for the considered setup. finally the abc algorithm is applied to a large size protein data. the suggested methodology is fairly general and not limited to the exemplified model and data.",10.1080/00949655.2014.1002101,2013-10-03,2014-12-22,"['umberto picchini', 'julie lyng forman']"
1310.1545,learning hidden structures with relational models by adequately   involving rich information in a network,cs.lg cs.si stat.ml,"effectively modelling hidden structures in a network is very practical but theoretically challenging. existing relational models only involve very limited information, namely the binary directional link data, embedded in a network to learn hidden networking structures. there is other rich and meaningful information (e.g., various attributes of entities and more granular information than binary elements such as ""like"" or ""dislike"") missed, which play a critical role in forming and understanding relations in a network. in this work, we propose an informative relational model (infrm) framework to adequately involve rich information and its granularity in a network, including metadata information about each entity and various forms of link data. firstly, an effective metadata information incorporation method is employed on the prior information from relational models mmsb and lfrm. this is to encourage the entities with similar metadata information to have similar hidden structures. secondly, we propose various solutions to cater for alternative forms of link data. substantial efforts have been made towards modelling appropriateness and efficiency, for example, using conjugate priors. we evaluate our framework and its inference algorithms in different datasets, which shows the generality and effectiveness of our models in capturing implicit structures in networks.",,2013-10-06,,"['xuhui fan', 'richard yi da xu', 'longbing cao', 'yin song']"
1310.2125,retrieval of experiments with sequential dirichlet process mixtures in   model space,stat.ml cs.ir stat.ap,"we address the problem of retrieving relevant experiments given a query experiment, motivated by the public databases of datasets in molecular biology and other experimental sciences, and the need of scientists to relate to earlier work on the level of actual measurement data. since experiments are inherently noisy and databases ever accumulating, we argue that a retrieval engine should possess two particular characteristics. first, it should compare models learnt from the experiments rather than the raw measurements themselves: this allows incorporating experiment-specific prior knowledge to suppress noise effects and focus on what is important. second, it should be updated sequentially from newly published experiments, without explicitly storing either the measurements or the models, which is critical for saving storage space and protecting data privacy: this promotes life long learning. we formulate the retrieval as a ``supermodelling'' problem, of sequentially learning a model of the set of posterior distributions, represented as sets of mcmc samples, and suggest the use of particle-learning-based sequential dirichlet process mixture (dpm) for this purpose. the relevance measure for retrieval is derived from the supermodel through the mixture representation. we demonstrate the performance of the proposed retrieval method on simulated data and molecular biological experiments.",,2013-10-08,2014-03-06,"['ritabrata dutta', 'sohan seth', 'samuel kaski']"
1310.3403,disease mapping via negative binomial regression m-quantiles,stat.me,"we introduce a semi-parametric approach to ecological regression for disease mapping, based on modelling the regression m-quantiles of a negative binomial variable. the proposed method is robust to outliers in the model covariates, including those due to measurement error, and can account for both spatial heterogeneity and spatial clustering. a simulation experiment based on the well-known scottish lip cancer data set is used to compare the m-quantile modelling approach and a random effects modelling approach for disease mapping. this suggests that the m-quantile approach leads to predicted relative risks with smaller root mean square error than standard disease mapping methods. the paper concludes with an illustrative application of the m-quantile approach, mapping low birth weight incidence data for english local authority districts for the years 2005-2010.",10.1002/sim.6256,2013-10-12,,"['ray chambers', 'emanuela dreassi', 'nicola salvati']"
1310.4456,"inference, sampling, and learning in copula cumulative distribution   networks",stat.ml cs.lg,"the cumulative distribution network (cdn) is a recently developed class of probabilistic graphical models (pgms) permitting a copula factorization, in which the cdf, rather than the density, is factored. despite there being much recent interest within the machine learning community about copula representations, there has been scarce research into the cdn, its amalgamation with copula theory, and no evaluation of its performance. algorithms for inference, sampling, and learning in these models are underdeveloped compared those of other pgms, hindering widerspread use.   one advantage of the cdn is that it allows the factors to be parameterized as copulae, combining the benefits of graphical models with those of copula theory. in brief, the use of a copula parameterization enables greater modelling flexibility by separating representation of the marginals from the dependence structure, permitting more efficient and robust learning. another advantage is that the cdn permits the representation of implicit latent variables, whose parameterization and connectivity are not required to be specified. unfortunately, that the model can encode only latent relationships between variables severely limits its utility.   in this thesis, we present inference, learning, and sampling for cdns, and further the state-of-the-art. first, we explain the basics of copula theory and the representation of copula cdns. then, we discuss inference in the models, and develop the first sampling algorithm. we explain standard learning methods, propose an algorithm for learning from data missing completely at random (mcar), and develop a novel algorithm for learning models of arbitrary treewidth and size. properties of the models and algorithms are investigated through monte carlo simulations. we conclude with further discussion of the advantages and limitations of cdns, and suggest future work.",,2013-10-16,,['stefan douglas webb']
1310.5177,the swells survey. vi. hierarchical inference of the initial mass   functions of bulges and discs,astro-ph.im astro-ph.ga physics.data-an stat.ap,"the long-standing assumption that the stellar initial mass function (imf) is universal has recently been challenged by a number of observations. several studies have shown that a ""heavy"" imf (e.g., with a salpeter-like abundance of low mass stars and thus normalisation) is preferred for massive early-type galaxies, while this imf is inconsistent with the properties of less massive, later-type galaxies. these discoveries motivate the hypothesis that the imf may vary (possibly very slightly) across galaxies and across components of individual galaxies (e.g. bulges vs discs). in this paper we use a sample of 19 late-type strong gravitational lenses from the swells survey to investigate the imfs of the bulges and discs in late-type galaxies. we perform a joint analysis of the galaxies' total masses (constrained by strong gravitational lensing) and stellar masses (constrained by optical and near-infrared colours in the context of a stellar population synthesis [sps] model, up to an imf normalisation parameter). using minimal assumptions apart from the physical constraint that the total stellar mass within any aperture must be less than the total mass within the aperture, we find that the bulges of the galaxies cannot have imfs heavier (i.e. implying high mass per unit luminosity) than salpeter, while the disc imfs are not well constrained by this data set. we also discuss the necessity for hierarchical modelling when combining incomplete information about multiple astronomical objects. this modelling approach allows us to place upper limits on the size of any departures from universality. more data, including spatially resolved kinematics (as in paper v) and stellar population diagnostics over a range of bulge and disc masses, are needed to robustly quantify how the imf varies within galaxies.",10.1093/mnras/stt2026,2013-10-18,,"['brendon j. brewer', 'philip j. marshall', 'matthew w. auger', 'tommaso treu', 'aaron a. dutton', 'matteo barnabè']"
1310.5288,gpatt: fast multidimensional pattern extrapolation with gaussian   processes,stat.ml cs.ai cs.lg stat.me,"gaussian processes are typically used for smoothing and interpolation on small datasets. we introduce a new bayesian nonparametric framework -- gpatt -- enabling automatic pattern extrapolation with gaussian processes on large multidimensional datasets. gpatt unifies and extends highly expressive kernels and fast exact inference techniques. without human intervention -- no hand crafting of kernel features, and no sophisticated initialisation procedures -- we show that gpatt can solve large scale pattern extrapolation, inpainting, and kernel discovery problems, including a problem with 383400 training points. we find that gpatt significantly outperforms popular alternative scalable gaussian process methods in speed and accuracy. moreover, we discover profound differences between each of these methods, suggesting expressive kernels, nonparametric representations, and exact inference are useful for modelling large scale multidimensional patterns.",,2013-10-19,2013-12-31,"['andrew gordon wilson', 'elad gilboa', 'arye nehorai', 'john p. cunningham']"
1310.7714,an improved bayesian semiparametric model for palaeoclimate   reconstruction: cross-validation based model assessment,stat.ap,"fossil-based palaeoclimate reconstruction is an important area of ecological science that has gained momentum in the backdrop of the global climate change debate. the hierarchical bayesian paradigm provides an interesting platform for studying such important scientific issue. however, our cross-validation based assessment of the existing bayesian hierarchical models with respect to two modern proxy data sets based on chironomid and pollen, respectively, revealed that the models are inadequate for the data sets.   in this paper, we model the species assemblages (compositional data) by the zero-inflated multinomial distribution, while modelling the species response functions using dirichlet process based gaussian mixtures. this modelling strategy yielded significantly improved performances, and a formal bayesian test of model adequacy, developed recently, showed that our new model is adequate for both the modern data sets. furthermore, combining together the zero-inflated assumption, importance resampling markov chain monte carlo (irmcmc) and the recently developed transformation-based markov chain monte carlo (tmcmc), we develop a powerful and efficient computational methodology.",,2013-10-29,2013-12-12,"['sabyasachi mukhopadhyay', 'sourabh bhattacharya']"
1310.7815,"efficient and automatic methods for flexible regression on   spatiotemporal data, with applications to groundwater monitoring",stat.ap,"fitting statistical models to spatiotemporal data requires finding the right balance between imposing smoothness and following the data. in the context of p-splines, we propose a bayesian framework for choosing the smoothing parameter which allows the construction of fully-automatic data-driven methods for fitting flexible models to spatiotemporal data. a computationally efficient implementation, exploiting the sparsity of the arising design and penalty matrices, is proposed. the findings are illustrated using a simulation and two examples, all concerned with the modelling of contaminants in groundwater, which suggest that the proposed strategy is more stable that competing strategies based on the use of criteria such as gcv and aic.",,2013-10-29,,"['a. w. bowman', 'l. evers', 'd. molinari', 'w. r. jones', 'm. j. spence']"
1310.8158,gwsdat - groundwater spatiotemporal data analysis tool,stat.ap,"periodic monitoring of groundwater quality at industrial and commercial sites generates large volumes of spatiotemporal concentration data. data modelling is typically restricted to either the analysis of monotonic trends in individual wells, or independent fitting of spatial concentration distributions (e.g. kriging) to separate monitoring time periods. neither of these techniques satisfactorily elucidate the interaction between spatial and temporal components of the data. potential negative consequences include an incomplete understanding of groundwater plume dynamics, which can lead to the selection of inappropriate remedial strategies. the groundwater spatiotemporal data analysis tool (gwsdat) is a user friendly, open source, decision support tool that has been developed to address these issues. uniquely, gwsdat applies a spatiotemporal model for a more coherent and smooth interpretation of the interaction in spatial and time-series components of groundwater solute concentrations. gwsdat has been designed to work with standard groundwater monitoring data sets, and has no special data requirements. data entry is via a standardised microsoft excel input template. the underlying statistical modelling and graphical output are generated using r. this paper describes in detail the various plotting options available and how the graphical user interface can be used for rapid, rigorous and interactive trend analysis with facilitated report generation. gwsdat has been used extensively in the assessment of soil and groundwater conditions at shell's downstream assets and the discussion section describes the benefits of its applied use. these include rapid interpretation of complex data sets, early identification of new spills, detection of off-site plume migration and simplified preparation of groundwater monitoring reports - all of which facilitate expedited risk assessment and remediation.",,2013-10-30,,"['wayne r. jones', 'michael j. spence', 'adrian w. bowman', 'ludger evers', 'daniel a. molinari']"
1310.8176,bayesian regression analysis of data with random effects covariates from   nonlinear longitudinal measurements,stat.me,"joint models for a wide class of response variables and longitudinal measurements consist on a mixed-effects model to fit longitudinal trajectories whose random effects enter as covariates in a generalized linear model for the primary response. they provide a useful way to asses association between these two kinds of data, which in clinical studies are often collected jointly on a series of individuals and may help understanding, for instance, the mechanisms of recovery of a certain disease or the efficacy of a given therapy. the most common joint model in this framework is based on a linear mixed model for the longitudinal data. however, for complex datasets the linearity assumption may be too restrictive. some works have considered generalizing this setting with the use of a nonlinear mixed-effects model for the longitudinal trajectories but the proposed estimation procedures based on likelihood approximations have been shown de la cruz et al. (2011) to exhibit some computational efficiency problems. in this article we propose an mcmc-based estimation procedure in the joint model with a nonlinear mixed-effects model for the longitudinal data and a generalized linear model for the primary response. moreover, we consider that the errors in the longitudinal model may be correlated. we apply our method to the analysis of hormone levels measured at the early stages of pregnancy that can be used to predict normal versus abnormal pregnancy outcomes. we also conduct a simulation study to asses the importance of modelling correlated errors and quantify the consequences of model misspecification.",,2013-10-30,2014-07-02,"['rolando de la cruz', 'cristian meza', 'ana arribas-gil', 'raymond j. carroll']"
1310.8527,a water level relationship between consecutive gauge stations along   solim\~oes/amazonas main channel: a wavelet approach,stat.ap physics.ao-ph,"gauge stations are distributed along the solim\~oes/amazonas main channel to monitor water level changes over time. those measurements help quantify both the water movement and its variability from one gauge station to the next downstream. the objective of this study is to detect changes in the water level relationship between consecutive gauge stations along the solim\~oes/amazonas main channel, since 1980. to carry out the analyses, data spanning from 1980 to 2010 from three consecutive gauges (tefe, manaus and obidos) were used to compute standardized daily anomalies. in particular for infra-annual periods it was possible to detect changes for the water level variability along the solim\~oes/amazonas main channel, by applying the morlet wavelet transformation (wt) and wavelet cross coherence (wcc) methods. it was possible to quantify the waves amplitude for the wt infra-annual scaled-period and were quite similar to the three gauge stations denoting that the water level variability are related to the same hydrological forcing functions. changes in the wcc was detected for the manaus-obidos river stretch and this characteristic might be associated with land cover changes in the floodplains. the next steps of this research, will be to test this hypotheses by integrating land cover changes into the floodplain with hydrological modelling simulations throughout the time-series.",10.2495/ws130051,2013-10-31,,"['r. d. somoza', 'e. s. pereira', 'e. m. l. novo', 'c. d. rennó']"
1311.0701,on fast dropout and its applicability to recurrent networks,stat.ml cs.lg cs.ne,"recurrent neural networks (rnns) are rich models for the processing of sequential data. recent work on advancing the state of the art has been focused on the optimization or modelling of rnns, mostly motivated by adressing the problems of the vanishing and exploding gradients. the control of overfitting has seen considerably less attention. this paper contributes to that by analyzing fast dropout, a recent regularization method for generalized linear models and neural networks from a back-propagation inspired perspective. we show that fast dropout implements a quadratic form of an adaptive, per-parameter regularizer, which rewards large weights in the light of underfitting, penalizes them for overconfident predictions and vanishes at minima of an unregularized training loss. the derivatives of that regularizer are exclusively based on the training error signal. one consequence of this is the absense of a global weight attractor, which is particularly appealing for rnns, since the dynamics are not biased towards a certain regime. we positively test the hypothesis that this improves the performance of rnns on four musical data sets.",,2013-11-04,2014-03-05,"['justin bayer', 'christian osendorfer', 'daniela korhammer', 'nutan chen', 'sebastian urban', 'patrick van der smagt']"
1311.1138,analysis of the gibbs sampler for hierarchical inverse problems,math.st stat.th,"many inverse problems arising in applications come from continuum models where the unknown parameter is a field. in practice the unknown field is discretized resulting in a problem in $\mathbb{r}^n$, with an understanding that refining the discretization, that is increasing $n$, will often be desirable. in the context of bayesian inversion this situation suggests the importance of two issues: (i) defining hyper-parameters in such a way that they are interpretable in the continuum limit $n \to \infty$ and so that their values may be compared between different discretization levels; (ii) understanding the efficiency of algorithms for probing the posterior distribution, as a function of large $n.$ here we address these two issues in the context of linear inverse problems subject to additive gaussian noise within a hierarchical modelling framework based on a gaussian prior for the unknown field and an inverse-gamma prior for a hyper-parameter, namely the amplitude of the prior variance. the structure of the model is such that the gibbs sampler can be easily implemented for probing the posterior distribution. subscribing to the dogma that one should think infinite-dimensionally before implementing in finite dimensions, we present function space intuition and provide rigorous theory showing that as $n$ increases, the component of the gibbs sampler for sampling the amplitude of the prior variance becomes increasingly slower. we discuss a reparametrization of the prior variance that is robust with respect to the increase in dimension; we give numerical experiments which exhibit that our reparametrization prevents the slowing down. our intuition on the behaviour of the prior hyper-parameter, with and without reparametrization, is sufficiently general to include a broad class of nonlinear inverse problems as well as other families of hyper-priors.",,2013-11-05,2014-07-15,"['sergios agapiou', 'johnathan m. bardsley', 'omiros papaspiliopoulos', 'andrew m. stuart']"
1311.2520,the infinite degree corrected stochastic block model,stat.ml,"in stochastic blockmodels, which are among the most prominent statistical models for cluster analysis of complex networks, clusters are defined as groups of nodes with statistically similar link probabilities within and between groups. a recent extension by karrer and newman incorporates a node degree correction to model degree heterogeneity within each group. although this demonstrably leads to better performance on several networks it is not obvious whether modelling node degree is always appropriate or necessary. we formulate the degree corrected stochastic blockmodel as a non-parametric bayesian model, incorporating a parameter to control the amount of degree correction which can then be inferred from data. additionally, our formulation yields principled ways of inferring the number of groups as well as predicting missing links in the network which can be used to quantify the model's predictive performance. on synthetic data we demonstrate that including the degree correction yields better performance both on recovering the true group structure and predicting missing links when degree heterogeneity is present, whereas performance is on par for data with no degree heterogeneity within clusters. on seven real networks (with no ground truth group structure available) we show that predictive performance is about equal whether or not degree correction is included; however, for some networks significantly fewer clusters are discovered when correcting for degree indicating that the data can be more compactly explained by clusters of heterogenous degree nodes.",10.1103/physreve.90.032819,2013-11-11,2014-05-30,"['tue herlau', 'mikkel n. schmidt', 'morten mørup']"
1311.2994,bayesian threshold selection for extremal models using measures of   surprise,stat.me stat.co,"statistical extreme value theory is concerned with the use of asymptotically motivated models to describe the extreme values of a process. a number of commonly used models are valid for observed data that exceed some high threshold. however, in practice a suitable threshold is unknown and must be determined for each analysis. while there are many threshold selection methods for univariate extremes, there are relatively few that can be applied in the multivariate setting. in addition, there are only a few bayesian-based methods, which are naturally attractive in the modelling of extremes due to data scarcity. the use of bayesian measures of surprise to determine suitable thresholds for extreme value models is proposed. such measures quantify the level of support for the proposed extremal model and threshold, without the need to specify any model alternatives. this approach is easily implemented for both univariate and multivariate extremes.",,2013-11-12,2014-12-09,"['j. lee', 'y. fan', 's. a. sisson']"
1311.3888,spline approximations to conditional archimedean copula,stat.me,we propose a flexible copula model to describe changes with a covariate in the dependence structure of (conditionally exchangeable) random variables. the starting point is a spline approximation to the generator of an archimedean copula. changes in the dependence structure with a covariate $x$ are modelled by flexible regression of the spline coefficients on $x$. the performances and properties of the spline estimate of the reference generator and the abilities of these conditional models to approximate conditional copulas are studied through simulations. inference is made using bayesian arguments with posterior distributions explored using importance sampling or adaptive mcmc algorithms. the modelling strategy is illustrated with two examples.,10.1002/sta4.55,2013-11-15,,['philippe lambert']
1311.5066,some context-specific graphical models for discrete longitudinal data,math.st stat.th,"ron et al (1998) introduced a rich family of models for discrete longitudinal data, called acyclic probabilistic finite automata. these may be described as context-specific graphical models, since they are represented as directed multigraphs that embody context-specific conditional independence relations. here we develop the methodology from a statistical modelling perspective. we show how likelihood ratio tests may be constructed using standard contingency table methods, and indicate how these may be used in model selection. we also show that the models generalize certain subclasses of conventional undirected and directed graphical models.",,2013-11-20,2014-08-13,"['david edwards', 'smitha ankinakatte']"
1311.5699,computational inference beyond kingman's coalescent,math.pr q-bio.pe stat.co,"full likelihood inference under kingman's coalescent is a computationally challenging problem to which importance sampling (is) and the product of approximate conditionals (pac) method have been applied successfully. both methods can be expressed in terms of families of intractable conditional sampling distributions (csds), and rely on principled approximations for accurate inference. recently, more general $\lambda$- and $\xi$-coalescents have been observed to provide better modelling fits to some genetic data sets. we derive families of approximate csds for finite sites $\lambda$- and $\xi$-coalescents, and use them to obtain ""approximately optimal"" is and pac algorithms for $\lambda$-coalescents, yielding substantial gains in efficiency over existing methods.",10.1239/jap/1437658613,2013-11-22,2015-12-16,"['jere koskela', 'paul a. jenkins', 'dario spano']"
1311.5769,"an exact, time-independent approach to clone size distributions in   normal and mutated cells",q-bio.qm stat.me,"biological tools such as genetic lineage tracing, three dimensional confocal microscopy and next generation dna sequencing are providing new ways to quantify the distribution of clones of normal and mutated cells. population-wide clone size distributions in vivo are complicated by multiple cell types, and overlapping birth and death processes. this has led to the increased need for mathematically informed models to understand their biological significance. standard approaches usually require knowledge of clonal age. we show that modelling on clone size independent of time is an alternative method that offers certain analytical advantages; it can help parameterize these models, and obtain distributions for counts of mutated or proliferating cells, for example. when applied to a general birth-death process common in epithelial progenitors this takes the form of a gamblers ruin problem, the solution of which relates to counting motzkin lattice paths. applying this approach to mutational processes, an alternative, exact, formulation of the classic luria delbruck problem emerges. this approach can be extended beyond neutral models of mutant clonal evolution, and also describe some distributions relating to sub-clones within a tumour. the approaches above are generally applicable to any markovian branching process where the dynamics of different ""coloured"" daughter branches are of interest.",,2013-11-22,,"['roshan a', 'jones ph', 'greenman cd']"
1311.6334,learning reputation in an authorship network,cs.si cs.ir cs.lg stat.ml,the problem of searching for experts in a given academic field is hugely important in both industry and academia. we study exactly this issue with respect to a database of authors and their publications. the idea is to use latent semantic indexing (lsi) and latent dirichlet allocation (lda) to perform topic modelling in order to find authors who have worked in a query field. we then construct a coauthorship graph and motivate the use of influence maximisation and a variety of graph centrality measures to obtain a ranked list of experts. the ranked lists are further improved using a markov chain-based rank aggregation approach. the complete method is readily scalable to large datasets. to demonstrate the efficacy of the approach we report on an extensive set of computational simulations using the arnetminer dataset. an improvement in mean average precision is demonstrated over the baseline case of simply using the order of authors found by the topic models.,,2013-11-25,,"['charanpal dhanjal', 'stéphan clémençon']"
1311.6530,a mixture of generalized hyperbolic factor analyzers,stat.me stat.ml,"model-based clustering imposes a finite mixture modelling structure on data for clustering. finite mixture models assume that the population is a convex combination of a finite number of densities, the distribution within each population is a basic assumption of each particular model. among all distributions that have been tried, the generalized hyperbolic distribution has the advantage that is a generalization of several other methods, such as the gaussian distribution, the skew t-distribution, etc. with specific parameters, it can represent either a symmetric or a skewed distribution. while its inherent flexibility is an advantage in many ways, it means the estimation of more parameters than its special and limiting cases. the aim of this work is to propose a mixture of generalized hyperbolic factor analyzers to introduce parsimony and extend the method to high dimensional data. this work can be seen as an extension of the mixture of factor analyzers model to generalized hyperbolic mixtures. the performance of our generalized hyperbolic factor analyzers is illustrated on real data, where it performs favourably compared to its gaussian analogue.",10.1007/s11634-015-0204-z,2013-11-25,2015-05-22,"['cristina tortora', 'paul d. mcnicholas', 'ryan p. browne']"
1311.7286,approximate bayesian computation with composite score functions,stat.co,"both approximate bayesian computation (abc) and composite likelihood methods are useful for bayesian and frequentist inference, respectively, when the likelihood function is intractable. we propose to use composite likelihood score functions as summary statistics in abc in order to obtain accurate approximations to the posterior distribution. this is motivated by the use of the score function of the full likelihood, and extended to general unbiased estimating functions in complex models. moreover, we show that if the composite score is suitably standardised, the resulting abc procedure is invariant to reparameterisations and automatically adjusts the curvature of the composite likelihood, and of the corresponding posterior distribution. the method is illustrated through examples with simulated data, and an application to modelling of spatial extreme rainfall data is discussed.",10.1007/s11222-015-9551-z,2013-11-28,2015-02-24,"['erlis ruli', 'nicola sartori', 'laura ventura']"
1311.7582,bayesian nonparametric location-scale-shape mixtures,stat.me,"discrete mixture models are one of the most successful approaches for density estimation. under a bayesian nonparametric framework, dirichlet process location-scale mixture of gaussian kernels is the golden standard, both having nice theoretical properties and computational tractability. in this paper we explore the use of the skew-normal kernel, which can naturally accommodate several degrees of skewness by the use of a third parameter. the choice of this kernel function allows us to formulate nonparametric location-scale-shape mixture prior with large support and good performance in different applications. asymptotically, we show that this modelling framework is consistent in frequentist sense. efficient gibbs sampling algorithms are also discussed and the performance of the methods are tested through simulations and applications to galaxy velocity and fertility data. extensions to accommodate discrete data are also discussed.",,2013-11-29,,"['antonio canale', 'bruno scarpa']"
1312.0020,sloshing in the lng shipping industry: risk modelling through   multivariate heavy-tail analysis,math.st stat.th,"in the liquefied natural gas (lng) shipping industry, the phenomenon of sloshing can lead to the occurrence of very high pressures in the tanks of the vessel. the issue of modelling or estimating the probability of the simultaneous occurrence of such extremal pressures is now crucial from the risk assessment point of view. in this paper, heavy-tail modelling, widely used as a conservative approach to risk assessment and corresponding to a worst-case risk analysis, is applied to the study of sloshing. multivariate heavy-tailed distributions are considered, with sloshing pressures investigated by means of small-scale replica tanks instrumented with d >1 sensors. when attempting to fit such nonparametric statistical models, one naturally faces computational issues inherent in the phenomenon of dimensionality. the primary purpose of this article is to overcome this barrier by introducing a novel methodology. for d-dimensional heavy-tailed distributions, the structure of extremal dependence is entirely characterised by the angular measure, a positive measure on the intersection of a sphere with the positive orthant in rd. as d increases, the mutual extremal dependence between variables becomes difficult to assess. based on a spectral clustering approach, we show here how a low dimensional approximation to the angular measure may be found. the nonparametric method proposed for model sloshing has been successfully applied to pressure data. the parsimonious representation thus obtained proves to be very convenient for the simulation of multivariate heavy-tailed distributions, allowing for the implementation of monte-carlo simulation schemes in estimating the probability of failure. besides confirming its performance on artificial data, the methodology has been implemented on a real data set specifically collected for risk assessment of sloshing in the lng shipping industry.",,2013-11-29,,"['antoine dematteo', 'stéphan clemencon', 'nicolas vayatis', 'mathilde mougeot']"
1312.0286,efficient learning and planning with compressed predictive states,cs.lg stat.ml,"predictive state representations (psrs) offer an expressive framework for modelling partially observable systems. by compactly representing systems as functions of observable quantities, the psr learning approach avoids using local-minima prone expectation-maximization and instead employs a globally optimal moment-based algorithm. moreover, since psrs do not require a predetermined latent state structure as an input, they offer an attractive framework for model-based reinforcement learning when agents must plan without a priori access to a system model. unfortunately, the expressiveness of psrs comes with significant computational cost, and this cost is a major factor inhibiting the use of psrs in applications. in order to alleviate this shortcoming, we introduce the notion of compressed psrs (cpsrs). the cpsr learning approach combines recent advancements in dimensionality reduction, incremental matrix decomposition, and compressed sensing. we show how this approach provides a principled avenue for learning accurate approximations of psrs, drastically reducing the computational costs associated with learning while also providing effective regularization. going further, we propose a planning framework which exploits these learned models. and we show that this approach facilitates model-learning and planning in large complex partially observable domains, a task that is infeasible without the principled use of compression.",,2013-12-01,2014-07-20,"['william l. hamilton', 'mahdi milani fard', 'joelle pineau']"
1312.0859,accelerated failure time models for competing risks in a cluster   weighted modelling framework,stat.me,"a novel approach for dealing with censored competing risks regression data is proposed. this is implemented by a mixture of accelerated failure time (aft) models for a competing risks scenario within a cluster-weighted modelling (cwm) framework. specifically, we make use of the log-normal aft model here but any commonly used aft model can be utilized. the alternating expectation conditional maximization algorithm (aecm) is used for parameter estimation and bootstrapping for standard error estimation. finally, we present our results on some simulated and real competing risks data.",,2013-12-03,,"['utkarsh j. dang', 'paul d. mcnicholas']"
1312.1794,statistical modelling of citation exchange between statistics journals,stat.ap cs.dl,"rankings of scholarly journals based on citation data are often met with skepticism by the scientific community. part of the skepticism is due to disparity between the common perception of journals' prestige and their ranking based on citation counts. a more serious concern is the inappropriate use of journal rankings to evaluate the scientific influence of authors. this paper focuses on analysis of the table of cross-citations among a selection of statistics journals. data are collected from the web of science database published by thomson reuters. our results suggest that modelling the exchange of citations between journals is useful to highlight the most prestigious journals, but also that journal citation data are characterized by considerable heterogeneity, which needs to be properly summarized. inferential conclusions require care in order to avoid potential over-interpretation of insignificant differences between journal ratings. comparison with published ratings of institutions from the uk's research assessment exercise shows strong correlation at aggregate level between assessed research quality and journal citation `export scores' within the discipline of statistics.",10.1111/rssa.12124,2013-12-06,2015-04-03,"['cristiano varin', 'manuela cattelan', 'david firth']"
1312.2240,testing for residual correlation of any order in the autoregressive   process,math.st stat.th,"we are interested in the implications of a linearly autocorrelated driven noise on the asymptotic behavior of the usual least squares estimator in a stable autoregressive process. we show that the least squares estimator is not consistent and we suggest a sharp analysis of its almost sure limiting value as well as its asymptotic normality. we also establish the almost sure convergence and the asymptotic normality of the estimated serial correlation parameter of the driven noise. then, we derive a statistical procedure enabling to test for correlation of any order in the residuals of an autoregressive modelling, giving clearly better results than the commonly used portmanteau tests of ljung-box and box-pierce, and appearing to outperform the breusch-godfrey procedure on small-sized samples.",,2013-12-08,2017-03-13,['frédéric proïa']
1312.2393,a dynamic probabilistic principal components model for the analysis of   longitudinal metabolomic data,stat.ap stat.me,"in a longitudinal metabolomics study, multiple metabolites are measured from several observations at many time points. interest lies in reducing the dimensionality of such data and in highlighting influential metabolites which change over time. a dynamic probabilistic principal components analysis (dppca) model is proposed to achieve dimension reduction while appropriately modelling the correlation due to repeated measurements. this is achieved by assuming an autoregressive model for some of the model parameters. linear mixed models are subsequently used to identify influential metabolites which change over time. the proposed model is used to analyse data from a longitudinal metabolomics animal study.",,2013-12-09,,"['gift nyamundanda', 'isobel claire gormley', 'lorraine brennan']"
1312.2423,the gamma-count distribution in the analysis of experimental   underdispersed data,stat.ap,"event counts are response variables with non-negative integer values representing the number of times that an event occurs within a fixed domain such as a time interval, a geographical area or a cell of a contingency table. analysis of counts by gaussian regression models ignores the discreteness, asymmetry and heterocedasticity and is inefficient, providing unrealistic standard errors or possibily negative predictions of the expected number of events. the poisson regression is the standard model for count data with underlying assumptions on the generating process which may be implausible in many applications. statisticians have long recognized the limitation of imposing equidispersion under the poisson regression model. a typical situation is when the conditional variance exceeds the conditional mean, in which case models allowing for overdispersion are routinely used. less reported is the case of underdispersion with fewer modelling alternatives and assessments available in the literature. one of such alternatives, the gamma-count model, is adopted here in the analysis of an agronomic experiment designed to investigate the effect of levels of defoliation on different phenological states upon the number of cotton bolls. results show improvements over the poisson model and the semiparametric quasi-poisson model in capturing the observed variability in the data. estimating rather than assuming the underlying variance process lead to important insights into the process.",10.1080/02664763.2014.922168,2013-12-09,,"['walmes marques zeviani', 'paulo justiniano ribeiro', 'wagner hugo bonat', 'silvia emiko shimakura', 'joel augusti muniz']"
1312.2923,lagrangian time series models for ocean surface drifter trajectories,stat.ap physics.ao-ph physics.flu-dyn stat.me,"this paper proposes stochastic models for the analysis of ocean surface trajectories obtained from freely-drifting satellite-tracked instruments. the proposed time series models are used to summarise large multivariate datasets and infer important physical parameters of inertial oscillations and other ocean processes. nonstationary time series methods are employed to account for the spatiotemporal variability of each trajectory. because the datasets are large, we construct computationally efficient methods through the use of frequency-domain modelling and estimation, with the data expressed as complex-valued time series. we detail how practical issues related to sampling and model misspecification may be addressed using semi-parametric techniques for time series, and we demonstrate the effectiveness of our stochastic models through application to both real-world data and to numerical model output.",10.1111/rssc.12112,2013-12-10,2015-04-21,"['adam m. sykulski', 'sofia c. olhede', 'jonathan m. lilly', 'eric danioux']"
1312.4479,parametric modelling of multivariate count data using probabilistic   graphical models,stat.ml cs.lg stat.me,"multivariate count data are defined as the number of items of different categories issued from sampling within a population, which individuals are grouped into categories. the analysis of multivariate count data is a recurrent and crucial issue in numerous modelling problems, particularly in the fields of biology and ecology (where the data can represent, for example, children counts associated with multitype branching processes), sociology and econometrics. we focus on i) identifying categories that appear simultaneously, or on the contrary that are mutually exclusive. this is achieved by identifying conditional independence relationships between the variables; ii)building parsimonious parametric models consistent with these relationships; iii) characterising and testing the effects of covariates on the joint distribution of the counts. to achieve these goals, we propose an approach based on graphical probabilistic models, and more specifically partially directed acyclic graphs.",,2013-12-16,,"['pierre fernique', 'jean-baptiste durand', 'yann guédon']"
1312.5911,estimating time-changes in noisy l\'evy models,math.st q-fin.st stat.th,"in quantitative finance, we often model asset prices as a noisy ito semimartingale. as this model is not identifiable, approximating by a time-changed levy process can be useful for generative modelling. we give a new estimate of the normalised volatility or time change in this model, which obtains minimax convergence rates, and is unaffected by infinite-variation jumps. in the semimartingale model, our estimate remains accurate for the normalised volatility, obtaining convergence rates as good as any previously implied in the literature.",10.1214/14-aos1250,2013-12-20,2014-11-14,['adam d. bull']
1312.6220,dynamic modelling of hepatitis c virus transmission among people who   inject drugs: a methodological review,stat.ap math.ds q-bio.qm,"equipment sharing among people who inject drugs (pwid) is a key risk factor in infection by hepatitis c virus (hcv). both the effectiveness and cost-effectiveness of interventions aimed at reducing hcv transmission in this population (such as opioid substitution therapy, needle exchange programs or improved treatment) are difficult to evaluate using field surveys. ethical issues and complicated access to the pwid population make it difficult to gather epidemiological data. in this context, mathematical modelling of hcv transmission is a useful alternative for comparing the cost and effectiveness of various interventions. several models have been developed in the past few years. they are often based on strong hypotheses concerning the population structure. this review presents compartmental and individual-based models in order to underline their strengths and limits in the context of hcv infection among pwid. the final section discusses the main results of the papers.",10.1111/jvh.12337,2013-12-21,2015-02-09,"['anthony cousien', 'viet chi tran', 'sylvie deuffic-burban', 'marie jauffret-roustide', 'jean-stephane dhersin', 'yazdan yazdanpanah']"
1312.6407,multivariate markov-switching models and tail risk interdependence,stat.me,"markov switching models are often used to analyze financial returns because of their ability to capture frequently observed stylized facts. in this paper we consider a multivariate student-t version of the model as a viable alternative to the usual multivariate gaussian distribution, providing a natural robust extension that accounts for heavy-tails and time varying non-linear correlations. moreover, these modelling assumptions allow us to capture extreme tail co-movements which are of fundamental importance to assess the underlying dependence structure of asset returns during extreme events such as financial crisis. for the considered model we provide new risk interdependence measures which generalize the existing ones, like the conditional value-at-risk (covar). the proposed measures aim to capture interconnections among multiple connecting market participants which is particularly relevant during period of crisis when several institutions may contemporaneously experience distress instances. those measures are analytically evaluated on the predictive distribution of the modes in order to provide a forward-looking risk quantification. application on a set of u.s. banks is considered to show that the right specification of the model conditional distribution along with a multiple risk interdependence measure may help to better understand how the overall risk is shared among institutions.",,2013-12-22,2014-03-02,"['mauro bernardi', 'antonello maruotti', 'lea petrella']"
1401.0168,efficient inference and simulation for elliptical pareto processes,stat.me,"recent advances in extreme value theory have established $\ell$-pareto processes as the natural limits for extreme events defined in terms of exceedances of a risk functional. here we provide methods for the practical modelling of data based on a tractable yet flexible dependence model. we introduce the class of elliptical $\ell$-pareto processes, which arise as the limit of threshold exceedances of certain elliptical processes characterized by a correlation function and a shape parameter. an efficient inference method based on maximizing a full likelihood with partial censoring is developed. novel procedures for exact conditional and unconditional simulation are proposed. these ideas are illustrated using precipitation extremes in switzerland.",10.1093/biomet/asv045,2013-12-31,2015-11-11,"['emeric thibaud', 'thomas opitz']"
1401.1052,inverse bayesian estimation of gravitational mass density in galaxies   from missing kinematic data,stat.ap astro-ph.ga stat.me,"in this paper we focus on a type of inverse problem in which the data is expressed as an unknown function of the sought and unknown model function (or its discretised representation as a model parameter vector). in particular, we deal with situations in which training data is not available. then we cannot model the unknown functional relationship between data and the unknown model function (or parameter vector) with a gaussian process of appropriate dimensionality. a bayesian method based on state space modelling is advanced instead. within this framework, the likelihood is expressed in terms of the probability density function ($pdf$) of the state space variable and the sought model parameter vector is embedded within the domain of this $pdf$. as the measurable vector lives only inside an identified sub-volume of the system state space, the $pdf$ of the state space variable is projected onto the space of the measurables, and it is in terms of the projected state space density that the likelihood is written; the final form of the likelihood is achieved after convolution with the distribution of measurement errors. application motivated vague priors are invoked and the posterior probability density of the model parameter vectors, given the data is computed. inference is performed by taking posterior samples with adaptive mcmc. the method is illustrated on synthetic as well as real galactic data.",,2014-01-06,,"['dalia chakrabarty', 'prasenjit saha']"
1401.2649,personalised medicine: taking a new look at the patient,stat.ap,"personalised medicine strives to identify the right treatment for the right patient at the right time, integrating different types of biological and environmental information. such information come from a variety of sources: omics data (genomic, proteomic, metabolomic, etc.), live molecular diagnostics, and other established diagnostics routinely used by medical doctors. integrating these different kinds of data, which are all high-dimensional, presents significant challenges in knowledge representation and subsequent reasoning. the ultimate goal of such a modelling effort is to elucidate the flow of information that links genes, protein signalling and other physiological responses to external stimuli such as environmental conditions or the progress of a disease.",,2014-01-12,,['marco scutari']
1401.4342,modelling a response as a function of high frequency count data: the   association between physical activity and fat mass,stat.ap,"we present a new statistical modelling approach where the response is a function of high frequency count data. our application is about investigating the relationship between the health outcome fat mass and physical activity (pa) measured by accelerometer. the accelerometer quantifies the intensity of physical activity as counts per epoch over a given period of time. we use data from the avon longitudinal study of parents and children (alspac) where accelerometer data is available as a time series of accelerometer counts per minute over seven days for a subset of children. in order to compare accelerometer profiles between individuals and to reduce the high dimension a functional summary of the profiles is used. we use the histogram as a functional summary due to its simplicity, suitability and ease of interpretation. our model is an extension of generalised regression of scalars on functions or signal regression. it allows also multi-dimensional functional predictors and additive non-linear predictors for metric covariates. the additive multidimensional functional predictors allow investigating specific questions about whether the effect of pa varies over its intensity, by gender, by time of day or by day of the week. the key feature of the model is that it utilises the full profile of measured pa without requiring cut-points defining intensity levels for light, moderate and vigorous activity. we show that the (not necessarily causal) effect of pa is not linear and not constant over the activity intensity. also, there is little evidence to suggest that the effect of pa intensity varies by gender or whether it happens on weekdays or on weekends.",10.1177/0962280215595832,2014-01-17,,"['nicole h. augustin', 'calum mattocks', 'julian j. faraway', 'sonja greven', 'andy r. ness']"
1401.5613,a precision of the sequential change point detection,math.st math.pr stat.ap stat.th,"a random sequence having two segments being the homogeneous markov processes is registered. each segment has his own transition probability law and the length of the segment is unknown and random. the transition probabilities of each process are known and a priori distribution of the disorder moment is given. the decision maker aim is to detect the moment of the transition probabilities change. the detection of the disorder rarely is precise. the decision maker accepts some deviation in estimation of the disorder moment. in the considered model the aim is to indicate the change point with fixed, bounded error with maximal probability. the case with various precision for over and under estimation of this point is analysed. the case when the disorder does not appears with positive probability is also included. the results insignificantly extends range of application, explain the structure of optimal detector in various circumstances and shows new details of the solution construction. the motivation for this investigation is the modelling of the attacks in the node of networks. the objectives is to detect one of the attack immediately or in very short time before or after it appearance with highest probability. the problem is reformulated to optimal stopping of the observed sequences. the detailed analysis of the problem is presented to show the form of optimal decision function.",,2014-01-22,,"['a. ochman-gozdek', 'w. sarnowski', 'k. j. szajowski']"
1401.5747,multiple imputation for continuous variables using a bayesian principal   component analysis,stat.me,"we propose a multiple imputation method based on principal component analysis (pca) to deal with incomplete continuous data. to reflect the uncertainty of the parameters from one imputation to the next, we use a bayesian treatment of the pca model. using a simulation study and real data sets, the method is compared to two classical approaches: multiple imputation based on joint modelling and on fully conditional modelling. contrary to the others, the proposed method can be easily used on data sets where the number of individuals is less than the number of variables and when the variables are highly correlated. in addition, it provides unbiased point estimates of quantities of interest, such as an expectation, a regression coefficient or a correlation coefficient, with a smaller mean squared error. furthermore, the widths of the confidence intervals built for the quantities of interest are often smaller whilst ensuring a valid coverage.",,2014-01-22,2015-08-19,"['vincent audigier', 'françois husson', 'julie josse']"
1401.7214,exploring dependence between categorical variables: benefits and   limitations of using variable selection within bayesian clustering in   relation to log-linear modelling with interaction terms,stat.me,"this manuscript is concerned with relating two approaches that can be used to explore complex dependence structures between categorical variables, namely bayesian partitioning of the covariate space incorporating a variable selection procedure that highlights the covariates that drive the clustering, and log-linear modelling with interaction terms. we derive theoretical results on this relation and discuss if they can be employed to assist log-linear model determination, demonstrating advantages and limitations with simulated and real data sets. the main advantage concerns sparse contingency tables. inferences from clustering can potentially reduce the number of covariates considered and, subsequently, the number of competing log-linear models, making the exploration of the model space feasible. variable selection within clustering can inform on marginal independence in general, thus allowing for a more efficient exploration of the log-linear model space. however, we show that the clustering structure is not informative on the existence of interactions in a consistent manner. this work is of interest to those who utilize log-linear models, as well as practitioners such as epidemiologists that use clustering models to reduce the dimensionality in the data and to reveal interesting patterns on how covariates combine.",,2014-01-28,2016-01-05,"['michail papathomas', 'sylvia richardson']"
1402.0859,the informed sampler: a discriminative approach to bayesian inference in   generative computer vision models,cs.cv cs.lg stat.ml,"computer vision is hard because of a large variability in lighting, shape, and texture; in addition the image signal is non-additive due to occlusion. generative models promised to account for this variability by accurately modelling the image formation process as a function of latent variables with prior beliefs. bayesian posterior inference could then, in principle, explain the observation. while intuitively appealing, generative models for computer vision have largely failed to deliver on that promise due to the difficulty of posterior inference. as a result the community has favoured efficient discriminative approaches. we still believe in the usefulness of generative models in computer vision, but argue that we need to leverage existing discriminative or even heuristic computer vision methods. we implement this idea in a principled way with an ""informed sampler"" and in careful experiments demonstrate it on challenging generative models which contain renderer programs as their components. we concentrate on the problem of inverting an existing graphics rendering engine, an approach that can be understood as ""inverse graphics"". the informed sampler, using simple discriminative proposals based on existing computer vision technology, achieves significant improvements of inference.",10.1016/j.cviu.2015.03.002,2014-02-04,2015-03-07,"['varun jampani', 'sebastian nowozin', 'matthew loper', 'peter v. gehler']"
1402.1213,a statistical modelling approach to detecting community in networks,cs.si stat.ap,"there has been considerable recent interest in algorithms for finding communities in networks - groups of vertex within which connections are dense (frequent), but between which connections are sparser (rare). most of the current literature advocates an heuristic approach to the removal of the edges (i.e., removing the links that are less significant using a well-designed function). in this article, we will investigate a technique for uncovering latent communities using a new modelling approach, based on how information spread within a network. it will prove to be easy to use, robust and scalable. it makes supplementary information related to the network/community structure (different communications, consecutive observations) easier to integrate. we will demonstrate the efficiency of our approach by providing some illustrating real-world applications, like the famous zachary karate club, or the amazon political books buyers network.",,2014-02-05,,['adrien ickowicz']
1402.1389,distributed variational inference in sparse gaussian process regression   and latent variable models,stat.ml cs.lg,"gaussian processes (gps) are a powerful tool for probabilistic inference over functions. they have been applied to both regression and non-linear dimensionality reduction, and offer desirable properties such as uncertainty estimates, robustness to over-fitting, and principled ways for tuning hyper-parameters. however the scalability of these models to big datasets remains an active topic of research. we introduce a novel re-parametrisation of variational inference for sparse gp regression and latent variable models that allows for an efficient distributed algorithm. this is done by exploiting the decoupling of the data given the inducing points to re-formulate the evidence lower bound in a map-reduce setting. we show that the inference scales well with data and computational resources, while preserving a balanced distribution of the load among the nodes. we further demonstrate the utility in scaling gaussian processes to big data. we show that gp performance improves with increasing amounts of data in regression (on flight data with 2 million records) and latent variable modelling (on mnist). the results show that gps perform better than many common models often used for big data.",,2014-02-06,2014-09-29,"['yarin gal', 'mark van der wilk', 'carl e. rasmussen']"
1402.1876,information theory and image understanding: an application to   polarimetric sar imagery,stat.me math.st stat.th,"this work presents a comprehensive examination of the use of information theory for understanding polarimetric synthetic aperture radar (polsar) images by means of contrast measures that can be used as test statistics. due to the phenomenon called `speckle', common to all images obtained with coherent illumination such as polsar imagery, accurate modelling is required in their processing and analysis. the scaled multilook complex wishart distribution has proven to be a successful approach for modelling radar backscatter from forest and pasture areas. classification, segmentation, and image analysis techniques which depend on this model have been devised, and many of them employ some kind of dissimilarity measure. specifically, we introduce statistical tests for analyzing contrast in such images. these tests are based on the chi-square, kullback-leibler, r\'enyi, bhattacharyya, and hellinger distances. results obtained by monte carlo experiments reveal the kullback-leibler distance as the best one with respect to the empirical test sizes under several situations which include pure and contaminated data. the proposed methodology was applied to actual data, obtained by an e-sar sensor over surroundings of we$\beta$ssling, bavaria, germany.",,2014-02-08,,"['a. c. frery', 'a. d. c. nascimento', 'r. j. cintra']"
1402.2492,risk margin quantile function via parametric and non-parametric bayesian   quantile regression,q-fin.rm stat.ap stat.co,"we develop quantile regression models in order to derive risk margin and to evaluate capital in non-life insurance applications. by utilizing the entire range of conditional quantile functions, especially higher quantile levels, we detail how quantile regression is capable of providing an accurate estimation of risk margin and an overview of implied capital based on the historical volatility of a general insurers loss portfolio. two modelling frameworks are considered based around parametric and nonparametric quantile regression models which we develop specifically in this insurance setting.   in the parametric quantile regression framework, several models including the flexible generalized beta distribution family, asymmetric laplace (al) distribution and power pareto distribution are considered under a bayesian regression framework. the bayesian posterior quantile regression models in each case are studied via markov chain monte carlo (mcmc) sampling strategies.   in the nonparametric quantile regression framework, that we contrast to the parametric bayesian models, we adopted an al distribution as a proxy and together with the parametric al model, we expressed the solution as a scale mixture of uniform distributions to facilitate implementation. the models are extended to adopt dynamic mean, variance and skewness and applied to analyze two real loss reserve data sets to perform inference and discuss interesting features of quantile regression for risk margin calculations.",,2014-02-11,,"['alice x. d. dong', 'jennifer s. k. chan', 'gareth w. peters']"
1402.3014,joint inference of misaligned irregular time series with application to   greenland ice core data,stat.ap,"ice cores provide insight into the past climate over many millennia. due to ice compaction, the raw data for any single core are irregular in time. multiple cores have different irregularities; jointly these series are misaligned. after processing, such data are made available to researchers as regular time series: a data product. typically, these cores are independently processed. in this paper, we consider a fast bayesian method for the joint processing of multiple irregular series. this is shown to be more efficient. further, our approach permits a realistic modelling of the impact of the multiple sources of uncertainty. the methodology is illustrated with the analysis of a pair of ice cores (gisp2 and grip). our data products, in the form of marginal posterior distributions on an arbitrary temporal grid, are finite gaussian mixtures. we can also produce sample paths from the joint posterior distribution to study non-linear functionals of interest. more generally, the concept of joint analysis via hierarchical gaussian process model can be widely extended as the models used can be viewed within the larger context of continuous space-time processes.",,2014-02-12,2014-09-22,"['thinh k. doan', 'andrew c. parnell', 'john haslett']"
1402.3070,squeezing bottlenecks: exploring the limits of autoencoder semantic   representation capabilities,cs.ir cs.lg stat.ml,"we present a comprehensive study on the use of autoencoders for modelling text data, in which (differently from previous studies) we focus our attention on the following issues: i) we explore the suitability of two different models bda and rsda for constructing deep autoencoders for text data at the sentence level; ii) we propose and evaluate two novel metrics for better assessing the text-reconstruction capabilities of autoencoders; and iii) we propose an automatic method to find the critical bottleneck dimensionality for text language representations (below which structural information is lost).",,2014-02-13,,"['parth gupta', 'rafael e. banchs', 'paolo rosso']"
1402.3433,thresholds in choice behaviour and the size of travel time savings,stat.me,"travel time savings are usually the most substantial economic benefit of transport infrastructure projects. however, questions surround whether small time savings are as valuable per unit as larger savings. thresholds in individual choice behaviour are one reason cited for a discounted unit value for small time savings. we demonstrate different approaches for modelling these thresholds using synthetic and stated choice data. we show that the consideration of thresholds is important, even if the discounted unit value for small travel time savings is rejected for transport project appraisal. if an existing threshold is ignored in model estimation, the value of travel time savings will be biased. the presented procedure might also be useful to model thresholds in other contexts of choice behaviour.",,2014-02-14,2015-09-02,"['andy obermeyer', 'martin treiber', 'christos evangelinos']"
1402.3783,map-aware models for indoor wireless localization systems: an   experimental study,cs.it math.it stat.ap,"the accuracy of indoor wireless localization systems can be substantially enhanced by map-awareness, i.e., by the knowledge of the map of the environment in which localization signals are acquired. in fact, this knowledge can be exploited to cancel out, at least to some extent, the signal degradation due to propagation through physical obstructions, i.e., to the so called non-line-of-sight bias. this result can be achieved by developing novel localization techniques that rely on proper map-aware statistical modelling of the measurements they process. in this manuscript a unified statistical model for the measurements acquired in map-aware localization systems based on time-of-arrival and received signal strength techniques is developed and its experimental validation is illustrated. finally, the accuracy of the proposed map-aware model is assessed and compared with that offered by its map-unaware counterparts. our numerical results show that, when the quality of acquired measurements is poor, map-aware modelling can enhance localization accuracy by up to 110% in certain scenarios.",,2014-02-16,,"['francesco montorsi', 'fabrizio pancaldi', 'giorgio m. vitetta']"
1402.5161,statistical constraints,cs.ai stat.me,"we introduce statistical constraints, a declarative modelling tool that links statistics and constraint programming. we discuss two statistical constraints and some associated filtering algorithms. finally, we illustrate applications to standard problems encountered in statistics and to a novel inspection scheduling problem in which the aim is to find inspection plans with desirable statistical properties.",10.3233/978-1-61499-419-0-777,2014-02-20,2014-08-14,"['roberto rossi', 'steven prestwich', 's. armagan tarim']"
1403.0510,bayesian density estimation via multiple sequential inversions of 2-d   images with application in electron microscopy,stat.ap cond-mat.mtrl-sci,"we present a new bayesian methodology to learn the unknown material density of a given sample by inverting its two-dimensional images that are taken with a scanning electron microscope. an image results from a sequence of projections of the convolution of the density function with the unknown microscopy correction function that we also learn from the data. we invoke a novel design of experiment, involving imaging at multiple values of the parameter that controls the sub-surface depth from which information about the density structure is carried, to result in the image. real-life material density functions are characterised by high density contrasts and typically are highly discontinuous, implying that they exhibit correlation structures that do not vary smoothly. in the absence of training data, modelling such correlation structures of real material density functions is not possible. so we discretise the material sample and treat values of the density function at chosen locations inside it as independent and distribution-free parameters. resolution of the available image dictates the discretisation length of the model; three models pertaining to distinct resolution classes are developed. we develop priors on the material density, such that these priors adapt to the sparsity inherent in the density function. the likelihood is defined in terms of the distance between the convolution of the unknown functions and the image data. the posterior probability density of the unknowns given the data is expressed using the developed priors on the density and priors on the microscopy correction function as elicitated from the microscopy literature. we achieve posterior samples using an adaptive metropolis-within-gibbs inference scheme. the method is applied to learn the material density of a 3-d sample of a real nano-structure and of simulated alloy samples.",,2014-03-03,,"['dalia chakrabarty', 'fabio rigat', 'nare gabrielyan', 'richard beanland', 'shashi paul']"
1403.0623,global solar irradiation prediction using a multi-gene genetic   programming approach,cs.ne cs.ce stat.ap,"in this paper, a nonlinear symbolic regression technique using an evolutionary algorithm known as multi-gene genetic programming (mggp) is applied for a data-driven modelling between the dependent and the independent variables. the technique is applied for modelling the measured global solar irradiation and validated through numerical simulations. the proposed modelling technique shows improved results over the fuzzy logic and artificial neural network (ann) based approaches as attempted by contemporary researchers. the method proposed here results in nonlinear analytical expressions, unlike those with neural networks which is essentially a black box modelling approach. this additional flexibility is an advantage from the modelling perspective and helps to discern the important variables which affect the prediction. due to the evolutionary nature of the algorithm, it is able to get out of local minima and converge to a global optimum unlike the back-propagation (bp) algorithm used for training neural networks. this results in a better percentage fit than the ones obtained using neural networks by contemporary researchers. also a hold-out cross validation is done on the obtained genetic programming (gp) results which show that the results generalize well to new data and do not over-fit the training samples. the multi-gene gp results are compared with those, obtained using its single-gene version and also the same with four classical regression models in order to show the effectiveness of the adopted approach.",10.1063/1.4850495,2014-03-03,,"['indranil pan', 'daya shankar pandey', 'saptarshi das']"
1403.0648,multi-period trading prediction markets with connections to machine   learning,cs.gt cs.lg q-fin.tr stat.ml,"we present a new model for prediction markets, in which we use risk measures to model agents and introduce a market maker to describe the trading process. this specific choice on modelling tools brings us mathematical convenience. the analysis shows that the whole market effectively approaches a global objective, despite that the market is designed such that each agent only cares about its own goal. additionally, the market dynamics provides a sensible algorithm for optimising the global objective. an intimate connection between machine learning and our markets is thus established, such that we could 1) analyse a market by applying machine learning methods to the global objective, and 2) solve machine learning problems by setting up and running certain markets.",,2014-03-03,,"['jinli hu', 'amos storkey']"
1403.0848,"netconomics: novel forecasting techniques from the combination of big   data, network science and economics",q-fin.gn physics.soc-ph stat.me,"the combination of the network theoretic approach with recently available abundant economic data leads to the development of novel analytic and computational tools for modelling and forecasting key economic indicators. the main idea is to introduce a topological component into the analysis, taking into account consistently all higher-order interactions. we present three basic methodologies to demonstrate different approaches to harness the resulting network gain. first, a multiple linear regression optimisation algorithm is used to generate a relational network between individual components of national balance of payment accounts. this model describes annual statistics with a high accuracy and delivers good forecasts for the majority of indicators. second, an early-warning mechanism for global financial crises is presented, which combines network measures with standard economic indicators. from the analysis of the cross-border portfolio investment network of long-term debt securities, the proliferation of a wide range of over-the-counter-traded financial derivative products, such as credit default swaps, can be described in terms of gross-market values and notional outstanding amounts, which are associated with increased levels of market interdependence and systemic risk. third, considering the flow-network of goods traded between g-20 economies, network statistics provide better proxies for key economic measures than conventional indicators. for example, it is shown that a country's gate-keeping potential, as a measure for local power, projects its annual change of gdp generally far better than the volume of its imports or exports.",,2014-03-04,,"['andreas joseph', 'irena vodenska', 'eugene stanley', 'guanrong chen']"
1403.1452,the evolution of boosting algorithms - from machine learning to   statistical modelling,stat.me,"the concept of boosting emerged from the field of machine learning. the basic idea is to boost the accuracy of a weak classifying tool by combining various instances into a more accurate prediction. this general concept was later adapted to the field of statistical modelling. this review article attempts to highlight this evolution of boosting algorithms from machine learning to statistical modelling. we describe the adaboost algorithm for classification as well as the two most prominent statistical boosting approaches, gradient boosting and likelihood-based boosting. although both appraoches are typically treated separately in the literature, they share the same methodological roots and follow the same fundamental concepts. compared to the initial machine learning algorithms, which must be seen as black-box prediction schemes, statistical boosting result in statistical models which offer a straight-forward interpretation. we highlight the methodological background and present the most common software implementations. worked out examples and corresponding r code can be found in the appendix.",10.3414/me13-01-0122,2014-03-06,2014-11-18,"['andreas mayr', 'harald binder', 'olaf gefeller', 'matthias schmid']"
1403.1692,extending statistical boosting - an overview of recent methodological   developments,stat.me,"boosting algorithms to simultaneously estimate and select predictor effects in statistical models have gained substantial interest during the last decade. this review article aims to highlight recent methodological developments regarding boosting algorithms for statistical modelling especially focusing on topics relevant for biomedical research. we suggest a unified framework for gradient boosting and likelihood-based boosting (statistical boosting) which have been addressed strictly separated in the literature up to now. statistical boosting algorithms have been adapted to carry out unbiased variable selection and automated model choice during the fitting process and can nowadays be applied in almost any possible type of regression setting in combination with a large amount of different types of predictor effects. the methodological developments on statistical boosting during the last ten years can be grouped into three different lines of research: (i) efforts to ensure variable selection leading to sparser models, (ii) developments regarding different types of predictor effects and their selection (model choice), (iii) approaches to extend the statistical boosting framework to new regression settings.",10.3414/me13-01-0123,2014-03-07,2014-11-18,"['andreas mayr', 'harald binder', 'olaf gefeller', 'matthias schmid']"
1403.1783,bayesian spatio-temporal epidemic models with applications to sheep pox,stat.ap,"epidemic data often possess certain characteristics, such as the presence of many zeros, the spatial nature of the disease spread mechanism or environmental noise. this paper addresses these issues via suitable bayesian modelling. in doing so we utilise stochastic regression models appropriate for spatio-temporal count data with an excess number of zeros. the developed regression framework can incorporate serial correlation and time varying covariates through an ornstein uhlenbeck process formulation. in addition, we explore the effect of different priors, including default options and techniques based upon variations of mixtures of $g$-priors. the effect of different distance kernels for the epidemic model component is investigated. we proceed by developing branching process-based methods for testing scenarios for disease control, thus linking traditional spatio-temporal models with epidemic processes, useful in policy-focused decision making. the approach is illustrated with an application to a sheep pox dataset from the evros region, greece.",,2014-03-07,,"['c. malesios', 'n. demiris', 'k. kalogeropoulos', 'i. ntzoufras']"
1403.4640,communication communities in moocs,cs.cy cs.si stat.ml,"massive open online courses (moocs) bring together thousands of people from different geographies and demographic backgrounds -- but to date, little is known about how they learn or communicate. we introduce a new content-analysed mooc dataset and use bayesian non-negative matrix factorization (bnmf) to extract communities of learners based on the nature of their online forum posts. we see that bnmf yields a superior probabilistic generative model for online discussions when compared to other models, and that the communities it learns are differentiated by their composite students' demographic and course performance indicators. these findings suggest that computationally efficient probabilistic generative modelling of moocs can reveal important insights for educational researchers and practitioners and help to develop more intelligent and responsive online learning environments.",,2014-03-18,2014-04-16,"['nabeel gillani', 'rebecca eynon', 'michael osborne', 'isis hjorth', 'stephen roberts']"
1404.1733,"comment on ""comparing two formulations of skew distributions with   special reference to model-based clustering"" by a. azzalini, r. browne, m.   genton, and p. mcnicholas",stat.me,"in this paper, we comment on the recent comparison in azzalini et al. (2014) of two different distributions proposed in the literature for the modelling of data that have asymmetric and possibly long-tailed clusters. they are referred to as the restricted and unrestricted skew t-distributions by lee and mclachlan (2013a). firstly, we wish to point out that in lee and mclachlan (2014b), which preceded this comparison, it is shown how a distribution belonging to the broader class, the canonical fundamental skew t (cfust) class, can be fitted with essentially no additional computational effort than for the unrestricted distribution. the cfust class includes the restricted and unrestricted distributions as special cases. thus the user now has the option of letting the data decide as to which model is appropriate for their particular dataset. secondly, we wish to identify several statements in the comparison by azzalini et al.(2014) that demonstrate a serious misunderstanding of the reporting of results in lee and mclachlan (2014a) on the relative performance of these two skew t-distributions. in particular, there is an apparent misunderstanding of the nomenclature that has been adopted to distinguish between these two models. thirdly, we take the opportunity to report here that we have obtained improved fits, in some cases a marked improvement, for the unrestricted model for various cases corresponding to different combinations of the variables in the two real datasets that were used in azzalini et al. (2014) to mount their claims on the relative superiority of the restricted and unrestricted models. for one case the misclassification rate of our fit under the unrestricted model is less than one third of their reported error rate. our results thus reverse their claims on the ranking of the restricted and unrestricted models in such cases.",,2014-04-07,,"['geoffrey j. mclachlan', 'sharon x. lee']"
1404.2189,data mining for censored time-to-event data: a bayesian network model   for predicting cardiovascular risk from electronic health record data,stat.ml stat.ap,"models for predicting the risk of cardiovascular events based on individual patient characteristics are important tools for managing patient care. most current and commonly used risk prediction models have been built from carefully selected epidemiological cohorts. however, the homogeneity and limited size of such cohorts restricts the predictive power and generalizability of these risk models to other populations. electronic health data (ehd) from large health care systems provide access to data on large, heterogeneous, and contemporaneous patient populations. the unique features and challenges of ehd, including missing risk factor information, non-linear relationships between risk factors and cardiovascular event outcomes, and differing effects from different patient subgroups, demand novel machine learning approaches to risk model development. in this paper, we present a machine learning approach based on bayesian networks trained on ehd to predict the probability of having a cardiovascular event within five years. in such data, event status may be unknown for some individuals as the event time is right-censored due to disenrollment and incomplete follow-up. since many traditional data mining methods are not well-suited for such data, we describe how to modify both modelling and assessment techniques to account for censored observation times. we show that our approach can lead to better predictive performance than the cox proportional hazards model (i.e., a regression-based approach commonly used for censored, time-to-event data) or a bayesian network with {\em{ad hoc}} approaches to right-censoring. our techniques are motivated by and illustrated on data from a large u.s. midwestern health care system.",,2014-04-08,,"['sunayan bandyopadhyay', 'julian wolfson', 'david m. vock', 'gabriela vazquez-benitez', 'gediminas adomavicius', 'mohamed elidrisi', 'paul e. johnson', ""patrick j. o'connor""]"
1404.3046,ecf identification of garch systems driven by l\'evy processes,math.st stat.th,"l\'evy processes are widely used in financial mathematics, telecommunication, economics, queueing theory and natural sciences for modelling. we propose an essentially asymptotically efficient estimation method for the system parameters of general autoregressive conditional heteroscedasticity (garch) processes. as an alternative to the maximum likelihood (ml) method we develop and analyze a novel identification method by adapting the so-called empirical characteristic function method (ecf) originally devised for estimating parameters of c.f.-s from i.i.d. samples. precise characterization of the errors of these estimators will be given, and their asymptotic covariance matrices will be obtained.",,2014-04-11,,"['máté mánfay', 'lászló gerencsér', 'zsanett orlovits']"
1404.4077,model-based clustering using copulas with applications,stat.me,"the majority of model-based clustering techniques is based on multivariate normal models and their variants. in this paper copulas are used for the construction of flexible families of models for clustering applications. the use of copulas in model-based clustering offers two direct advantages over current methods: i) the appropriate choice of copulas provides the ability to obtain a range of exotic shapes for the clusters, and ii) the explicit choice of marginal distributions for the clusters allows the modelling of multivariate data of various modes (either discrete or continuous) in a natural way. this paper introduces and studies the framework of copula-based finite mixture models for clustering applications. estimation in the general case can be performed using standard em, and, depending on the mode of the data, more efficient procedures are provided that can fully exploit the copula structure. the closure properties of the mixture models under marginalization are discussed, and for continuous, real-valued data parametric rotations in the sample space are introduced, with a parallel discussion on parameter identifiability depending on the choice of copulas for the components. the exposition of the methodology is accompanied and motivated by the analysis of real and artificial data.",10.1007/s11222-015-9590-5,2014-04-15,2015-07-02,"['ioannis kosmidis', 'dimitris karlis']"
1404.4175,meg decoding across subjects,stat.ml cs.lg q-bio.nc,"brain decoding is a data analysis paradigm for neuroimaging experiments that is based on predicting the stimulus presented to the subject from the concurrent brain activity. in order to make inference at the group level, a straightforward but sometimes unsuccessful approach is to train a classifier on the trials of a group of subjects and then to test it on unseen trials from new subjects. the extreme difficulty is related to the structural and functional variability across the subjects. we call this approach ""decoding across subjects"". in this work, we address the problem of decoding across subjects for magnetoencephalographic (meg) experiments and we provide the following contributions: first, we formally describe the problem and show that it belongs to a machine learning sub-field called transductive transfer learning (ttl). second, we propose to use a simple ttl technique that accounts for the differences between train data and test data. third, we propose the use of ensemble learning, and specifically of stacked generalization, to address the variability across subjects within train data, with the aim of producing more stable classifiers. on a face vs. scramble task meg dataset of 16 subjects, we compare the standard approach of not modelling the differences across subjects, to the proposed one of combining ttl and ensemble learning. we show that the proposed approach is consistently more accurate than the standard one.",,2014-04-16,,"['emanuele olivetti', 'seyed mostafa kia', 'paolo avesani']"
1404.4414,probit transformation for nonparametric kernel estimation of the copula   density,stat.me math.st stat.th,"copula modelling has become ubiquitous in modern statistics. here, the problem of nonparametrically estimating a copula density is addressed. arguably the most popular nonparametric density estimator, the kernel estimator is not suitable for the unit-square-supported copula densities, mainly because it is heavily affected by boundary bias issues. in addition, most common copulas admit unbounded densities, and kernel methods are not consistent in that case. in this paper, a kernel-type copula density estimator is proposed. it is based on the idea of transforming the uniform marginals of the copula density into normal distributions via the probit function, estimating the density in the transformed domain, which can be accomplished without boundary problems, and obtaining an estimate of the copula density through back-transformation. although natural, a raw application of this procedure was, however, seen not to perform very well in the earlier literature. here, it is shown that, if combined with local likelihood density estimation methods, the idea yields very good and easy to implement estimators, fixing boundary issues in a natural way and able to cope with unbounded copula densities. the asymptotic properties of the suggested estimators are derived, and a practical way of selecting the crucially important smoothing parameters is devised. finally, extensive simulation studies and a real data analysis evidence their excellent performance compared to their main competitors.",,2014-04-16,,"['gery geenens', 'arthur charpentier', 'davy paindaveine']"
1405.1299,model-based clustering of gaussian copulas for mixed data,stat.me,"clustering task of mixed data is a challenging problem. in a probabilistic framework, the main difficulty is due to a shortage of conventional distributions for such data. in this paper, we propose to achieve the mixed data clustering with a gaussian copula mixture model, since copulas, and in particular the gaussian ones, are powerful tools for easily modelling the distribution of multivariate variables. indeed, considering a mixing of continuous, integer and ordinal variables (thus all having a cumulative distribution function), this copula mixture model defines intra-component dependencies similar to a gaussian mixture, so with classical correlation meaning. simultaneously, it preserves standard margins associated to continuous, integer and ordered features, namely the gaussian, the poisson and the ordered multinomial distributions. as an interesting by-product, the proposed mixture model generalizes many well-known ones and also provides tools of visualization based on the parameters. at a practical level, the bayesian inference is retained and it is achieved with a metropolis-within-gibbs sampler. experiments on simulated and real data sets finally illustrate the expected advantages of the proposed model for mixed data: flexible and meaningful parametrization combined with visualization features.",,2014-05-06,2015-09-29,"['matthieu marbac', 'christophe biernacki', 'vincent vandewalle']"
1405.2105,hybrid copula estimators,stat.me,an extension of the empirical copula is considered by combining an estimator of a multivariate cumulative distribution function with estimators of the marginal cumulative distribution functions for marginal estimators that are not necessarily equal to the margins of the joint estimator. such a hybrid estimator may be reasonable when there is additional information available for some margins in the form of additional data or stronger modelling assumptions. a functional central limit theorem is established and some examples are developed.,,2014-05-08,2014-11-28,['johan segers']
1406.0182,discriminant functions arising from selection distributions: theory and   simulation,stat.co stat.me,"the assumption of normality in data has been considered in the field of statistical analysis for a long time. however, in many practical situations, this assumption is clearly unrealistic. it has recently been suggested that the use of distributions indexed by skewness/shape parameters produce more exibility in the modelling of different applications. consequently, the results show a more realistic interpretation for these problems. for these reasons, the aim of this paper is to investigate the effects of the generalisation of a discrimination function method through the class of multivariate extended skew-elliptical distributions, study in detail the multivariate extended skew-normal case and develop a quadratic approximation function for this family of distributions. a simulation study is reported to evaluate the adequacy of the proposed classification rule as well as the performance of the em algorithm to estimate the model parameters.",,2014-06-01,,"['reinaldo b. arellano-valle', 'javier e. contreras-reyes']"
1406.0808,"robust improper maximum likelihood: tuning, computation, and a   comparison with other methods for robust gaussian clustering",stat.me stat.co,"the two main topics of this paper are the introduction of the ""optimally tuned improper maximum likelihood estimator"" (otrimle) for robust clustering based on the multivariate gaussian model for clusters, and a comprehensive simulation study comparing the otrimle to maximum likelihood in gaussian mixtures with and without noise component, mixtures of t-distributions, and the tclust approach for trimmed clustering. the otrimle uses an improper constant density for modelling outliers and noise. this can be chosen optimally so that the non-noise part of the data looks as close to a gaussian mixture as possible. some deviation from gaussianity can be traded in for lowering the estimated noise proportion. covariance matrix constraints and computation of the otrimle are also treated. in the simulation study, all methods are confronted with setups in which their model assumptions are not exactly fulfilled, and in order to evaluate the experiments in a standardized way by misclassification rates, a new model-based definition of ""true clusters"" is introduced that deviates from the usual identification of mixture components with clusters. in the study, every method turns out to be superior for one or more setups, but the otrimle achieves the most satisfactory overall performance. the methods are also applied to two real datasets, one without and one with known ""true"" clusters.",10.1080/01621459.2015.1100996,2014-06-02,2017-01-28,"['pietro coretto', 'christian hennig']"
1406.1245,modelling receiver operating characteristic curves using gaussian   mixtures,stat.me stat.ap stat.co,"the receiver operating characteristic curve is widely applied in measuring the performance of diagnostic tests. many direct and indirect approaches have been proposed for modelling the roc curve, and because of its tractability, the gaussian distribution has typically been used to model both populations. we propose using a gaussian mixture model, leading to a more flexible approach that better accounts for atypical data. monte carlo simulation is used to circumvent the issue of absence of a closed-form. we show that our method performs favourably when compared to the crude binormal curve and to the semi-parametric frequentist binormal roc using the famous labroc procedure.",10.1016/j.csda.2015.04.010,2014-06-04,,"['amay cheam', 'paul d. mcnicholas']"
1406.1655,variational inference of latent state sequences using recurrent networks,stat.ml cs.lg,"recent advances in the estimation of deep directed graphical models and recurrent networks let us contribute to the removal of a blind spot in the area of probabilistc modelling of time series. the proposed methods i) can infer distributed latent state-space trajectories with nonlinear transitions, ii) scale to large data sets thanks to the use of a stochastic objective and fast, approximate inference, iii) enable the design of rich emission models which iv) will naturally lead to structured outputs. two different paths of introducing latent state sequences are pursued, leading to the variational recurrent auto encoder (vrae) and the variational one step predictor (vosp). the use of independent wiener processes as priors on the latent state sequence is a viable compromise between efficient computation of the kullback-leibler divergence from the variational approximation of the posterior and maintaining a reasonable belief in the dynamics. we verify our methods empirically, obtaining results close or superior to the state of the art. we also show qualitative results for denoising and missing value imputation.",,2014-06-06,2014-09-30,"['justin bayer', 'christian osendorfer']"
1406.2933,optimal designs for copula models,stat.me,"copula modelling has in the past decade become a standard tool in many areas of applied statistics. however, a largely neglected aspect concerns the design of related experiments. particularly the issue of whether the estimation of copula parameters can be enhanced by optimizing experimental conditions and how robust all the parameter estimates for the model are with respect to the type of copula employed. in this paper an equivalence theorem for (bivariate) copula models is provided that allows formulation of efficient design algorithms and quick checks of whether designs are optimal or at least efficient. some examples illustrate that in practical situations considerable gains in design efficiency can be achieved. a natural comparison between different copula models with respect to design efficiency is provided as well.",,2014-06-11,,"['elisa perrone', 'werner g. müller']"
1406.3106,on lindley-exponential distribution: properties and application,stat.ap stat.me,"in this paper, we introduce a new distribution generated by lindley random variable which offers a more flexible model for modelling lifetime data. various statistical properties like distribution function, survival function, moments, entropy, and limiting distribution of extreme order statistics are established. inference for a random sample from the proposed distribution is investigated and maximum likelihood estimation method is used for estimating parameters of this distribution. the applicability of the proposed distribution is shown through real data sets.",,2014-06-11,,"['deepesh bhati', 'mohd. aamir malik']"
1406.3774,markov-switching generalized additive models,stat.me stat.co,"we consider markov-switching regression models, i.e. models for time series regression analyses where the functional relationship between covariates and response is subject to regime switching controlled by an unobservable markov chain. building on the powerful hidden markov model machinery and the methods for penalized b-splines routinely used in regression analyses, we develop a framework for nonparametrically estimating the functional form of the effect of the covariates in such a regression model, assuming an additive structure of the predictor. the resulting class of markov-switching generalized additive models is immensely flexible, and contains as special cases the common parametric markov-switching regression models and also generalized additive and generalized linear models. the feasibility of the suggested maximum penalized likelihood approach is demonstrated by simulation and further illustrated by modelling how energy price in spain depends on the euro/dollar exchange rate.",,2014-06-14,2015-05-10,"['roland langrock', 'thomas kneib', 'richard glennie', 'théo michelot']"
1406.3830,"modelling, visualising and summarising documents with a single   convolutional neural network",cs.cl cs.lg stat.ml,"capturing the compositional process which maps the meaning of words to that of documents is a central challenge for researchers in natural language processing and information retrieval. we introduce a model that is able to represent the meaning of documents by embedding them in a low dimensional vector space, while preserving distinctions of word and sentence order crucial for capturing nuanced semantics. our model is based on an extended dynamic convolution neural network, which learns convolution filters at both the sentence and document level, hierarchically learning to capture and compose low level lexical features into high level semantic concepts. we demonstrate the effectiveness of this model on a range of document modelling tasks, achieving strong results with no feature engineering and with a more compact model. inspired by recent advances in visualising deep convolution networks for computer vision, we present a novel visualisation technique for our document networks which not only provides insight into their learning process, but also can be interpreted to produce a compelling automatic summarisation system for texts.",,2014-06-15,,"['misha denil', 'alban demiraj', 'nal kalchbrenner', 'phil blunsom', 'nando de freitas']"
1406.4643,vector quantile regression: an optimal transport approach,stat.me,"we propose a notion of conditional vector quantile function and a vector quantile regression. a \emph{conditional vector quantile function} (cvqf) of a random vector $y$, taking values in $\mathbb{r}^d$ given covariates $z=z$, taking values in $\mathbb{r}% ^k$, is a map $u \longmapsto q_{y\mid z}(u,z)$, which is monotone, in the sense of being a gradient of a convex function, and such that given that vector $u$ follows a reference non-atomic distribution $f_u$, for instance uniform distribution on a unit cube in $\mathbb{r}^d$, the random vector $q_{y\mid z}(u,z)$ has the distribution of $y$ conditional on $z=z$. moreover, we have a strong representation, $y = q_{y\mid z}(u,z)$ almost surely, for some version of $u$. the \emph{vector quantile regression} (vqr) is a linear model for cvqf of $y$ given $z$. under correct specification, the notion produces strong representation, $y=\beta \left(u\right) ^\top f(z)$, for $f(z)$ denoting a known set of transformations of $z$, where $u \longmapsto \beta(u)^\top f(z)$ is a monotone map, the gradient of a convex function, and the quantile regression coefficients $u \longmapsto \beta(u)$ have the interpretations analogous to that of the standard scalar quantile regression. as $f(z)$ becomes a richer class of transformations of $z$, the model becomes nonparametric, as in series modelling. a key property of vqr is the embedding of the classical monge-kantorovich's optimal transportation problem at its core as a special case. in the classical case, where $y$ is scalar, vqr reduces to a version of the classical qr, and cvqf reduces to the scalar conditional quantile function. an application to multiple engel curve estimation is considered.",,2014-06-18,2015-09-27,"['guillaume carlier', 'victor chernozhukov', 'alfred galichon']"
1406.5005,computation and visualisation for large-scale gaussian updates,stat.co,"in geostatistics, and also in other applications in science and engineering, we are now performing updates on gaussian process models with many thousands or even millions of components. these large-scale inferences involve computational challenges, because the updating equations cannot be solved as written, owing to the size and cost of the matrix operations. they also involve representational challenges, to account for judgements of heterogeneity concerning the underlying fields, and diverse sources of observations.   diagnostics are particularly valuable in this situation. we present a diagnostic and visualisation tool for large-scale gaussian updates, the `medal plot'. this shows the updated uncertainty for each observation, and also summarises the sharing of information across observations, as a proxy for the sharing of information across the state vector. it allows us to `sanity-check' the code implementing the update, but it can also reveal unexpected features in our modelling. we discuss computational issues for large-scale updates, and we illustrate with an application to assess mass trends in the antarctic ice sheet.",,2014-06-19,,"['jonathan rougier', 'andrew zammit mangion', 'nana schoen']"
1406.6728,bayesian survival modelling of university outcomes,stat.ap,"the aim of this paper is to model the length of registration at university and its associated academic outcome for undergraduate students at the pontificia universidad cat\'olica de chile. survival time is defined as the time until the end of the enrollment period, which can relate to different reasons - graduation or two types of dropout - that are driven by different processes. hence, a competing risks model is employed for the analysis. the issue of separation of the outcomes (which precludes maximum likelihood estimation) is handled through the use of bayesian inference with an appropriately chosen prior. we are interested in identifying important determinants of university outcomes and the associ- ated model uncertainty is formally addressed through bayesian model averaging. the methodology introduced for modelling university outcomes is applied to three selected degree programmes, which are particularly affected by dropout and late graduation.",,2014-06-25,,"['catalina a. vallejos', 'mark f. j. steel']"
1406.7665,interleaved factorial non-homogeneous hidden markov models for energy   disaggregation,stat.ap,"to reduce energy demand in households it is useful to know which electrical appliances are in use at what times. monitoring individual appliances is costly and intrusive, whereas data on overall household electricity use is more easily obtained. in this paper, we consider the energy disaggregation problem where a household's electricity consumption is disaggregated into the component appliances. the factorial hidden markov model (fhmm) is a natural model to fit this data. we enhance this generic model by introducing two constraints on the state sequence of the fhmm. the first is to use a non-homogeneous markov chain, modelling how appliance usage varies over the day, and the other is to enforce that at most one chain changes state at each time step. this yields a new model which we call the interleaved factorial non-homogeneous hidden markov model (ifnhmm). we evaluated the ability of this model to perform disaggregation in an ultra-low frequency setting, over a data set of 251 english households. in this new setting, the ifnhmm outperforms the fhmm in terms of recovering the energy used by the component appliances, due to that stronger constraints have been imposed on the states of the hidden markov chains. interestingly, we find that the variability in model performance across households is significant, underscoring the importance of using larger scale data in the disaggregation problem.",,2014-06-30,,"['mingjun zhong', 'nigel goddard', 'charles sutton']"
1407.0064,the zero & $n$-inflated binomial distribution with applications,stat.me,"in this article we consider the distribution arising when two zero-inflated poisson count processes are constrained by their sum total, resulting in a novel zero & $n$-inflated binomial distribution. this result motivates a general class of model for applications in which a sum-constrained count response is subject to multiple sources of heterogeneity, principally an excess of zeroes and $n$'s in the underlying count generating process. two examples from the ecological regression literature are used to illustrate the wide applicability of the proposed model, and serve to detail its substantial superiority in modelling performance as compared to competing models. we also present an extension to the modelling framework for more complex cases, considering a gender study dataset which is overdispersed relative to the new likelihood, and conclude the article with the description of a general framework for a zero & $n$-inflated multinomial distribution.",,2014-06-30,2016-02-17,"['james sweeney', 'john haslett', 'andrew c. parnell']"
1407.1576,a statistical modelling and analysis of phevs' power demand in smart   grids,cs.ce stat.ap,"electric vehicles (evs) and particularly plug-in hybrid electric vehicles (phevs) are foreseen to become popular in the near future. not only are they much more environmentally friendly than conventional internal combustion engine (ice) vehicles, their fuel can also be catered from diverse energy sources and resources. however, they add significant load on the power grid as they become widespread. the characteristics of this extra load follow the patterns of people's driving behaviours. in particular, random parameters such as arrival time and driven distance of the vehicles determine their expected demand profile from the power grid. in this paper, we first present a model for uncoordinated charging power demand of phevs based on a stochastic process and accordingly we characterize the ev's expected daily power demand profile. next, we adopt different distributions for the ev's charging time following some available empirical research data in the literature. simulation results show that the ev's expected daily power demand profiles obtained under the uniform, gaussian with positive support and rician distributions for charging time are identical when the first and second order statistics of these distributions are the same. this gives us useful insights into the long-term planning for upgrading power systems' infrastructure to accommodate phevs. in addition, the results from this modelling can be incorporated into designing demand response (dr) algorithms and evaluating the available dr techniques more accurately.",,2014-07-07,,"['farshad rassaei', 'wee-seng soh', 'kee-chaing chua']"
1407.4139,"subjectivity, bayesianism, and causality",cs.ai stat.me stat.ml,"bayesian probability theory is one of the most successful frameworks to model reasoning under uncertainty. its defining property is the interpretation of probabilities as degrees of belief in propositions about the state of the world relative to an inquiring subject. this essay examines the notion of subjectivity by drawing parallels between lacanian theory and bayesian probability theory, and concludes that the latter must be enriched with causal interventions to model agency. the central contribution of this work is an abstract model of the subject that accommodates causal interventions in a measure-theoretic formalisation. this formalisation is obtained through a game-theoretic ansatz based on modelling the inside and outside of the subject as an extensive-form game with imperfect information between two players. finally, i illustrate the expressiveness of this model with an example of causal induction.",,2014-07-15,2015-04-24,['pedro a. ortega']
1407.4184,inference for biased models: a quasi-instrumental variable approach,stat.me,"for linear regression models who are not exactly sparse in the sense that the coefficients of the insignificant variables are not exactly zero, the working models obtained by a variable selection are often biased. even in sparse cases, after a variable selection, when some significant variables are missing, the working models are biased as well. thus, under such situations, root-n consistent estimation and accurate prediction could not be expected. in this paper, a novel remodelling method is proposed to produce an unbiased model when quasi-instrumental variables are introduced. the root-n estimation consistency and the asymptotic normality can be achieved, and the prediction accuracy can be promoted as well. the performance of the new method is examined through simulation studies.",,2014-07-15,,"['lu lin', 'lixing zhu', 'yujie gai']"
1407.5155,sparse and spurious: dictionary learning with noise and outliers,cs.lg stat.ml,"a popular approach within the signal processing and machine learning communities consists in modelling signals as sparse linear combinations of atoms selected from a learned dictionary. while this paradigm has led to numerous empirical successes in various fields ranging from image to audio processing, there have only been a few theoretical arguments supporting these evidences. in particular, sparse coding, or sparse dictionary learning, relies on a non-convex procedure whose local minima have not been fully analyzed yet. in this paper, we consider a probabilistic model of sparse signals, and show that, with high probability, sparse coding admits a local minimum around the reference dictionary generating the signals. our study takes into account the case of over-complete dictionaries, noisy signals, and possible outliers, thus extending previous work limited to noiseless settings and/or under-complete dictionaries. the analysis we conduct is non-asymptotic and makes it possible to understand how the key quantities of the problem, such as the coherence or the level of noise, can scale with respect to the dimension of the signals, the number of atoms, the sparsity and the number of observations.",,2014-07-19,2015-08-22,"['rémi gribonval', 'rodolphe jenatton', 'francis bach']"
1407.5290,event-controlled constructions of random fields of maxima with   non-max-stable dependence,stat.me physics.geo-ph,"max-stable random fields can be constructed according to schlather (2002) with a random function or a stationary process and a kind of random event magnitude. these are applied for the modelling of natural hazards. we simply extend these event-controlled constructions to random fields of maxima with non-max-stable dependence structure (copula). the theory for the variant with a stationary process is obvious; the parameter(s) of its correlation function is/are determined by the event magnitude. the introduced variant with random functions can only be researched numerically. the scaling of the random function is exponentially determined by the event magnitude. the location parameter of the gumbel margins depends only on this exponential function in the researched examples; the scale parameter of the margins is normalized. in addition, we propose a method for the parameter estimation for such constructions by using kendall's tau. the spatial dependence in relation to the block size is considered therein. finally, we briefly discuss some issues like the sampling.",,2014-07-20,,['mathias raschke']
1407.6128,permutation models for collaborative ranking,cs.ir cs.lg stat.ml,"we study the problem of collaborative filtering where ranking information is available. focusing on the core of the collaborative ranking process, the user and their community, we propose new models for representation of the underlying permutations and prediction of ranks. the first approach is based on the assumption that the user makes successive choice of items in a stage-wise manner. in particular, we extend the plackett-luce model in two ways - introducing parameter factoring to account for user-specific contribution, and modelling the latent community in a generative setting. the second approach relies on log-linear parameterisation, which relaxes the discrete-choice assumption, but makes learning and inference much more involved. we propose mcmc-based learning and inference methods and derive linear-time prediction algorithms.",,2014-07-23,,"['truyen tran', 'svetha venkatesh']"
1407.6895,bayesian exponential random graph models with nodal random effects,stat.ap stat.me,we extend the well-known and widely used exponential random graph model (ergm) by including nodal random effects to compensate for heterogeneity in the nodes of a network. the bayesian framework for ergms proposed by caimo and friel (2011) yields the basis of our modelling algorithm. a central question in network models is the question of model selection and following the bayesian paradigm we focus on estimating bayes factors. to do so we develop an approximate but feasible calculation of the bayes factor which allows one to pursue model selection. two data examples and a small simulation study illustrate our mixed model approach and the corresponding model selection.,,2014-07-25,2015-01-12,"['stephanie thiemichen', 'nial friel', 'alberto caimo', 'göran kauermann']"
1408.0047,cumulative restricted boltzmann machines for ordinal matrix data   analysis,stat.ml cs.ir cs.lg stat.ap stat.me,"ordinal data is omnipresent in almost all multiuser-generated feedback - questionnaires, preferences etc. this paper investigates modelling of ordinal data with gaussian restricted boltzmann machines (rbms). in particular, we present the model architecture, learning and inference procedures for both vector-variate and matrix-variate ordinal data. we show that our model is able to capture latent opinion profile of citizens around the world, and is competitive against state-of-art collaborative filtering techniques on large-scale public datasets. the model thus has the potential to extend application of rbms to diverse domains such as recommendation systems, product reviews and expert assessments.",,2014-07-31,,"['truyen tran', 'dinh phung', 'svetha venkatesh']"
1408.0711,clustering using skewed multivariate heavy tailed distributions with   flexible tail behaviour,stat.me stat.ap,"the family of location and scale mixtures of gaussians has the ability to generate a number of flexible distributional forms. it nests as particular cases several important asymmetric distributions like the generalised hyperbolic distribution. the generalised hyperbolic distribution in turn nests many other well known distributions such as the normal inverse gaussian (nig) whose practical relevance has been widely documented in the literature. in a multivariate setting, we propose to extend the standard location and scale mixture concept into a so called multiple scaled framework which has the advantage of allowing different tail and skewness behaviours in each dimension of the variable space with arbitrary correlation between dimensions. estimation of the parameters is provided via an em algorithm with a particular focus on nig distributions. inference is then extended to cover the case of mixtures of such multiple scaled distributions for application to clustering. assessments on simulated and real data confirm the gain in degrees of freedom and flexibility in modelling data of varying tail behaviour and directional shape.",,2014-08-01,,"['darren wraith', 'florence forbes']"
1408.1160,mixed-variate restricted boltzmann machines,stat.ml cs.lg stat.me,"modern datasets are becoming heterogeneous. to this end, we present in this paper mixed-variate restricted boltzmann machines for simultaneously modelling variables of multiple types and modalities, including binary and continuous responses, categorical options, multicategorical choices, ordinal assessment and category-ranked preferences. dependency among variables is modeled using latent binary variables, each of which can be interpreted as a particular hidden aspect of the data. the proposed model, similar to the standard rbms, allows fast evaluation of the posterior for the latent variables. hence, it is naturally suitable for many common tasks including, but not limited to, (a) as a pre-processing step to convert complex input data into a more convenient vectorial representation through the latent posteriors, thereby offering a dimensionality reduction capacity, (b) as a classifier supporting binary, multiclass, multilabel, and label-ranking outputs, or a regression tool for continuous outputs and (c) as a data completion tool for multimodal and heterogeneous data. we evaluate the proposed model on a large-scale dataset using the world opinion survey results on three tasks: feature extraction and visualization, data completion and prediction.",,2014-08-05,,"['truyen tran', 'dinh phung', 'svetha venkatesh']"
1408.1191,cluster detection and risk estimation for spatio-temporal health data,stat.me,"in epidemiological disease mapping one aims to estimate the spatio-temporal pattern in disease risk and identify high-risk clusters, allowing health interventions to be appropriately targeted. bayesian spatio-temporal models are used to estimate smoothed risk surfaces, but this is contrary to the aim of identifying groups of areal units that exhibit elevated risks compared with their neighbours. therefore, in this paper we propose a new bayesian hierarchical modelling approach for simultaneously estimating disease risk and identifying high-risk clusters in space and time. inference for this model is based on markov chain monte carlo simulation, using the freely available r package carbayesst that has been developed in conjunction with this paper. our methodology is motivated by two case studies, the first of which assesses if there is a relationship between public health districts and colon cancer clusters in georgia, while the second looks at the impact of the smoking ban in public places in england on cardiovascular disease clusters.",,2014-08-06,2014-11-10,"['duncan lee', 'andrew lawson']"
1408.1554,a complete data frame work for fitting power law distributions,stat.co physics.soc-ph,"over the last few decades power law distributions have been suggested as forming generative mechanisms in a variety of disparate fields, such as, astrophysics, criminology and database curation. however, fitting these heavy tailed distributions requires care, especially since the power law behaviour may only be present in the distributional tail. current state of the art methods for fitting these models rely on estimating the cut-off parameter $x_{\min}$. this results in the majority of collected data being discarded. this paper provides an alternative, principled approached for fitting heavy tailed distributions. by directly modelling the deviation from the power law distribution, we can fit and compare a variety of competing models in a single unified framework.",,2014-08-07,2014-08-24,['colin s. gillespie']
1408.5060,modelling across extremal dependence classes,stat.me,"different dependence scenarios can arise in multivariate extremes, entailing careful selection of an appropriate class of models. in bivariate extremes, the variables are either asymptotically dependent or are asymptotically independent. most available statistical models suit one or other of these cases, but not both, resulting in a stage in the inference that is unaccounted for, but can substantially impact subsequent extrapolation. existing modelling solutions to this problem are either applicable only on sub-domains, or appeal to multiple limit theories. we introduce a unified representation for bivariate extremes that encompasses a wide variety of dependence scenarios, and applies when at least one variable is large. our representation motivates a parametric model that encompasses both dependence classes. we implement a simple version of this model, and show that it performs well in a range of settings.",,2014-08-21,2015-10-29,"['jennifer wadsworth', 'jonathan tawn', 'anthony davison', 'daniel elton']"
1408.7025,synthesising evidence to estimate pandemic (2009) a/h1n1 influenza   severity in 2009-2011,stat.ap,"knowledge of the severity of an influenza outbreak is crucial for informing and monitoring appropriate public health responses, both during and after an epidemic. however, case-fatality, case-intensive care admission and case-hospitalisation risks are difficult to measure directly. bayesian evidence synthesis methods have previously been employed to combine fragmented, under-ascertained and biased surveillance data coherently and consistently, to estimate case-severity risks in the first two waves of the 2009 a/h1n1 influenza pandemic experienced in england. we present in detail the complex probabilistic model underlying this evidence synthesis, and extend the analysis to also estimate severity in the third wave of the pandemic strain during the 2010/2011 influenza season. we adapt the model to account for changes in the surveillance data available over the three waves. we consider two approaches: (a) a two-stage approach using posterior distributions from the model for the first two waves to inform priors for the third wave model; and (b) a one-stage approach modelling all three waves simultaneously. both approaches result in the same key conclusions: (1) that the age-distribution of the case-severity risks is ""u""-shaped, with children and older adults having the highest severity; (2) that the age-distribution of the infection attack rate changes over waves, school-age children being most affected in the first two waves and the attack rate in adults over 25 increasing from the second to third waves; and (3) that when averaged over all age groups, case-severity appears to increase over the three waves. the extent to which the final conclusion is driven by the change in age-distribution of those infected over time is subject to discussion.",10.1214/14-aoas775,2014-08-29,2015-02-03,"['anne m. presanis', 'richard g. pebody', 'paul j. birrell', 'brian d. m. tom', 'helen k. green', 'hayley durnall', 'douglas fleming', 'daniela de angelis']"
1409.0733,integral approximation by kernel smoothing,math.st stat.th,"let $(x_1,\ldots,x_n)$ be an i.i.d. sequence of random variables in $\mathbb{r}^d$, $d\geq 1$. we show that, for any function $\varphi :\mathbb{r}^d\rightarrow\mathbb{r}$, under regularity conditions, \[n^ {1/2}\biggl(n^{-1}\sum_{i=1}^n\frac{\varphi(x_i)}{\widehat{f}^(x_i)}- \int \varphi(x)\,dx\biggr)\stackrel{\mathbb{p}}{\longrightarrow}0,\] where $\widehat{f}$ is the classical kernel estimator of the density of $x_1$. this result is striking because it speeds up traditional rates, in root $n$, derived from the central limit theorem when $\widehat{f}=f$. although this paper highlights some applications, we mainly address theoretical issues related to the later result. we derive upper bounds for the rate of convergence in probability. these bounds depend on the regularity of the functions $\varphi$ and $f$, the dimension $d$ and the bandwidth of the kernel estimator $\widehat{f}$. moreover, they are shown to be accurate since they are used as renormalizing sequences in two central limit theorems each reflecting different degrees of smoothness of $\varphi$. as an application to regression modelling with random design, we provide the asymptotic normality of the estimation of the linear functionals of a regression function. as a consequence of the above result, the asymptotic variance does not depend on the regression function. finally, we debate the choice of the bandwidth for integral approximation and we highlight the good behavior of our procedure through simulations.",10.3150/15-bej725,2014-09-02,2016-06-06,"['bernard delyon', 'françois portier']"
1409.0743,does non-stationary spatial data always require non-stationary random   fields?,stat.me stat.ap,"a stationary spatial model is an idealization and we expect that the true dependence structures of physical phenomena are spatially varying, but how should we handle this non-stationarity in practice? we study the challenges involved in applying a flexible non-stationary model to a dataset of annual precipitation in the conterminous us, where exploratory data analysis shows strong evidence of a non-stationary covariance structure.   the aim of this paper is to investigate the modelling pipeline once non-stationarity has been detected in spatial data. we show that there is a real danger of over-fitting the model and that careful modelling is necessary in order to properly account for varying second-order structure. in fact, the example shows that sometimes non-stationary gaussian random fields are not necessary to model non-stationary spatial data.",,2014-09-02,2015-09-14,"['geir-arne fuglstad', 'daniel simpson', 'finn lindgren', 'håvard rue']"
1409.0940,high-performance kernel machines with implicit distributed optimization   and randomization,stat.ml cs.dc cs.lg,"in order to fully utilize ""big data"", it is often required to use ""big models"". such models tend to grow with the complexity and size of the training data, and do not make strong parametric assumptions upfront on the nature of the underlying statistical dependencies. kernel methods fit this need well, as they constitute a versatile and principled statistical methodology for solving a wide range of non-parametric modelling problems. however, their high computational costs (in storage and time) pose a significant barrier to their widespread adoption in big data applications.   we propose an algorithmic framework and high-performance implementation for massive-scale training of kernel-based statistical models, based on combining two key technical ingredients: (i) distributed general purpose convex optimization, and (ii) the use of randomization to improve the scalability of kernel methods. our approach is based on a block-splitting variant of the alternating directions method of multipliers, carefully reconfigured to handle very large random feature matrices, while exploiting hybrid parallelism typically found in modern clusters of multicore machines. our implementation supports a variety of statistical learning tasks by enabling several loss functions, regularization schemes, kernels, and layers of randomized approximations for both dense and sparse datasets, in a highly extensible framework. we evaluate the ability of our framework to learn models on data from applications, and provide a comparison against existing sequential and parallel libraries.",,2014-09-02,2015-04-16,"['vikas sindhwani', 'haim avron']"
1409.2642,exploiting timss and pirls combined data: multivariate multilevel   modelling of student achievement,stat.ap,"we exploit a multivariate multilevel model for the analysis of the italian sample of the timss\&pirls 2011 combined international database on fourth grade students. the multivariate approach jointly considers educational achievement on reading, mathematics and science, thus allowing us to test for differential associations of the covariates with the three outcomes, and to estimate the residual correlations between pairs of outcomes at student and class levels. multilevel modelling allows us to disentangle student and contextual factors affecting achievement. we also account for territorial differences in wealth by means of an index from an external source. the model residuals point out classes with high or low performance. as educational achievement is measured by plausible values, the estimates are obtained through multiple imputation formulas. the results, while confirming the role of traditional student and contextual factors, reveal interesting patterns of achievement in italian primary schools.",,2014-09-09,2015-08-16,"['leonardo grilli', 'fulvia pennoni', 'carla rampichini', 'isabella romeo']"
1409.2824,scalable bayesian modelling of paired symbols,stat.ml,"we present a novel, scalable and bayesian approach to modelling the occurrence of pairs of symbols (i,j) drawn from a large vocabulary. observed pairs are assumed to be generated by a simple popularity based selection process followed by censoring using a preference function. by basing inference on the well-founded principle of variational bounding, and using new site-independent bounds, we show how a scalable inference procedure can be obtained for large data sets. state of the art results are presented on real-world movie viewing data.",,2014-09-09,2014-09-10,"['ulrich paquet', 'noam koenigstein', 'ole winther']"
1409.4512,estimation and testing for covariance-spectral spatial-temporal models,stat.me stat.ap,in this paper we explore a covariance spectral modelling strategy for spatial-temporal processes which involves a spectral approach for time but a covariance approach for space.it facilitates the analysis of coherence between the temporal frequency components at different spatial sites. stein(2005) developed a semi-parametric model within this framework.the purpose of this paper is to give a deeper insight into the properties of his model and to develop simple and more intuitive methods of estimation and testing. an example is given using the irish wind speed data.,,2014-09-16,,"['a. m. mosammam', 'j. t. kent']"
1409.6219,"flexible modelling in statistics: past, present and future",stat.me math.st stat.th,"in times where more and more data become available and where the data exhibit rather complex structures (significant departure from symmetry, heavy or light tails), flexible modelling has become an essential task for statisticians as well as researchers and practitioners from domains such as economics, finance or environmental sciences. this is reflected by the wealth of existing proposals for flexible distributions; well-known examples are azzalini's skew-normal, tukey's $g$-and-$h$, mixture and two-piece distributions, to cite but these. my aim in the present paper is to provide an introduction to this research field, intended to be useful both for novices and professionals of the domain. after a description of the research stream itself, i will narrate the gripping history of flexible modelling, starring emblematic heroes from the past such as edgeworth and pearson, then depict three of the most used flexible families of distributions, and finally provide an outlook on future flexible modelling research by posing challenging open questions.",,2014-09-22,,['christophe ley']
1409.7454,a bayesian spatial temporal mixtures approach to kinetic parametric   images in dynamic positron emission tomography,stat.ap,"we present a fully bayesian statistical approach to the problem of compartmental modelling in the context of positron emission tomography. we cluster homogeneous region of interest and perform kinetic parameter estimation simultaneously. a mixture modelling approach is adopted, incorporating both spatial and temporal information based on reconstructed dynamic pet image. our modelling approach is flexible, and provides uncertainty estimates for the estimated kinetic parameters. crucially, the proposed method allows us to determine the unknown number of clusters, which has a great impact on resulting estimated kinetic parameters. we demonstrate our method on simulated dynamic myocardial pet data, and show that our method is superior to standard curve-fitting approach.",,2014-09-25,2016-02-07,"['wanchuang zhu', 'jinsong ouyang', 'yothin rakvongthai', 'n. j. guehl', 'd. w. wooten', 'g. el fakhri', 'm. d. normandin', 'yanan fan']"
1409.8083,variational inference for probabilistic latent tensor factorization with   kl divergence,stat.co cs.na,probabilistic latent tensor factorization (pltf) is a recently proposed probabilistic framework for modelling multi-way data. not only the common tensor factorization models but also any arbitrary tensor factorization structure can be realized by the pltf framework. this paper presents full bayesian inference via variational bayes that facilitates more powerful modelling and allows more sophisticated inference on the pltf framework. we illustrate our approach on model order selection and link prediction.,,2014-09-29,,"['beyza ermis', 'y. kenan yılmaz', 'a. taylan cemgil', 'evrim acar']"
1409.8202,short-term predictability of photovoltaic production over italy,cs.lg stat.ap,"photovoltaic (pv) power production increased drastically in europe throughout the last years. about the 6% of electricity in italy comes from pv and for an efficient management of the power grid an accurate and reliable forecasting of production would be needed. starting from a dataset of electricity production of 65 italian solar plants for the years 2011-2012 we investigate the possibility to forecast daily production from one to ten days of lead time without using on site measurements. our study is divided in two parts: an assessment of the predictability of meteorological variables using weather forecasts and an analysis on the application of data-driven modelling in predicting solar power production. we calibrate a svm model using available observations and then we force the same model with the predicted variables from weather forecasts with a lead time from one to ten days. as expected, solar power production is strongly influenced by cloudiness and clear sky, in fact we observe that while during summer we obtain a general error under the 10% (slightly lower in south italy), during winter the error is abundantly above the 20%.",,2014-09-29,,"['matteo de felice', 'marcello petitta', 'paolo m. ruti']"
1410.0347,bootstrap confidence sets under model misspecification,math.st stat.th,"a multiplier bootstrap procedure for construction of likelihood-based confidence sets is considered for finite samples and a possible model misspecification. theoretical results justify the bootstrap validity for a small or moderate sample size and allow to control the impact of the parameter dimension $p$: the bootstrap approximation works if $p^3/n$ is small. the main result about bootstrap validity continues to apply even if the underlying parametric model is misspecified under the so-called small modelling bias condition. in the case when the true model deviates significantly from the considered parametric family, the bootstrap procedure is still applicable but it becomes a bit conservative: the size of the constructed confidence sets is increased by the modelling bias. we illustrate the results with numerical examples for misspecified linear and logistic regressions.",10.1214/15-aos1355,2014-10-01,2015-11-17,"['vladimir spokoiny', 'mayya zhilova']"
1410.0908,probit normal correlated topic models,stat.ml cs.ir cs.lg,"the logistic normal distribution has recently been adapted via the transformation of multivariate gaus- sian variables to model the topical distribution of documents in the presence of correlations among topics. in this paper, we propose a probit normal alternative approach to modelling correlated topical structures. our use of the probit model in the context of topic discovery is novel, as many authors have so far con- centrated solely of the logistic model partly due to the formidable inefficiency of the multinomial probit model even in the case of very small topical spaces. we herein circumvent the inefficiency of multinomial probit estimation by using an adaptation of the diagonal orthant multinomial probit in the topic models context, resulting in the ability of our topic modelling scheme to handle corpuses with a large number of latent topics. an additional and very important benefit of our method lies in the fact that unlike with the logistic normal model whose non-conjugacy leads to the need for sophisticated sampling schemes, our ap- proach exploits the natural conjugacy inherent in the auxiliary formulation of the probit model to achieve greater simplicity. the application of our proposed scheme to a well known associated press corpus not only helps discover a large number of meaningful topics but also reveals the capturing of compellingly intuitive correlations among certain topics. besides, our proposed approach lends itself to even further scalability thanks to various existing high performance algorithms and architectures capable of handling millions of documents.",,2014-10-03,,"['xingchen yu', 'ernest fokoue']"
1410.2323,principal component analysis for second-order stationary vector time   series,stat.me,"we extend the principal component analysis (pca) to second-order stationary vector time series in the sense that we seek for a contemporaneous linear transformation for a $p$-variate time series such that the transformed series is segmented into several lower-dimensional subseries, and those subseries are uncorrelated with each other both contemporaneously and serially. therefore those lower-dimensional series can be analysed separately as far as the linear dynamic structure is concerned. technically it boils down to an eigenanalysis for a positive definite matrix. when $p$ is large, an additional step is required to perform a permutation in terms of either maximum cross-correlations or fdr based on multiple tests. the asymptotic theory is established for both fixed $p$ and diverging $p$ when the sample size $n$ tends to infinity. numerical experiments with both simulated and real data sets indicate that the proposed method is an effective initial step in analysing multiple time series data, which leads to substantial dimension reduction in modelling and forecasting high-dimensional linear dynamical structures. unlike pca for independent data, there is no guarantee that the required linear transformation exists. when it does not, the proposed method provides an approximate segmentation which leads to the advantages in, for example, forecasting for future values. the method can also be adapted to segment multiple volatility processes.",10.1214/17-aos1613,2014-10-08,2017-04-12,"['jinyuan chang', 'bin guo', 'qiwei yao']"
1410.5362,prediction of synchrostate transitions in eeg signals using markov chain   models,q-bio.nc physics.med-ph stat.ap stat.ml,"this paper proposes a stochastic model using the concept of markov chains for the inter-state transitions of the millisecond order quasi-stable phase synchronized patterns or synchrostates, found in multi-channel electroencephalogram (eeg) signals. first and second order transition probability matrices are estimated for markov chain modelling from 100 trials of 128-channel eeg signals during two different face perception tasks. prediction accuracies with such finite markov chain models for synchrostate transition are also compared, under a data-partitioning based cross-validation scheme.",10.1109/lsp.2014.2352251,2014-10-20,,"['wasifa jamal', 'saptarshi das', 'ioana-anastasia oprescu', 'koushik maharatna']"
1410.7365,multiple output regression with latent noise,stat.ml,"in high-dimensional data, structured noise caused by observed and unobserved factors affecting multiple target variables simultaneously, imposes a serious challenge for modeling, by masking the often weak signal. therefore, (1) explaining away the structured noise in multiple-output regression is of paramount importance. additionally, (2) assumptions about the correlation structure of the regression weights are needed. we note that both can be formulated in a natural way in a latent variable model, in which both the interesting signal and the noise are mediated through the same latent factors. under this assumption, the signal model then borrows strength from the noise model by encouraging similar effects on correlated targets. we introduce a hyperparameter for the \emph{latent signal-to-noise ratio} which turns out to be important for modelling weak signals, and an ordered infinite-dimensional shrinkage prior that resolves the rotational unidentifiability in reduced-rank regression models. simulations and prediction experiments with metabolite, gene expression, fmri measurement, and macroeconomic time series data show that our model equals or exceeds the state-of-the-art performance and, in particular, outperforms the standard approach of assuming independent noise and signal models.",,2014-10-27,2016-02-03,"['jussi gillberg', 'pekka marttinen', 'matti pirinen', 'antti j. kangas', 'pasi soininen', 'mehreen ali', 'aki s. havulinna', 'marjo-riitta marjo-riitta järvelin', 'mika ala-korpela', 'samuel kaski']"
1410.8276,functional regression approximate bayesian computation for gaussian   process density estimation,stat.co,"we propose a novel bayesian nonparametric method for hierarchical modelling on a set of related density functions, where grouped data in the form of samples from each density function are available. borrowing strength across the groups is a major challenge in this context. to address this problem, we introduce a hierarchically structured prior, defined over a set of univariate density functions, using convenient transformations of gaussian processes. inference is performed through approximate bayesian computation (abc), via a novel functional regression adjustment. the performance of the proposed method is illustrated via a simulation study and an analysis of rural high school exam performance in brazil.",,2014-10-30,,"['g. s. rodrigues', 'david j. nott', 's. a. sisson']"
1410.8570,a partially linear framework for massive heterogeneous data,math.st stat.th,"we consider a partially linear framework for modelling massive heterogeneous data. the major goal is to extract common features across all sub-populations while exploring heterogeneity of each sub-population. in particular, we propose an aggregation type estimator for the commonality parameter that possesses the (non-asymptotic) minimax optimal bound and asymptotic distribution as if there were no heterogeneity. this oracular result holds when the number of sub-populations does not grow too fast. a plug-in estimator for the heterogeneity parameter is further constructed, and shown to possess the asymptotic distribution as if the commonality information were available. we also test the heterogeneity among a large number of sub-populations. all the above results require to regularize each sub-estimation as though it had the entire sample size. our general theory applies to the divide-and-conquer approach that is often used to deal with massive homogeneous data. a technical by-product of this paper is the statistical inferences for the general kernel ridge regression. thorough numerical results are also provided to back up our theory.",,2014-10-30,2016-01-24,"['tianqi zhao', 'guang cheng', 'han liu']"
1411.0414,nonparametric estimation of extremal dependence,stat.me,"there is an increasing interest to understand the dependence structure of a random vector not only in the center of its distribution but also in the tails. extreme-value theory tackles the problem of modelling the joint tail of a multivariate distribution by modelling the marginal distributions and the dependence structure separately. for estimating dependence at high levels, the stable tail dependence function and the spectral measure are particularly convenient. these objects also lie at the basis of nonparametric techniques for modelling the dependence among extremes in the max-domain of attraction setting. in case of asymptotic independence, this setting is inadequate, and more refined tail dependence coefficients exist, serving, among others, to discriminate between asymptotic dependence and independence. throughout, the methods are illustrated on financial data.",,2014-11-03,,"['anna kiriliouk', 'johan segers', 'michal warchol']"
1411.0560,multivariate response and parsimony for gaussian cluster-weighted models,stat.co stat.me stat.ml,"a family of parsimonious gaussian cluster-weighted models is presented. this family concerns a multivariate extension to cluster-weighted modelling that can account for correlations between multivariate responses. parsimony is attained by constraining parts of an eigen-decomposition imposed on the component covariance matrices. a sufficient condition for identifiability is provided and an expectation-maximization algorithm is presented for parameter estimation. model performance is investigated on both synthetic and classical real data sets and compared with some popular approaches. finally, accounting for linear dependencies in the presence of a linear regression structure is shown to offer better performance, vis-\`{a}-vis clustering, over existing methodologies.",,2014-11-03,2016-02-26,"['utkarsh j. dang', 'antonio punzo', 'paul d. mcnicholas', 'salvatore ingrassia', 'ryan p. browne']"
1411.0606,clustvarsel: a package implementing variable selection for model-based   clustering in r,stat.co,"finite mixture modelling provides a framework for cluster analysis based on parsimonious gaussian mixture models. variable or feature selection is of particular importance in situations where only a subset of the available variables provide clustering information. this enables the selection of a more parsimonious model, yielding more efficient estimates, a clearer interpretation and, often, improved clustering partitions. this paper describes the r package clustvarsel which performs subset selection for model-based clustering. an improved version of the methodology of raftery and dean (2006) is implemented in the new version 2 of the package to find the (locally) optimal subset of variables with group/cluster information in a dataset. search over the solution space is performed using either a stepwise greedy search or a headlong algorithm. adjustments for speeding up these algorithms are discussed, as well as a parallel implementation of the stepwise search. usage of the package is presented through the discussion of several data examples.",,2014-11-03,,"['luca scrucca', 'adrian e. raftery']"
1411.1243,using twitter to predict football outcomes,stat.ml cs.cl cs.si,"twitter has been proven to be a notable source for predictive modelling on various domains such as the stock market, the dissemination of diseases or sports outcomes. however, such a study has not been conducted in football (soccer) so far. the purpose of this research was to study whether data mined from twitter can be used for this purpose. we built a set of predictive models for the outcome of football games of the english premier league for a 3 month period based on tweets and we studied whether these models can overcome predictive models which use only historical data and simple football statistics. moreover, combined models are constructed using both twitter and historical data. the final results indicate that data mined from twitter can indeed be a useful source for predicting games in the premier league. the final twitter-based model performs significantly better than chance when measured by cohen's kappa and is comparable to the model that uses simple statistics and historical data. combining both models raises the performance higher than it was achieved by each individual model. thereby, this study provides evidence that twitter derived features can indeed provide useful information for the prediction of football (soccer) outcomes.",,2014-11-05,,"['stylianos kampakis', 'andreas adamides']"
1411.1292,monitoring count time series in r: aberration detection in public health   surveillance,stat.co,"public health surveillance aims at lessening disease burden, e.g., in case of infectious diseases by timely recognizing emerging outbreaks. seen from a statistical perspective, this implies the use of appropriate methods for monitoring time series of aggregated case reports. this paper presents the tools for such automatic aberration detection offered by the r package surveillance. we introduce the functionality for the visualization, modelling and monitoring of surveillance time series. with respect to modelling we focus on univariate time series modelling based on generalized linear models (glms), multivariate glms, generalized additive models and generalized additive models for location, shape and scale. this ranges from illustrating implementational improvements and extensions of the well-known farrington algorithm, e.g, by spline-modelling or by treating it in a bayesian context. furthermore, we look at categorical time series and address overdispersion using beta-binomial or dirichlet-multinomial modelling. with respect to monitoring we consider detectors based on either a shewhart-like single timepoint comparison between the observed count and the predictive distribution or by likelihood-ratio based cumulative sum methods. finally, we illustrate how surveillance can support aberration detection in practice by integrating it into the monitoring workflow of a public health institution. altogether, the present article shows how well surveillance can support automatic aberration detection in a public health surveillance context.",10.18637/jss.v070.i10,2014-11-05,,"['salmon maëlle', 'schumacher dirk', 'höhle michael']"
1411.2182,a censored bayesian hierarchical model for precipitation,stat.ap,"modelling of precipitation, including extremes, is important for hydrological and agricultural applications. traditionally, because of large sample properties for data over a large threshold value, generalised pareto (gp) distributions are often used for modelling extreme rainfall. it can be shown that under certain conditions the generalised hyperbolic (gh) distributions can approximate the power law decay of the gp distribution in the tails. given their flexible form, this raises the possibility that distributions from the gh family serve as a model for the entire rainfall distribution thus avoiding the need to select a threshold. in this paper, we use a flexible censored hierarchical model that leverages the gh distribution to accommodate data subject to heavy tails and an excessive number of zeros. the fitted model allows estimation of probabilities and return periods of the rainfall extremes, and it produces narrower credible intervals in the tails than the traditional gp method. the model not only fits the tails of the rainfall distribution, but fits the whole distribution very well. it also efficiently represents short-term dependencies in the data so it is suitable for evaluating duration over and below thresholds as well as duration of zero rainfall.",,2014-11-08,,"['yang liu', 'philip kokic', 'k. shuvo bakar']"
1411.2624,bayesian non-parametric inference for infectious disease data,stat.me stat.ap stat.co,we propose a framework for bayesian non-parametric estimation of the rate at which new infections occur assuming that the epidemic is partially observed. the developed methodology relies on modelling the rate at which new infections occur as a function which only depends on time. two different types of prior distributions are proposed namely using step-functions and b-splines. the methodology is illustrated using both simulated and real datasets and we show that certain aspects of the epidemic such as seasonality and super-spreading events are picked up without having to explicitly incorporate them into a parametric model.,,2014-11-10,2014-12-15,"['edward s. knock', 'theodore kypraios']"
1411.3921,inference for trans-dimensional bayesian models with diffusive nested   sampling,stat.co astro-ph.im physics.data-an,"many inference problems involve inferring the number $n$ of components in some region, along with their properties $\{\mathbf{x}_i\}_{i=1}^n$, from a dataset $\mathcal{d}$. a common statistical example is finite mixture modelling. in the bayesian framework, these problems are typically solved using one of the following two methods: i) by executing a monte carlo algorithm (such as nested sampling) once for each possible value of $n$, and calculating the marginal likelihood or evidence as a function of $n$; or ii) by doing a single run that allows the model dimension $n$ to change (such as markov chain monte carlo with birth/death moves), and obtaining the posterior for $n$ directly. in this paper we present a general approach to this problem that uses trans-dimensional mcmc embedded within a nested sampling algorithm, allowing us to explore the posterior distribution and calculate the marginal likelihood (summed over $n$) even if the problem contains a phase transition or other difficult features such as multimodality. we present two example problems, finding sinusoidal signals in noisy data, and finding and measuring galaxies in a noisy astronomical image. both of the examples demonstrate phase transitions in the relationship between the likelihood and the cumulative prior mass, highlighting the need for nested sampling.",,2014-11-14,2015-01-14,['brendon j. brewer']
1411.4715,predictive inference for spatio-temporal precipitation data and its   extremes,stat.ap stat.me,"modelling of precipitation and its extremes is important for urban and agriculture planning purposes. we present a method for producing spatial predictions and measures of uncertainty for spatio-temporal data that is heavy-tailed and subject to substaintial skewness which often arise in measurements of many environmental processes, and we apply the method to precipitation data in south-west western australia. a generalised hyperbolic bayesian hierarchical model is constructed for the intensity, frequency and duration of daily precipitation, including the extremes. unlike models based on extreme value theory, which only model maxima of finite-sized blocks or exceedances above a large threshold, the proposed model uses all the data available efficiently, and hence not only fits the extremes but also models the entire rainfall distribution. it captures spatial and temporal clustering, as well as spatially and temporally varying volatility and skewness. the model assumes that the regional precipitation is driven by a latent process characterised by geographical and climatological covariates. effects not fully described by the covariates are captured by spatial and temporal structure in the hierarchies. inference is provided by mcmc using a metropolis-hastings algorithm and spatial interpolation method, which provide a natural approach for estimating uncertainty. similarly both spatial and temporal predictions with uncertainty can be produced with the model.",,2014-11-17,,"['yang liu', 'philip kokic']"
1411.4846,modelling and analysis of time in-homogeneous recurrent event processes   in a heterogeneous population: a case study of hrts,stat.ap,"in this work we present a method for the statistical analysis of continually monitored data arising in a recurrent diseases problem. the model enables individual level inference in the presence of time transience and population heterogeneity. this is achieved by applying bayesian hierarchical modelling, where marked point processes are used as descriptions of the individual data, with latent variables providing a means of modelling long range dependence and transience over time. in addition to providing a sound probabilistic formulation of a rather complex data set, the proposed method is also successful in prediction of future outcomes. computational difficulties arising from the analytic intractability of this bayesian model were solved by implementing the method into the bugs software and using standard computational facilities.   we illustrate this approach by an analysis of a data set on hormone replacement therapies (hrts). the data contain, in the form of diaries on bleeding patterns maintained by individual patients, detailed information on how they responded to different hrts. the proposed model is able to capture the essential features of these treatments as well as provide realistic individual level predictions on the future bleeding patterns.",,2014-11-18,,"['madhuchhanda bhattacharjee', 'elja arjas']"
1411.5988,clustering evolving data using kernel-based methods,cs.si cs.lg stat.ml,"in this thesis, we propose several modelling strategies to tackle evolving data in different contexts. in the framework of static clustering, we start by introducing a soft kernel spectral clustering (sksc) algorithm, which can better deal with overlapping clusters with respect to kernel spectral clustering (ksc) and provides more interpretable outcomes. afterwards, a whole strategy based upon ksc for community detection of static networks is proposed, where the extraction of a high quality training sub-graph, the choice of the kernel function, the model selection and the applicability to large-scale data are key aspects. this paves the way for the development of a novel clustering algorithm for the analysis of evolving networks called kernel spectral clustering with memory effect (mksc), where the temporal smoothness between clustering results in successive time steps is incorporated at the level of the primal optimization problem, by properly modifying the ksc formulation. later on, an application of ksc to fault detection of an industrial machine is presented. here, a smart pre-processing of the data by means of a proper windowing operation is necessary to catch the ongoing degradation process affecting the machine. in this way, in a genuinely unsupervised manner, it is possible to raise an early warning when necessary, in an online fashion. finally, we propose a new algorithm called incremental kernel spectral clustering (iksc) for online learning of non-stationary data. this ambitious challenge is faced by taking advantage of the out-of-sample property of kernel spectral clustering (ksc) to adapt the initial model, in order to tackle merging, splitting or drifting of clusters across time. real-world applications considered in this thesis include image segmentation, time-series clustering, community detection of static and evolving networks.",,2014-11-20,,['rocco langone']
1412.1344,estimation of space deformation model for non-stationary random   functions,stat.me stat.ap,"stationary random functions have been successfully applied in geostatistical applications for decades. in some instances, the assumption of a homogeneous spatial dependence structure across the entire domain of interest is unrealistic. a practical approach for modelling and estimating non-stationary spatial dependence structure is considered. this consists in transforming a non-stationary random function into a stationary and isotropic one via a bijective continuous deformation of the index space. so far, this approach has been successfully applied in the context of data from several independent realizations of a random function. in this work, we propose an approach for non-stationary geostatistical modelling using space deformation in the context of a single realization with possibly irregularly spaced data. the estimation method is based on a non-stationary variogram kernel estimator which serves as a dissimilarity measure between two locations in the geographical space. the proposed procedure combines aspects of kernel smoothing, weighted non-metric multi-dimensional scaling and thin-plate spline radial basis functions. on a simulated data, the method is able to retrieve the true deformation. performances are assessed on both synthetic and real datasets. it is shown in particular that our approach outperforms the stationary approach. beyond the prediction, the proposed method can also serve as a tool for exploratory analysis of the non-stationarity.",,2014-12-03,,"['francky fouedjio', 'nicolas desassis', 'thomas romary']"
1412.1370,nested variational compression in deep gaussian processes,stat.ml,"deep gaussian processes provide a flexible approach to probabilistic modelling of data using either supervised or unsupervised learning. for tractable inference approximations to the marginal likelihood of the model must be made. the original approach to approximate inference in these models used variational compression to allow for approximate variational marginalization of the hidden variables leading to a lower bound on the marginal likelihood of the model [damianou and lawrence, 2013]. in this paper we extend this idea with a nested variational compression. the resulting lower bound on the likelihood can be easily parallelized or adapted for stochastic variational inference.",,2014-12-03,,"['james hensman', 'neil d. lawrence']"
1412.3230,max-factor individual risk models with application to credit portfolios,stat.me q-fin.rm q-fin.st,"individual risk models need to capture possible correlations as failing to do so typically results in an underestimation of extreme quantiles of the aggregate loss. such dependence modelling is particularly important for managing credit risk, for instance, where joint defaults are a major cause of concern. often, the dependence between the individual loss occurrence indicators is driven by a small number of unobservable factors. conditional loss probabilities are then expressed as monotone functions of linear combinations of these hidden factors. however, combining the factors in a linear way allows for some compensation between them. such diversification effects are not always desirable and this is why the present work proposes a new model replacing linear combinations with maxima. these max-factor models give more insight into which of the factors is dominant.",,2014-12-10,,"['michel denuit', 'anna kiriliouk', 'johan segers']"
1412.4355,designs for generalized linear models with random block effects via   information matrix approximations,stat.me,"the selection of optimal designs for generalized linear mixed models is complicated by the fact that the fisher information matrix, on which most optimality criteria depend, is computationally expensive to evaluate. our focus is on the design of experiments for likelihood estimation of parameters in the conditional model. we provide two novel approximations that substantially reduce the computational cost of evaluating the information matrix by complete enumeration of response outcomes, or monte carlo approximations thereof: (i) an asymptotic approximation which is accurate when there is strong dependence between observations in the same block; (ii) an approximation via kriging interpolators. for logistic random intercept models, we show how interpolation can be especially effective for finding pseudo-bayesian designs that incorporate uncertainty in the values of the model parameters. the new results are used to provide the first evaluation of the efficiency, for estimating conditional models, of optimal designs from closed-form approximations to the information matrix derived from marginal models. it is found that correcting for the marginal attenuation of parameters in binary-response models yields much improved designs, typically with very high efficiencies. however, in some experiments exhibiting strong dependence, designs for marginal models may still be inefficient for conditional modelling. our asymptotic results provide some theoretical insights into why such inefficiencies occur.",10.1093/biomet/asv005,2014-12-14,,"['timothy w. waite', 'david c. woods']"
1412.5236,the supervised hierarchical dirichlet process,stat.ml cs.lg,"we propose the supervised hierarchical dirichlet process (shdp), a nonparametric generative model for the joint distribution of a group of observations and a response variable directly associated with that whole group. we compare the shdp with another leading method for regression on grouped data, the supervised latent dirichlet allocation (slda) model. we evaluate our method on two real-world classification problems and two real-world regression problems. bayesian nonparametric regression models based on the dirichlet process, such as the dirichlet process-generalised linear models (dp-glm) have previously been explored; these models allow flexibility in modelling nonlinear relationships. however, until now, hierarchical dirichlet process (hdp) mixtures have not seen significant use in supervised problems with grouped data since a straightforward application of the hdp on the grouped data results in learnt clusters that are not predictive of the responses. the shdp solves this problem by allowing for clusters to be learnt jointly from the group structure and from the label assigned to each group.",10.1109/tpami.2014.2315802,2014-12-16,,"['andrew m. dai', 'amos j. storkey']"
1412.5351,a comparative analysis of the uk and italian small businesses using   generalised extreme value models,stat.ap q-fin.gn,"this paper presents a cross-country comparison of significant predictors of small business failure between italy and the uk. financial measures of profitability, leverage, coverage, liquidity, scale and non-financial information are explored, some commonalities and differences are highlighted. several models are considered, starting with the logis- tic regression which is a standard approach in credit risk modelling. some important improvements are investigated. generalised extreme value (gev) regression is applied to correct for the symmetric link function of the logistic regression. the assumption of non-linearity is relaxed through application of bgeva, non-parametric additive model based on the gev link function. two methods of handling missing values are compared: multiple imputation and weights of evidence (woe) transformation. the results suggest that the best predictive performance is obtained by bgeva, thus implying the necessity of taking into account the relative volume of defaults and non-linear patterns when modelling sme performance. woe for the majority of models considered show better prediction as compared to multiple imputation, suggesting that missing values could be informative and should not be assumed to be missing at random.",,2014-12-17,,"['galina andreeva', 'raffaella calabrese', 'silvia angela osmetti']"
1412.7334,a hidden markov approach to disability insurance,stat.ap,"point and interval estimation of future disability inception and recovery rates are predominantly carried out by combining generalized linear models (glm) with time series forecasting techniques into a two-step method involving parameter estimation from historical data and subsequent calibration of a time series model. this approach may in fact lead to both conceptual and numerical problems since any time trend components of the model are incoherently treated as both model parameters and realizations of a stochastic process. we suggest that this general two-step approach can be improved in the following way: first, we assume a stochastic process form for the time trend component. the corresponding transition densities are then incorporated into the likelihood, and the model parameters are estimated using the expectation-maximization algorithm. we illustrate the modelling procedure by fitting the model to swedish disability claims data.",,2014-12-23,,"['boualem djehiche', 'björn löfdahl']"
1501.02157,quantile regression for longitudinal data: unobserved heterogeneity and   informative missingness,stat.me,"linear quantile regression models aim at providing a detailed and robust picture of the (conditional) response distribution as function of a set of observed covariates. longitudinal data represent an interesting field of application of such models; due to their peculiar features, they represent a substantial challenge, in that the standard, cross-sectional, model representation needs to be extended for dealing with such kind of data. in fact, repeated observations from the same statistical unit poses a problem of dependence; in a conditional perspective, this dependence could be ascribed to sources of unobserved, individual-specific, heterogeneity. along these lines, quantile regression models have recently been extended to the analysis of longitudinal, continuous, responses, by modelling dependence via time-constant or time-varying random effects. in this manuscript, we introduce a general quantile regression model for longitudinal, continuous, responses where time-varying and time-constant random parameters are jointly taken into account. a further feature of longitudinal designs is the presence of partially incomplete sequences, due to some individuals leaving the study before its designed end. the missing data process may produce a selection of units which can be informative with respect to the parameters of the longitudinal data model. to deal with the case of irretrievable drop-out, we introduce a pattern mixture version of the linear quantile hidden markov model, where we account for time-varying heterogeneity and for changes in the fixed effect vector due to differential propensities to stay in the study. the proposed models are illustrated using a well known benchmark dataset on longitudinal dynamics of cd4 cells and by means of a large scale simulation study, entailing different quantiles and both complete and partially complete (ie subject to drop-out) individual sequences.",,2015-01-09,2015-07-29,"['maria francesca marino', 'nikos tzavidis', ""marco alfo'""]"
1501.02248,a particle multi-target tracker for superpositional measurements using   labeled random finite sets,stat.me stat.co,"in this paper we present a general solution for multi-target tracking with superpositional measurements. measurements that are functions of the sum of the contributions of the targets present in the surveillance area are called superpositional measurements. we base our modelling on labeled random finite set (rfs) in order to jointly estimate the number of targets and their trajectories. this modelling leads to a labeled version of mahler's multi-target bayes filter. however, a straightforward implementation of this tracker using sequential monte carlo (smc) methods is not feasible due to the difficulties of sampling in high dimensional spaces. we propose an efficient multi-target sampling strategy based on superpositional approximate cphd (sa-cphd) filter and the recently introduced labeled multi-bernoulli (lmb) and vo-vo densities. the applicability of the proposed approach is verified through simulation in a challenging radar application with closely spaced targets and low signal-to-noise ratio.",,2014-12-19,2015-06-02,"['francesco papi', 'du yong kim']"
1501.03731,robust linear spectral unmixing using anomaly detection,stat.me,"this paper presents a bayesian algorithm for linear spectral unmixing of hyperspectral images that accounts for anomalies present in the data. the model proposed assumes that the pixel reflectances are linear mixtures of unknown endmembers, corrupted by an additional nonlinear term modelling anomalies and additive gaussian noise. a markov random field is used for anomaly detection based on the spatial and spectral structures of the anomalies. this allows outliers to be identified in particular regions and wavelengths of the data cube. a bayesian algorithm is proposed to estimate the parameters involved in the model yielding a joint linear unmixing and anomaly detection algorithm. simulations conducted with synthetic and real hyperspectral images demonstrate the accuracy of the proposed unmixing and outlier detection strategy for the analysis of hyperspectral images.",,2015-01-15,2015-10-03,"['yoann altmann', 'steve mclaughlin', 'alfred hero']"
